	.text
	.file	"main.cpp"
	.globl	_ZN7mX_real8toStringB5cxx11ENS_9AlgorithmE # -- Begin function _ZN7mX_real8toStringB5cxx11ENS_9AlgorithmE
	.p2align	4, 0x90
	.type	_ZN7mX_real8toStringB5cxx11ENS_9AlgorithmE,@function
_ZN7mX_real8toStringB5cxx11ENS_9AlgorithmE: # 
	.cfi_startproc
# %bb.0:
	movq	%rdi, %rax
	leaq	16(%rdi), %rcx
	movq	%rcx, (%rdi)
	testl	%esi, %esi
	je	.LBB0_5
# %bb.1:
	cmpl	$1, %esi
	je	.LBB0_4
# %bb.2:
	cmpl	$2, %esi
	jne	.LBB0_6
# %bb.3:
	movb	$105, 4(%rcx)
	movl	$1935766865, (%rcx)             # imm = 0x73617551
	movq	$5, 8(%rax)
	movb	$0, 21(%rax)
	retq
.LBB0_5:
	movabsq	$7310575239352771393, %rcx      # imm = 0x6574617275636341
	movq	%rcx, 16(%rax)
	movq	$8, 8(%rax)
	movb	$0, 24(%rax)
	retq
.LBB0_4:
	movw	$31088, 4(%rcx)                 # imm = 0x7970
	movl	$1886350419, (%rcx)             # imm = 0x706F6C53
	movq	$6, 8(%rax)
	movb	$0, 22(%rax)
	retq
.LBB0_6:
	movw	$30575, 4(%rcx)                 # imm = 0x776F
	movl	$1852534357, (%rcx)             # imm = 0x6E6B6E55
	movq	$6, 8(%rax)
	movb	$0, 22(%rax)
	retq
.Lfunc_end0:
	.size	_ZN7mX_real8toStringB5cxx11ENS_9AlgorithmE, .Lfunc_end0-_ZN7mX_real8toStringB5cxx11ENS_9AlgorithmE
	.cfi_endproc
                                        # -- End function
	.section	.text.__clang_call_terminate,"axG",@progbits,__clang_call_terminate,comdat
	.hidden	__clang_call_terminate          # -- Begin function __clang_call_terminate
	.weak	__clang_call_terminate
	.p2align	4, 0x90
	.type	__clang_call_terminate,@function
__clang_call_terminate:                 # 
# %bb.0:
	pushq	%rax
	callq	__cxa_begin_catch
	callq	_ZSt9terminatev
.Lfunc_end1:
	.size	__clang_call_terminate, .Lfunc_end1-__clang_call_terminate
                                        # -- End function
	.text
	.globl	_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKfb # -- Begin function _Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKfb
	.p2align	4, 0x90
	.type	_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKfb,@function
_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKfb: # 
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	subq	$40, %rsp
	.cfi_def_cfa_offset 64
	.cfi_offset %rbx, -24
	.cfi_offset %rbp, -16
	movl	%edx, %ebp
	movq	%rsi, %rbx
	vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 16(%rsp)
	flds	16(%rsp)
	fstpt	20(%rsp)                        # 10-byte Folded Spill
	wait
	movq	(%rdi), %rsi
	movq	8(%rdi), %rdx
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
	movl	(%rbx), %esi
	fldt	20(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	testb	%bpl, %bpl
	je	.LBB2_2
# %bb.1:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.6, %esi
	movl	$4, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
	movl	(%rbx), %eax
	movq	%rax, 32(%rsp)
	leaq	32(%rsp), %rsi
	movl	$_ZSt4cout, %edi
	callq	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
	fldt	20(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.7, %edi
	xorl	%eax, %eax
	callq	printf
.LBB2_2:
	addq	$40, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end2:
	.size	_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKfb, .Lfunc_end2-_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKfb
	.cfi_endproc
                                        # -- End function
	.section	.text._ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE,"axG",@progbits,_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE,comdat
	.weak	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE # -- Begin function _ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
	.p2align	4, 0x90
	.type	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE,@function
_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE: # 
.Lfunc_begin0:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception0
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$40, %rsp
	.cfi_def_cfa_offset 96
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rsi, %r14
	movq	%rdi, %rbx
	leaq	16(%rsp), %r13
	movq	%r13, (%rsp)
	movq	$0, 8(%rsp)
	movb	$0, 16(%rsp)
	movq	(%rdi), %rax
	movq	-24(%rax), %rax
	leaq	(%rdi,%rax), %rsi
	addq	$208, %rsi
	leaq	32(%rsp), %r15
	movq	%r15, %rdi
	callq	_ZNSt6localeC1ERKS_
.Ltmp0:
	movq	%r15, %rdi
	callq	_ZSt9use_facetISt5ctypeIcEERKT_RKSt6locale
.Ltmp1:
# %bb.1:
	movq	%rax, %r15
	leaq	32(%rsp), %rdi
	callq	_ZNSt6localeD1Ev
	cmpb	$0, 56(%r15)
	je	.LBB3_3
# %bb.2:
	movzbl	105(%r15), %r12d
	movzbl	106(%r15), %ebp
	jmp	.LBB3_9
.LBB3_3:
.Ltmp3:
	movq	%r15, %rdi
	callq	_ZNKSt5ctypeIcE13_M_widen_initEv
.Ltmp4:
# %bb.4:
	movq	(%r15), %rax
.Ltmp5:
	movq	%r15, %rdi
	movl	$48, %esi
	callq	*48(%rax)
.Ltmp6:
# %bb.5:
	movl	%eax, %r12d
	cmpb	$0, 56(%r15)
	je	.LBB3_7
# %bb.6:
	movzbl	106(%r15), %ebp
	jmp	.LBB3_9
.LBB3_7:
.Ltmp7:
	movq	%r15, %rdi
	callq	_ZNKSt5ctypeIcE13_M_widen_initEv
.Ltmp8:
# %bb.8:
	movq	(%r15), %rax
.Ltmp9:
	movq	%r15, %rdi
	movl	$49, %esi
	callq	*48(%rax)
	movl	%eax, %ebp
.Ltmp10:
.LBB3_9:
	movq	8(%rsp), %rdx
.Ltmp11:
	movsbl	%r12b, %r8d
	movq	%rsp, %rdi
	movl	$32, %ecx
	xorl	%esi, %esi
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE14_M_replace_auxEmmmc
.Ltmp12:
# %bb.10:
	movq	(%r14), %rax
	testl	%eax, %eax
	js	.LBB3_11
# %bb.12:
	testl	$1073741824, %eax               # imm = 0x40000000
	jne	.LBB3_13
.LBB3_14:
	testl	$536870912, %eax                # imm = 0x20000000
	jne	.LBB3_15
.LBB3_16:
	testl	$268435456, %eax                # imm = 0x10000000
	jne	.LBB3_17
.LBB3_18:
	testl	$134217728, %eax                # imm = 0x8000000
	jne	.LBB3_19
.LBB3_20:
	testl	$67108864, %eax                 # imm = 0x4000000
	jne	.LBB3_21
.LBB3_22:
	testl	$33554432, %eax                 # imm = 0x2000000
	jne	.LBB3_23
.LBB3_24:
	testl	$16777216, %eax                 # imm = 0x1000000
	jne	.LBB3_25
.LBB3_26:
	testl	$8388608, %eax                  # imm = 0x800000
	jne	.LBB3_27
.LBB3_28:
	testl	$4194304, %eax                  # imm = 0x400000
	jne	.LBB3_29
.LBB3_30:
	testl	$2097152, %eax                  # imm = 0x200000
	jne	.LBB3_31
.LBB3_32:
	testl	$1048576, %eax                  # imm = 0x100000
	jne	.LBB3_33
.LBB3_34:
	testl	$524288, %eax                   # imm = 0x80000
	jne	.LBB3_35
.LBB3_36:
	testl	$262144, %eax                   # imm = 0x40000
	jne	.LBB3_37
.LBB3_38:
	testl	$131072, %eax                   # imm = 0x20000
	jne	.LBB3_39
.LBB3_40:
	testl	$65536, %eax                    # imm = 0x10000
	jne	.LBB3_41
.LBB3_42:
	testw	%ax, %ax
	js	.LBB3_43
.LBB3_44:
	testl	$16384, %eax                    # imm = 0x4000
	jne	.LBB3_45
.LBB3_46:
	testl	$8192, %eax                     # imm = 0x2000
	jne	.LBB3_47
.LBB3_48:
	testl	$4096, %eax                     # imm = 0x1000
	jne	.LBB3_49
.LBB3_50:
	testl	$2048, %eax                     # imm = 0x800
	jne	.LBB3_51
.LBB3_52:
	testl	$1024, %eax                     # imm = 0x400
	jne	.LBB3_53
.LBB3_54:
	testl	$512, %eax                      # imm = 0x200
	jne	.LBB3_55
.LBB3_56:
	testl	$256, %eax                      # imm = 0x100
	jne	.LBB3_57
.LBB3_58:
	testb	%al, %al
	js	.LBB3_59
.LBB3_60:
	testb	$64, %al
	jne	.LBB3_61
.LBB3_62:
	testb	$32, %al
	jne	.LBB3_63
.LBB3_64:
	testb	$16, %al
	jne	.LBB3_65
.LBB3_66:
	testb	$8, %al
	jne	.LBB3_67
.LBB3_68:
	testb	$4, %al
	jne	.LBB3_69
.LBB3_70:
	testb	$2, %al
	jne	.LBB3_71
.LBB3_72:
	movq	(%rsp), %rsi
	testb	$1, %al
	je	.LBB3_74
.LBB3_73:
	movb	%bpl, 31(%rsi)
.LBB3_74:
	movq	8(%rsp), %rdx
.Ltmp13:
	movq	%rbx, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp14:
# %bb.75:
	movq	%rax, %rbx
	movq	(%rsp), %rdi
	cmpq	%r13, %rdi
	je	.LBB3_77
# %bb.76:
	callq	_ZdlPv
.LBB3_77:
	movq	%rbx, %rax
	addq	$40, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB3_11:
	.cfi_def_cfa_offset 96
	movq	(%rsp), %rax
	movb	%bpl, (%rax)
	movq	(%r14), %rax
	testl	$1073741824, %eax               # imm = 0x40000000
	je	.LBB3_14
.LBB3_13:
	movq	(%rsp), %rax
	movb	%bpl, 1(%rax)
	movq	(%r14), %rax
	testl	$536870912, %eax                # imm = 0x20000000
	je	.LBB3_16
.LBB3_15:
	movq	(%rsp), %rax
	movb	%bpl, 2(%rax)
	movq	(%r14), %rax
	testl	$268435456, %eax                # imm = 0x10000000
	je	.LBB3_18
.LBB3_17:
	movq	(%rsp), %rax
	movb	%bpl, 3(%rax)
	movq	(%r14), %rax
	testl	$134217728, %eax                # imm = 0x8000000
	je	.LBB3_20
.LBB3_19:
	movq	(%rsp), %rax
	movb	%bpl, 4(%rax)
	movq	(%r14), %rax
	testl	$67108864, %eax                 # imm = 0x4000000
	je	.LBB3_22
.LBB3_21:
	movq	(%rsp), %rax
	movb	%bpl, 5(%rax)
	movq	(%r14), %rax
	testl	$33554432, %eax                 # imm = 0x2000000
	je	.LBB3_24
.LBB3_23:
	movq	(%rsp), %rax
	movb	%bpl, 6(%rax)
	movq	(%r14), %rax
	testl	$16777216, %eax                 # imm = 0x1000000
	je	.LBB3_26
.LBB3_25:
	movq	(%rsp), %rax
	movb	%bpl, 7(%rax)
	movq	(%r14), %rax
	testl	$8388608, %eax                  # imm = 0x800000
	je	.LBB3_28
.LBB3_27:
	movq	(%rsp), %rax
	movb	%bpl, 8(%rax)
	movq	(%r14), %rax
	testl	$4194304, %eax                  # imm = 0x400000
	je	.LBB3_30
.LBB3_29:
	movq	(%rsp), %rax
	movb	%bpl, 9(%rax)
	movq	(%r14), %rax
	testl	$2097152, %eax                  # imm = 0x200000
	je	.LBB3_32
.LBB3_31:
	movq	(%rsp), %rax
	movb	%bpl, 10(%rax)
	movq	(%r14), %rax
	testl	$1048576, %eax                  # imm = 0x100000
	je	.LBB3_34
.LBB3_33:
	movq	(%rsp), %rax
	movb	%bpl, 11(%rax)
	movq	(%r14), %rax
	testl	$524288, %eax                   # imm = 0x80000
	je	.LBB3_36
.LBB3_35:
	movq	(%rsp), %rax
	movb	%bpl, 12(%rax)
	movq	(%r14), %rax
	testl	$262144, %eax                   # imm = 0x40000
	je	.LBB3_38
.LBB3_37:
	movq	(%rsp), %rax
	movb	%bpl, 13(%rax)
	movq	(%r14), %rax
	testl	$131072, %eax                   # imm = 0x20000
	je	.LBB3_40
.LBB3_39:
	movq	(%rsp), %rax
	movb	%bpl, 14(%rax)
	movq	(%r14), %rax
	testl	$65536, %eax                    # imm = 0x10000
	je	.LBB3_42
.LBB3_41:
	movq	(%rsp), %rax
	movb	%bpl, 15(%rax)
	movq	(%r14), %rax
	testw	%ax, %ax
	jns	.LBB3_44
.LBB3_43:
	movq	(%rsp), %rax
	movb	%bpl, 16(%rax)
	movq	(%r14), %rax
	testl	$16384, %eax                    # imm = 0x4000
	je	.LBB3_46
.LBB3_45:
	movq	(%rsp), %rax
	movb	%bpl, 17(%rax)
	movq	(%r14), %rax
	testl	$8192, %eax                     # imm = 0x2000
	je	.LBB3_48
.LBB3_47:
	movq	(%rsp), %rax
	movb	%bpl, 18(%rax)
	movq	(%r14), %rax
	testl	$4096, %eax                     # imm = 0x1000
	je	.LBB3_50
.LBB3_49:
	movq	(%rsp), %rax
	movb	%bpl, 19(%rax)
	movq	(%r14), %rax
	testl	$2048, %eax                     # imm = 0x800
	je	.LBB3_52
.LBB3_51:
	movq	(%rsp), %rax
	movb	%bpl, 20(%rax)
	movq	(%r14), %rax
	testl	$1024, %eax                     # imm = 0x400
	je	.LBB3_54
.LBB3_53:
	movq	(%rsp), %rax
	movb	%bpl, 21(%rax)
	movq	(%r14), %rax
	testl	$512, %eax                      # imm = 0x200
	je	.LBB3_56
.LBB3_55:
	movq	(%rsp), %rax
	movb	%bpl, 22(%rax)
	movq	(%r14), %rax
	testl	$256, %eax                      # imm = 0x100
	je	.LBB3_58
.LBB3_57:
	movq	(%rsp), %rax
	movb	%bpl, 23(%rax)
	movq	(%r14), %rax
	testb	%al, %al
	jns	.LBB3_60
.LBB3_59:
	movq	(%rsp), %rax
	movb	%bpl, 24(%rax)
	movq	(%r14), %rax
	testb	$64, %al
	je	.LBB3_62
.LBB3_61:
	movq	(%rsp), %rax
	movb	%bpl, 25(%rax)
	movq	(%r14), %rax
	testb	$32, %al
	je	.LBB3_64
.LBB3_63:
	movq	(%rsp), %rax
	movb	%bpl, 26(%rax)
	movq	(%r14), %rax
	testb	$16, %al
	je	.LBB3_66
.LBB3_65:
	movq	(%rsp), %rax
	movb	%bpl, 27(%rax)
	movq	(%r14), %rax
	testb	$8, %al
	je	.LBB3_68
.LBB3_67:
	movq	(%rsp), %rax
	movb	%bpl, 28(%rax)
	movq	(%r14), %rax
	testb	$4, %al
	je	.LBB3_70
.LBB3_69:
	movq	(%rsp), %rax
	movb	%bpl, 29(%rax)
	movq	(%r14), %rax
	testb	$2, %al
	je	.LBB3_72
.LBB3_71:
	movq	(%rsp), %rax
	movb	%bpl, 30(%rax)
	movq	(%r14), %rax
	movq	(%rsp), %rsi
	testb	$1, %al
	jne	.LBB3_73
	jmp	.LBB3_74
.LBB3_78:
.Ltmp2:
	movq	%rax, %rbx
	leaq	32(%rsp), %rdi
	callq	_ZNSt6localeD1Ev
	jmp	.LBB3_80
.LBB3_79:
.Ltmp15:
	movq	%rax, %rbx
.LBB3_80:
	movq	(%rsp), %rdi
	cmpq	%r13, %rdi
	je	.LBB3_82
# %bb.81:
	callq	_ZdlPv
.LBB3_82:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end3:
	.size	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE, .Lfunc_end3-_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
	.cfi_endproc
	.section	.gcc_except_table._ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE,"aG",@progbits,_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE,comdat
	.p2align	2, 0x0
GCC_except_table3:
.Lexception0:
	.byte	255                             # @LPStart Encoding = omit
	.byte	255                             # @TType Encoding = omit
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end0-.Lcst_begin0
.Lcst_begin0:
	.uleb128 .Ltmp0-.Lfunc_begin0           # >> Call Site 1 <<
	.uleb128 .Ltmp1-.Ltmp0                  #   Call between .Ltmp0 and .Ltmp1
	.uleb128 .Ltmp2-.Lfunc_begin0           #     jumps to .Ltmp2
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3-.Lfunc_begin0           # >> Call Site 2 <<
	.uleb128 .Ltmp14-.Ltmp3                 #   Call between .Ltmp3 and .Ltmp14
	.uleb128 .Ltmp15-.Lfunc_begin0          #     jumps to .Ltmp15
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp14-.Lfunc_begin0          # >> Call Site 3 <<
	.uleb128 .Lfunc_end3-.Ltmp14            #   Call between .Ltmp14 and .Lfunc_end3
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end0:
	.p2align	2, 0x0
                                        # -- End function
	.text
	.globl	_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKdb # -- Begin function _Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKdb
	.p2align	4, 0x90
	.type	_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKdb,@function
_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKdb: # 
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r14
	.cfi_def_cfa_offset 24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	subq	$64, %rsp
	.cfi_def_cfa_offset 96
	.cfi_offset %rbx, -32
	.cfi_offset %r14, -24
	.cfi_offset %rbp, -16
	movl	%edx, %ebp
	movq	%rsi, %rbx
	vmovsd	(%rsi), %xmm0                   # xmm0 = mem[0],zero
	vmovsd	%xmm0, 48(%rsp)
	fldl	48(%rsp)
	fstpt	36(%rsp)                        # 10-byte Folded Spill
	wait
	movq	(%rdi), %rsi
	movq	8(%rdi), %rdx
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
	movq	(%rbx), %r14
	fldt	36(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.8, %edi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	printf
	testb	%bpl, %bpl
	je	.LBB4_2
# %bb.1:
	vmovq	%r14, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm1
	vcvtss2sd	%xmm1, %xmm1, %xmm2
	vsubsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, 28(%rsp)
	flds	28(%rsp)
	wait
	vmovss	%xmm1, 32(%rsp)
	flds	32(%rsp)
	faddp	%st, %st(1)
	fld	%st(0)
	fstpt	36(%rsp)                        # 10-byte Folded Spill
	wait
	vmovd	%xmm1, %esi
	vmovd	%xmm0, %edx
	fstpt	(%rsp)
	wait
	movl	$.L.str.9, %edi
	xorl	%eax, %eax
	callq	printf
	movl	$_ZSt4cout, %edi
	movl	$.L.str.6, %esi
	movl	$4, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
	movq	(%rbx), %rax
	movq	%rax, 56(%rsp)
	leaq	56(%rsp), %rsi
	movl	$_ZSt4cout, %edi
	callq	_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
	fldt	36(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.7, %edi
	xorl	%eax, %eax
	callq	printf
.LBB4_2:
	addq	$64, %rsp
	.cfi_def_cfa_offset 32
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end4:
	.size	_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKdb, .Lfunc_end4-_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKdb
	.cfi_endproc
                                        # -- End function
	.section	.text._ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE,"axG",@progbits,_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE,comdat
	.weak	_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE # -- Begin function _ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
	.p2align	4, 0x90
	.type	_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE,@function
_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE: # 
.Lfunc_begin1:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception1
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$40, %rsp
	.cfi_def_cfa_offset 96
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rsi, %r14
	movq	%rdi, %rbx
	leaq	16(%rsp), %r13
	movq	%r13, (%rsp)
	movq	$0, 8(%rsp)
	movb	$0, 16(%rsp)
	movq	(%rdi), %rax
	movq	-24(%rax), %rax
	leaq	(%rdi,%rax), %rsi
	addq	$208, %rsi
	leaq	32(%rsp), %r15
	movq	%r15, %rdi
	callq	_ZNSt6localeC1ERKS_
.Ltmp16:
	movq	%r15, %rdi
	callq	_ZSt9use_facetISt5ctypeIcEERKT_RKSt6locale
.Ltmp17:
# %bb.1:
	movq	%rax, %r15
	leaq	32(%rsp), %rdi
	callq	_ZNSt6localeD1Ev
	cmpb	$0, 56(%r15)
	je	.LBB5_3
# %bb.2:
	movzbl	105(%r15), %r12d
	movzbl	106(%r15), %ebp
	jmp	.LBB5_9
.LBB5_3:
.Ltmp19:
	movq	%r15, %rdi
	callq	_ZNKSt5ctypeIcE13_M_widen_initEv
.Ltmp20:
# %bb.4:
	movq	(%r15), %rax
.Ltmp21:
	movq	%r15, %rdi
	movl	$48, %esi
	callq	*48(%rax)
.Ltmp22:
# %bb.5:
	movl	%eax, %r12d
	cmpb	$0, 56(%r15)
	je	.LBB5_7
# %bb.6:
	movzbl	106(%r15), %ebp
	jmp	.LBB5_9
.LBB5_7:
.Ltmp23:
	movq	%r15, %rdi
	callq	_ZNKSt5ctypeIcE13_M_widen_initEv
.Ltmp24:
# %bb.8:
	movq	(%r15), %rax
.Ltmp25:
	movq	%r15, %rdi
	movl	$49, %esi
	callq	*48(%rax)
	movl	%eax, %ebp
.Ltmp26:
.LBB5_9:
	movq	8(%rsp), %rdx
.Ltmp27:
	movsbl	%r12b, %r8d
	movq	%rsp, %rdi
	movl	$64, %ecx
	xorl	%esi, %esi
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE14_M_replace_auxEmmmc
.Ltmp28:
# %bb.10:
	movl	$56, %eax
	xorl	%ecx, %ecx
	jmp	.LBB5_11
	.p2align	4, 0x90
.LBB5_27:                               #   in Loop: Header=BB5_11 Depth=1
	addq	$8, %rcx
	addq	$-8, %rax
	cmpq	$64, %rcx
	je	.LBB5_28
.LBB5_11:                               # =>This Inner Loop Header: Depth=1
	leal	7(%rax), %esi
	movq	(%r14), %rdx
	movzbl	%sil, %esi
	btq	%rsi, %rdx
	jae	.LBB5_13
# %bb.12:                               #   in Loop: Header=BB5_11 Depth=1
	movq	(%rsp), %rdx
	movb	%bpl, (%rdx,%rcx)
	movq	(%r14), %rdx
.LBB5_13:                               #   in Loop: Header=BB5_11 Depth=1
	leal	6(%rax), %esi
	movzbl	%sil, %esi
	btq	%rsi, %rdx
	jae	.LBB5_15
# %bb.14:                               #   in Loop: Header=BB5_11 Depth=1
	movq	(%rsp), %rdx
	movb	%bpl, 1(%rdx,%rcx)
	movq	(%r14), %rdx
.LBB5_15:                               #   in Loop: Header=BB5_11 Depth=1
	leal	5(%rax), %esi
	movzbl	%sil, %esi
	btq	%rsi, %rdx
	jae	.LBB5_17
# %bb.16:                               #   in Loop: Header=BB5_11 Depth=1
	movq	(%rsp), %rdx
	movb	%bpl, 2(%rdx,%rcx)
	movq	(%r14), %rdx
.LBB5_17:                               #   in Loop: Header=BB5_11 Depth=1
	leal	4(%rax), %esi
	movzbl	%sil, %esi
	btq	%rsi, %rdx
	jae	.LBB5_19
# %bb.18:                               #   in Loop: Header=BB5_11 Depth=1
	movq	(%rsp), %rdx
	movb	%bpl, 3(%rdx,%rcx)
	movq	(%r14), %rdx
.LBB5_19:                               #   in Loop: Header=BB5_11 Depth=1
	leal	3(%rax), %esi
	movzbl	%sil, %esi
	btq	%rsi, %rdx
	jae	.LBB5_21
# %bb.20:                               #   in Loop: Header=BB5_11 Depth=1
	movq	(%rsp), %rdx
	movb	%bpl, 4(%rdx,%rcx)
	movq	(%r14), %rdx
.LBB5_21:                               #   in Loop: Header=BB5_11 Depth=1
	leal	2(%rax), %esi
	movzbl	%sil, %esi
	btq	%rsi, %rdx
	jae	.LBB5_23
# %bb.22:                               #   in Loop: Header=BB5_11 Depth=1
	movq	(%rsp), %rdx
	movb	%bpl, 5(%rdx,%rcx)
	movq	(%r14), %rdx
.LBB5_23:                               #   in Loop: Header=BB5_11 Depth=1
	leal	1(%rax), %esi
	movzbl	%sil, %esi
	btq	%rsi, %rdx
	jae	.LBB5_25
# %bb.24:                               #   in Loop: Header=BB5_11 Depth=1
	movq	(%rsp), %rdx
	movb	%bpl, 6(%rdx,%rcx)
	movq	(%r14), %rdx
.LBB5_25:                               #   in Loop: Header=BB5_11 Depth=1
	btq	%rax, %rdx
	jae	.LBB5_27
# %bb.26:                               #   in Loop: Header=BB5_11 Depth=1
	movq	(%rsp), %rdx
	movb	%bpl, 7(%rdx,%rcx)
	jmp	.LBB5_27
.LBB5_28:
	movq	(%rsp), %rsi
	movq	8(%rsp), %rdx
.Ltmp29:
	movq	%rbx, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp30:
# %bb.29:
	movq	%rax, %rbx
	movq	(%rsp), %rdi
	cmpq	%r13, %rdi
	je	.LBB5_31
# %bb.30:
	callq	_ZdlPv
.LBB5_31:
	movq	%rbx, %rax
	addq	$40, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB5_32:
	.cfi_def_cfa_offset 96
.Ltmp18:
	movq	%rax, %rbx
	leaq	32(%rsp), %rdi
	callq	_ZNSt6localeD1Ev
	jmp	.LBB5_34
.LBB5_33:
.Ltmp31:
	movq	%rax, %rbx
.LBB5_34:
	movq	(%rsp), %rdi
	cmpq	%r13, %rdi
	je	.LBB5_36
# %bb.35:
	callq	_ZdlPv
.LBB5_36:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end5:
	.size	_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE, .Lfunc_end5-_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
	.cfi_endproc
	.section	.gcc_except_table._ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE,"aG",@progbits,_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE,comdat
	.p2align	2, 0x0
GCC_except_table5:
.Lexception1:
	.byte	255                             # @LPStart Encoding = omit
	.byte	255                             # @TType Encoding = omit
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end1-.Lcst_begin1
.Lcst_begin1:
	.uleb128 .Ltmp16-.Lfunc_begin1          # >> Call Site 1 <<
	.uleb128 .Ltmp17-.Ltmp16                #   Call between .Ltmp16 and .Ltmp17
	.uleb128 .Ltmp18-.Lfunc_begin1          #     jumps to .Ltmp18
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp19-.Lfunc_begin1          # >> Call Site 2 <<
	.uleb128 .Ltmp30-.Ltmp19                #   Call between .Ltmp19 and .Ltmp30
	.uleb128 .Ltmp31-.Lfunc_begin1          #     jumps to .Ltmp31
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp30-.Lfunc_begin1          # >> Call Site 3 <<
	.uleb128 .Lfunc_end5-.Ltmp30            #   Call between .Ltmp30 and .Lfunc_end5
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end1:
	.p2align	2, 0x0
                                        # -- End function
	.text
	.globl	_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERK7dd_realb # -- Begin function _Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERK7dd_realb
	.p2align	4, 0x90
	.type	_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERK7dd_realb,@function
_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERK7dd_realb: # 
	.cfi_startproc
# %bb.0:
	pushq	%rbx
	.cfi_def_cfa_offset 16
	subq	$48, %rsp
	.cfi_def_cfa_offset 64
	.cfi_offset %rbx, -16
	movq	%rsi, %rbx
	vmovsd	(%rsi), %xmm0                   # xmm0 = mem[0],zero
	vmovsd	8(%rsi), %xmm1                  # xmm1 = mem[0],zero
	vmovsd	%xmm1, 16(%rsp)
	fldl	16(%rsp)
	wait
	vmovsd	%xmm0, 24(%rsp)
	fldl	24(%rsp)
	faddp	%st, %st(1)
	fstpt	36(%rsp)                        # 10-byte Folded Spill
	wait
	movq	(%rdi), %rsi
	movq	8(%rdi), %rdx
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
	movq	(%rbx), %rsi
	movq	8(%rbx), %rdx
	fldt	36(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.10, %edi
	xorl	%eax, %eax
	callq	printf
	addq	$48, %rsp
	.cfi_def_cfa_offset 16
	popq	%rbx
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end6:
	.size	_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERK7dd_realb, .Lfunc_end6-_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERK7dd_realb
	.cfi_endproc
                                        # -- End function
	.globl	_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERK7qd_realb # -- Begin function _Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERK7qd_realb
	.p2align	4, 0x90
	.type	_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERK7qd_realb,@function
_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERK7qd_realb: # 
	.cfi_startproc
# %bb.0:
	pushq	%rbx
	.cfi_def_cfa_offset 16
	subq	$64, %rsp
	.cfi_def_cfa_offset 80
	.cfi_offset %rbx, -16
	movq	%rsi, %rbx
	vmovsd	24(%rsi), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 16(%rsp)
	fldl	16(%rsp)
	wait
	vmovsd	16(%rsi), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 24(%rsp)
	fldl	24(%rsp)
	faddp	%st, %st(1)
	wait
	vmovsd	(%rsi), %xmm0                   # xmm0 = mem[0],zero
	vmovsd	8(%rsi), %xmm1                  # xmm1 = mem[0],zero
	vmovsd	%xmm1, 32(%rsp)
	fldl	32(%rsp)
	faddp	%st, %st(1)
	wait
	vmovsd	%xmm0, 40(%rsp)
	fldl	40(%rsp)
	faddp	%st, %st(1)
	fstpt	52(%rsp)                        # 10-byte Folded Spill
	wait
	movq	(%rdi), %rsi
	movq	8(%rdi), %rdx
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
	movq	(%rbx), %rsi
	movq	8(%rbx), %rdx
	movq	16(%rbx), %rcx
	movq	24(%rbx), %r8
	fldt	52(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.11, %edi
	xorl	%eax, %eax
	callq	printf
	addq	$64, %rsp
	.cfi_def_cfa_offset 16
	popq	%rbx
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end7:
	.size	_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERK7qd_realb, .Lfunc_end7-_Z5printNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERK7qd_realb
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3, 0x0                          # -- Begin function _ZN7mX_real9print_subEN4mpfr6mprealE
.LCPI8_0:
	.quad	0x4000000000000000              #  2
.LCPI8_1:
	.quad	0x4024000000000000              #  10
	.text
	.globl	_ZN7mX_real9print_subEN4mpfr6mprealE
	.p2align	4, 0x90
	.type	_ZN7mX_real9print_subEN4mpfr6mprealE,@function
_ZN7mX_real9print_subEN4mpfr6mprealE:   # 
	.cfi_startproc
# %bb.0:
	pushq	%rbx
	.cfi_def_cfa_offset 16
	subq	$16, %rsp
	.cfi_def_cfa_offset 32
	.cfi_offset %rbx, -16
	movq	%rdi, %rbx
	movl	$256, %eax                      # imm = 0x100
	vcvtsi2sd	%eax, %xmm0, %xmm0
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
	vmovsd	.LCPI8_0(%rip), %xmm0           # xmm0 = mem[0],zero
	callq	log
	vmulsd	8(%rsp), %xmm0, %xmm0           # 8-byte Folded Reload
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
	vmovsd	.LCPI8_1(%rip), %xmm0           # xmm0 = mem[0],zero
	callq	log
	vmovsd	8(%rsp), %xmm1                  # 8-byte Reload
                                        # xmm1 = mem[0],zero
	vdivsd	%xmm0, %xmm1, %xmm0
	vroundsd	$10, %xmm0, %xmm0, %xmm0
	vcvttsd2si	%xmm0, %eax
	movq	_ZSt4cout(%rip), %rcx
	movq	-24(%rcx), %rdx
	cltq
	movq	%rax, _ZSt4cout+8(%rdx)
	movq	-24(%rcx), %rax
	movl	$-261, %edx                     # imm = 0xFEFB
	andl	_ZSt4cout+24(%rax), %edx
	orl	$256, %edx                      # imm = 0x100
	movl	%edx, _ZSt4cout+24(%rax)
	movq	-24(%rcx), %rax
	orl	$2048, _ZSt4cout+24(%rax)       # imm = 0x800
	movl	$_ZSt4cout, %esi
	movq	%rbx, %rdi
	callq	_ZNK4mpfr6mpreal6outputERSo
	movq	_ZSt4cout(%rip), %rax
	movq	-24(%rax), %rax
	movq	_ZSt4cout+240(%rax), %rbx
	testq	%rbx, %rbx
	je	.LBB8_5
# %bb.1:
	cmpb	$0, 56(%rbx)
	je	.LBB8_3
# %bb.2:
	movzbl	67(%rbx), %eax
	jmp	.LBB8_4
.LBB8_3:
	movq	%rbx, %rdi
	callq	_ZNKSt5ctypeIcE13_M_widen_initEv
	movq	(%rbx), %rax
	movq	%rbx, %rdi
	movl	$10, %esi
	callq	*48(%rax)
.LBB8_4:
	movsbl	%al, %esi
	movl	$_ZSt4cout, %edi
	callq	_ZNSo3putEc
	movq	%rax, %rdi
	addq	$16, %rsp
	.cfi_def_cfa_offset 16
	popq	%rbx
	.cfi_def_cfa_offset 8
	jmp	_ZNSo5flushEv                   # TAILCALL
.LBB8_5:
	.cfi_def_cfa_offset 32
	callq	_ZSt16__throw_bad_castv
.Lfunc_end8:
	.size	_ZN7mX_real9print_subEN4mpfr6mprealE, .Lfunc_end8-_ZN7mX_real9print_subEN4mpfr6mprealE
	.cfi_endproc
                                        # -- End function
	.section	.text._ZNK4mpfr6mpreal6outputERSo,"axG",@progbits,_ZNK4mpfr6mpreal6outputERSo,comdat
	.weak	_ZNK4mpfr6mpreal6outputERSo     # -- Begin function _ZNK4mpfr6mpreal6outputERSo
	.p2align	4, 0x90
	.type	_ZNK4mpfr6mpreal6outputERSo,@function
_ZNK4mpfr6mpreal6outputERSo:            # 
.Lfunc_begin2:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception2
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r12
	.cfi_def_cfa_offset 40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	subq	$432, %rsp                      # imm = 0x1B0
	.cfi_def_cfa_offset 480
	.cfi_offset %rbx, -48
	.cfi_offset %r12, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rsi, %rbx
	movq	%rdi, %r14
	leaq	56(%rsp), %r15
	movq	%r15, %rdi
	callq	_ZNSt7__cxx1119basic_ostringstreamIcSt11char_traitsIcESaIcEEC1Ev
	movq	(%rbx), %rax
	movq	-24(%rax), %rax
	movl	24(%rbx,%rax), %ebp
	xorl	%edx, %edx
	testl	$2048, %ebp                     # imm = 0x800
	movl	$.L.str.13, %eax
	movl	$.L.str.12, %esi
	cmoveq	%rax, %rsi
	setne	%dl
	incq	%rdx
.Ltmp32:
	movq	%r15, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp33:
# %bb.1:
	movq	(%rbx), %rax
	movq	-24(%rax), %rax
	cmpq	$0, 8(%rbx,%rax)
	js	.LBB9_6
# %bb.2:
	movb	$46, 8(%rsp)
.Ltmp36:
	leaq	56(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	$1, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp37:
# %bb.3:
	movq	(%rbx), %rcx
	movq	-24(%rcx), %rcx
	movq	8(%rbx,%rcx), %rsi
.Ltmp38:
	movq	%rax, %rdi
	callq	_ZNSo9_M_insertIlEERSoT_
.Ltmp39:
# %bb.4:
.Ltmp40:
	movq	%rax, %r15
	movl	$.L.str.14, %esi
	movl	$2, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp41:
# %bb.5:
	andl	$260, %ebp                      # imm = 0x104
	cmpl	$256, %ebp                      # imm = 0x100
	setne	%al
	addb	%al, %al
	orb	$101, %al
	cmpl	$4, %ebp
	movzbl	%al, %eax
	movl	$102, %ecx
	cmovnel	%eax, %ecx
	movb	%cl, 8(%rsp)
.Ltmp42:
	leaq	8(%rsp), %rsi
	movl	$1, %edx
	movq	%r15, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp43:
	jmp	.LBB9_7
.LBB9_6:
.Ltmp34:
	leaq	56(%rsp), %rdi
	movl	$.L.str.15, %esi
	movl	$3, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp35:
.LBB9_7:
	movq	$0, 40(%rsp)
	leaq	24(%rsp), %r12
	movq	%r12, 8(%rsp)
	movq	$0, 16(%rsp)
	movb	$0, 24(%rsp)
	movq	88(%rsp), %r8
	movq	104(%rsp), %rax
	cmpq	%r8, %rax
	cmovaq	%rax, %r8
	testq	%rax, %rax
	je	.LBB9_10
# %bb.8:
	testq	%r8, %r8
	je	.LBB9_10
# %bb.9:
	movq	96(%rsp), %rcx
	subq	%rcx, %r8
.Ltmp45:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE10_M_replaceEmmPKcm
.Ltmp46:
	jmp	.LBB9_11
.LBB9_10:
	leaq	136(%rsp), %rsi
.Ltmp47:
	leaq	8(%rsp), %rdi
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_assignERKS4_
.Ltmp48:
.LBB9_11:
	movq	8(%rsp), %r15
.Ltmp50:
	callq	mpfr_get_default_rounding_mode
.Ltmp51:
# %bb.12:
.Ltmp52:
	leaq	40(%rsp), %rdi
	movq	%r15, %rsi
	movl	%eax, %edx
	movq	%r14, %rcx
	xorl	%eax, %eax
	callq	mpfr_asprintf
.Ltmp53:
# %bb.13:
	movl	%eax, %ebp
	movq	8(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB9_15
# %bb.14:
	callq	_ZdlPv
.LBB9_15:
	testl	%ebp, %ebp
	js	.LBB9_28
# %bb.16:
	movq	40(%rsp), %r14
	movq	%r12, 8(%rsp)
	testq	%r14, %r14
	je	.LBB9_31
# %bb.17:
	movq	%r14, %rdi
	callq	strlen
	movq	%rax, %r15
	movq	%rax, 48(%rsp)
	movq	%r12, %rax
	cmpq	$16, %r15
	jb	.LBB9_20
# %bb.18:
.Ltmp55:
	leaq	8(%rsp), %rdi
	leaq	48(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp56:
# %bb.19:
	movq	%rax, 8(%rsp)
	movq	48(%rsp), %rcx
	movq	%rcx, 24(%rsp)
.LBB9_20:
	testq	%r15, %r15
	je	.LBB9_24
# %bb.21:
	cmpq	$1, %r15
	jne	.LBB9_23
# %bb.22:
	movzbl	(%r14), %ecx
	movb	%cl, (%rax)
	jmp	.LBB9_24
.LBB9_23:
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r15, %rdx
	callq	_intel_fast_memcpy@PLT
.LBB9_24:
	movq	48(%rsp), %rax
	movq	%rax, 16(%rsp)
	movq	8(%rsp), %rcx
	movb	$0, (%rcx,%rax)
	movq	8(%rsp), %rsi
	movq	16(%rsp), %rdx
.Ltmp57:
	movq	%rbx, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp58:
# %bb.25:
	movq	8(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB9_27
# %bb.26:
	callq	_ZdlPv
.LBB9_27:
	movq	40(%rsp), %rdi
.Ltmp60:
	callq	mpfr_free_str
.Ltmp61:
.LBB9_28:
	movq	_ZTTNSt7__cxx1119basic_ostringstreamIcSt11char_traitsIcESaIcEEE(%rip), %rax
	movq	%rax, 56(%rsp)
	movq	_ZTTNSt7__cxx1119basic_ostringstreamIcSt11char_traitsIcESaIcEEE+24(%rip), %rcx
	movq	-24(%rax), %rax
	movq	%rcx, 56(%rsp,%rax)
	movq	$_ZTVNSt7__cxx1115basic_stringbufIcSt11char_traitsIcESaIcEEE+16, 64(%rsp)
	movq	136(%rsp), %rdi
	leaq	152(%rsp), %rax
	cmpq	%rax, %rdi
	je	.LBB9_30
# %bb.29:
	callq	_ZdlPv
.LBB9_30:
	movq	$_ZTVSt15basic_streambufIcSt11char_traitsIcEE+16, 64(%rsp)
	leaq	120(%rsp), %rdi
	callq	_ZNSt6localeD1Ev
	leaq	168(%rsp), %rdi
	callq	_ZNSt8ios_baseD2Ev
	movq	%rbx, %rax
	addq	$432, %rsp                      # imm = 0x1B0
	.cfi_def_cfa_offset 48
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB9_31:
	.cfi_def_cfa_offset 480
.Ltmp63:
	movl	$.L.str.4, %edi
	callq	_ZSt19__throw_logic_errorPKc
.Ltmp64:
# %bb.32:
.LBB9_33:
.Ltmp62:
	jmp	.LBB9_42
.LBB9_34:
.Ltmp59:
	movq	%rax, %rbx
	movq	8(%rsp), %rdi
	cmpq	%r12, %rdi
	jne	.LBB9_40
	jmp	.LBB9_43
.LBB9_36:
.Ltmp65:
	jmp	.LBB9_42
.LBB9_37:
.Ltmp49:
	jmp	.LBB9_39
.LBB9_38:
.Ltmp54:
.LBB9_39:
	movq	%rax, %rbx
	movq	8(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB9_43
.LBB9_40:
	callq	_ZdlPv
	jmp	.LBB9_43
.LBB9_41:
.Ltmp44:
.LBB9_42:
	movq	%rax, %rbx
.LBB9_43:
	leaq	56(%rsp), %rdi
	callq	_ZNSt7__cxx1119basic_ostringstreamIcSt11char_traitsIcESaIcEED1Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end9:
	.size	_ZNK4mpfr6mpreal6outputERSo, .Lfunc_end9-_ZNK4mpfr6mpreal6outputERSo
	.cfi_endproc
	.section	.gcc_except_table._ZNK4mpfr6mpreal6outputERSo,"aG",@progbits,_ZNK4mpfr6mpreal6outputERSo,comdat
	.p2align	2, 0x0
GCC_except_table9:
.Lexception2:
	.byte	255                             # @LPStart Encoding = omit
	.byte	255                             # @TType Encoding = omit
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end2-.Lcst_begin2
.Lcst_begin2:
	.uleb128 .Lfunc_begin2-.Lfunc_begin2    # >> Call Site 1 <<
	.uleb128 .Ltmp32-.Lfunc_begin2          #   Call between .Lfunc_begin2 and .Ltmp32
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp32-.Lfunc_begin2          # >> Call Site 2 <<
	.uleb128 .Ltmp35-.Ltmp32                #   Call between .Ltmp32 and .Ltmp35
	.uleb128 .Ltmp44-.Lfunc_begin2          #     jumps to .Ltmp44
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp45-.Lfunc_begin2          # >> Call Site 3 <<
	.uleb128 .Ltmp48-.Ltmp45                #   Call between .Ltmp45 and .Ltmp48
	.uleb128 .Ltmp49-.Lfunc_begin2          #     jumps to .Ltmp49
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp50-.Lfunc_begin2          # >> Call Site 4 <<
	.uleb128 .Ltmp53-.Ltmp50                #   Call between .Ltmp50 and .Ltmp53
	.uleb128 .Ltmp54-.Lfunc_begin2          #     jumps to .Ltmp54
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp55-.Lfunc_begin2          # >> Call Site 5 <<
	.uleb128 .Ltmp56-.Ltmp55                #   Call between .Ltmp55 and .Ltmp56
	.uleb128 .Ltmp65-.Lfunc_begin2          #     jumps to .Ltmp65
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp56-.Lfunc_begin2          # >> Call Site 6 <<
	.uleb128 .Ltmp57-.Ltmp56                #   Call between .Ltmp56 and .Ltmp57
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp57-.Lfunc_begin2          # >> Call Site 7 <<
	.uleb128 .Ltmp58-.Ltmp57                #   Call between .Ltmp57 and .Ltmp58
	.uleb128 .Ltmp59-.Lfunc_begin2          #     jumps to .Ltmp59
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp60-.Lfunc_begin2          # >> Call Site 8 <<
	.uleb128 .Ltmp61-.Ltmp60                #   Call between .Ltmp60 and .Ltmp61
	.uleb128 .Ltmp62-.Lfunc_begin2          #     jumps to .Ltmp62
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp63-.Lfunc_begin2          # >> Call Site 9 <<
	.uleb128 .Ltmp64-.Ltmp63                #   Call between .Ltmp63 and .Ltmp64
	.uleb128 .Ltmp65-.Lfunc_begin2          #     jumps to .Ltmp65
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp64-.Lfunc_begin2          # >> Call Site 10 <<
	.uleb128 .Lfunc_end9-.Ltmp64            #   Call between .Ltmp64 and .Lfunc_end9
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end2:
	.p2align	2, 0x0
                                        # -- End function
	.text
	.globl	_ZN7mX_real7convertI7dd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS3_ # -- Begin function _ZN7mX_real7convertI7dd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS3_
	.p2align	4, 0x90
	.type	_ZN7mX_real7convertI7dd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS3_,@function
_ZN7mX_real7convertI7dd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS3_: # 
.Lfunc_begin3:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception3
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r12
	.cfi_def_cfa_offset 40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	subq	$32, %rsp
	.cfi_def_cfa_offset 80
	.cfi_offset %rbx, -48
	.cfi_offset %r12, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rsi, %r14
	movq	%rdi, %rbx
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	movq	%rsp, %rax
	cmpq	%rbx, %rax
	je	.LBB10_1
# %bb.10:
.Ltmp66:
	movq	%rsp, %rdi
	movq	%rbx, %rsi
	movq	%r14, %rdx
	callq	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp67:
# %bb.11:
.Ltmp68:
	movq	%rbx, %rdi
	callq	mpfr_get_prec
.Ltmp69:
# %bb.12:
.Ltmp70:
	movq	%rax, %r12
	movq	%rsp, %rdi
	callq	mpfr_get_prec
.Ltmp71:
# %bb.13:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB10_17
# %bb.14:
	cmpq	$0, 24(%rbx)
	je	.LBB10_16
# %bb.15:
.Ltmp72:
	movq	%rbx, %rdi
	callq	mpfr_clear
.Ltmp73:
.LBB10_16:
.Ltmp74:
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp75:
.LBB10_17:
.Ltmp76:
	callq	mpfr_get_default_rounding_mode
.Ltmp77:
# %bb.18:
.Ltmp78:
	movq	%rsp, %rsi
	movq	%rbx, %rdi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp79:
# %bb.19:
	cmpq	$0, 24(%rsp)
	je	.LBB10_21
# %bb.20:
.Ltmp80:
	movq	%rsp, %rdi
	callq	mpfr_clear
.Ltmp81:
.LBB10_21:
	addq	$8, %r14
.Ltmp82:
	movq	%rsp, %rdi
	movq	%rbx, %rsi
	movq	%r14, %rdx
	callq	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp83:
# %bb.22:
.Ltmp85:
	movq	%rbx, %rdi
	callq	mpfr_get_prec
.Ltmp86:
# %bb.23:
.Ltmp87:
	movq	%rax, %r15
	movq	%rsp, %rdi
	callq	mpfr_get_prec
.Ltmp88:
# %bb.24:
	movq	%rax, %r14
	cmpq	%rax, %r15
	je	.LBB10_28
# %bb.25:
	cmpq	$0, 24(%rbx)
	je	.LBB10_27
# %bb.26:
.Ltmp89:
	movq	%rbx, %rdi
	callq	mpfr_clear
.Ltmp90:
.LBB10_27:
.Ltmp91:
	movq	%rbx, %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp92:
.LBB10_28:
.Ltmp93:
	callq	mpfr_get_default_rounding_mode
.Ltmp94:
# %bb.29:
.Ltmp95:
	movq	%rsp, %rsi
	movq	%rbx, %rdi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp96:
# %bb.30:
	cmpq	$0, 24(%rsp)
	je	.LBB10_7
# %bb.31:
.Ltmp98:
	movq	%rsp, %rdi
	callq	mpfr_clear
.Ltmp99:
	jmp	.LBB10_7
.LBB10_1:
.Ltmp101:
	movq	%rsp, %rdi
	movq	%rdi, %rsi
	movq	%r14, %rdx
	callq	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp102:
# %bb.2:
	cmpq	$0, 24(%rsp)
	je	.LBB10_4
# %bb.3:
.Ltmp103:
	movq	%rsp, %rdi
	callq	mpfr_clear
.Ltmp104:
.LBB10_4:
	addq	$8, %r14
.Ltmp105:
	movq	%rsp, %rdi
	movq	%rdi, %rsi
	movq	%r14, %rdx
	callq	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp106:
# %bb.5:
	cmpq	$0, 24(%rsp)
	je	.LBB10_7
# %bb.6:
.Ltmp108:
	movq	%rsp, %rdi
	callq	mpfr_clear
.Ltmp109:
.LBB10_7:
	movq	%rbx, %rax
	addq	$32, %rsp
	.cfi_def_cfa_offset 48
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB10_9:
	.cfi_def_cfa_offset 80
.Ltmp110:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB10_8:
.Ltmp107:
	movq	%rax, %r14
	jmp	.LBB10_36
.LBB10_32:
.Ltmp100:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB10_34:
.Ltmp84:
	movq	%rax, %r14
	jmp	.LBB10_36
.LBB10_35:
.Ltmp97:
	movq	%rax, %r14
	movq	%rsp, %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB10_36:
	movq	%rbx, %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%r14, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end10:
	.size	_ZN7mX_real7convertI7dd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS3_, .Lfunc_end10-_ZN7mX_real7convertI7dd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS3_
	.cfi_endproc
	.section	.gcc_except_table,"a",@progbits
	.p2align	2, 0x0
GCC_except_table10:
.Lexception3:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase0-.Lttbaseref0
.Lttbaseref0:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end3-.Lcst_begin3
.Lcst_begin3:
	.uleb128 .Lfunc_begin3-.Lfunc_begin3    # >> Call Site 1 <<
	.uleb128 .Ltmp66-.Lfunc_begin3          #   Call between .Lfunc_begin3 and .Ltmp66
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp66-.Lfunc_begin3          # >> Call Site 2 <<
	.uleb128 .Ltmp67-.Ltmp66                #   Call between .Ltmp66 and .Ltmp67
	.uleb128 .Ltmp84-.Lfunc_begin3          #     jumps to .Ltmp84
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp68-.Lfunc_begin3          # >> Call Site 3 <<
	.uleb128 .Ltmp79-.Ltmp68                #   Call between .Ltmp68 and .Ltmp79
	.uleb128 .Ltmp97-.Lfunc_begin3          #     jumps to .Ltmp97
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp80-.Lfunc_begin3          # >> Call Site 4 <<
	.uleb128 .Ltmp81-.Ltmp80                #   Call between .Ltmp80 and .Ltmp81
	.uleb128 .Ltmp100-.Lfunc_begin3         #     jumps to .Ltmp100
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp82-.Lfunc_begin3          # >> Call Site 5 <<
	.uleb128 .Ltmp83-.Ltmp82                #   Call between .Ltmp82 and .Ltmp83
	.uleb128 .Ltmp84-.Lfunc_begin3          #     jumps to .Ltmp84
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp85-.Lfunc_begin3          # >> Call Site 6 <<
	.uleb128 .Ltmp96-.Ltmp85                #   Call between .Ltmp85 and .Ltmp96
	.uleb128 .Ltmp97-.Lfunc_begin3          #     jumps to .Ltmp97
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp98-.Lfunc_begin3          # >> Call Site 7 <<
	.uleb128 .Ltmp99-.Ltmp98                #   Call between .Ltmp98 and .Ltmp99
	.uleb128 .Ltmp100-.Lfunc_begin3         #     jumps to .Ltmp100
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp101-.Lfunc_begin3         # >> Call Site 8 <<
	.uleb128 .Ltmp102-.Ltmp101              #   Call between .Ltmp101 and .Ltmp102
	.uleb128 .Ltmp107-.Lfunc_begin3         #     jumps to .Ltmp107
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp103-.Lfunc_begin3         # >> Call Site 9 <<
	.uleb128 .Ltmp104-.Ltmp103              #   Call between .Ltmp103 and .Ltmp104
	.uleb128 .Ltmp110-.Lfunc_begin3         #     jumps to .Ltmp110
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp105-.Lfunc_begin3         # >> Call Site 10 <<
	.uleb128 .Ltmp106-.Ltmp105              #   Call between .Ltmp105 and .Ltmp106
	.uleb128 .Ltmp107-.Lfunc_begin3         #     jumps to .Ltmp107
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp108-.Lfunc_begin3         # >> Call Site 11 <<
	.uleb128 .Ltmp109-.Ltmp108              #   Call between .Ltmp108 and .Ltmp109
	.uleb128 .Ltmp110-.Lfunc_begin3         #     jumps to .Ltmp110
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp109-.Lfunc_begin3         # >> Call Site 12 <<
	.uleb128 .Lfunc_end10-.Ltmp109          #   Call between .Ltmp109 and .Lfunc_end10
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end3:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase0:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_,"axG",@progbits,_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_,comdat
	.weak	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_ # -- Begin function _ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
	.p2align	4, 0x90
	.type	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_,@function
_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_: # 
.Lfunc_begin4:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception4
# %bb.0:
	pushq	%r15
	.cfi_def_cfa_offset 16
	pushq	%r14
	.cfi_def_cfa_offset 24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	subq	$48, %rsp
	.cfi_def_cfa_offset 80
	.cfi_offset %rbx, -32
	.cfi_offset %r14, -24
	.cfi_offset %r15, -16
	movq	%rsi, %r14
	movq	%rdi, %rbx
	vmovsd	(%rdx), %xmm0                   # xmm0 = mem[0],zero
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
	movq	%rsi, %rdi
	callq	mpfr_get_prec
	leaq	16(%rsp), %r15
	movq	%r15, %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
	callq	mpfr_get_default_rounding_mode
	movq	%r15, %rdi
	movq	%r14, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp111:
	callq	mpfr_get_default_rounding_mode
.Ltmp112:
# %bb.1:
.Ltmp113:
	leaq	16(%rsp), %rdi
	movq	%rdi, %rsi
	vmovsd	8(%rsp), %xmm0                  # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%eax, %edx
	callq	mpfr_add_d
.Ltmp114:
# %bb.2:
.Ltmp115:
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp116:
# %bb.3:
.Ltmp117:
	movq	%rbx, %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
.Ltmp118:
# %bb.4:
.Ltmp119:
	callq	mpfr_get_default_rounding_mode
.Ltmp120:
# %bb.5:
.Ltmp121:
	leaq	16(%rsp), %rsi
	movq	%rbx, %rdi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp122:
# %bb.6:
	cmpq	$0, 40(%rsp)
	je	.LBB11_8
# %bb.7:
.Ltmp124:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp125:
.LBB11_8:
	movq	%rbx, %rax
	addq	$48, %rsp
	.cfi_def_cfa_offset 32
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	retq
.LBB11_10:
	.cfi_def_cfa_offset 80
.Ltmp126:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB11_9:
.Ltmp123:
	movq	%rax, %rbx
	leaq	16(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end11:
	.size	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_, .Lfunc_end11-_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
	.cfi_endproc
	.section	.gcc_except_table._ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_,"aG",@progbits,_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_,comdat
	.p2align	2, 0x0
GCC_except_table11:
.Lexception4:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase1-.Lttbaseref1
.Lttbaseref1:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end4-.Lcst_begin4
.Lcst_begin4:
	.uleb128 .Lfunc_begin4-.Lfunc_begin4    # >> Call Site 1 <<
	.uleb128 .Ltmp111-.Lfunc_begin4         #   Call between .Lfunc_begin4 and .Ltmp111
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp111-.Lfunc_begin4         # >> Call Site 2 <<
	.uleb128 .Ltmp122-.Ltmp111              #   Call between .Ltmp111 and .Ltmp122
	.uleb128 .Ltmp123-.Lfunc_begin4         #     jumps to .Ltmp123
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp124-.Lfunc_begin4         # >> Call Site 3 <<
	.uleb128 .Ltmp125-.Ltmp124              #   Call between .Ltmp124 and .Ltmp125
	.uleb128 .Ltmp126-.Lfunc_begin4         #     jumps to .Ltmp126
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp125-.Lfunc_begin4         # >> Call Site 4 <<
	.uleb128 .Lfunc_end11-.Ltmp125          #   Call between .Ltmp125 and .Lfunc_end11
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end4:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase1:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._ZN4mpfr6mprealD2Ev,"axG",@progbits,_ZN4mpfr6mprealD2Ev,comdat
	.weak	_ZN4mpfr6mprealD2Ev             # -- Begin function _ZN4mpfr6mprealD2Ev
	.p2align	4, 0x90
	.type	_ZN4mpfr6mprealD2Ev,@function
_ZN4mpfr6mprealD2Ev:                    # 
.Lfunc_begin5:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception5
# %bb.0:
	pushq	%rax
	.cfi_def_cfa_offset 16
	cmpq	$0, 24(%rdi)
	je	.LBB12_2
# %bb.1:
.Ltmp127:
	callq	mpfr_clear
.Ltmp128:
.LBB12_2:
	popq	%rax
	.cfi_def_cfa_offset 8
	retq
.LBB12_3:
	.cfi_def_cfa_offset 16
.Ltmp129:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.Lfunc_end12:
	.size	_ZN4mpfr6mprealD2Ev, .Lfunc_end12-_ZN4mpfr6mprealD2Ev
	.cfi_endproc
	.section	.gcc_except_table._ZN4mpfr6mprealD2Ev,"aG",@progbits,_ZN4mpfr6mprealD2Ev,comdat
	.p2align	2, 0x0
GCC_except_table12:
.Lexception5:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase2-.Lttbaseref2
.Lttbaseref2:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end5-.Lcst_begin5
.Lcst_begin5:
	.uleb128 .Ltmp127-.Lfunc_begin5         # >> Call Site 1 <<
	.uleb128 .Ltmp128-.Ltmp127              #   Call between .Ltmp127 and .Ltmp128
	.uleb128 .Ltmp129-.Lfunc_begin5         #     jumps to .Ltmp129
	.byte	1                               #   On action: 1
.Lcst_end5:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase2:
	.p2align	2, 0x0
                                        # -- End function
	.text
	.globl	_ZN7mX_real7convertI7qd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS3_ # -- Begin function _ZN7mX_real7convertI7qd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS3_
	.p2align	4, 0x90
	.type	_ZN7mX_real7convertI7qd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS3_,@function
_ZN7mX_real7convertI7qd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS3_: # 
.Lfunc_begin6:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception6
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r12
	.cfi_def_cfa_offset 40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	subq	$32, %rsp
	.cfi_def_cfa_offset 80
	.cfi_offset %rbx, -48
	.cfi_offset %r12, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rsi, %r14
	movq	%rdi, %rbx
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	movq	%rsp, %rax
	cmpq	%rbx, %rax
	je	.LBB13_1
# %bb.16:
.Ltmp130:
	movq	%rsp, %rdi
	movq	%rbx, %rsi
	movq	%r14, %rdx
	callq	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp131:
# %bb.17:
.Ltmp132:
	movq	%rbx, %rdi
	callq	mpfr_get_prec
.Ltmp133:
# %bb.18:
.Ltmp134:
	movq	%rax, %r12
	movq	%rsp, %rdi
	callq	mpfr_get_prec
.Ltmp135:
# %bb.19:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB13_23
# %bb.20:
	cmpq	$0, 24(%rbx)
	je	.LBB13_22
# %bb.21:
.Ltmp136:
	movq	%rbx, %rdi
	callq	mpfr_clear
.Ltmp137:
.LBB13_22:
.Ltmp138:
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp139:
.LBB13_23:
.Ltmp140:
	callq	mpfr_get_default_rounding_mode
.Ltmp141:
# %bb.24:
.Ltmp142:
	movq	%rsp, %rsi
	movq	%rbx, %rdi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp143:
# %bb.25:
	cmpq	$0, 24(%rsp)
	je	.LBB13_27
# %bb.26:
.Ltmp144:
	movq	%rsp, %rdi
	callq	mpfr_clear
.Ltmp145:
.LBB13_27:
	leaq	8(%r14), %rdx
.Ltmp146:
	movq	%rsp, %rdi
	movq	%rbx, %rsi
	callq	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp147:
# %bb.28:
.Ltmp148:
	movq	%rbx, %rdi
	callq	mpfr_get_prec
.Ltmp149:
# %bb.29:
.Ltmp150:
	movq	%rax, %r12
	movq	%rsp, %rdi
	callq	mpfr_get_prec
.Ltmp151:
# %bb.30:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB13_34
# %bb.31:
	cmpq	$0, 24(%rbx)
	je	.LBB13_33
# %bb.32:
.Ltmp152:
	movq	%rbx, %rdi
	callq	mpfr_clear
.Ltmp153:
.LBB13_33:
.Ltmp154:
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp155:
.LBB13_34:
.Ltmp156:
	callq	mpfr_get_default_rounding_mode
.Ltmp157:
# %bb.35:
.Ltmp158:
	movq	%rsp, %rsi
	movq	%rbx, %rdi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp159:
# %bb.36:
	cmpq	$0, 24(%rsp)
	je	.LBB13_38
# %bb.37:
.Ltmp160:
	movq	%rsp, %rdi
	callq	mpfr_clear
.Ltmp161:
.LBB13_38:
	leaq	16(%r14), %rdx
.Ltmp162:
	movq	%rsp, %rdi
	movq	%rbx, %rsi
	callq	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp163:
# %bb.39:
.Ltmp164:
	movq	%rbx, %rdi
	callq	mpfr_get_prec
.Ltmp165:
# %bb.40:
.Ltmp166:
	movq	%rax, %r12
	movq	%rsp, %rdi
	callq	mpfr_get_prec
.Ltmp167:
# %bb.41:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB13_45
# %bb.42:
	cmpq	$0, 24(%rbx)
	je	.LBB13_44
# %bb.43:
.Ltmp168:
	movq	%rbx, %rdi
	callq	mpfr_clear
.Ltmp169:
.LBB13_44:
.Ltmp170:
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp171:
.LBB13_45:
.Ltmp172:
	callq	mpfr_get_default_rounding_mode
.Ltmp173:
# %bb.46:
.Ltmp174:
	movq	%rsp, %rsi
	movq	%rbx, %rdi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp175:
# %bb.47:
	cmpq	$0, 24(%rsp)
	je	.LBB13_49
# %bb.48:
.Ltmp176:
	movq	%rsp, %rdi
	callq	mpfr_clear
.Ltmp177:
.LBB13_49:
	addq	$24, %r14
.Ltmp178:
	movq	%rsp, %rdi
	movq	%rbx, %rsi
	movq	%r14, %rdx
	callq	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp179:
# %bb.50:
.Ltmp181:
	movq	%rbx, %rdi
	callq	mpfr_get_prec
.Ltmp182:
# %bb.51:
.Ltmp183:
	movq	%rax, %r15
	movq	%rsp, %rdi
	callq	mpfr_get_prec
.Ltmp184:
# %bb.52:
	movq	%rax, %r14
	cmpq	%rax, %r15
	je	.LBB13_56
# %bb.53:
	cmpq	$0, 24(%rbx)
	je	.LBB13_55
# %bb.54:
.Ltmp185:
	movq	%rbx, %rdi
	callq	mpfr_clear
.Ltmp186:
.LBB13_55:
.Ltmp187:
	movq	%rbx, %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp188:
.LBB13_56:
.Ltmp189:
	callq	mpfr_get_default_rounding_mode
.Ltmp190:
# %bb.57:
.Ltmp191:
	movq	%rsp, %rsi
	movq	%rbx, %rdi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp192:
# %bb.58:
	cmpq	$0, 24(%rsp)
	je	.LBB13_13
# %bb.59:
.Ltmp194:
	movq	%rsp, %rdi
	callq	mpfr_clear
.Ltmp195:
	jmp	.LBB13_13
.LBB13_1:
.Ltmp197:
	movq	%rsp, %rdi
	movq	%rdi, %rsi
	movq	%r14, %rdx
	callq	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp198:
# %bb.2:
	cmpq	$0, 24(%rsp)
	je	.LBB13_4
# %bb.3:
.Ltmp199:
	movq	%rsp, %rdi
	callq	mpfr_clear
.Ltmp200:
.LBB13_4:
	leaq	8(%r14), %rdx
.Ltmp201:
	movq	%rsp, %rdi
	movq	%rdi, %rsi
	callq	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp202:
# %bb.5:
	cmpq	$0, 24(%rsp)
	je	.LBB13_7
# %bb.6:
.Ltmp203:
	movq	%rsp, %rdi
	callq	mpfr_clear
.Ltmp204:
.LBB13_7:
	leaq	16(%r14), %rdx
.Ltmp205:
	movq	%rsp, %rdi
	movq	%rdi, %rsi
	callq	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp206:
# %bb.8:
	cmpq	$0, 24(%rsp)
	je	.LBB13_10
# %bb.9:
.Ltmp207:
	movq	%rsp, %rdi
	callq	mpfr_clear
.Ltmp208:
.LBB13_10:
	addq	$24, %r14
.Ltmp209:
	movq	%rsp, %rdi
	movq	%rdi, %rsi
	movq	%r14, %rdx
	callq	_ZN4mpfrplIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp210:
# %bb.11:
	cmpq	$0, 24(%rsp)
	je	.LBB13_13
# %bb.12:
.Ltmp212:
	movq	%rsp, %rdi
	callq	mpfr_clear
.Ltmp213:
.LBB13_13:
	movq	%rbx, %rax
	addq	$32, %rsp
	.cfi_def_cfa_offset 48
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB13_15:
	.cfi_def_cfa_offset 80
.Ltmp214:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB13_14:
.Ltmp211:
	movq	%rax, %r14
	jmp	.LBB13_64
.LBB13_60:
.Ltmp196:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB13_62:
.Ltmp180:
	movq	%rax, %r14
	jmp	.LBB13_64
.LBB13_63:
.Ltmp193:
	movq	%rax, %r14
	movq	%rsp, %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB13_64:
	movq	%rbx, %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%r14, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end13:
	.size	_ZN7mX_real7convertI7qd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS3_, .Lfunc_end13-_ZN7mX_real7convertI7qd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS3_
	.cfi_endproc
	.section	.gcc_except_table,"a",@progbits
	.p2align	2, 0x0
GCC_except_table13:
.Lexception6:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase3-.Lttbaseref3
.Lttbaseref3:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end6-.Lcst_begin6
.Lcst_begin6:
	.uleb128 .Lfunc_begin6-.Lfunc_begin6    # >> Call Site 1 <<
	.uleb128 .Ltmp130-.Lfunc_begin6         #   Call between .Lfunc_begin6 and .Ltmp130
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp130-.Lfunc_begin6         # >> Call Site 2 <<
	.uleb128 .Ltmp131-.Ltmp130              #   Call between .Ltmp130 and .Ltmp131
	.uleb128 .Ltmp180-.Lfunc_begin6         #     jumps to .Ltmp180
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp132-.Lfunc_begin6         # >> Call Site 3 <<
	.uleb128 .Ltmp143-.Ltmp132              #   Call between .Ltmp132 and .Ltmp143
	.uleb128 .Ltmp193-.Lfunc_begin6         #     jumps to .Ltmp193
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp144-.Lfunc_begin6         # >> Call Site 4 <<
	.uleb128 .Ltmp145-.Ltmp144              #   Call between .Ltmp144 and .Ltmp145
	.uleb128 .Ltmp196-.Lfunc_begin6         #     jumps to .Ltmp196
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp146-.Lfunc_begin6         # >> Call Site 5 <<
	.uleb128 .Ltmp147-.Ltmp146              #   Call between .Ltmp146 and .Ltmp147
	.uleb128 .Ltmp180-.Lfunc_begin6         #     jumps to .Ltmp180
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp148-.Lfunc_begin6         # >> Call Site 6 <<
	.uleb128 .Ltmp159-.Ltmp148              #   Call between .Ltmp148 and .Ltmp159
	.uleb128 .Ltmp193-.Lfunc_begin6         #     jumps to .Ltmp193
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp160-.Lfunc_begin6         # >> Call Site 7 <<
	.uleb128 .Ltmp161-.Ltmp160              #   Call between .Ltmp160 and .Ltmp161
	.uleb128 .Ltmp196-.Lfunc_begin6         #     jumps to .Ltmp196
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp162-.Lfunc_begin6         # >> Call Site 8 <<
	.uleb128 .Ltmp163-.Ltmp162              #   Call between .Ltmp162 and .Ltmp163
	.uleb128 .Ltmp180-.Lfunc_begin6         #     jumps to .Ltmp180
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp164-.Lfunc_begin6         # >> Call Site 9 <<
	.uleb128 .Ltmp175-.Ltmp164              #   Call between .Ltmp164 and .Ltmp175
	.uleb128 .Ltmp193-.Lfunc_begin6         #     jumps to .Ltmp193
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp176-.Lfunc_begin6         # >> Call Site 10 <<
	.uleb128 .Ltmp177-.Ltmp176              #   Call between .Ltmp176 and .Ltmp177
	.uleb128 .Ltmp196-.Lfunc_begin6         #     jumps to .Ltmp196
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp178-.Lfunc_begin6         # >> Call Site 11 <<
	.uleb128 .Ltmp179-.Ltmp178              #   Call between .Ltmp178 and .Ltmp179
	.uleb128 .Ltmp180-.Lfunc_begin6         #     jumps to .Ltmp180
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp181-.Lfunc_begin6         # >> Call Site 12 <<
	.uleb128 .Ltmp192-.Ltmp181              #   Call between .Ltmp181 and .Ltmp192
	.uleb128 .Ltmp193-.Lfunc_begin6         #     jumps to .Ltmp193
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp194-.Lfunc_begin6         # >> Call Site 13 <<
	.uleb128 .Ltmp195-.Ltmp194              #   Call between .Ltmp194 and .Ltmp195
	.uleb128 .Ltmp196-.Lfunc_begin6         #     jumps to .Ltmp196
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp197-.Lfunc_begin6         # >> Call Site 14 <<
	.uleb128 .Ltmp198-.Ltmp197              #   Call between .Ltmp197 and .Ltmp198
	.uleb128 .Ltmp211-.Lfunc_begin6         #     jumps to .Ltmp211
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp199-.Lfunc_begin6         # >> Call Site 15 <<
	.uleb128 .Ltmp200-.Ltmp199              #   Call between .Ltmp199 and .Ltmp200
	.uleb128 .Ltmp214-.Lfunc_begin6         #     jumps to .Ltmp214
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp201-.Lfunc_begin6         # >> Call Site 16 <<
	.uleb128 .Ltmp202-.Ltmp201              #   Call between .Ltmp201 and .Ltmp202
	.uleb128 .Ltmp211-.Lfunc_begin6         #     jumps to .Ltmp211
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp203-.Lfunc_begin6         # >> Call Site 17 <<
	.uleb128 .Ltmp204-.Ltmp203              #   Call between .Ltmp203 and .Ltmp204
	.uleb128 .Ltmp214-.Lfunc_begin6         #     jumps to .Ltmp214
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp205-.Lfunc_begin6         # >> Call Site 18 <<
	.uleb128 .Ltmp206-.Ltmp205              #   Call between .Ltmp205 and .Ltmp206
	.uleb128 .Ltmp211-.Lfunc_begin6         #     jumps to .Ltmp211
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp207-.Lfunc_begin6         # >> Call Site 19 <<
	.uleb128 .Ltmp208-.Ltmp207              #   Call between .Ltmp207 and .Ltmp208
	.uleb128 .Ltmp214-.Lfunc_begin6         #     jumps to .Ltmp214
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp209-.Lfunc_begin6         # >> Call Site 20 <<
	.uleb128 .Ltmp210-.Ltmp209              #   Call between .Ltmp209 and .Ltmp210
	.uleb128 .Ltmp211-.Lfunc_begin6         #     jumps to .Ltmp211
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp212-.Lfunc_begin6         # >> Call Site 21 <<
	.uleb128 .Ltmp213-.Ltmp212              #   Call between .Ltmp212 and .Ltmp213
	.uleb128 .Ltmp214-.Lfunc_begin6         #     jumps to .Ltmp214
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp213-.Lfunc_begin6         # >> Call Site 22 <<
	.uleb128 .Lfunc_end13-.Ltmp213          #   Call between .Ltmp213 and .Lfunc_end13
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end6:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase3:
	.p2align	2, 0x0
                                        # -- End function
	.text
	.globl	_ZN7mX_real7convertI7dd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES3_E4typeERKN4mpfr6mprealE # -- Begin function _ZN7mX_real7convertI7dd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES3_E4typeERKN4mpfr6mprealE
	.p2align	4, 0x90
	.type	_ZN7mX_real7convertI7dd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES3_E4typeERKN4mpfr6mprealE,@function
_ZN7mX_real7convertI7dd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES3_E4typeERKN4mpfr6mprealE: # 
.Lfunc_begin7:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception7
# %bb.0:
	pushq	%r14
	.cfi_def_cfa_offset 16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	subq	$120, %rsp
	.cfi_def_cfa_offset 144
	.cfi_offset %rbx, -24
	.cfi_offset %r14, -16
	movq	%rdi, %rbx
	callq	mpfr_get_prec
	leaq	8(%rsp), %r14
	movq	%r14, %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
	callq	mpfr_get_default_rounding_mode
	movq	%r14, %rdi
	movq	%rbx, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp215:
	movq	%r14, %rdi
	xorl	%esi, %esi
	callq	mpfr_get_d
	vmovsd	%xmm0, (%rsp)                   # 8-byte Spill
.Ltmp216:
# %bb.1:
.Ltmp217:
	callq	mpfr_get_default_prec
.Ltmp218:
# %bb.2:
.Ltmp219:
	movq	%rax, %r14
	callq	mpfr_get_default_rounding_mode
.Ltmp220:
# %bb.3:
.Ltmp221:
	movl	%eax, %ebx
	leaq	48(%rsp), %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp222:
# %bb.4:
.Ltmp223:
	leaq	48(%rsp), %rdi
	vmovsd	(%rsp), %xmm0                   # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%ebx, %esi
	callq	mpfr_set_d
.Ltmp224:
# %bb.5:
.Ltmp225:
	leaq	8(%rsp), %rdi
	callq	mpfr_nan_p
.Ltmp226:
# %bb.6:
	testl	%eax, %eax
	je	.LBB14_13
.LBB14_7:
.Ltmp231:
	leaq	8(%rsp), %rdi
	callq	mpfr_nan_p
.Ltmp232:
# %bb.8:
	testl	%eax, %eax
	jne	.LBB14_20
# %bb.9:
.Ltmp233:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	callq	mpfr_cmp_si
.Ltmp234:
# %bb.10:
	testl	%eax, %eax
	jns	.LBB14_20
# %bb.11:
.Ltmp235:
	leaq	48(%rsp), %rdi
	leaq	8(%rsp), %rsi
	callq	mpfr_less_p
.Ltmp236:
# %bb.12:
	testl	%eax, %eax
	jne	.LBB14_17
	jmp	.LBB14_20
.LBB14_13:
.Ltmp227:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	callq	mpfr_cmp_si
.Ltmp228:
# %bb.14:
	testl	%eax, %eax
	jle	.LBB14_7
# %bb.15:
.Ltmp229:
	leaq	48(%rsp), %rdi
	leaq	8(%rsp), %rsi
	callq	mpfr_greater_p
.Ltmp230:
# %bb.16:
	testl	%eax, %eax
	je	.LBB14_7
.LBB14_17:
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovsd	(%rsp), %xmm1                   # 8-byte Reload
                                        # xmm1 = mem[0],zero
	vucomisd	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	vmovapd	%xmm1, %xmm0
	testb	%al, %cl
	jne	.LBB14_19
# %bb.18:
	vmovq	%xmm1, %rax
	movabsq	$-9223372036854775808, %rcx     # imm = 0x8000000000000000
	andq	%rax, %rcx
	movabsq	$-4503599627370496, %rdx        # imm = 0xFFF0000000000000
	andq	%rax, %rdx
	movabsq	$-234187180623265792, %rax      # imm = 0xFCC0000000000000
	addq	%rdx, %rax
	xorl	%edx, %edx
	testq	%rax, %rax
	cmovgq	%rax, %rdx
	orq	%rcx, %rdx
	vmovq	%rdx, %xmm0
.LBB14_19:
	vsubsd	%xmm0, %xmm1, %xmm1
	vmovsd	%xmm1, (%rsp)                   # 8-byte Spill
.LBB14_20:
	vmovsd	(%rsp), %xmm0                   # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	%xmm0, 80(%rsp)
.Ltmp238:
	leaq	88(%rsp), %rdi
	leaq	8(%rsp), %rsi
	leaq	80(%rsp), %rdx
	callq	_ZN4mpfrmiIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp239:
# %bb.21:
.Ltmp241:
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp242:
# %bb.22:
.Ltmp243:
	movq	%rax, %r14
	leaq	88(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp244:
# %bb.23:
	movq	%rax, %rbx
	cmpq	%rax, %r14
	je	.LBB14_27
# %bb.24:
	cmpq	$0, 32(%rsp)
	je	.LBB14_26
# %bb.25:
.Ltmp245:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp246:
.LBB14_26:
.Ltmp247:
	leaq	8(%rsp), %rdi
	movq	%rbx, %rsi
	callq	mpfr_init2
.Ltmp248:
.LBB14_27:
.Ltmp249:
	callq	mpfr_get_default_rounding_mode
.Ltmp250:
# %bb.28:
.Ltmp251:
	leaq	8(%rsp), %rdi
	leaq	88(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp252:
# %bb.29:
	cmpq	$0, 112(%rsp)
	je	.LBB14_31
# %bb.30:
.Ltmp254:
	leaq	88(%rsp), %rdi
	callq	mpfr_clear
.Ltmp255:
.LBB14_31:
	cmpq	$0, 72(%rsp)
	je	.LBB14_33
# %bb.32:
.Ltmp257:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp258:
.LBB14_33:
.Ltmp259:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	callq	mpfr_get_d
	vmovsd	%xmm0, 40(%rsp)                 # 8-byte Spill
.Ltmp260:
# %bb.34:
.Ltmp262:
	callq	mpfr_get_default_prec
.Ltmp263:
# %bb.35:
.Ltmp264:
	movq	%rax, %r14
	callq	mpfr_get_default_rounding_mode
.Ltmp265:
# %bb.36:
.Ltmp266:
	movl	%eax, %ebx
	leaq	48(%rsp), %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp267:
# %bb.37:
.Ltmp268:
	leaq	48(%rsp), %rdi
	vmovsd	40(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%ebx, %esi
	callq	mpfr_set_d
.Ltmp269:
# %bb.38:
	cmpq	$0, 72(%rsp)
	je	.LBB14_40
# %bb.39:
.Ltmp271:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp272:
.LBB14_40:
	cmpq	$0, 32(%rsp)
	je	.LBB14_42
# %bb.41:
.Ltmp274:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp275:
.LBB14_42:
	vmovsd	(%rsp), %xmm0                   # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	40(%rsp), %xmm1                 # 8-byte Reload
                                        # xmm1 = mem[0],zero
	addq	$120, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%r14
	.cfi_def_cfa_offset 8
	retq
.LBB14_43:
	.cfi_def_cfa_offset 144
.Ltmp276:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB14_44:
.Ltmp256:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB14_45:
.Ltmp240:
	jmp	.LBB14_49
.LBB14_46:
.Ltmp273:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB14_47:
.Ltmp261:
	jmp	.LBB14_53
.LBB14_48:
.Ltmp237:
.LBB14_49:
	movq	%rax, %rbx
	jmp	.LBB14_51
.LBB14_50:
.Ltmp253:
	movq	%rax, %rbx
	leaq	88(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB14_51:
	leaq	48(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB14_54
.LBB14_52:
.Ltmp270:
.LBB14_53:
	movq	%rax, %rbx
.LBB14_54:
	leaq	8(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end14:
	.size	_ZN7mX_real7convertI7dd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES3_E4typeERKN4mpfr6mprealE, .Lfunc_end14-_ZN7mX_real7convertI7dd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES3_E4typeERKN4mpfr6mprealE
	.cfi_endproc
	.section	.gcc_except_table,"a",@progbits
	.p2align	2, 0x0
GCC_except_table14:
.Lexception7:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase4-.Lttbaseref4
.Lttbaseref4:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end7-.Lcst_begin7
.Lcst_begin7:
	.uleb128 .Lfunc_begin7-.Lfunc_begin7    # >> Call Site 1 <<
	.uleb128 .Ltmp215-.Lfunc_begin7         #   Call between .Lfunc_begin7 and .Ltmp215
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp215-.Lfunc_begin7         # >> Call Site 2 <<
	.uleb128 .Ltmp216-.Ltmp215              #   Call between .Ltmp215 and .Ltmp216
	.uleb128 .Ltmp261-.Lfunc_begin7         #     jumps to .Ltmp261
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp217-.Lfunc_begin7         # >> Call Site 3 <<
	.uleb128 .Ltmp224-.Ltmp217              #   Call between .Ltmp217 and .Ltmp224
	.uleb128 .Ltmp270-.Lfunc_begin7         #     jumps to .Ltmp270
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp225-.Lfunc_begin7         # >> Call Site 4 <<
	.uleb128 .Ltmp230-.Ltmp225              #   Call between .Ltmp225 and .Ltmp230
	.uleb128 .Ltmp237-.Lfunc_begin7         #     jumps to .Ltmp237
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp238-.Lfunc_begin7         # >> Call Site 5 <<
	.uleb128 .Ltmp239-.Ltmp238              #   Call between .Ltmp238 and .Ltmp239
	.uleb128 .Ltmp240-.Lfunc_begin7         #     jumps to .Ltmp240
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp241-.Lfunc_begin7         # >> Call Site 6 <<
	.uleb128 .Ltmp252-.Ltmp241              #   Call between .Ltmp241 and .Ltmp252
	.uleb128 .Ltmp253-.Lfunc_begin7         #     jumps to .Ltmp253
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp254-.Lfunc_begin7         # >> Call Site 7 <<
	.uleb128 .Ltmp255-.Ltmp254              #   Call between .Ltmp254 and .Ltmp255
	.uleb128 .Ltmp256-.Lfunc_begin7         #     jumps to .Ltmp256
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp257-.Lfunc_begin7         # >> Call Site 8 <<
	.uleb128 .Ltmp258-.Ltmp257              #   Call between .Ltmp257 and .Ltmp258
	.uleb128 .Ltmp273-.Lfunc_begin7         #     jumps to .Ltmp273
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp259-.Lfunc_begin7         # >> Call Site 9 <<
	.uleb128 .Ltmp260-.Ltmp259              #   Call between .Ltmp259 and .Ltmp260
	.uleb128 .Ltmp261-.Lfunc_begin7         #     jumps to .Ltmp261
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp262-.Lfunc_begin7         # >> Call Site 10 <<
	.uleb128 .Ltmp269-.Ltmp262              #   Call between .Ltmp262 and .Ltmp269
	.uleb128 .Ltmp270-.Lfunc_begin7         #     jumps to .Ltmp270
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp271-.Lfunc_begin7         # >> Call Site 11 <<
	.uleb128 .Ltmp272-.Ltmp271              #   Call between .Ltmp271 and .Ltmp272
	.uleb128 .Ltmp273-.Lfunc_begin7         #     jumps to .Ltmp273
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp274-.Lfunc_begin7         # >> Call Site 12 <<
	.uleb128 .Ltmp275-.Ltmp274              #   Call between .Ltmp274 and .Ltmp275
	.uleb128 .Ltmp276-.Lfunc_begin7         #     jumps to .Ltmp276
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp275-.Lfunc_begin7         # >> Call Site 13 <<
	.uleb128 .Lfunc_end14-.Ltmp275          #   Call between .Ltmp275 and .Lfunc_end14
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end7:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase4:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._ZN4mpfrmiIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_,"axG",@progbits,_ZN4mpfrmiIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_,comdat
	.weak	_ZN4mpfrmiIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_ # -- Begin function _ZN4mpfrmiIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
	.p2align	4, 0x90
	.type	_ZN4mpfrmiIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_,@function
_ZN4mpfrmiIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_: # 
.Lfunc_begin8:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception8
# %bb.0:
	pushq	%r15
	.cfi_def_cfa_offset 16
	pushq	%r14
	.cfi_def_cfa_offset 24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	subq	$48, %rsp
	.cfi_def_cfa_offset 80
	.cfi_offset %rbx, -32
	.cfi_offset %r14, -24
	.cfi_offset %r15, -16
	movq	%rsi, %r14
	movq	%rdi, %rbx
	vmovsd	(%rdx), %xmm0                   # xmm0 = mem[0],zero
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
	movq	%rsi, %rdi
	callq	mpfr_get_prec
	leaq	16(%rsp), %r15
	movq	%r15, %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
	callq	mpfr_get_default_rounding_mode
	movq	%r15, %rdi
	movq	%r14, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp277:
	callq	mpfr_get_default_rounding_mode
.Ltmp278:
# %bb.1:
.Ltmp279:
	leaq	16(%rsp), %rdi
	movq	%rdi, %rsi
	vmovsd	8(%rsp), %xmm0                  # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%eax, %edx
	callq	mpfr_sub_d
.Ltmp280:
# %bb.2:
.Ltmp281:
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp282:
# %bb.3:
.Ltmp283:
	movq	%rbx, %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
.Ltmp284:
# %bb.4:
.Ltmp285:
	callq	mpfr_get_default_rounding_mode
.Ltmp286:
# %bb.5:
.Ltmp287:
	leaq	16(%rsp), %rsi
	movq	%rbx, %rdi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp288:
# %bb.6:
	cmpq	$0, 40(%rsp)
	je	.LBB15_8
# %bb.7:
.Ltmp290:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp291:
.LBB15_8:
	movq	%rbx, %rax
	addq	$48, %rsp
	.cfi_def_cfa_offset 32
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	retq
.LBB15_10:
	.cfi_def_cfa_offset 80
.Ltmp292:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB15_9:
.Ltmp289:
	movq	%rax, %rbx
	leaq	16(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end15:
	.size	_ZN4mpfrmiIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_, .Lfunc_end15-_ZN4mpfrmiIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
	.cfi_endproc
	.section	.gcc_except_table._ZN4mpfrmiIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_,"aG",@progbits,_ZN4mpfrmiIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_,comdat
	.p2align	2, 0x0
GCC_except_table15:
.Lexception8:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase5-.Lttbaseref5
.Lttbaseref5:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end8-.Lcst_begin8
.Lcst_begin8:
	.uleb128 .Lfunc_begin8-.Lfunc_begin8    # >> Call Site 1 <<
	.uleb128 .Ltmp277-.Lfunc_begin8         #   Call between .Lfunc_begin8 and .Ltmp277
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp277-.Lfunc_begin8         # >> Call Site 2 <<
	.uleb128 .Ltmp288-.Ltmp277              #   Call between .Ltmp277 and .Ltmp288
	.uleb128 .Ltmp289-.Lfunc_begin8         #     jumps to .Ltmp289
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp290-.Lfunc_begin8         # >> Call Site 3 <<
	.uleb128 .Ltmp291-.Ltmp290              #   Call between .Ltmp290 and .Ltmp291
	.uleb128 .Ltmp292-.Lfunc_begin8         #     jumps to .Ltmp292
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp291-.Lfunc_begin8         # >> Call Site 4 <<
	.uleb128 .Lfunc_end15-.Ltmp291          #   Call between .Ltmp291 and .Lfunc_end15
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end8:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase5:
	.p2align	2, 0x0
                                        # -- End function
	.text
	.globl	_ZN7mX_real7convertI7qd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES3_E4typeERKN4mpfr6mprealE # -- Begin function _ZN7mX_real7convertI7qd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES3_E4typeERKN4mpfr6mprealE
	.p2align	4, 0x90
	.type	_ZN7mX_real7convertI7qd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES3_E4typeERKN4mpfr6mprealE,@function
_ZN7mX_real7convertI7qd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES3_E4typeERKN4mpfr6mprealE: # 
.Lfunc_begin9:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception9
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	subq	$120, %rsp
	.cfi_def_cfa_offset 160
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rsi, %r14
	movq	%rdi, %rbx
	movq	%rsi, %rdi
	callq	mpfr_get_prec
	leaq	16(%rsp), %r15
	movq	%r15, %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
	callq	mpfr_get_default_rounding_mode
	movq	%r15, %rdi
	movq	%r14, %rsi
	movl	%eax, %edx
	callq	mpfr_set
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rbx)
.Ltmp293:
	movq	%r15, %rdi
	xorl	%esi, %esi
	vzeroupper
	callq	mpfr_get_d
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
.Ltmp294:
# %bb.1:
.Ltmp295:
	callq	mpfr_get_default_prec
.Ltmp296:
# %bb.2:
.Ltmp297:
	movq	%rax, %r14
	callq	mpfr_get_default_rounding_mode
.Ltmp298:
# %bb.3:
.Ltmp299:
	movl	%eax, %ebp
	leaq	56(%rsp), %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp300:
# %bb.4:
.Ltmp301:
	leaq	56(%rsp), %rdi
	vmovsd	8(%rsp), %xmm0                  # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%ebp, %esi
	callq	mpfr_set_d
.Ltmp302:
# %bb.5:
.Ltmp303:
	leaq	16(%rsp), %rdi
	callq	mpfr_nan_p
.Ltmp304:
# %bb.6:
	testl	%eax, %eax
	je	.LBB16_13
.LBB16_7:
.Ltmp309:
	leaq	16(%rsp), %rdi
	callq	mpfr_nan_p
.Ltmp310:
# %bb.8:
	testl	%eax, %eax
	jne	.LBB16_20
# %bb.9:
.Ltmp311:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	callq	mpfr_cmp_si
.Ltmp312:
# %bb.10:
	testl	%eax, %eax
	jns	.LBB16_20
# %bb.11:
.Ltmp313:
	leaq	56(%rsp), %rdi
	leaq	16(%rsp), %rsi
	callq	mpfr_less_p
.Ltmp314:
# %bb.12:
	testl	%eax, %eax
	jne	.LBB16_17
	jmp	.LBB16_20
.LBB16_13:
.Ltmp305:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	callq	mpfr_cmp_si
.Ltmp306:
# %bb.14:
	testl	%eax, %eax
	jle	.LBB16_7
# %bb.15:
.Ltmp307:
	leaq	56(%rsp), %rdi
	leaq	16(%rsp), %rsi
	callq	mpfr_greater_p
.Ltmp308:
# %bb.16:
	testl	%eax, %eax
	je	.LBB16_7
.LBB16_17:
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovsd	8(%rsp), %xmm1                  # 8-byte Reload
                                        # xmm1 = mem[0],zero
	vucomisd	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	vmovapd	%xmm1, %xmm0
	testb	%al, %cl
	jne	.LBB16_19
# %bb.18:
	vmovq	%xmm1, %rax
	movabsq	$-9223372036854775808, %rcx     # imm = 0x8000000000000000
	andq	%rax, %rcx
	movabsq	$-4503599627370496, %rdx        # imm = 0xFFF0000000000000
	andq	%rax, %rdx
	movabsq	$-234187180623265792, %rax      # imm = 0xFCC0000000000000
	addq	%rdx, %rax
	xorl	%edx, %edx
	testq	%rax, %rax
	cmovgq	%rax, %rdx
	orq	%rcx, %rdx
	vmovq	%rdx, %xmm0
.LBB16_19:
	vsubsd	%xmm0, %xmm1, %xmm1
	vmovsd	%xmm1, 8(%rsp)                  # 8-byte Spill
.LBB16_20:
	vmovsd	8(%rsp), %xmm0                  # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	%xmm0, 48(%rsp)
.Ltmp315:
	leaq	88(%rsp), %rdi
	leaq	16(%rsp), %rsi
	leaq	48(%rsp), %rdx
	callq	_ZN4mpfrmiIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp316:
# %bb.21:
.Ltmp317:
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp318:
# %bb.22:
.Ltmp319:
	movq	%rax, %r15
	leaq	88(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp320:
# %bb.23:
	movq	%rax, %r14
	cmpq	%rax, %r15
	je	.LBB16_27
# %bb.24:
	cmpq	$0, 40(%rsp)
	je	.LBB16_26
# %bb.25:
.Ltmp321:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp322:
.LBB16_26:
.Ltmp323:
	leaq	16(%rsp), %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp324:
.LBB16_27:
.Ltmp325:
	callq	mpfr_get_default_rounding_mode
.Ltmp326:
# %bb.28:
.Ltmp327:
	leaq	16(%rsp), %rdi
	leaq	88(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp328:
# %bb.29:
	cmpq	$0, 112(%rsp)
	je	.LBB16_31
# %bb.30:
.Ltmp329:
	leaq	88(%rsp), %rdi
	callq	mpfr_clear
.Ltmp330:
.LBB16_31:
	vmovsd	8(%rsp), %xmm0                  # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	%xmm0, (%rbx)
	cmpq	$0, 80(%rsp)
	je	.LBB16_33
# %bb.32:
.Ltmp331:
	leaq	56(%rsp), %rdi
	callq	mpfr_clear
.Ltmp332:
.LBB16_33:
.Ltmp333:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	callq	mpfr_get_d
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
.Ltmp334:
# %bb.34:
.Ltmp335:
	callq	mpfr_get_default_prec
.Ltmp336:
# %bb.35:
.Ltmp337:
	movq	%rax, %r14
	callq	mpfr_get_default_rounding_mode
.Ltmp338:
# %bb.36:
.Ltmp339:
	movl	%eax, %ebp
	leaq	56(%rsp), %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp340:
# %bb.37:
.Ltmp341:
	leaq	56(%rsp), %rdi
	vmovsd	8(%rsp), %xmm0                  # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%ebp, %esi
	callq	mpfr_set_d
.Ltmp342:
# %bb.38:
.Ltmp343:
	leaq	16(%rsp), %rdi
	callq	mpfr_nan_p
.Ltmp344:
# %bb.39:
	testl	%eax, %eax
	je	.LBB16_46
.LBB16_40:
.Ltmp349:
	leaq	16(%rsp), %rdi
	callq	mpfr_nan_p
.Ltmp350:
# %bb.41:
	testl	%eax, %eax
	jne	.LBB16_53
# %bb.42:
.Ltmp351:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	callq	mpfr_cmp_si
.Ltmp352:
# %bb.43:
	testl	%eax, %eax
	jns	.LBB16_53
# %bb.44:
.Ltmp353:
	leaq	56(%rsp), %rdi
	leaq	16(%rsp), %rsi
	callq	mpfr_less_p
.Ltmp354:
# %bb.45:
	testl	%eax, %eax
	jne	.LBB16_50
	jmp	.LBB16_53
.LBB16_46:
.Ltmp345:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	callq	mpfr_cmp_si
.Ltmp346:
# %bb.47:
	testl	%eax, %eax
	jle	.LBB16_40
# %bb.48:
.Ltmp347:
	leaq	56(%rsp), %rdi
	leaq	16(%rsp), %rsi
	callq	mpfr_greater_p
.Ltmp348:
# %bb.49:
	testl	%eax, %eax
	je	.LBB16_40
.LBB16_50:
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovsd	8(%rsp), %xmm1                  # 8-byte Reload
                                        # xmm1 = mem[0],zero
	vucomisd	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	vmovapd	%xmm1, %xmm0
	testb	%al, %cl
	jne	.LBB16_52
# %bb.51:
	vmovq	%xmm1, %rax
	movabsq	$-9223372036854775808, %rcx     # imm = 0x8000000000000000
	andq	%rax, %rcx
	movabsq	$-4503599627370496, %rdx        # imm = 0xFFF0000000000000
	andq	%rax, %rdx
	movabsq	$-234187180623265792, %rax      # imm = 0xFCC0000000000000
	addq	%rdx, %rax
	xorl	%edx, %edx
	testq	%rax, %rax
	cmovgq	%rax, %rdx
	orq	%rcx, %rdx
	vmovq	%rdx, %xmm0
.LBB16_52:
	vsubsd	%xmm0, %xmm1, %xmm1
	vmovsd	%xmm1, 8(%rsp)                  # 8-byte Spill
.LBB16_53:
	vmovsd	8(%rsp), %xmm0                  # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	%xmm0, 48(%rsp)
.Ltmp355:
	leaq	88(%rsp), %rdi
	leaq	16(%rsp), %rsi
	leaq	48(%rsp), %rdx
	callq	_ZN4mpfrmiIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp356:
# %bb.54:
.Ltmp357:
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp358:
# %bb.55:
.Ltmp359:
	movq	%rax, %r15
	leaq	88(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp360:
# %bb.56:
	movq	%rax, %r14
	cmpq	%rax, %r15
	je	.LBB16_60
# %bb.57:
	cmpq	$0, 40(%rsp)
	je	.LBB16_59
# %bb.58:
.Ltmp361:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp362:
.LBB16_59:
.Ltmp363:
	leaq	16(%rsp), %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp364:
.LBB16_60:
.Ltmp365:
	callq	mpfr_get_default_rounding_mode
.Ltmp366:
# %bb.61:
.Ltmp367:
	leaq	16(%rsp), %rdi
	leaq	88(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp368:
# %bb.62:
	cmpq	$0, 112(%rsp)
	je	.LBB16_64
# %bb.63:
.Ltmp369:
	leaq	88(%rsp), %rdi
	callq	mpfr_clear
.Ltmp370:
.LBB16_64:
	vmovsd	8(%rsp), %xmm0                  # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	%xmm0, 8(%rbx)
	cmpq	$0, 80(%rsp)
	je	.LBB16_66
# %bb.65:
.Ltmp371:
	leaq	56(%rsp), %rdi
	callq	mpfr_clear
.Ltmp372:
.LBB16_66:
.Ltmp373:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	callq	mpfr_get_d
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
.Ltmp374:
# %bb.67:
.Ltmp375:
	callq	mpfr_get_default_prec
.Ltmp376:
# %bb.68:
.Ltmp377:
	movq	%rax, %r14
	callq	mpfr_get_default_rounding_mode
.Ltmp378:
# %bb.69:
.Ltmp379:
	movl	%eax, %ebp
	leaq	56(%rsp), %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp380:
# %bb.70:
.Ltmp381:
	leaq	56(%rsp), %rdi
	vmovsd	8(%rsp), %xmm0                  # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%ebp, %esi
	callq	mpfr_set_d
.Ltmp382:
# %bb.71:
.Ltmp383:
	leaq	16(%rsp), %rdi
	callq	mpfr_nan_p
.Ltmp384:
# %bb.72:
	testl	%eax, %eax
	je	.LBB16_79
.LBB16_73:
.Ltmp389:
	leaq	16(%rsp), %rdi
	callq	mpfr_nan_p
.Ltmp390:
# %bb.74:
	testl	%eax, %eax
	jne	.LBB16_86
# %bb.75:
.Ltmp391:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	callq	mpfr_cmp_si
.Ltmp392:
# %bb.76:
	testl	%eax, %eax
	jns	.LBB16_86
# %bb.77:
.Ltmp393:
	leaq	56(%rsp), %rdi
	leaq	16(%rsp), %rsi
	callq	mpfr_less_p
.Ltmp394:
# %bb.78:
	testl	%eax, %eax
	jne	.LBB16_83
	jmp	.LBB16_86
.LBB16_79:
.Ltmp385:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	callq	mpfr_cmp_si
.Ltmp386:
# %bb.80:
	testl	%eax, %eax
	jle	.LBB16_73
# %bb.81:
.Ltmp387:
	leaq	56(%rsp), %rdi
	leaq	16(%rsp), %rsi
	callq	mpfr_greater_p
.Ltmp388:
# %bb.82:
	testl	%eax, %eax
	je	.LBB16_73
.LBB16_83:
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovsd	8(%rsp), %xmm1                  # 8-byte Reload
                                        # xmm1 = mem[0],zero
	vucomisd	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	vmovapd	%xmm1, %xmm0
	testb	%al, %cl
	jne	.LBB16_85
# %bb.84:
	vmovq	%xmm1, %rax
	movabsq	$-9223372036854775808, %rcx     # imm = 0x8000000000000000
	andq	%rax, %rcx
	movabsq	$-4503599627370496, %rdx        # imm = 0xFFF0000000000000
	andq	%rax, %rdx
	movabsq	$-234187180623265792, %rax      # imm = 0xFCC0000000000000
	addq	%rdx, %rax
	xorl	%edx, %edx
	testq	%rax, %rax
	cmovgq	%rax, %rdx
	orq	%rcx, %rdx
	vmovq	%rdx, %xmm0
.LBB16_85:
	vsubsd	%xmm0, %xmm1, %xmm1
	vmovsd	%xmm1, 8(%rsp)                  # 8-byte Spill
.LBB16_86:
	vmovsd	8(%rsp), %xmm0                  # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	%xmm0, 48(%rsp)
.Ltmp396:
	leaq	88(%rsp), %rdi
	leaq	16(%rsp), %rsi
	leaq	48(%rsp), %rdx
	callq	_ZN4mpfrmiIdEEKNS_8internal11result_typeIT_E4typeERKNS_6mprealERKS3_
.Ltmp397:
# %bb.87:
.Ltmp399:
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp400:
# %bb.88:
.Ltmp401:
	movq	%rax, %r15
	leaq	88(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp402:
# %bb.89:
	movq	%rax, %r14
	cmpq	%rax, %r15
	je	.LBB16_93
# %bb.90:
	cmpq	$0, 40(%rsp)
	je	.LBB16_92
# %bb.91:
.Ltmp403:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp404:
.LBB16_92:
.Ltmp405:
	leaq	16(%rsp), %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp406:
.LBB16_93:
.Ltmp407:
	callq	mpfr_get_default_rounding_mode
.Ltmp408:
# %bb.94:
.Ltmp409:
	leaq	16(%rsp), %rdi
	leaq	88(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp410:
# %bb.95:
	cmpq	$0, 112(%rsp)
	je	.LBB16_97
# %bb.96:
.Ltmp412:
	leaq	88(%rsp), %rdi
	callq	mpfr_clear
.Ltmp413:
.LBB16_97:
	vmovsd	8(%rsp), %xmm0                  # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	%xmm0, 16(%rbx)
	cmpq	$0, 80(%rsp)
	je	.LBB16_99
# %bb.98:
.Ltmp415:
	leaq	56(%rsp), %rdi
	callq	mpfr_clear
.Ltmp416:
.LBB16_99:
.Ltmp417:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	callq	mpfr_get_d
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
.Ltmp418:
# %bb.100:
.Ltmp420:
	callq	mpfr_get_default_prec
.Ltmp421:
# %bb.101:
.Ltmp422:
	movq	%rax, %r14
	callq	mpfr_get_default_rounding_mode
.Ltmp423:
# %bb.102:
.Ltmp424:
	movl	%eax, %ebp
	leaq	56(%rsp), %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp425:
# %bb.103:
.Ltmp426:
	leaq	56(%rsp), %rdi
	vmovsd	8(%rsp), %xmm0                  # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%ebp, %esi
	callq	mpfr_set_d
.Ltmp427:
# %bb.104:
	vmovsd	8(%rsp), %xmm0                  # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	%xmm0, 24(%rbx)
	cmpq	$0, 80(%rsp)
	je	.LBB16_106
# %bb.105:
.Ltmp429:
	leaq	56(%rsp), %rdi
	callq	mpfr_clear
.Ltmp430:
.LBB16_106:
	cmpq	$0, 40(%rsp)
	je	.LBB16_108
# %bb.107:
.Ltmp432:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp433:
.LBB16_108:
	movq	%rbx, %rax
	addq	$120, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB16_109:
	.cfi_def_cfa_offset 160
.Ltmp434:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB16_110:
.Ltmp414:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB16_111:
.Ltmp431:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB16_112:
.Ltmp398:
	jmp	.LBB16_115
.LBB16_113:
.Ltmp419:
	jmp	.LBB16_119
.LBB16_114:
.Ltmp395:
.LBB16_115:
	movq	%rax, %rbx
	jmp	.LBB16_117
.LBB16_116:
.Ltmp411:
	movq	%rax, %rbx
	leaq	88(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB16_117:
	leaq	56(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB16_120
.LBB16_118:
.Ltmp428:
.LBB16_119:
	movq	%rax, %rbx
.LBB16_120:
	leaq	16(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end16:
	.size	_ZN7mX_real7convertI7qd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES3_E4typeERKN4mpfr6mprealE, .Lfunc_end16-_ZN7mX_real7convertI7qd_realEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES3_E4typeERKN4mpfr6mprealE
	.cfi_endproc
	.section	.gcc_except_table,"a",@progbits
	.p2align	2, 0x0
GCC_except_table16:
.Lexception9:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase6-.Lttbaseref6
.Lttbaseref6:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end9-.Lcst_begin9
.Lcst_begin9:
	.uleb128 .Lfunc_begin9-.Lfunc_begin9    # >> Call Site 1 <<
	.uleb128 .Ltmp293-.Lfunc_begin9         #   Call between .Lfunc_begin9 and .Ltmp293
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp293-.Lfunc_begin9         # >> Call Site 2 <<
	.uleb128 .Ltmp294-.Ltmp293              #   Call between .Ltmp293 and .Ltmp294
	.uleb128 .Ltmp419-.Lfunc_begin9         #     jumps to .Ltmp419
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp295-.Lfunc_begin9         # >> Call Site 3 <<
	.uleb128 .Ltmp302-.Ltmp295              #   Call between .Ltmp295 and .Ltmp302
	.uleb128 .Ltmp428-.Lfunc_begin9         #     jumps to .Ltmp428
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp303-.Lfunc_begin9         # >> Call Site 4 <<
	.uleb128 .Ltmp308-.Ltmp303              #   Call between .Ltmp303 and .Ltmp308
	.uleb128 .Ltmp395-.Lfunc_begin9         #     jumps to .Ltmp395
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp315-.Lfunc_begin9         # >> Call Site 5 <<
	.uleb128 .Ltmp316-.Ltmp315              #   Call between .Ltmp315 and .Ltmp316
	.uleb128 .Ltmp398-.Lfunc_begin9         #     jumps to .Ltmp398
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp317-.Lfunc_begin9         # >> Call Site 6 <<
	.uleb128 .Ltmp328-.Ltmp317              #   Call between .Ltmp317 and .Ltmp328
	.uleb128 .Ltmp411-.Lfunc_begin9         #     jumps to .Ltmp411
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp329-.Lfunc_begin9         # >> Call Site 7 <<
	.uleb128 .Ltmp330-.Ltmp329              #   Call between .Ltmp329 and .Ltmp330
	.uleb128 .Ltmp414-.Lfunc_begin9         #     jumps to .Ltmp414
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp331-.Lfunc_begin9         # >> Call Site 8 <<
	.uleb128 .Ltmp332-.Ltmp331              #   Call between .Ltmp331 and .Ltmp332
	.uleb128 .Ltmp431-.Lfunc_begin9         #     jumps to .Ltmp431
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp333-.Lfunc_begin9         # >> Call Site 9 <<
	.uleb128 .Ltmp334-.Ltmp333              #   Call between .Ltmp333 and .Ltmp334
	.uleb128 .Ltmp419-.Lfunc_begin9         #     jumps to .Ltmp419
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp335-.Lfunc_begin9         # >> Call Site 10 <<
	.uleb128 .Ltmp342-.Ltmp335              #   Call between .Ltmp335 and .Ltmp342
	.uleb128 .Ltmp428-.Lfunc_begin9         #     jumps to .Ltmp428
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp343-.Lfunc_begin9         # >> Call Site 11 <<
	.uleb128 .Ltmp348-.Ltmp343              #   Call between .Ltmp343 and .Ltmp348
	.uleb128 .Ltmp395-.Lfunc_begin9         #     jumps to .Ltmp395
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp355-.Lfunc_begin9         # >> Call Site 12 <<
	.uleb128 .Ltmp356-.Ltmp355              #   Call between .Ltmp355 and .Ltmp356
	.uleb128 .Ltmp398-.Lfunc_begin9         #     jumps to .Ltmp398
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp357-.Lfunc_begin9         # >> Call Site 13 <<
	.uleb128 .Ltmp368-.Ltmp357              #   Call between .Ltmp357 and .Ltmp368
	.uleb128 .Ltmp411-.Lfunc_begin9         #     jumps to .Ltmp411
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp369-.Lfunc_begin9         # >> Call Site 14 <<
	.uleb128 .Ltmp370-.Ltmp369              #   Call between .Ltmp369 and .Ltmp370
	.uleb128 .Ltmp414-.Lfunc_begin9         #     jumps to .Ltmp414
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp371-.Lfunc_begin9         # >> Call Site 15 <<
	.uleb128 .Ltmp372-.Ltmp371              #   Call between .Ltmp371 and .Ltmp372
	.uleb128 .Ltmp431-.Lfunc_begin9         #     jumps to .Ltmp431
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp373-.Lfunc_begin9         # >> Call Site 16 <<
	.uleb128 .Ltmp374-.Ltmp373              #   Call between .Ltmp373 and .Ltmp374
	.uleb128 .Ltmp419-.Lfunc_begin9         #     jumps to .Ltmp419
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp375-.Lfunc_begin9         # >> Call Site 17 <<
	.uleb128 .Ltmp382-.Ltmp375              #   Call between .Ltmp375 and .Ltmp382
	.uleb128 .Ltmp428-.Lfunc_begin9         #     jumps to .Ltmp428
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp383-.Lfunc_begin9         # >> Call Site 18 <<
	.uleb128 .Ltmp388-.Ltmp383              #   Call between .Ltmp383 and .Ltmp388
	.uleb128 .Ltmp395-.Lfunc_begin9         #     jumps to .Ltmp395
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp396-.Lfunc_begin9         # >> Call Site 19 <<
	.uleb128 .Ltmp397-.Ltmp396              #   Call between .Ltmp396 and .Ltmp397
	.uleb128 .Ltmp398-.Lfunc_begin9         #     jumps to .Ltmp398
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp399-.Lfunc_begin9         # >> Call Site 20 <<
	.uleb128 .Ltmp410-.Ltmp399              #   Call between .Ltmp399 and .Ltmp410
	.uleb128 .Ltmp411-.Lfunc_begin9         #     jumps to .Ltmp411
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp412-.Lfunc_begin9         # >> Call Site 21 <<
	.uleb128 .Ltmp413-.Ltmp412              #   Call between .Ltmp412 and .Ltmp413
	.uleb128 .Ltmp414-.Lfunc_begin9         #     jumps to .Ltmp414
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp415-.Lfunc_begin9         # >> Call Site 22 <<
	.uleb128 .Ltmp416-.Ltmp415              #   Call between .Ltmp415 and .Ltmp416
	.uleb128 .Ltmp431-.Lfunc_begin9         #     jumps to .Ltmp431
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp417-.Lfunc_begin9         # >> Call Site 23 <<
	.uleb128 .Ltmp418-.Ltmp417              #   Call between .Ltmp417 and .Ltmp418
	.uleb128 .Ltmp419-.Lfunc_begin9         #     jumps to .Ltmp419
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp420-.Lfunc_begin9         # >> Call Site 24 <<
	.uleb128 .Ltmp427-.Ltmp420              #   Call between .Ltmp420 and .Ltmp427
	.uleb128 .Ltmp428-.Lfunc_begin9         #     jumps to .Ltmp428
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp429-.Lfunc_begin9         # >> Call Site 25 <<
	.uleb128 .Ltmp430-.Ltmp429              #   Call between .Ltmp429 and .Ltmp430
	.uleb128 .Ltmp431-.Lfunc_begin9         #     jumps to .Ltmp431
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp432-.Lfunc_begin9         # >> Call Site 26 <<
	.uleb128 .Ltmp433-.Ltmp432              #   Call between .Ltmp432 and .Ltmp433
	.uleb128 .Ltmp434-.Lfunc_begin9         #     jumps to .Ltmp434
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp433-.Lfunc_begin9         # >> Call Site 27 <<
	.uleb128 .Lfunc_end16-.Ltmp433          #   Call between .Ltmp433 and .Lfunc_end16
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end9:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase6:
	.p2align	2, 0x0
                                        # -- End function
	.text
	.globl	_Z4initiRKN4mpfr6mprealEPS0_S3_S3_ # -- Begin function _Z4initiRKN4mpfr6mprealEPS0_S3_S3_
	.p2align	4, 0x90
	.type	_Z4initiRKN4mpfr6mprealEPS0_S3_S3_,@function
_Z4initiRKN4mpfr6mprealEPS0_S3_S3_:     # 
.Lfunc_begin10:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception10
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$216, %rsp
	.cfi_def_cfa_offset 272
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, %rbx
	movq	%rcx, 96(%rsp)                  # 8-byte Spill
	movq	%rdx, %r15
	movq	%rsi, %r12
	movl	%edi, %r14d
	movl	%edi, 4(%rsp)
	leaq	184(%rsp), %rdi
	movl	$1, %esi
	callq	_ZN4mpfr6randomEj
	cmpq	$0, 208(%rsp)
	je	.LBB17_3
# %bb.1:
.Ltmp435:
	leaq	184(%rsp), %rdi
	callq	mpfr_clear
.Ltmp436:
# %bb.2:
	movl	4(%rsp), %r14d
.LBB17_3:
	testl	%r14d, %r14d
	movq	%rbx, 80(%rsp)                  # 8-byte Spill
	movq	%r12, 176(%rsp)                 # 8-byte Spill
	movq	%r15, 168(%rsp)                 # 8-byte Spill
	jle	.LBB17_122
# %bb.4:
	leaq	256(%rbx), %rbp
	xorl	%eax, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	leaq	8(%rsp), %r13
	movq	%r15, %rbx
	movq	96(%rsp), %r15                  # 8-byte Reload
	jmp	.LBB17_8
	.p2align	4, 0x90
.LBB17_5:                               #   in Loop: Header=BB17_8 Depth=1
	movq	%rbp, %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.LBB17_6:                               #   in Loop: Header=BB17_8 Depth=1
	callq	mpfr_get_default_rounding_mode
	movq	%rbp, %rdi
	movq	%r15, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.LBB17_7:                               #   in Loop: Header=BB17_8 Depth=1
	movq	72(%rsp), %rcx                  # 8-byte Reload
	incq	%rcx
	movslq	4(%rsp), %r14
	addq	$32, %rbp
	addq	$32, %r15
	addq	$32, %rbx
	movq	%rcx, %rax
	movq	%rcx, 72(%rsp)                  # 8-byte Spill
	cmpq	%r14, %rcx
	jge	.LBB17_96
.LBB17_8:                               # =>This Inner Loop Header: Depth=1
	cmpb	$0, _ZZN4mpfr6randomEjE10initialize(%rip)
	je	.LBB17_10
# %bb.9:                                #   in Loop: Header=BB17_8 Depth=1
	movl	$_ZZN4mpfr6randomEjE5state, %edi
	callq	__gmp_randinit_default
	movl	$_ZZN4mpfr6randomEjE5state, %edi
	xorl	%esi, %esi
	callq	__gmp_randseed_ui
	movb	$0, _ZZN4mpfr6randomEjE10initialize(%rip)
.LBB17_10:                              #   in Loop: Header=BB17_8 Depth=1
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %r14d
	callq	mpfr_get_default_prec
	leaq	104(%rsp), %r12
	movq	%r12, %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
	movabsq	$-9223372036854775807, %rax     # imm = 0x8000000000000001
	movq	%rax, 120(%rsp)
.Ltmp438:
	movl	$_ZZN4mpfr6randomEjE5state, %esi
	movq	%r12, %rdi
	movl	%r14d, %edx
	callq	mpfr_urandom
.Ltmp439:
# %bb.11:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp444:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp445:
# %bb.12:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp446:
	movq	%r13, %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
.Ltmp447:
# %bb.13:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp448:
	callq	mpfr_get_default_rounding_mode
.Ltmp449:
# %bb.14:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp450:
	movq	%r13, %rdi
	leaq	104(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp451:
# %bb.15:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp453:
	callq	mpfr_get_default_rounding_mode
.Ltmp454:
# %bb.16:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp455:
	movl	$2, %edx
	movq	%r13, %rdi
	movq	%r13, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul_si
.Ltmp456:
# %bb.17:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp457:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp458:
# %bb.18:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp459:
	leaq	136(%rsp), %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
.Ltmp460:
# %bb.19:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp461:
	callq	mpfr_get_default_rounding_mode
.Ltmp462:
# %bb.20:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp463:
	leaq	136(%rsp), %rdi
	movq	%r13, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp464:
# %bb.21:                               #   in Loop: Header=BB17_8 Depth=1
	cmpq	$0, 32(%rsp)
	je	.LBB17_23
# %bb.22:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp466:
	movq	%r13, %rdi
	callq	mpfr_clear
.Ltmp467:
.LBB17_23:                              #   in Loop: Header=BB17_8 Depth=1
.Ltmp469:
	leaq	136(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp470:
# %bb.24:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp471:
	movq	%r13, %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
.Ltmp472:
# %bb.25:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp473:
	callq	mpfr_get_default_rounding_mode
.Ltmp474:
# %bb.26:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp475:
	movq	%r13, %rdi
	leaq	136(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp476:
# %bb.27:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp478:
	callq	mpfr_get_default_rounding_mode
.Ltmp479:
# %bb.28:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp480:
	movl	$1, %edx
	movq	%r13, %rdi
	movq	%r13, %rsi
	movl	%eax, %ecx
	callq	mpfr_sub_si
.Ltmp481:
# %bb.29:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp482:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp483:
# %bb.30:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp484:
	leaq	40(%rsp), %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
.Ltmp485:
# %bb.31:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp486:
	callq	mpfr_get_default_rounding_mode
.Ltmp487:
# %bb.32:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp488:
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp489:
# %bb.33:                               #   in Loop: Header=BB17_8 Depth=1
	cmpq	$0, 32(%rsp)
	je	.LBB17_35
# %bb.34:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp491:
	movq	%r13, %rdi
	callq	mpfr_clear
.Ltmp492:
.LBB17_35:                              #   in Loop: Header=BB17_8 Depth=1
	leaq	40(%rsp), %rax
	cmpq	%rax, %rbx
	je	.LBB17_44
# %bb.36:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp494:
	movq	%rbx, %rdi
	callq	mpfr_get_prec
.Ltmp495:
# %bb.37:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp496:
	movq	%rax, %r12
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp497:
# %bb.38:                               #   in Loop: Header=BB17_8 Depth=1
	movq	%rax, %r14
	cmpq	%rax, %r12
	je	.LBB17_42
# %bb.39:                               #   in Loop: Header=BB17_8 Depth=1
	cmpq	$0, 24(%rbx)
	je	.LBB17_41
# %bb.40:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp498:
	movq	%rbx, %rdi
	callq	mpfr_clear
.Ltmp499:
.LBB17_41:                              #   in Loop: Header=BB17_8 Depth=1
.Ltmp500:
	movq	%rbx, %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp501:
.LBB17_42:                              #   in Loop: Header=BB17_8 Depth=1
.Ltmp502:
	callq	mpfr_get_default_rounding_mode
.Ltmp503:
# %bb.43:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp504:
	movq	%rbx, %rdi
	leaq	40(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp505:
.LBB17_44:                              #   in Loop: Header=BB17_8 Depth=1
	cmpq	$0, 64(%rsp)
	je	.LBB17_46
# %bb.45:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp507:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp508:
.LBB17_46:                              #   in Loop: Header=BB17_8 Depth=1
	cmpq	$0, 160(%rsp)
	je	.LBB17_48
# %bb.47:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp510:
	leaq	136(%rsp), %rdi
	callq	mpfr_clear
.Ltmp511:
.LBB17_48:                              #   in Loop: Header=BB17_8 Depth=1
	cmpq	$0, 128(%rsp)
	je	.LBB17_50
# %bb.49:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp513:
	leaq	104(%rsp), %rdi
	callq	mpfr_clear
.Ltmp514:
.LBB17_50:                              #   in Loop: Header=BB17_8 Depth=1
	cmpb	$0, _ZZN4mpfr6randomEjE10initialize(%rip)
	je	.LBB17_52
# %bb.51:                               #   in Loop: Header=BB17_8 Depth=1
	movl	$_ZZN4mpfr6randomEjE5state, %edi
	callq	__gmp_randinit_default
	movl	$_ZZN4mpfr6randomEjE5state, %edi
	xorl	%esi, %esi
	callq	__gmp_randseed_ui
	movb	$0, _ZZN4mpfr6randomEjE10initialize(%rip)
.LBB17_52:                              #   in Loop: Header=BB17_8 Depth=1
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %r14d
	callq	mpfr_get_default_prec
	leaq	104(%rsp), %r12
	movq	%r12, %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
	movabsq	$-9223372036854775807, %rax     # imm = 0x8000000000000001
	movq	%rax, 120(%rsp)
.Ltmp516:
	movl	$_ZZN4mpfr6randomEjE5state, %esi
	movq	%r12, %rdi
	movl	%r14d, %edx
	callq	mpfr_urandom
.Ltmp517:
# %bb.53:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp522:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp523:
# %bb.54:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp524:
	movq	%r13, %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
.Ltmp525:
# %bb.55:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp526:
	callq	mpfr_get_default_rounding_mode
.Ltmp527:
# %bb.56:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp528:
	movq	%r13, %rdi
	leaq	104(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp529:
# %bb.57:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp531:
	callq	mpfr_get_default_rounding_mode
.Ltmp532:
# %bb.58:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp533:
	movl	$2, %edx
	movq	%r13, %rdi
	movq	%r13, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul_si
.Ltmp534:
# %bb.59:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp535:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp536:
# %bb.60:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp537:
	leaq	136(%rsp), %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
.Ltmp538:
# %bb.61:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp539:
	callq	mpfr_get_default_rounding_mode
.Ltmp540:
# %bb.62:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp541:
	leaq	136(%rsp), %rdi
	movq	%r13, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp542:
# %bb.63:                               #   in Loop: Header=BB17_8 Depth=1
	cmpq	$0, 32(%rsp)
	je	.LBB17_65
# %bb.64:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp544:
	movq	%r13, %rdi
	callq	mpfr_clear
.Ltmp545:
.LBB17_65:                              #   in Loop: Header=BB17_8 Depth=1
.Ltmp547:
	leaq	136(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp548:
# %bb.66:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp549:
	movq	%r13, %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
.Ltmp550:
# %bb.67:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp551:
	callq	mpfr_get_default_rounding_mode
.Ltmp552:
# %bb.68:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp553:
	movq	%r13, %rdi
	leaq	136(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp554:
# %bb.69:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp556:
	callq	mpfr_get_default_rounding_mode
.Ltmp557:
# %bb.70:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp558:
	movl	$1, %edx
	movq	%r13, %rdi
	movq	%r13, %rsi
	movl	%eax, %ecx
	callq	mpfr_sub_si
.Ltmp559:
# %bb.71:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp560:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp561:
# %bb.72:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp562:
	leaq	40(%rsp), %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
.Ltmp563:
# %bb.73:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp564:
	callq	mpfr_get_default_rounding_mode
.Ltmp565:
# %bb.74:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp566:
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp567:
# %bb.75:                               #   in Loop: Header=BB17_8 Depth=1
	cmpq	$0, 32(%rsp)
	je	.LBB17_77
# %bb.76:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp569:
	movq	%r13, %rdi
	callq	mpfr_clear
.Ltmp570:
.LBB17_77:                              #   in Loop: Header=BB17_8 Depth=1
	leaq	40(%rsp), %rax
	cmpq	%rax, %r15
	je	.LBB17_86
# %bb.78:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp572:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp573:
# %bb.79:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp574:
	movq	%rax, %r12
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp575:
# %bb.80:                               #   in Loop: Header=BB17_8 Depth=1
	movq	%rax, %r14
	cmpq	%rax, %r12
	je	.LBB17_84
# %bb.81:                               #   in Loop: Header=BB17_8 Depth=1
	cmpq	$0, 24(%r15)
	je	.LBB17_83
# %bb.82:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp576:
	movq	%r15, %rdi
	callq	mpfr_clear
.Ltmp577:
.LBB17_83:                              #   in Loop: Header=BB17_8 Depth=1
.Ltmp578:
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp579:
.LBB17_84:                              #   in Loop: Header=BB17_8 Depth=1
.Ltmp580:
	callq	mpfr_get_default_rounding_mode
.Ltmp581:
# %bb.85:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp582:
	movq	%r15, %rdi
	leaq	40(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp583:
.LBB17_86:                              #   in Loop: Header=BB17_8 Depth=1
	cmpq	$0, 64(%rsp)
	je	.LBB17_88
# %bb.87:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp585:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp586:
.LBB17_88:                              #   in Loop: Header=BB17_8 Depth=1
	cmpq	$0, 160(%rsp)
	je	.LBB17_90
# %bb.89:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp588:
	leaq	136(%rsp), %rdi
	callq	mpfr_clear
.Ltmp589:
.LBB17_90:                              #   in Loop: Header=BB17_8 Depth=1
	cmpq	$0, 128(%rsp)
	je	.LBB17_92
# %bb.91:                               #   in Loop: Header=BB17_8 Depth=1
.Ltmp591:
	leaq	104(%rsp), %rdi
	callq	mpfr_clear
.Ltmp592:
.LBB17_92:                              #   in Loop: Header=BB17_8 Depth=1
	cmpq	%rbp, %r15
	je	.LBB17_7
# %bb.93:                               #   in Loop: Header=BB17_8 Depth=1
	movq	%rbp, %rdi
	callq	mpfr_get_prec
	movq	%rax, %r12
	movq	%r15, %rdi
	callq	mpfr_get_prec
	cmpq	%rax, %r12
	je	.LBB17_6
# %bb.94:                               #   in Loop: Header=BB17_8 Depth=1
	movq	%rax, %r14
	cmpq	$0, 24(%rbp)
	je	.LBB17_5
# %bb.95:                               #   in Loop: Header=BB17_8 Depth=1
	movq	%rbp, %rdi
	callq	mpfr_clear
	jmp	.LBB17_5
.LBB17_96:
	movq	80(%rsp), %rbx                  # 8-byte Reload
	leaq	256(%rbx), %r12
	testl	%r14d, %r14d
	jle	.LBB17_123
# %bb.97:
	xorl	%eax, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	movq	168(%rsp), %r15                 # 8-byte Reload
	movq	%r12, 88(%rsp)                  # 8-byte Spill
	jmp	.LBB17_99
	.p2align	4, 0x90
.LBB17_98:                              #   in Loop: Header=BB17_99 Depth=1
	movq	72(%rsp), %rcx                  # 8-byte Reload
	incq	%rcx
	movslq	4(%rsp), %r14
	addq	$32, %r12
	addq	$32, %r15
	movq	%rcx, %rax
	movq	%rcx, 72(%rsp)                  # 8-byte Spill
	cmpq	%r14, %rcx
	jge	.LBB17_121
.LBB17_99:                              # =>This Inner Loop Header: Depth=1
	movq	176(%rsp), %r14                 # 8-byte Reload
	movq	%r14, %rdi
	callq	mpfr_get_prec
	movq	%rax, %rbx
	movq	%r15, %rdi
	callq	mpfr_get_prec
	cmpq	%rax, %rbx
	cmovleq	%rax, %rbx
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	40(%rsp), %r13
	movq	%r13, %rdi
	movq	%rbx, %rsi
	callq	mpfr_init2
	movq	%r13, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp594:
	callq	mpfr_get_default_rounding_mode
.Ltmp595:
# %bb.100:                              #   in Loop: Header=BB17_99 Depth=1
.Ltmp596:
	movq	%r13, %rdi
	movq	%r14, %rsi
	movq	%r15, %rdx
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp597:
# %bb.101:                              #   in Loop: Header=BB17_99 Depth=1
.Ltmp599:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp600:
# %bb.102:                              #   in Loop: Header=BB17_99 Depth=1
.Ltmp601:
	movq	%rax, %r14
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp602:
# %bb.103:                              #   in Loop: Header=BB17_99 Depth=1
.Ltmp603:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp604:
# %bb.104:                              #   in Loop: Header=BB17_99 Depth=1
	movl	%eax, %ebx
	cmpq	%rbp, %r14
	cmovgq	%r14, %rbp
.Ltmp605:
	leaq	8(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp606:
# %bb.105:                              #   in Loop: Header=BB17_99 Depth=1
.Ltmp607:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebx, %edx
	callq	mpfr_set_si
.Ltmp608:
# %bb.106:                              #   in Loop: Header=BB17_99 Depth=1
.Ltmp610:
	callq	mpfr_get_default_rounding_mode
.Ltmp611:
	leaq	8(%rsp), %rbx
# %bb.107:                              #   in Loop: Header=BB17_99 Depth=1
.Ltmp612:
	movq	%rbx, %rdi
	leaq	40(%rsp), %rsi
	movq	%r12, %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp613:
# %bb.108:                              #   in Loop: Header=BB17_99 Depth=1
	cmpq	%rbx, %r12
	je	.LBB17_117
# %bb.109:                              #   in Loop: Header=BB17_99 Depth=1
.Ltmp615:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp616:
# %bb.110:                              #   in Loop: Header=BB17_99 Depth=1
.Ltmp617:
	movq	%rax, %rbx
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp618:
# %bb.111:                              #   in Loop: Header=BB17_99 Depth=1
	movq	%rax, %r14
	cmpq	%rax, %rbx
	je	.LBB17_115
# %bb.112:                              #   in Loop: Header=BB17_99 Depth=1
	cmpq	$0, 24(%r12)
	je	.LBB17_114
# %bb.113:                              #   in Loop: Header=BB17_99 Depth=1
.Ltmp619:
	movq	%r12, %rdi
	callq	mpfr_clear
.Ltmp620:
.LBB17_114:                             #   in Loop: Header=BB17_99 Depth=1
.Ltmp621:
	movq	%r12, %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp622:
.LBB17_115:                             #   in Loop: Header=BB17_99 Depth=1
.Ltmp623:
	callq	mpfr_get_default_rounding_mode
.Ltmp624:
# %bb.116:                              #   in Loop: Header=BB17_99 Depth=1
.Ltmp625:
	movq	%r12, %rdi
	leaq	8(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp626:
.LBB17_117:                             #   in Loop: Header=BB17_99 Depth=1
	cmpq	$0, 32(%rsp)
	je	.LBB17_119
# %bb.118:                              #   in Loop: Header=BB17_99 Depth=1
.Ltmp628:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp629:
.LBB17_119:                             #   in Loop: Header=BB17_99 Depth=1
	cmpq	$0, 64(%rsp)
	je	.LBB17_98
# %bb.120:                              #   in Loop: Header=BB17_99 Depth=1
.Ltmp631:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp632:
	jmp	.LBB17_98
.LBB17_121:
	movq	80(%rsp), %rbx                  # 8-byte Reload
	movq	88(%rsp), %r12                  # 8-byte Reload
	jmp	.LBB17_123
.LBB17_122:
	leaq	256(%rbx), %r12
.LBB17_123:
	movq	%r12, 88(%rsp)                  # 8-byte Spill
	vcvtsi2sd	%r14d, %xmm0, %xmm0
	callq	sqrt
	vcvttsd2si	%xmm0, %ecx
	movl	%ecx, 40(%rsp)
	movl	4(%rsp), %eax
	testl	%ecx, %ecx
	jle	.LBB17_132
# %bb.124:
	xorl	%r13d, %r13d
	movq	96(%rsp), %r14                  # 8-byte Reload
	jmp	.LBB17_128
	.p2align	4, 0x90
.LBB17_125:                             #   in Loop: Header=BB17_128 Depth=1
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	mpfr_init2
.LBB17_126:                             #   in Loop: Header=BB17_128 Depth=1
	callq	mpfr_get_default_rounding_mode
	movq	%r15, %rdi
	movq	%r14, %rsi
	movl	%eax, %edx
	callq	mpfr_set
	movl	40(%rsp), %ecx
	movl	4(%rsp), %eax
.LBB17_127:                             #   in Loop: Header=BB17_128 Depth=1
	incq	%r13
	movslq	%ecx, %rdx
	addq	$32, %r14
	cmpq	%rdx, %r13
	movq	80(%rsp), %rbx                  # 8-byte Reload
	jge	.LBB17_132
.LBB17_128:                             # =>This Inner Loop Header: Depth=1
	leal	(%rax,%r13), %edx
	addl	$8, %edx
	movslq	%edx, %rbp
	shlq	$5, %rbp
	leaq	(%rbx,%rbp), %r15
	cmpq	%r15, %r14
	je	.LBB17_127
# %bb.129:                              #   in Loop: Header=BB17_128 Depth=1
	movq	%r15, %rdi
	callq	mpfr_get_prec
	movq	%rax, %r12
	movq	%r14, %rdi
	callq	mpfr_get_prec
	cmpq	%rax, %r12
	je	.LBB17_126
# %bb.130:                              #   in Loop: Header=BB17_128 Depth=1
	movq	%rax, %rbx
	movq	80(%rsp), %rax                  # 8-byte Reload
	cmpq	$0, 24(%rax,%rbp)
	je	.LBB17_125
# %bb.131:                              #   in Loop: Header=BB17_128 Depth=1
	movq	%r15, %rdi
	callq	mpfr_clear
	jmp	.LBB17_125
.LBB17_132:
	cltq
	shlq	$5, %rax
	leaq	(%rbx,%rax), %r9
	addq	$256, %r9                       # imm = 0x100
	leaq	40(%rsp), %rdi
	movq	%rdi, %rsi
	movq	176(%rsp), %rdx                 # 8-byte Reload
	movq	168(%rsp), %r15                 # 8-byte Reload
	movq	%r15, %rcx
	movq	96(%rsp), %r12                  # 8-byte Reload
	movq	%r12, %r8
	callq	_Z4gemvIN4mpfr6mprealEEvRKiS3_RKT_PS5_S7_PS4_
	leaq	8(%rsp), %r14
	leaq	4(%rsp), %rsi
	movq	%r14, %rdi
	movq	%r15, %rdx
	movq	%r12, %rcx
	callq	_Z3dotIN4mpfr6mprealEET_RKiPKS2_S6_
	cmpq	%rbx, %r14
	je	.LBB17_134
# %bb.133:
.Ltmp634:
	leaq	8(%rsp), %rsi
	movq	%rbx, %rdi
	callq	mpfr_swap
.Ltmp635:
.LBB17_134:
	cmpq	$0, 32(%rsp)
	je	.LBB17_136
# %bb.135:
.Ltmp637:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp638:
.LBB17_136:
	leaq	4(%rsp), %rsi
	movq	%r14, %rdi
	movq	%r15, %rdx
	callq	_Z4nrm2IN4mpfr6mprealEET_RKiPKS2_
	leaq	32(%rbx), %rdi
	cmpq	%r14, %rdi
	je	.LBB17_138
# %bb.137:
.Ltmp640:
	leaq	8(%rsp), %rsi
	callq	mpfr_swap
.Ltmp641:
.LBB17_138:
	cmpq	$0, 32(%rsp)
	je	.LBB17_140
# %bb.139:
.Ltmp643:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp644:
.LBB17_140:
	leaq	4(%rsp), %rsi
	movq	%r14, %rdi
	movq	%r15, %rdx
	callq	_Z4asumIN4mpfr6mprealEET_RKiPKS2_
	leaq	64(%rbx), %rdi
	cmpq	%r14, %rdi
	je	.LBB17_142
# %bb.141:
.Ltmp646:
	leaq	8(%rsp), %rsi
	callq	mpfr_swap
.Ltmp647:
.LBB17_142:
	cmpq	$0, 32(%rsp)
	movq	88(%rsp), %r15                  # 8-byte Reload
	je	.LBB17_144
# %bb.143:
.Ltmp649:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp650:
.LBB17_144:
	leaq	4(%rsp), %rsi
	movq	%r14, %rdi
	movq	%r15, %rdx
	callq	_Z4nrm2IN4mpfr6mprealEET_RKiPKS2_
	leaq	96(%rbx), %rdi
	cmpq	%r14, %rdi
	je	.LBB17_146
# %bb.145:
.Ltmp652:
	leaq	8(%rsp), %rsi
	callq	mpfr_swap
.Ltmp653:
.LBB17_146:
	cmpq	$0, 32(%rsp)
	je	.LBB17_148
# %bb.147:
.Ltmp655:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp656:
.LBB17_148:
	leaq	256(%rbx), %rax
	movslq	4(%rsp), %rdx
	shlq	$5, %rdx
	addq	%rax, %rdx
	leaq	40(%rsp), %rsi
	movq	%r14, %rdi
	callq	_Z4nrm2IN4mpfr6mprealEET_RKiPKS2_
	subq	$-128, %rbx
	cmpq	%r14, %rbx
	je	.LBB17_150
# %bb.149:
.Ltmp658:
	leaq	8(%rsp), %rsi
	movq	%rbx, %rdi
	callq	mpfr_swap
.Ltmp659:
.LBB17_150:
	cmpq	$0, 32(%rsp)
	je	.LBB17_152
# %bb.151:
.Ltmp661:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp662:
.LBB17_152:
	addq	$216, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB17_153:
	.cfi_def_cfa_offset 272
.Ltmp663:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_154:
.Ltmp660:
	jmp	.LBB17_163
.LBB17_155:
.Ltmp657:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_156:
.Ltmp654:
	jmp	.LBB17_163
.LBB17_157:
.Ltmp651:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_158:
.Ltmp648:
	jmp	.LBB17_163
.LBB17_159:
.Ltmp645:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_160:
.Ltmp642:
	jmp	.LBB17_163
.LBB17_161:
.Ltmp639:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_162:
.Ltmp636:
.LBB17_163:
	movq	%rax, %rbx
	leaq	8(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB17_164:
.Ltmp437:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_165:
.Ltmp633:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_166:
.Ltmp630:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_167:
.Ltmp593:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_168:
.Ltmp590:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_169:
.Ltmp587:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_170:
.Ltmp571:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_171:
.Ltmp546:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_172:
.Ltmp515:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_173:
.Ltmp512:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_174:
.Ltmp509:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_175:
.Ltmp493:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_176:
.Ltmp468:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_177:
.Ltmp518:
	movq	%rax, %rbx
	cmpq	$0, 128(%rsp)
	je	.LBB17_182
# %bb.178:
.Ltmp519:
	leaq	104(%rsp), %rdi
	callq	mpfr_clear
.Ltmp520:
	jmp	.LBB17_182
.LBB17_179:
.Ltmp521:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_180:
.Ltmp440:
	movq	%rax, %rbx
	cmpq	$0, 128(%rsp)
	je	.LBB17_182
# %bb.181:
.Ltmp441:
	leaq	104(%rsp), %rdi
	callq	mpfr_clear
.Ltmp442:
.LBB17_182:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB17_183:
.Ltmp443:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB17_184:
.Ltmp614:
	jmp	.LBB17_187
.LBB17_185:
.Ltmp598:
	jmp	.LBB17_192
.LBB17_186:
.Ltmp627:
.LBB17_187:
	movq	%rax, %rbx
	leaq	8(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB17_193
.LBB17_188:
.Ltmp584:
	jmp	.LBB17_190
.LBB17_189:
.Ltmp506:
.LBB17_190:
	movq	%rax, %rbx
	leaq	40(%rsp), %rdi
	jmp	.LBB17_202
.LBB17_191:
.Ltmp609:
.LBB17_192:
	movq	%rax, %rbx
.LBB17_193:
	leaq	40(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB17_194:
.Ltmp555:
	movq	%rax, %rbx
	jmp	.LBB17_203
.LBB17_195:
.Ltmp530:
	movq	%rax, %rbx
	jmp	.LBB17_207
.LBB17_196:
.Ltmp477:
	movq	%rax, %rbx
	jmp	.LBB17_203
.LBB17_197:
.Ltmp452:
	movq	%rax, %rbx
	jmp	.LBB17_207
.LBB17_198:
.Ltmp568:
	jmp	.LBB17_201
.LBB17_199:
.Ltmp543:
	jmp	.LBB17_205
.LBB17_200:
.Ltmp490:
.LBB17_201:
	movq	%rax, %rbx
	leaq	8(%rsp), %rdi
.LBB17_202:
	callq	_ZN4mpfr6mprealD2Ev
.LBB17_203:
	leaq	136(%rsp), %rdi
	jmp	.LBB17_206
.LBB17_204:
.Ltmp465:
.LBB17_205:
	movq	%rax, %rbx
	leaq	8(%rsp), %rdi
.LBB17_206:
	callq	_ZN4mpfr6mprealD2Ev
.LBB17_207:
	leaq	104(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end17:
	.size	_Z4initiRKN4mpfr6mprealEPS0_S3_S3_, .Lfunc_end17-_Z4initiRKN4mpfr6mprealEPS0_S3_S3_
	.cfi_endproc
	.section	.gcc_except_table,"a",@progbits
	.p2align	2, 0x0
GCC_except_table17:
.Lexception10:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase7-.Lttbaseref7
.Lttbaseref7:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end10-.Lcst_begin10
.Lcst_begin10:
	.uleb128 .Lfunc_begin10-.Lfunc_begin10  # >> Call Site 1 <<
	.uleb128 .Ltmp435-.Lfunc_begin10        #   Call between .Lfunc_begin10 and .Ltmp435
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp435-.Lfunc_begin10        # >> Call Site 2 <<
	.uleb128 .Ltmp436-.Ltmp435              #   Call between .Ltmp435 and .Ltmp436
	.uleb128 .Ltmp437-.Lfunc_begin10        #     jumps to .Ltmp437
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp436-.Lfunc_begin10        # >> Call Site 3 <<
	.uleb128 .Ltmp438-.Ltmp436              #   Call between .Ltmp436 and .Ltmp438
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp438-.Lfunc_begin10        # >> Call Site 4 <<
	.uleb128 .Ltmp439-.Ltmp438              #   Call between .Ltmp438 and .Ltmp439
	.uleb128 .Ltmp440-.Lfunc_begin10        #     jumps to .Ltmp440
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp444-.Lfunc_begin10        # >> Call Site 5 <<
	.uleb128 .Ltmp451-.Ltmp444              #   Call between .Ltmp444 and .Ltmp451
	.uleb128 .Ltmp452-.Lfunc_begin10        #     jumps to .Ltmp452
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp453-.Lfunc_begin10        # >> Call Site 6 <<
	.uleb128 .Ltmp464-.Ltmp453              #   Call between .Ltmp453 and .Ltmp464
	.uleb128 .Ltmp465-.Lfunc_begin10        #     jumps to .Ltmp465
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp466-.Lfunc_begin10        # >> Call Site 7 <<
	.uleb128 .Ltmp467-.Ltmp466              #   Call between .Ltmp466 and .Ltmp467
	.uleb128 .Ltmp468-.Lfunc_begin10        #     jumps to .Ltmp468
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp469-.Lfunc_begin10        # >> Call Site 8 <<
	.uleb128 .Ltmp476-.Ltmp469              #   Call between .Ltmp469 and .Ltmp476
	.uleb128 .Ltmp477-.Lfunc_begin10        #     jumps to .Ltmp477
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp478-.Lfunc_begin10        # >> Call Site 9 <<
	.uleb128 .Ltmp489-.Ltmp478              #   Call between .Ltmp478 and .Ltmp489
	.uleb128 .Ltmp490-.Lfunc_begin10        #     jumps to .Ltmp490
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp491-.Lfunc_begin10        # >> Call Site 10 <<
	.uleb128 .Ltmp492-.Ltmp491              #   Call between .Ltmp491 and .Ltmp492
	.uleb128 .Ltmp493-.Lfunc_begin10        #     jumps to .Ltmp493
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp494-.Lfunc_begin10        # >> Call Site 11 <<
	.uleb128 .Ltmp505-.Ltmp494              #   Call between .Ltmp494 and .Ltmp505
	.uleb128 .Ltmp506-.Lfunc_begin10        #     jumps to .Ltmp506
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp507-.Lfunc_begin10        # >> Call Site 12 <<
	.uleb128 .Ltmp508-.Ltmp507              #   Call between .Ltmp507 and .Ltmp508
	.uleb128 .Ltmp509-.Lfunc_begin10        #     jumps to .Ltmp509
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp510-.Lfunc_begin10        # >> Call Site 13 <<
	.uleb128 .Ltmp511-.Ltmp510              #   Call between .Ltmp510 and .Ltmp511
	.uleb128 .Ltmp512-.Lfunc_begin10        #     jumps to .Ltmp512
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp513-.Lfunc_begin10        # >> Call Site 14 <<
	.uleb128 .Ltmp514-.Ltmp513              #   Call between .Ltmp513 and .Ltmp514
	.uleb128 .Ltmp515-.Lfunc_begin10        #     jumps to .Ltmp515
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp514-.Lfunc_begin10        # >> Call Site 15 <<
	.uleb128 .Ltmp516-.Ltmp514              #   Call between .Ltmp514 and .Ltmp516
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp516-.Lfunc_begin10        # >> Call Site 16 <<
	.uleb128 .Ltmp517-.Ltmp516              #   Call between .Ltmp516 and .Ltmp517
	.uleb128 .Ltmp518-.Lfunc_begin10        #     jumps to .Ltmp518
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp522-.Lfunc_begin10        # >> Call Site 17 <<
	.uleb128 .Ltmp529-.Ltmp522              #   Call between .Ltmp522 and .Ltmp529
	.uleb128 .Ltmp530-.Lfunc_begin10        #     jumps to .Ltmp530
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp531-.Lfunc_begin10        # >> Call Site 18 <<
	.uleb128 .Ltmp542-.Ltmp531              #   Call between .Ltmp531 and .Ltmp542
	.uleb128 .Ltmp543-.Lfunc_begin10        #     jumps to .Ltmp543
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp544-.Lfunc_begin10        # >> Call Site 19 <<
	.uleb128 .Ltmp545-.Ltmp544              #   Call between .Ltmp544 and .Ltmp545
	.uleb128 .Ltmp546-.Lfunc_begin10        #     jumps to .Ltmp546
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp547-.Lfunc_begin10        # >> Call Site 20 <<
	.uleb128 .Ltmp554-.Ltmp547              #   Call between .Ltmp547 and .Ltmp554
	.uleb128 .Ltmp555-.Lfunc_begin10        #     jumps to .Ltmp555
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp556-.Lfunc_begin10        # >> Call Site 21 <<
	.uleb128 .Ltmp567-.Ltmp556              #   Call between .Ltmp556 and .Ltmp567
	.uleb128 .Ltmp568-.Lfunc_begin10        #     jumps to .Ltmp568
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp569-.Lfunc_begin10        # >> Call Site 22 <<
	.uleb128 .Ltmp570-.Ltmp569              #   Call between .Ltmp569 and .Ltmp570
	.uleb128 .Ltmp571-.Lfunc_begin10        #     jumps to .Ltmp571
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp572-.Lfunc_begin10        # >> Call Site 23 <<
	.uleb128 .Ltmp583-.Ltmp572              #   Call between .Ltmp572 and .Ltmp583
	.uleb128 .Ltmp584-.Lfunc_begin10        #     jumps to .Ltmp584
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp585-.Lfunc_begin10        # >> Call Site 24 <<
	.uleb128 .Ltmp586-.Ltmp585              #   Call between .Ltmp585 and .Ltmp586
	.uleb128 .Ltmp587-.Lfunc_begin10        #     jumps to .Ltmp587
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp588-.Lfunc_begin10        # >> Call Site 25 <<
	.uleb128 .Ltmp589-.Ltmp588              #   Call between .Ltmp588 and .Ltmp589
	.uleb128 .Ltmp590-.Lfunc_begin10        #     jumps to .Ltmp590
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp591-.Lfunc_begin10        # >> Call Site 26 <<
	.uleb128 .Ltmp592-.Ltmp591              #   Call between .Ltmp591 and .Ltmp592
	.uleb128 .Ltmp593-.Lfunc_begin10        #     jumps to .Ltmp593
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp592-.Lfunc_begin10        # >> Call Site 27 <<
	.uleb128 .Ltmp594-.Ltmp592              #   Call between .Ltmp592 and .Ltmp594
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp594-.Lfunc_begin10        # >> Call Site 28 <<
	.uleb128 .Ltmp597-.Ltmp594              #   Call between .Ltmp594 and .Ltmp597
	.uleb128 .Ltmp598-.Lfunc_begin10        #     jumps to .Ltmp598
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp599-.Lfunc_begin10        # >> Call Site 29 <<
	.uleb128 .Ltmp608-.Ltmp599              #   Call between .Ltmp599 and .Ltmp608
	.uleb128 .Ltmp609-.Lfunc_begin10        #     jumps to .Ltmp609
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp610-.Lfunc_begin10        # >> Call Site 30 <<
	.uleb128 .Ltmp613-.Ltmp610              #   Call between .Ltmp610 and .Ltmp613
	.uleb128 .Ltmp614-.Lfunc_begin10        #     jumps to .Ltmp614
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp615-.Lfunc_begin10        # >> Call Site 31 <<
	.uleb128 .Ltmp626-.Ltmp615              #   Call between .Ltmp615 and .Ltmp626
	.uleb128 .Ltmp627-.Lfunc_begin10        #     jumps to .Ltmp627
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp628-.Lfunc_begin10        # >> Call Site 32 <<
	.uleb128 .Ltmp629-.Ltmp628              #   Call between .Ltmp628 and .Ltmp629
	.uleb128 .Ltmp630-.Lfunc_begin10        #     jumps to .Ltmp630
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp631-.Lfunc_begin10        # >> Call Site 33 <<
	.uleb128 .Ltmp632-.Ltmp631              #   Call between .Ltmp631 and .Ltmp632
	.uleb128 .Ltmp633-.Lfunc_begin10        #     jumps to .Ltmp633
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp632-.Lfunc_begin10        # >> Call Site 34 <<
	.uleb128 .Ltmp634-.Ltmp632              #   Call between .Ltmp632 and .Ltmp634
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp634-.Lfunc_begin10        # >> Call Site 35 <<
	.uleb128 .Ltmp635-.Ltmp634              #   Call between .Ltmp634 and .Ltmp635
	.uleb128 .Ltmp636-.Lfunc_begin10        #     jumps to .Ltmp636
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp637-.Lfunc_begin10        # >> Call Site 36 <<
	.uleb128 .Ltmp638-.Ltmp637              #   Call between .Ltmp637 and .Ltmp638
	.uleb128 .Ltmp639-.Lfunc_begin10        #     jumps to .Ltmp639
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp638-.Lfunc_begin10        # >> Call Site 37 <<
	.uleb128 .Ltmp640-.Ltmp638              #   Call between .Ltmp638 and .Ltmp640
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp640-.Lfunc_begin10        # >> Call Site 38 <<
	.uleb128 .Ltmp641-.Ltmp640              #   Call between .Ltmp640 and .Ltmp641
	.uleb128 .Ltmp642-.Lfunc_begin10        #     jumps to .Ltmp642
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp643-.Lfunc_begin10        # >> Call Site 39 <<
	.uleb128 .Ltmp644-.Ltmp643              #   Call between .Ltmp643 and .Ltmp644
	.uleb128 .Ltmp645-.Lfunc_begin10        #     jumps to .Ltmp645
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp644-.Lfunc_begin10        # >> Call Site 40 <<
	.uleb128 .Ltmp646-.Ltmp644              #   Call between .Ltmp644 and .Ltmp646
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp646-.Lfunc_begin10        # >> Call Site 41 <<
	.uleb128 .Ltmp647-.Ltmp646              #   Call between .Ltmp646 and .Ltmp647
	.uleb128 .Ltmp648-.Lfunc_begin10        #     jumps to .Ltmp648
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp649-.Lfunc_begin10        # >> Call Site 42 <<
	.uleb128 .Ltmp650-.Ltmp649              #   Call between .Ltmp649 and .Ltmp650
	.uleb128 .Ltmp651-.Lfunc_begin10        #     jumps to .Ltmp651
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp650-.Lfunc_begin10        # >> Call Site 43 <<
	.uleb128 .Ltmp652-.Ltmp650              #   Call between .Ltmp650 and .Ltmp652
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp652-.Lfunc_begin10        # >> Call Site 44 <<
	.uleb128 .Ltmp653-.Ltmp652              #   Call between .Ltmp652 and .Ltmp653
	.uleb128 .Ltmp654-.Lfunc_begin10        #     jumps to .Ltmp654
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp655-.Lfunc_begin10        # >> Call Site 45 <<
	.uleb128 .Ltmp656-.Ltmp655              #   Call between .Ltmp655 and .Ltmp656
	.uleb128 .Ltmp657-.Lfunc_begin10        #     jumps to .Ltmp657
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp656-.Lfunc_begin10        # >> Call Site 46 <<
	.uleb128 .Ltmp658-.Ltmp656              #   Call between .Ltmp656 and .Ltmp658
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp658-.Lfunc_begin10        # >> Call Site 47 <<
	.uleb128 .Ltmp659-.Ltmp658              #   Call between .Ltmp658 and .Ltmp659
	.uleb128 .Ltmp660-.Lfunc_begin10        #     jumps to .Ltmp660
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp661-.Lfunc_begin10        # >> Call Site 48 <<
	.uleb128 .Ltmp662-.Ltmp661              #   Call between .Ltmp661 and .Ltmp662
	.uleb128 .Ltmp663-.Lfunc_begin10        #     jumps to .Ltmp663
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp662-.Lfunc_begin10        # >> Call Site 49 <<
	.uleb128 .Ltmp519-.Ltmp662              #   Call between .Ltmp662 and .Ltmp519
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp519-.Lfunc_begin10        # >> Call Site 50 <<
	.uleb128 .Ltmp520-.Ltmp519              #   Call between .Ltmp519 and .Ltmp520
	.uleb128 .Ltmp521-.Lfunc_begin10        #     jumps to .Ltmp521
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp441-.Lfunc_begin10        # >> Call Site 51 <<
	.uleb128 .Ltmp442-.Ltmp441              #   Call between .Ltmp441 and .Ltmp442
	.uleb128 .Ltmp443-.Lfunc_begin10        #     jumps to .Ltmp443
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp442-.Lfunc_begin10        # >> Call Site 52 <<
	.uleb128 .Lfunc_end17-.Ltmp442          #   Call between .Ltmp442 and .Lfunc_end17
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end10:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase7:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._ZN4mpfr6randomEj,"axG",@progbits,_ZN4mpfr6randomEj,comdat
	.weak	_ZN4mpfr6randomEj               # -- Begin function _ZN4mpfr6randomEj
	.p2align	4, 0x90
	.type	_ZN4mpfr6randomEj,@function
_ZN4mpfr6randomEj:                      # 
.Lfunc_begin11:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception11
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r14
	.cfi_def_cfa_offset 24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset %rbx, -32
	.cfi_offset %r14, -24
	.cfi_offset %rbp, -16
	movl	%esi, %ebp
	movq	%rdi, %rbx
	cmpb	$0, _ZZN4mpfr6randomEjE10initialize(%rip)
	je	.LBB18_2
# %bb.1:
	movl	$_ZZN4mpfr6randomEjE5state, %edi
	callq	__gmp_randinit_default
	movl	$_ZZN4mpfr6randomEjE5state, %edi
	xorl	%esi, %esi
	callq	__gmp_randseed_ui
	movb	$0, _ZZN4mpfr6randomEjE10initialize(%rip)
.LBB18_2:
	testl	%ebp, %ebp
	je	.LBB18_4
# %bb.3:
	movl	%ebp, %esi
	movl	$_ZZN4mpfr6randomEjE5state, %edi
	callq	__gmp_randseed_ui
.LBB18_4:
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	callq	mpfr_get_default_prec
	movq	%rbx, %rdi
	movq	%rax, %rsi
	callq	mpfr_init2
	movabsq	$-9223372036854775807, %rax     # imm = 0x8000000000000001
	movq	%rax, 16(%rbx)
.Ltmp664:
	movl	$_ZZN4mpfr6randomEjE5state, %esi
	movq	%rbx, %rdi
	movl	%ebp, %edx
	callq	mpfr_urandom
.Ltmp665:
# %bb.5:
	movq	%rbx, %rax
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB18_6:
	.cfi_def_cfa_offset 32
.Ltmp666:
	movq	%rax, %r14
	cmpq	$0, 24(%rbx)
	je	.LBB18_8
# %bb.7:
.Ltmp667:
	movq	%rbx, %rdi
	callq	mpfr_clear
.Ltmp668:
.LBB18_8:
	movq	%r14, %rdi
	callq	_Unwind_Resume@PLT
.LBB18_9:
.Ltmp669:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.Lfunc_end18:
	.size	_ZN4mpfr6randomEj, .Lfunc_end18-_ZN4mpfr6randomEj
	.cfi_endproc
	.section	.gcc_except_table._ZN4mpfr6randomEj,"aG",@progbits,_ZN4mpfr6randomEj,comdat
	.p2align	2, 0x0
GCC_except_table18:
.Lexception11:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase8-.Lttbaseref8
.Lttbaseref8:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end11-.Lcst_begin11
.Lcst_begin11:
	.uleb128 .Lfunc_begin11-.Lfunc_begin11  # >> Call Site 1 <<
	.uleb128 .Ltmp664-.Lfunc_begin11        #   Call between .Lfunc_begin11 and .Ltmp664
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp664-.Lfunc_begin11        # >> Call Site 2 <<
	.uleb128 .Ltmp665-.Ltmp664              #   Call between .Ltmp664 and .Ltmp665
	.uleb128 .Ltmp666-.Lfunc_begin11        #     jumps to .Ltmp666
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp667-.Lfunc_begin11        # >> Call Site 3 <<
	.uleb128 .Ltmp668-.Ltmp667              #   Call between .Ltmp667 and .Ltmp668
	.uleb128 .Ltmp669-.Lfunc_begin11        #     jumps to .Ltmp669
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp668-.Lfunc_begin11        # >> Call Site 4 <<
	.uleb128 .Lfunc_end18-.Ltmp668          #   Call between .Ltmp668 and .Lfunc_end18
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end11:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase8:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._Z4gemvIN4mpfr6mprealEEvRKiS3_RKT_PS5_S7_PS4_,"axG",@progbits,_Z4gemvIN4mpfr6mprealEEvRKiS3_RKT_PS5_S7_PS4_,comdat
	.weak	_Z4gemvIN4mpfr6mprealEEvRKiS3_RKT_PS5_S7_PS4_ # -- Begin function _Z4gemvIN4mpfr6mprealEEvRKiS3_RKT_PS5_S7_PS4_
	.p2align	4, 0x90
	.type	_Z4gemvIN4mpfr6mprealEEvRKiS3_RKT_PS5_S7_PS4_,@function
_Z4gemvIN4mpfr6mprealEEvRKiS3_RKT_PS5_S7_PS4_: # 
.Lfunc_begin12:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception12
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$152, %rsp
	.cfi_def_cfa_offset 208
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r9, 80(%rsp)                   # 8-byte Spill
	movq	%r8, 104(%rsp)                  # 8-byte Spill
	movq	%rcx, 112(%rsp)                 # 8-byte Spill
	movq	%rdi, (%rsp)                    # 8-byte Spill
	movq	%rsi, 96(%rsp)                  # 8-byte Spill
	cmpl	$0, (%rsi)
	jle	.LBB19_37
# %bb.1:
	movq	%rdx, %r14
	xorl	%r15d, %r15d
	leaq	120(%rsp), %r12
	movq	%rdx, 88(%rsp)                  # 8-byte Spill
	jmp	.LBB19_3
	.p2align	4, 0x90
.LBB19_2:                               #   in Loop: Header=BB19_3 Depth=1
	movq	8(%rsp), %r15                   # 8-byte Reload
	incq	%r15
	movq	96(%rsp), %rax                  # 8-byte Reload
	movslq	(%rax), %rax
	cmpq	%rax, %r15
	movq	88(%rsp), %r14                  # 8-byte Reload
	jge	.LBB19_37
.LBB19_3:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB19_8 Depth 2
	movq	%r14, %rdi
	callq	mpfr_get_prec
	movq	%rax, %rbx
	movq	%r15, 8(%rsp)                   # 8-byte Spill
	shlq	$5, %r15
	addq	104(%rsp), %r15                 # 8-byte Folded Reload
	movq	%r15, %rdi
	callq	mpfr_get_prec
	cmpq	%rax, %rbx
	cmovleq	%rax, %rbx
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp670:
	callq	mpfr_get_default_rounding_mode
.Ltmp671:
# %bb.4:                                #   in Loop: Header=BB19_3 Depth=1
.Ltmp672:
	movq	%r12, %rdi
	movq	%r14, %rsi
	movq	%r15, %rdx
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp673:
# %bb.5:                                #   in Loop: Header=BB19_3 Depth=1
	movq	(%rsp), %rax                    # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	jle	.LBB19_35
# %bb.6:                                #   in Loop: Header=BB19_3 Depth=1
	movq	80(%rsp), %r12                  # 8-byte Reload
	xorl	%r13d, %r13d
	jmp	.LBB19_8
	.p2align	4, 0x90
.LBB19_7:                               #   in Loop: Header=BB19_8 Depth=2
	incq	%r13
	movq	(%rsp), %rax                    # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r12
	cmpq	%rax, %r13
	jge	.LBB19_35
.LBB19_8:                               #   Parent Loop BB19_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	imull	8(%rsp), %eax                   # 4-byte Folded Reload
	movslq	%eax, %r15
	addq	%r13, %r15
	shlq	$5, %r15
	addq	112(%rsp), %r15                 # 8-byte Folded Reload
.Ltmp675:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp676:
# %bb.9:                                #   in Loop: Header=BB19_8 Depth=2
.Ltmp677:
	movq	%rax, %rbp
	leaq	120(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp678:
# %bb.10:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp679:
	movq	%rax, %r14
	callq	mpfr_get_default_rounding_mode
.Ltmp680:
# %bb.11:                               #   in Loop: Header=BB19_8 Depth=2
	movl	%eax, %ebx
	cmpq	%r14, %rbp
	cmovgq	%rbp, %r14
.Ltmp681:
	leaq	16(%rsp), %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp682:
# %bb.12:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp683:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebx, %edx
	callq	mpfr_set_si
.Ltmp684:
# %bb.13:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp686:
	callq	mpfr_get_default_rounding_mode
.Ltmp687:
# %bb.14:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp688:
	leaq	16(%rsp), %rdi
	movq	%r15, %rsi
	leaq	120(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp689:
# %bb.15:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp691:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp692:
# %bb.16:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp693:
	movq	%rax, %r15
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp694:
# %bb.17:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp695:
	movq	%rax, %r14
	callq	mpfr_get_default_rounding_mode
.Ltmp696:
# %bb.18:                               #   in Loop: Header=BB19_8 Depth=2
	movl	%eax, %ebx
	cmpq	%r14, %r15
	cmovgq	%r15, %r14
.Ltmp697:
	leaq	48(%rsp), %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp698:
# %bb.19:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp699:
	leaq	48(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebx, %edx
	callq	mpfr_set_si
.Ltmp700:
# %bb.20:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp702:
	callq	mpfr_get_default_rounding_mode
.Ltmp703:
	leaq	48(%rsp), %rbx
# %bb.21:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp704:
	movq	%rbx, %rdi
	movq	%r12, %rsi
	leaq	16(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp705:
# %bb.22:                               #   in Loop: Header=BB19_8 Depth=2
	cmpq	%rbx, %r12
	je	.LBB19_31
# %bb.23:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp707:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp708:
# %bb.24:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp709:
	movq	%rax, %rbx
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp710:
# %bb.25:                               #   in Loop: Header=BB19_8 Depth=2
	movq	%rax, %r15
	cmpq	%rax, %rbx
	je	.LBB19_29
# %bb.26:                               #   in Loop: Header=BB19_8 Depth=2
	cmpq	$0, 24(%r12)
	je	.LBB19_28
# %bb.27:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp711:
	movq	%r12, %rdi
	callq	mpfr_clear
.Ltmp712:
.LBB19_28:                              #   in Loop: Header=BB19_8 Depth=2
.Ltmp713:
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp714:
.LBB19_29:                              #   in Loop: Header=BB19_8 Depth=2
.Ltmp715:
	callq	mpfr_get_default_rounding_mode
.Ltmp716:
# %bb.30:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp717:
	movq	%r12, %rdi
	leaq	48(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp718:
.LBB19_31:                              #   in Loop: Header=BB19_8 Depth=2
	cmpq	$0, 72(%rsp)
	je	.LBB19_33
# %bb.32:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp720:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp721:
.LBB19_33:                              #   in Loop: Header=BB19_8 Depth=2
	cmpq	$0, 40(%rsp)
	je	.LBB19_7
# %bb.34:                               #   in Loop: Header=BB19_8 Depth=2
.Ltmp723:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp724:
	jmp	.LBB19_7
	.p2align	4, 0x90
.LBB19_35:                              #   in Loop: Header=BB19_3 Depth=1
	cmpq	$0, 144(%rsp)
	leaq	120(%rsp), %r12
	je	.LBB19_2
# %bb.36:                               #   in Loop: Header=BB19_3 Depth=1
.Ltmp726:
	movq	%r12, %rdi
	callq	mpfr_clear
.Ltmp727:
	jmp	.LBB19_2
.LBB19_37:
	addq	$152, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB19_38:
	.cfi_def_cfa_offset 208
.Ltmp728:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB19_39:
.Ltmp674:
	jmp	.LBB19_50
.LBB19_40:
.Ltmp725:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB19_41:
.Ltmp722:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB19_42:
.Ltmp706:
	jmp	.LBB19_45
.LBB19_43:
.Ltmp690:
	jmp	.LBB19_47
.LBB19_44:
.Ltmp719:
.LBB19_45:
	movq	%rax, %rbx
	leaq	48(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB19_48
.LBB19_46:
.Ltmp701:
.LBB19_47:
	movq	%rax, %rbx
.LBB19_48:
	leaq	16(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB19_51
.LBB19_49:
.Ltmp685:
.LBB19_50:
	movq	%rax, %rbx
.LBB19_51:
	leaq	120(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end19:
	.size	_Z4gemvIN4mpfr6mprealEEvRKiS3_RKT_PS5_S7_PS4_, .Lfunc_end19-_Z4gemvIN4mpfr6mprealEEvRKiS3_RKT_PS5_S7_PS4_
	.cfi_endproc
	.section	.gcc_except_table._Z4gemvIN4mpfr6mprealEEvRKiS3_RKT_PS5_S7_PS4_,"aG",@progbits,_Z4gemvIN4mpfr6mprealEEvRKiS3_RKT_PS5_S7_PS4_,comdat
	.p2align	2, 0x0
GCC_except_table19:
.Lexception12:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase9-.Lttbaseref9
.Lttbaseref9:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end12-.Lcst_begin12
.Lcst_begin12:
	.uleb128 .Lfunc_begin12-.Lfunc_begin12  # >> Call Site 1 <<
	.uleb128 .Ltmp670-.Lfunc_begin12        #   Call between .Lfunc_begin12 and .Ltmp670
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp670-.Lfunc_begin12        # >> Call Site 2 <<
	.uleb128 .Ltmp673-.Ltmp670              #   Call between .Ltmp670 and .Ltmp673
	.uleb128 .Ltmp674-.Lfunc_begin12        #     jumps to .Ltmp674
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp675-.Lfunc_begin12        # >> Call Site 3 <<
	.uleb128 .Ltmp684-.Ltmp675              #   Call between .Ltmp675 and .Ltmp684
	.uleb128 .Ltmp685-.Lfunc_begin12        #     jumps to .Ltmp685
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp686-.Lfunc_begin12        # >> Call Site 4 <<
	.uleb128 .Ltmp689-.Ltmp686              #   Call between .Ltmp686 and .Ltmp689
	.uleb128 .Ltmp690-.Lfunc_begin12        #     jumps to .Ltmp690
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp691-.Lfunc_begin12        # >> Call Site 5 <<
	.uleb128 .Ltmp700-.Ltmp691              #   Call between .Ltmp691 and .Ltmp700
	.uleb128 .Ltmp701-.Lfunc_begin12        #     jumps to .Ltmp701
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp702-.Lfunc_begin12        # >> Call Site 6 <<
	.uleb128 .Ltmp705-.Ltmp702              #   Call between .Ltmp702 and .Ltmp705
	.uleb128 .Ltmp706-.Lfunc_begin12        #     jumps to .Ltmp706
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp707-.Lfunc_begin12        # >> Call Site 7 <<
	.uleb128 .Ltmp718-.Ltmp707              #   Call between .Ltmp707 and .Ltmp718
	.uleb128 .Ltmp719-.Lfunc_begin12        #     jumps to .Ltmp719
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp720-.Lfunc_begin12        # >> Call Site 8 <<
	.uleb128 .Ltmp721-.Ltmp720              #   Call between .Ltmp720 and .Ltmp721
	.uleb128 .Ltmp722-.Lfunc_begin12        #     jumps to .Ltmp722
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp723-.Lfunc_begin12        # >> Call Site 9 <<
	.uleb128 .Ltmp724-.Ltmp723              #   Call between .Ltmp723 and .Ltmp724
	.uleb128 .Ltmp725-.Lfunc_begin12        #     jumps to .Ltmp725
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp726-.Lfunc_begin12        # >> Call Site 10 <<
	.uleb128 .Ltmp727-.Ltmp726              #   Call between .Ltmp726 and .Ltmp727
	.uleb128 .Ltmp728-.Lfunc_begin12        #     jumps to .Ltmp728
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp727-.Lfunc_begin12        # >> Call Site 11 <<
	.uleb128 .Lfunc_end19-.Ltmp727          #   Call between .Ltmp727 and .Lfunc_end19
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end12:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase9:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._Z3dotIN4mpfr6mprealEET_RKiPKS2_S6_,"axG",@progbits,_Z3dotIN4mpfr6mprealEET_RKiPKS2_S6_,comdat
	.weak	_Z3dotIN4mpfr6mprealEET_RKiPKS2_S6_ # -- Begin function _Z3dotIN4mpfr6mprealEET_RKiPKS2_S6_
	.p2align	4, 0x90
	.type	_Z3dotIN4mpfr6mprealEET_RKiPKS2_S6_,@function
_Z3dotIN4mpfr6mprealEET_RKiPKS2_S6_:    # 
.Lfunc_begin13:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception13
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rcx, %r14
	movq	%rdx, %r15
	movq	%rsi, %r13
	movq	%rdi, %rbx
	callq	mpfr_get_default_prec
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	movq	%rbx, %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
	movq	%rbx, 8(%rsp)                   # 8-byte Spill
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	movq	%r13, 80(%rsp)                  # 8-byte Spill
	cmpl	$0, (%r13)
	jle	.LBB20_30
# %bb.1:
	xorl	%ebx, %ebx
	jmp	.LBB20_3
	.p2align	4, 0x90
.LBB20_2:                               #   in Loop: Header=BB20_3 Depth=1
	incq	%rbx
	movq	80(%rsp), %rax                  # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r15
	addq	$32, %r14
	cmpq	%rax, %rbx
	jge	.LBB20_30
.LBB20_3:                               # =>This Inner Loop Header: Depth=1
.Ltmp729:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp730:
# %bb.4:                                #   in Loop: Header=BB20_3 Depth=1
.Ltmp731:
	movq	%rax, %r12
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp732:
# %bb.5:                                #   in Loop: Header=BB20_3 Depth=1
.Ltmp733:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp734:
# %bb.6:                                #   in Loop: Header=BB20_3 Depth=1
	movl	%eax, %r13d
	cmpq	%rbp, %r12
	cmovgq	%r12, %rbp
.Ltmp735:
	leaq	16(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp736:
# %bb.7:                                #   in Loop: Header=BB20_3 Depth=1
.Ltmp737:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r13d, %edx
	callq	mpfr_set_si
.Ltmp738:
# %bb.8:                                #   in Loop: Header=BB20_3 Depth=1
.Ltmp740:
	callq	mpfr_get_default_rounding_mode
.Ltmp741:
# %bb.9:                                #   in Loop: Header=BB20_3 Depth=1
.Ltmp742:
	leaq	16(%rsp), %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp743:
# %bb.10:                               #   in Loop: Header=BB20_3 Depth=1
.Ltmp745:
	movq	8(%rsp), %rdi                   # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp746:
# %bb.11:                               #   in Loop: Header=BB20_3 Depth=1
.Ltmp747:
	movq	%rax, %r12
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp748:
# %bb.12:                               #   in Loop: Header=BB20_3 Depth=1
.Ltmp749:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp750:
# %bb.13:                               #   in Loop: Header=BB20_3 Depth=1
	movl	%eax, %r13d
	cmpq	%rbp, %r12
	cmovgq	%r12, %rbp
.Ltmp751:
	leaq	48(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp752:
# %bb.14:                               #   in Loop: Header=BB20_3 Depth=1
.Ltmp753:
	leaq	48(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r13d, %edx
	callq	mpfr_set_si
.Ltmp754:
# %bb.15:                               #   in Loop: Header=BB20_3 Depth=1
.Ltmp756:
	movq	8(%rsp), %r12                   # 8-byte Reload
	callq	mpfr_get_default_rounding_mode
.Ltmp757:
	leaq	48(%rsp), %r13
# %bb.16:                               #   in Loop: Header=BB20_3 Depth=1
.Ltmp758:
	movq	%r13, %rdi
	movq	%r12, %rsi
	leaq	16(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp759:
# %bb.17:                               #   in Loop: Header=BB20_3 Depth=1
	cmpq	%r12, %r13
	je	.LBB20_26
# %bb.18:                               #   in Loop: Header=BB20_3 Depth=1
.Ltmp761:
	movq	8(%rsp), %rdi                   # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp762:
# %bb.19:                               #   in Loop: Header=BB20_3 Depth=1
.Ltmp763:
	movq	%rax, %r13
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp764:
# %bb.20:                               #   in Loop: Header=BB20_3 Depth=1
	movq	%rax, %r12
	cmpq	%rax, %r13
	je	.LBB20_24
# %bb.21:                               #   in Loop: Header=BB20_3 Depth=1
	movq	8(%rsp), %rax                   # 8-byte Reload
	cmpq	$0, 24(%rax)
	je	.LBB20_23
# %bb.22:                               #   in Loop: Header=BB20_3 Depth=1
.Ltmp765:
	movq	8(%rsp), %rdi                   # 8-byte Reload
	callq	mpfr_clear
.Ltmp766:
.LBB20_23:                              #   in Loop: Header=BB20_3 Depth=1
.Ltmp767:
	movq	8(%rsp), %rdi                   # 8-byte Reload
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp768:
.LBB20_24:                              #   in Loop: Header=BB20_3 Depth=1
.Ltmp769:
	callq	mpfr_get_default_rounding_mode
.Ltmp770:
# %bb.25:                               #   in Loop: Header=BB20_3 Depth=1
.Ltmp771:
	movq	8(%rsp), %rdi                   # 8-byte Reload
	leaq	48(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp772:
.LBB20_26:                              #   in Loop: Header=BB20_3 Depth=1
	cmpq	$0, 72(%rsp)
	je	.LBB20_28
# %bb.27:                               #   in Loop: Header=BB20_3 Depth=1
.Ltmp774:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp775:
.LBB20_28:                              #   in Loop: Header=BB20_3 Depth=1
	cmpq	$0, 40(%rsp)
	je	.LBB20_2
# %bb.29:                               #   in Loop: Header=BB20_3 Depth=1
.Ltmp777:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp778:
	jmp	.LBB20_2
.LBB20_30:
	movq	8(%rsp), %rax                   # 8-byte Reload
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB20_31:
	.cfi_def_cfa_offset 144
.Ltmp779:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB20_32:
.Ltmp776:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB20_33:
.Ltmp760:
	jmp	.LBB20_36
.LBB20_34:
.Ltmp744:
	jmp	.LBB20_38
.LBB20_35:
.Ltmp773:
.LBB20_36:
	movq	%rax, %r14
	leaq	48(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB20_39
.LBB20_37:
.Ltmp755:
.LBB20_38:
	movq	%rax, %r14
.LBB20_39:
	leaq	16(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB20_41
.LBB20_40:
.Ltmp739:
	movq	%rax, %r14
.LBB20_41:
	movq	8(%rsp), %rdi                   # 8-byte Reload
	callq	_ZN4mpfr6mprealD2Ev
	movq	%r14, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end20:
	.size	_Z3dotIN4mpfr6mprealEET_RKiPKS2_S6_, .Lfunc_end20-_Z3dotIN4mpfr6mprealEET_RKiPKS2_S6_
	.cfi_endproc
	.section	.gcc_except_table._Z3dotIN4mpfr6mprealEET_RKiPKS2_S6_,"aG",@progbits,_Z3dotIN4mpfr6mprealEET_RKiPKS2_S6_,comdat
	.p2align	2, 0x0
GCC_except_table20:
.Lexception13:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase10-.Lttbaseref10
.Lttbaseref10:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end13-.Lcst_begin13
.Lcst_begin13:
	.uleb128 .Lfunc_begin13-.Lfunc_begin13  # >> Call Site 1 <<
	.uleb128 .Ltmp729-.Lfunc_begin13        #   Call between .Lfunc_begin13 and .Ltmp729
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp729-.Lfunc_begin13        # >> Call Site 2 <<
	.uleb128 .Ltmp738-.Ltmp729              #   Call between .Ltmp729 and .Ltmp738
	.uleb128 .Ltmp739-.Lfunc_begin13        #     jumps to .Ltmp739
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp740-.Lfunc_begin13        # >> Call Site 3 <<
	.uleb128 .Ltmp743-.Ltmp740              #   Call between .Ltmp740 and .Ltmp743
	.uleb128 .Ltmp744-.Lfunc_begin13        #     jumps to .Ltmp744
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp745-.Lfunc_begin13        # >> Call Site 4 <<
	.uleb128 .Ltmp754-.Ltmp745              #   Call between .Ltmp745 and .Ltmp754
	.uleb128 .Ltmp755-.Lfunc_begin13        #     jumps to .Ltmp755
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp756-.Lfunc_begin13        # >> Call Site 5 <<
	.uleb128 .Ltmp759-.Ltmp756              #   Call between .Ltmp756 and .Ltmp759
	.uleb128 .Ltmp760-.Lfunc_begin13        #     jumps to .Ltmp760
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp761-.Lfunc_begin13        # >> Call Site 6 <<
	.uleb128 .Ltmp772-.Ltmp761              #   Call between .Ltmp761 and .Ltmp772
	.uleb128 .Ltmp773-.Lfunc_begin13        #     jumps to .Ltmp773
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp774-.Lfunc_begin13        # >> Call Site 7 <<
	.uleb128 .Ltmp775-.Ltmp774              #   Call between .Ltmp774 and .Ltmp775
	.uleb128 .Ltmp776-.Lfunc_begin13        #     jumps to .Ltmp776
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp777-.Lfunc_begin13        # >> Call Site 8 <<
	.uleb128 .Ltmp778-.Ltmp777              #   Call between .Ltmp777 and .Ltmp778
	.uleb128 .Ltmp779-.Lfunc_begin13        #     jumps to .Ltmp779
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp778-.Lfunc_begin13        # >> Call Site 9 <<
	.uleb128 .Lfunc_end20-.Ltmp778          #   Call between .Ltmp778 and .Lfunc_end20
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end13:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase10:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._Z4nrm2IN4mpfr6mprealEET_RKiPKS2_,"axG",@progbits,_Z4nrm2IN4mpfr6mprealEET_RKiPKS2_,comdat
	.weak	_Z4nrm2IN4mpfr6mprealEET_RKiPKS2_ # -- Begin function _Z4nrm2IN4mpfr6mprealEET_RKiPKS2_
	.p2align	4, 0x90
	.type	_Z4nrm2IN4mpfr6mprealEET_RKiPKS2_,@function
_Z4nrm2IN4mpfr6mprealEET_RKiPKS2_:      # 
.Lfunc_begin14:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception14
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$120, %rsp
	.cfi_def_cfa_offset 176
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rdx, %r14
	movq	%rsi, %r12
	movq	%rdi, 72(%rsp)                  # 8-byte Spill
	callq	mpfr_get_default_prec
	movq	%rax, %rbx
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	8(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	mpfr_init2
	movq	%r15, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	movq	%r12, 80(%rsp)                  # 8-byte Spill
	cmpl	$0, (%r12)
	jle	.LBB21_29
# %bb.1:
	xorl	%r12d, %r12d
	leaq	88(%rsp), %rbp
	jmp	.LBB21_3
	.p2align	4, 0x90
.LBB21_2:                               #   in Loop: Header=BB21_3 Depth=1
	incq	%r12
	movq	80(%rsp), %rax                  # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r14
	cmpq	%rax, %r12
	jge	.LBB21_29
.LBB21_3:                               # =>This Inner Loop Header: Depth=1
.Ltmp780:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp781:
# %bb.4:                                #   in Loop: Header=BB21_3 Depth=1
.Ltmp782:
	movq	%rax, %rbx
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp783:
# %bb.5:                                #   in Loop: Header=BB21_3 Depth=1
.Ltmp784:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp785:
# %bb.6:                                #   in Loop: Header=BB21_3 Depth=1
	movl	%eax, %r15d
	cmpq	%r13, %rbx
	cmovgq	%rbx, %r13
.Ltmp786:
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp787:
# %bb.7:                                #   in Loop: Header=BB21_3 Depth=1
.Ltmp788:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp789:
# %bb.8:                                #   in Loop: Header=BB21_3 Depth=1
.Ltmp791:
	callq	mpfr_get_default_rounding_mode
.Ltmp792:
# %bb.9:                                #   in Loop: Header=BB21_3 Depth=1
.Ltmp793:
	leaq	40(%rsp), %rdi
	movq	%r14, %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp794:
# %bb.10:                               #   in Loop: Header=BB21_3 Depth=1
.Ltmp796:
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp797:
# %bb.11:                               #   in Loop: Header=BB21_3 Depth=1
.Ltmp798:
	movq	%rax, %rbx
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp799:
# %bb.12:                               #   in Loop: Header=BB21_3 Depth=1
.Ltmp800:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp801:
# %bb.13:                               #   in Loop: Header=BB21_3 Depth=1
	movl	%eax, %r15d
	cmpq	%r13, %rbx
	cmovgq	%rbx, %r13
.Ltmp802:
	movq	%rbp, %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp803:
# %bb.14:                               #   in Loop: Header=BB21_3 Depth=1
.Ltmp804:
	movq	%rbp, %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp805:
# %bb.15:                               #   in Loop: Header=BB21_3 Depth=1
.Ltmp807:
	callq	mpfr_get_default_rounding_mode
.Ltmp808:
	leaq	8(%rsp), %rbx
# %bb.16:                               #   in Loop: Header=BB21_3 Depth=1
.Ltmp809:
	movq	%rbp, %rdi
	movq	%rbx, %rsi
	leaq	40(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp810:
# %bb.17:                               #   in Loop: Header=BB21_3 Depth=1
.Ltmp812:
	movq	%rbx, %rdi
	callq	mpfr_get_prec
.Ltmp813:
# %bb.18:                               #   in Loop: Header=BB21_3 Depth=1
.Ltmp814:
	movq	%rax, %r15
	movq	%rbp, %rdi
	callq	mpfr_get_prec
.Ltmp815:
# %bb.19:                               #   in Loop: Header=BB21_3 Depth=1
	movq	%rax, %rbx
	cmpq	%rax, %r15
	je	.LBB21_23
# %bb.20:                               #   in Loop: Header=BB21_3 Depth=1
	cmpq	$0, 32(%rsp)
	je	.LBB21_22
# %bb.21:                               #   in Loop: Header=BB21_3 Depth=1
.Ltmp816:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp817:
.LBB21_22:                              #   in Loop: Header=BB21_3 Depth=1
.Ltmp818:
	leaq	8(%rsp), %rdi
	movq	%rbx, %rsi
	callq	mpfr_init2
.Ltmp819:
.LBB21_23:                              #   in Loop: Header=BB21_3 Depth=1
.Ltmp820:
	callq	mpfr_get_default_rounding_mode
.Ltmp821:
# %bb.24:                               #   in Loop: Header=BB21_3 Depth=1
.Ltmp822:
	leaq	8(%rsp), %rdi
	movq	%rbp, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp823:
# %bb.25:                               #   in Loop: Header=BB21_3 Depth=1
	cmpq	$0, 112(%rsp)
	je	.LBB21_27
# %bb.26:                               #   in Loop: Header=BB21_3 Depth=1
.Ltmp825:
	movq	%rbp, %rdi
	callq	mpfr_clear
.Ltmp826:
.LBB21_27:                              #   in Loop: Header=BB21_3 Depth=1
	cmpq	$0, 64(%rsp)
	je	.LBB21_2
# %bb.28:                               #   in Loop: Header=BB21_3 Depth=1
.Ltmp828:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp829:
	jmp	.LBB21_2
.LBB21_29:
.Ltmp831:
	callq	mpfr_get_default_rounding_mode
.Ltmp832:
	movq	72(%rsp), %r15                  # 8-byte Reload
# %bb.30:
.Ltmp833:
	movl	%eax, %ebp
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp834:
# %bb.31:
.Ltmp835:
	movq	%rax, %r14
	callq	mpfr_get_default_rounding_mode
.Ltmp836:
# %bb.32:
.Ltmp837:
	movl	%eax, %ebx
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	mpfr_init2
.Ltmp838:
# %bb.33:
.Ltmp839:
	movq	%r15, %rdi
	xorl	%esi, %esi
	movl	%ebx, %edx
	callq	mpfr_set_si
.Ltmp840:
# %bb.34:
.Ltmp842:
	leaq	8(%rsp), %rsi
	movq	%r15, %rdi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp843:
# %bb.35:
	cmpq	$0, 32(%rsp)
	je	.LBB21_37
# %bb.36:
.Ltmp845:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp846:
.LBB21_37:
	movq	%r15, %rax
	addq	$120, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB21_38:
	.cfi_def_cfa_offset 176
.Ltmp847:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB21_39:
.Ltmp844:
	movq	%rax, %r14
	movq	%r15, %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB21_52
.LBB21_40:
.Ltmp841:
	jmp	.LBB21_51
.LBB21_41:
.Ltmp830:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB21_42:
.Ltmp827:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB21_43:
.Ltmp811:
	jmp	.LBB21_46
.LBB21_44:
.Ltmp795:
	jmp	.LBB21_48
.LBB21_45:
.Ltmp824:
.LBB21_46:
	movq	%rax, %r14
	leaq	88(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB21_49
.LBB21_47:
.Ltmp806:
.LBB21_48:
	movq	%rax, %r14
.LBB21_49:
	leaq	40(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB21_52
.LBB21_50:
.Ltmp790:
.LBB21_51:
	movq	%rax, %r14
.LBB21_52:
	leaq	8(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%r14, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end21:
	.size	_Z4nrm2IN4mpfr6mprealEET_RKiPKS2_, .Lfunc_end21-_Z4nrm2IN4mpfr6mprealEET_RKiPKS2_
	.cfi_endproc
	.section	.gcc_except_table._Z4nrm2IN4mpfr6mprealEET_RKiPKS2_,"aG",@progbits,_Z4nrm2IN4mpfr6mprealEET_RKiPKS2_,comdat
	.p2align	2, 0x0
GCC_except_table21:
.Lexception14:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase11-.Lttbaseref11
.Lttbaseref11:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end14-.Lcst_begin14
.Lcst_begin14:
	.uleb128 .Lfunc_begin14-.Lfunc_begin14  # >> Call Site 1 <<
	.uleb128 .Ltmp780-.Lfunc_begin14        #   Call between .Lfunc_begin14 and .Ltmp780
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp780-.Lfunc_begin14        # >> Call Site 2 <<
	.uleb128 .Ltmp789-.Ltmp780              #   Call between .Ltmp780 and .Ltmp789
	.uleb128 .Ltmp790-.Lfunc_begin14        #     jumps to .Ltmp790
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp791-.Lfunc_begin14        # >> Call Site 3 <<
	.uleb128 .Ltmp794-.Ltmp791              #   Call between .Ltmp791 and .Ltmp794
	.uleb128 .Ltmp795-.Lfunc_begin14        #     jumps to .Ltmp795
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp796-.Lfunc_begin14        # >> Call Site 4 <<
	.uleb128 .Ltmp805-.Ltmp796              #   Call between .Ltmp796 and .Ltmp805
	.uleb128 .Ltmp806-.Lfunc_begin14        #     jumps to .Ltmp806
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp807-.Lfunc_begin14        # >> Call Site 5 <<
	.uleb128 .Ltmp810-.Ltmp807              #   Call between .Ltmp807 and .Ltmp810
	.uleb128 .Ltmp811-.Lfunc_begin14        #     jumps to .Ltmp811
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp812-.Lfunc_begin14        # >> Call Site 6 <<
	.uleb128 .Ltmp823-.Ltmp812              #   Call between .Ltmp812 and .Ltmp823
	.uleb128 .Ltmp824-.Lfunc_begin14        #     jumps to .Ltmp824
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp825-.Lfunc_begin14        # >> Call Site 7 <<
	.uleb128 .Ltmp826-.Ltmp825              #   Call between .Ltmp825 and .Ltmp826
	.uleb128 .Ltmp827-.Lfunc_begin14        #     jumps to .Ltmp827
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp828-.Lfunc_begin14        # >> Call Site 8 <<
	.uleb128 .Ltmp829-.Ltmp828              #   Call between .Ltmp828 and .Ltmp829
	.uleb128 .Ltmp830-.Lfunc_begin14        #     jumps to .Ltmp830
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp831-.Lfunc_begin14        # >> Call Site 9 <<
	.uleb128 .Ltmp840-.Ltmp831              #   Call between .Ltmp831 and .Ltmp840
	.uleb128 .Ltmp841-.Lfunc_begin14        #     jumps to .Ltmp841
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp842-.Lfunc_begin14        # >> Call Site 10 <<
	.uleb128 .Ltmp843-.Ltmp842              #   Call between .Ltmp842 and .Ltmp843
	.uleb128 .Ltmp844-.Lfunc_begin14        #     jumps to .Ltmp844
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp845-.Lfunc_begin14        # >> Call Site 11 <<
	.uleb128 .Ltmp846-.Ltmp845              #   Call between .Ltmp845 and .Ltmp846
	.uleb128 .Ltmp847-.Lfunc_begin14        #     jumps to .Ltmp847
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp846-.Lfunc_begin14        # >> Call Site 12 <<
	.uleb128 .Lfunc_end21-.Ltmp846          #   Call between .Ltmp846 and .Lfunc_end21
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end14:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase11:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._Z4asumIN4mpfr6mprealEET_RKiPKS2_,"axG",@progbits,_Z4asumIN4mpfr6mprealEET_RKiPKS2_,comdat
	.weak	_Z4asumIN4mpfr6mprealEET_RKiPKS2_ # -- Begin function _Z4asumIN4mpfr6mprealEET_RKiPKS2_
	.p2align	4, 0x90
	.type	_Z4asumIN4mpfr6mprealEET_RKiPKS2_,@function
_Z4asumIN4mpfr6mprealEET_RKiPKS2_:      # 
.Lfunc_begin15:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception15
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$72, %rsp
	.cfi_def_cfa_offset 128
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rdx, %r14
	movq	%rsi, %r12
	movq	%rdi, %rbx
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	movq	%r12, 64(%rsp)                  # 8-byte Spill
	cmpl	$0, (%r12)
	jle	.LBB22_29
# %bb.1:
	xorl	%r13d, %r13d
	jmp	.LBB22_3
	.p2align	4, 0x90
.LBB22_2:                               #   in Loop: Header=BB22_3 Depth=1
	incq	%r13
	movq	64(%rsp), %rax                  # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r14
	cmpq	%rax, %r13
	jge	.LBB22_29
.LBB22_3:                               # =>This Inner Loop Header: Depth=1
.Ltmp848:
	callq	mpfr_get_default_rounding_mode
.Ltmp849:
# %bb.4:                                #   in Loop: Header=BB22_3 Depth=1
.Ltmp850:
	movl	%eax, %ebp
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp851:
# %bb.5:                                #   in Loop: Header=BB22_3 Depth=1
.Ltmp852:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp853:
# %bb.6:                                #   in Loop: Header=BB22_3 Depth=1
.Ltmp854:
	movl	%eax, %r15d
	movq	%rsp, %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp855:
# %bb.7:                                #   in Loop: Header=BB22_3 Depth=1
.Ltmp856:
	movq	%rsp, %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp857:
# %bb.8:                                #   in Loop: Header=BB22_3 Depth=1
.Ltmp859:
	movq	%rsp, %rdi
	movq	%r14, %rsi
	movl	%ebp, %edx
	callq	mpfr_abs
.Ltmp860:
# %bb.9:                                #   in Loop: Header=BB22_3 Depth=1
.Ltmp862:
	movq	%rbx, %rdi
	callq	mpfr_get_prec
.Ltmp863:
# %bb.10:                               #   in Loop: Header=BB22_3 Depth=1
.Ltmp864:
	movq	%rax, %rbp
	movq	%rsp, %rdi
	callq	mpfr_get_prec
.Ltmp865:
# %bb.11:                               #   in Loop: Header=BB22_3 Depth=1
.Ltmp866:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp867:
# %bb.12:                               #   in Loop: Header=BB22_3 Depth=1
	movl	%eax, %r15d
	cmpq	%r12, %rbp
	cmovgq	%rbp, %r12
.Ltmp868:
	leaq	32(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp869:
# %bb.13:                               #   in Loop: Header=BB22_3 Depth=1
.Ltmp870:
	leaq	32(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp871:
# %bb.14:                               #   in Loop: Header=BB22_3 Depth=1
.Ltmp873:
	callq	mpfr_get_default_rounding_mode
.Ltmp874:
	leaq	32(%rsp), %r15
# %bb.15:                               #   in Loop: Header=BB22_3 Depth=1
.Ltmp875:
	movq	%r15, %rdi
	movq	%rbx, %rsi
	movq	%rsp, %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp876:
# %bb.16:                               #   in Loop: Header=BB22_3 Depth=1
	cmpq	%rbx, %r15
	je	.LBB22_25
# %bb.17:                               #   in Loop: Header=BB22_3 Depth=1
.Ltmp878:
	movq	%rbx, %rdi
	callq	mpfr_get_prec
.Ltmp879:
# %bb.18:                               #   in Loop: Header=BB22_3 Depth=1
.Ltmp880:
	movq	%rax, %r15
	leaq	32(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp881:
# %bb.19:                               #   in Loop: Header=BB22_3 Depth=1
	movq	%rax, %rbp
	cmpq	%rax, %r15
	je	.LBB22_23
# %bb.20:                               #   in Loop: Header=BB22_3 Depth=1
	cmpq	$0, 24(%rbx)
	je	.LBB22_22
# %bb.21:                               #   in Loop: Header=BB22_3 Depth=1
.Ltmp882:
	movq	%rbx, %rdi
	callq	mpfr_clear
.Ltmp883:
.LBB22_22:                              #   in Loop: Header=BB22_3 Depth=1
.Ltmp884:
	movq	%rbx, %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp885:
.LBB22_23:                              #   in Loop: Header=BB22_3 Depth=1
.Ltmp886:
	callq	mpfr_get_default_rounding_mode
.Ltmp887:
# %bb.24:                               #   in Loop: Header=BB22_3 Depth=1
.Ltmp888:
	movq	%rbx, %rdi
	leaq	32(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp889:
.LBB22_25:                              #   in Loop: Header=BB22_3 Depth=1
	cmpq	$0, 56(%rsp)
	je	.LBB22_27
# %bb.26:                               #   in Loop: Header=BB22_3 Depth=1
.Ltmp891:
	leaq	32(%rsp), %rdi
	callq	mpfr_clear
.Ltmp892:
.LBB22_27:                              #   in Loop: Header=BB22_3 Depth=1
	cmpq	$0, 24(%rsp)
	je	.LBB22_2
# %bb.28:                               #   in Loop: Header=BB22_3 Depth=1
.Ltmp894:
	movq	%rsp, %rdi
	callq	mpfr_clear
.Ltmp895:
	jmp	.LBB22_2
.LBB22_29:
	movq	%rbx, %rax
	addq	$72, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB22_30:
	.cfi_def_cfa_offset 128
.Ltmp896:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB22_31:
.Ltmp893:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB22_32:
.Ltmp861:
	jmp	.LBB22_37
.LBB22_33:
.Ltmp877:
	jmp	.LBB22_35
.LBB22_34:
.Ltmp890:
.LBB22_35:
	movq	%rax, %r14
	leaq	32(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB22_38
.LBB22_36:
.Ltmp872:
.LBB22_37:
	movq	%rax, %r14
.LBB22_38:
	movq	%rsp, %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB22_40
.LBB22_39:
.Ltmp858:
	movq	%rax, %r14
.LBB22_40:
	movq	%rbx, %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%r14, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end22:
	.size	_Z4asumIN4mpfr6mprealEET_RKiPKS2_, .Lfunc_end22-_Z4asumIN4mpfr6mprealEET_RKiPKS2_
	.cfi_endproc
	.section	.gcc_except_table._Z4asumIN4mpfr6mprealEET_RKiPKS2_,"aG",@progbits,_Z4asumIN4mpfr6mprealEET_RKiPKS2_,comdat
	.p2align	2, 0x0
GCC_except_table22:
.Lexception15:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase12-.Lttbaseref12
.Lttbaseref12:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end15-.Lcst_begin15
.Lcst_begin15:
	.uleb128 .Lfunc_begin15-.Lfunc_begin15  # >> Call Site 1 <<
	.uleb128 .Ltmp848-.Lfunc_begin15        #   Call between .Lfunc_begin15 and .Ltmp848
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp848-.Lfunc_begin15        # >> Call Site 2 <<
	.uleb128 .Ltmp857-.Ltmp848              #   Call between .Ltmp848 and .Ltmp857
	.uleb128 .Ltmp858-.Lfunc_begin15        #     jumps to .Ltmp858
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp859-.Lfunc_begin15        # >> Call Site 3 <<
	.uleb128 .Ltmp860-.Ltmp859              #   Call between .Ltmp859 and .Ltmp860
	.uleb128 .Ltmp861-.Lfunc_begin15        #     jumps to .Ltmp861
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp862-.Lfunc_begin15        # >> Call Site 4 <<
	.uleb128 .Ltmp871-.Ltmp862              #   Call between .Ltmp862 and .Ltmp871
	.uleb128 .Ltmp872-.Lfunc_begin15        #     jumps to .Ltmp872
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp873-.Lfunc_begin15        # >> Call Site 5 <<
	.uleb128 .Ltmp876-.Ltmp873              #   Call between .Ltmp873 and .Ltmp876
	.uleb128 .Ltmp877-.Lfunc_begin15        #     jumps to .Ltmp877
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp878-.Lfunc_begin15        # >> Call Site 6 <<
	.uleb128 .Ltmp889-.Ltmp878              #   Call between .Ltmp878 and .Ltmp889
	.uleb128 .Ltmp890-.Lfunc_begin15        #     jumps to .Ltmp890
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp891-.Lfunc_begin15        # >> Call Site 7 <<
	.uleb128 .Ltmp892-.Ltmp891              #   Call between .Ltmp891 and .Ltmp892
	.uleb128 .Ltmp893-.Lfunc_begin15        #     jumps to .Ltmp893
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp894-.Lfunc_begin15        # >> Call Site 8 <<
	.uleb128 .Ltmp895-.Ltmp894              #   Call between .Ltmp894 and .Ltmp895
	.uleb128 .Ltmp896-.Lfunc_begin15        #     jumps to .Ltmp896
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp895-.Lfunc_begin15        # >> Call Site 9 <<
	.uleb128 .Lfunc_end22-.Ltmp895          #   Call between .Ltmp895 and .Lfunc_end22
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end15:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase12:
	.p2align	2, 0x0
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2, 0x0                          # -- Begin function main
.LCPI23_0:
	.long	0x34000000                      #  1.1920929E-7
.LCPI23_1:
	.long	0x30000000                      #  4.65661287E-10
.LCPI23_2:
	.long	0x20800000                      #  2.16840434E-19
.LCPI23_3:
	.long	0x11000000                      #  1.00974196E-28
.LCPI23_15:
	.long	0x80000000                      #  -0
.LCPI23_16:
	.long	0x27800000                      #  3.55271368E-15
.LCPI23_17:
	.long	0x1b800000                      #  2.11758237E-22
.LCPI23_18:
	.long	0x0f800000                      #  1.26217745E-29
.LCPI23_19:
	.long	0x00800000                      #  1.17549435E-38
.LCPI23_22:
	.long	0x4b000000                      #  8388608
.LCPI23_23:
	.long	0x7f7fffff                      #  3.40282347E+38
.LCPI23_29:
	.long	0x3dcccccd                      #  0.100000001
.LCPI23_30:
	.long	0x3f800000                      #  1
.LCPI23_31:
	.long	0x3f000000                      #  0.5
.LCPI23_32:
	.long	0x3ea1e89b                      #  0.316227764
.LCPI23_33:
	.long	0x3e800000                      #  0.25
.LCPI23_34:
	.long	0x3f4ccccd                      #  0.800000011
.LCPI23_35:
	.long	0x7fc00000                      #  NaN
.LCPI23_37:
	.long	0xffc00000                      #  NaN
.LCPI23_38:
	.long	0x7f800000                      #  +Inf
.LCPI23_39:
	.long	0xff800000                      #  -Inf
.LCPI23_40:
	.long	0x00000000                      #  0
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3, 0x0
.LCPI23_4:
	.quad	0x3e00000000000000              #  4.6566128730773926E-10
.LCPI23_5:
	.quad	0x3c10000000000000              #  2.1684043449710089E-19
.LCPI23_6:
	.quad	0x3a20000000000000              #  1.0097419586828951E-28
.LCPI23_7:
	.quad	0x3830000000000000              #  4.70197740328915E-38
.LCPI23_8:
	.quad	0x3640000000000000              #  2.1895288505075267E-47
.LCPI23_9:
	.quad	0x3450000000000000              #  1.0195788231247695E-56
.LCPI23_10:
	.quad	0x3cb0000000000000              #  2.2204460492503131E-16
.LCPI23_12:
	.quad	0x3950000000000000              #  1.2325951644078309E-32
.LCPI23_13:
	.quad	0x3600000000000000              #  1.3684555315672042E-48
.LCPI23_14:
	.quad	0x32b0000000000000              #  1.5192908393215678E-64
.LCPI23_20:
	.quad	0x0010000000000000              #  2.2250738585072014E-308
.LCPI23_21:
	.quad	0x4330000000000000              #  4503599627370496
.LCPI23_25:
	.quad	0x7fefffffffffffff              #  1.7976931348623157E+308
.LCPI23_27:
	.quad	0x3fb999999999999a              #  0.10000000000000001
.LCPI23_28:
	.quad	0xbfb999999999999a              #  -0.10000000000000001
.LCPI23_36:
	.quad	0x3fd3333333333333              #  0.29999999999999999
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4, 0x0
.LCPI23_11:
	.quad	0x8000000000000000              #  -0
	.quad	0x8000000000000000              #  -0
.LCPI23_24:
	.long	0x7f7fffff                      #  3.40282347E+38
	.long	0x737fffff                      #  2.02824084E+31
	.zero	4
	.zero	4
.LCPI23_26:
	.quad	0x7fefffffffffffff              #  1.7976931348623157E+308
	.quad	0x7c9fffffffffffff              #  1.9958403095347196E+292
	.text
	.globl	main
	.p2align	4, 0x90
	.type	main,@function
main:                                   # 
.Lfunc_begin16:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception16
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$2472, %rsp                     # imm = 0x9A8
	.cfi_def_cfa_offset 2528
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	_ZSt4cout(%rip), %rax
	movq	-24(%rax), %rcx
	movl	$-261, %edx                     # imm = 0xFEFB
	andl	_ZSt4cout+24(%rcx), %edx
	orl	$256, %edx                      # imm = 0x100
	movl	%edx, _ZSt4cout+24(%rcx)
	movq	-24(%rax), %rax
	movq	$15, _ZSt4cout+8(%rax)
	movl	$256, %edi                      # imm = 0x100
	callq	mpfr_set_default_prec
	movl	$1000000, 52(%rsp)              # imm = 0xF4240
	movl	$1000000, %eax                  # imm = 0xF4240
	vcvtsi2sd	%eax, %xmm0, %xmm0
	callq	sqrt
	vcvttsd2si	%xmm0, %eax
	movl	%eax, 196(%rsp)                 # 4-byte Spill
	movl	$32000008, %edi                 # imm = 0x1E84808
	callq	_Znam
	movq	%rax, %r12
	movq	%rax, %r13
	movq	$1000000, (%rax)                # imm = 0xF4240
	leaq	8(%rax), %rbp
	xorl	%r14d, %r14d
	movabsq	$-9223372036854775807, %rbx     # imm = 0x8000000000000001
	.p2align	4, 0x90
.LBB23_1:                               # =>This Inner Loop Header: Depth=1
.Ltmp897:
	callq	mpfr_get_default_prec
.Ltmp898:
# %bb.2:                                #   in Loop: Header=BB23_1 Depth=1
	leaq	(%r12,%r14), %rdi
	addq	$8, %rdi
.Ltmp899:
	movq	%rax, %rsi
	callq	mpfr_init2
.Ltmp900:
# %bb.3:                                #   in Loop: Header=BB23_1 Depth=1
	movq	%rbx, 24(%r13,%r14)
	addq	$32, %r14
	cmpq	$32000000, %r14                 # imm = 0x1E84800
	jne	.LBB23_1
# %bb.4:
	movl	$32000008, %edi                 # imm = 0x1E84808
	callq	_Znam
	movq	%rax, %r14
	movq	$1000000, (%rax)                # imm = 0xF4240
	addq	$8, %rax
	movq	%rax, 536(%rsp)                 # 8-byte Spill
	xorl	%r15d, %r15d
	.p2align	4, 0x90
.LBB23_5:                               # =>This Inner Loop Header: Depth=1
.Ltmp902:
	callq	mpfr_get_default_prec
.Ltmp903:
# %bb.6:                                #   in Loop: Header=BB23_5 Depth=1
	leaq	(%r14,%r15), %rdi
	addq	$8, %rdi
.Ltmp904:
	movq	%rax, %rsi
	callq	mpfr_init2
.Ltmp905:
# %bb.7:                                #   in Loop: Header=BB23_5 Depth=1
	movq	%rbx, 24(%r14,%r15)
	addq	$32, %r15
	cmpq	$32000000, %r15                 # imm = 0x1E84800
	jne	.LBB23_5
# %bb.8:
	movq	%rbp, 528(%rsp)                 # 8-byte Spill
	movq	%r13, 288(%rsp)                 # 8-byte Spill
	movslq	196(%rsp), %r15                 # 4-byte Folded Reload
	leaq	1000008(%r15), %r13
	movq	%r13, %rbp
	shlq	$5, %rbp
	leaq	8(%rbp), %rax
	cmpl	$-1000008, %r15d                # imm = 0xFFF0BDB8
	movq	$-1, %rdi
	cmovgeq	%rax, %rdi
	callq	_Znam
	leaq	8(%rax), %rcx
	movq	%rcx, 296(%rsp)                 # 8-byte Spill
	movq	%rax, %rcx
	movq	%rax, 232(%rsp)                 # 8-byte Spill
	movq	%r13, (%rax)
	cmpl	$-1000008, %r15d                # imm = 0xFFF0BDB8
	je	.LBB23_13
# %bb.9:
	xorl	%r15d, %r15d
	.p2align	4, 0x90
.LBB23_10:                              # =>This Inner Loop Header: Depth=1
.Ltmp907:
	callq	mpfr_get_default_prec
.Ltmp908:
# %bb.11:                               #   in Loop: Header=BB23_10 Depth=1
	movq	296(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%r15), %rdi
.Ltmp909:
	movq	%rax, %rsi
	callq	mpfr_init2
.Ltmp910:
# %bb.12:                               #   in Loop: Header=BB23_10 Depth=1
	movq	232(%rsp), %rax                 # 8-byte Reload
	movq	%rbx, 24(%rax,%r15)
	addq	$32, %r15
	cmpq	%r15, %rbp
	jne	.LBB23_10
.LBB23_13:
	callq	mpfr_get_default_prec
	movq	%rax, %rbx
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	96(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	mpfr_init2
	movl	$2, %esi
	movq	%r15, %rdi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp912:
	callq	mpfr_get_default_rounding_mode
.Ltmp913:
# %bb.14:
.Ltmp914:
	movl	%eax, %ebp
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp915:
# %bb.15:
.Ltmp916:
	movq	%rax, %rbx
	callq	mpfr_get_default_rounding_mode
.Ltmp917:
# %bb.16:
.Ltmp918:
	movl	%eax, %r15d
	leaq	200(%rsp), %rdi
	movq	%rbx, %rsi
	callq	mpfr_init2
.Ltmp919:
# %bb.17:
.Ltmp920:
	leaq	200(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp921:
# %bb.18:
.Ltmp923:
	leaq	200(%rsp), %rdi
	leaq	96(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp924:
# %bb.19:
	cmpq	$0, 120(%rsp)
	je	.LBB23_21
# %bb.20:
.Ltmp926:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp927:
.LBB23_21:
	movl	$1, %eax
	vcvtsi2ss	%eax, %xmm1, %xmm0
	movl	$4, %eax
	vcvtsi2ss	%eax, %xmm1, %xmm1
	vmovss	%xmm0, 24(%rsp)
	vmovss	.LCPI23_0(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vdivss	%xmm1, %xmm2, %xmm0
	vmovss	%xmm0, 28(%rsp)
	movl	$-1, %ecx
	vcvtsi2ss	%ecx, %xmm3, %xmm0
	vmovss	%xmm0, 144(%rsp)
	vcvtsi2ss	%eax, %xmm3, %xmm0
	vdivss	%xmm0, %xmm2, %xmm0
	vmovss	%xmm0, 148(%rsp)
	leaq	1560(%rsp), %rbx
	movq	%rbx, 1544(%rsp)
	movabsq	$8098352192221705553, %rax      # imm = 0x7063206973617551
	movq	%rax, 1560(%rsp)
	movw	$61, 1568(%rsp)
	movq	$9, 1552(%rsp)
.Ltmp929:
	leaq	1544(%rsp), %rdi
	leaq	24(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp930:
# %bb.22:
	movq	1544(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_24
# %bb.23:
	callq	_ZdlPv
.LBB23_24:
	leaq	1528(%rsp), %rbx
	movq	%rbx, 1512(%rsp)
	movabsq	$7954237004145849681, %rax      # imm = 0x6E63206973617551
	movq	%rax, 1528(%rsp)
	movw	$61, 1536(%rsp)
	movq	$9, 1520(%rsp)
.Ltmp932:
	leaq	1512(%rsp), %rdi
	leaq	144(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp933:
# %bb.25:
	movq	1512(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_27
# %bb.26:
	callq	_ZdlPv
.LBB23_27:
	vmovss	144(%rsp), %xmm0                # xmm0 = mem[0],zero,zero,zero
	vmovss	24(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	vmovss	28(%rsp), %xmm2                 # xmm2 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vsubss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	148(%rsp), %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vinsertps	$16, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0],xmm0[0],xmm3[2,3]
	vmovlps	%xmm0, 1504(%rsp)
	leaq	384(%rsp), %rbx
	movq	%rbx, 368(%rsp)
	movq	$16, 96(%rsp)
.Ltmp935:
	leaq	368(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp936:
# %bb.28:
	movq	%rax, 368(%rsp)
	movq	96(%rsp), %rcx
	movq	%rcx, 384(%rsp)
	vmovups	.L.str.20(%rip), %xmm0
	vmovups	%xmm0, (%rax)
	movq	%rcx, 376(%rsp)
	movq	368(%rsp), %rax
	movb	$0, (%rax,%rcx)
.Ltmp938:
	leaq	368(%rsp), %rdi
	leaq	1504(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp939:
# %bb.29:
	movq	368(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_31
# %bb.30:
	callq	_ZdlPv
.LBB23_31:
	vmovss	24(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vmovss	28(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	144(%rsp), %xmm2                # xmm2 = mem[0],zero,zero,zero
	vmovss	148(%rsp), %xmm3                # xmm3 = mem[0],zero,zero,zero
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vaddss	%xmm3, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm1, %xmm1
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm0, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm6
	vsubss	%xmm6, %xmm0, %xmm0
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	vmovlps	%xmm0, 1496(%rsp)
	leaq	352(%rsp), %rbx
	movq	%rbx, 336(%rsp)
	movq	$26, 96(%rsp)
.Ltmp941:
	leaq	336(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp942:
# %bb.32:
	movq	%rax, 336(%rsp)
	movq	96(%rsp), %rcx
	movq	%rcx, 352(%rsp)
	vmovups	.L.str.21+10(%rip), %xmm0
	vmovups	%xmm0, 10(%rax)
	vmovups	.L.str.21(%rip), %xmm0
	vmovups	%xmm0, (%rax)
	movq	%rcx, 344(%rsp)
	movq	336(%rsp), %rax
	movb	$0, (%rax,%rcx)
.Ltmp944:
	leaq	336(%rsp), %rdi
	leaq	1496(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp945:
# %bb.33:
	movq	336(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_35
# %bb.34:
	callq	_ZdlPv
.LBB23_35:
	leaq	1480(%rsp), %rbx
	movq	%rbx, 1464(%rsp)
	movl	$1684955506, 1480(%rsp)         # imm = 0x646E6172
	movq	$4, 1472(%rsp)
	movb	$0, 1484(%rsp)
	callq	rand
	movzwl	%ax, %ecx
	andl	$2147418112, %eax               # imm = 0x7FFF0000
	vcvtsi2ss	%eax, %xmm7, %xmm0
	vmovss	.LCPI23_1(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm6, %xmm1, %xmm2
	vsubss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm3, %xmm6, %xmm3
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm2
	vsubss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vcvtsi2ss	%ecx, %xmm7, %xmm2
	vmulss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm6, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm5, %xmm0, %xmm0
	vsubss	%xmm4, %xmm6, %xmm4
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vmovss	%xmm1, 32(%rsp)                 # 4-byte Spill
	vsubss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, 80(%rsp)                 # 4-byte Spill
	callq	rand
	movzwl	%ax, %ecx
	andl	$2147418112, %eax               # imm = 0x7FFF0000
	vcvtsi2ss	%eax, %xmm7, %xmm0
	vmovss	.LCPI23_2(%rip), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm0, %xmm0
	vmovss	32(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm4, %xmm1, %xmm2
	vsubss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm3, %xmm4, %xmm3
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmovss	80(%rsp), %xmm7                 # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vaddss	%xmm6, %xmm7, %xmm2
	vsubss	%xmm7, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm7, %xmm4
	vsubss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm3, %xmm4, %xmm3
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm2
	vsubss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vcvtsi2ss	%ecx, %xmm8, %xmm3
	vaddss	%xmm0, %xmm2, %xmm0
	vmulss	%xmm5, %xmm3, %xmm2
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm6, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm5, %xmm0, %xmm0
	vsubss	%xmm4, %xmm6, %xmm4
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	vmovlps	%xmm0, 96(%rsp)
.Ltmp947:
	leaq	1464(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp948:
# %bb.36:
	movq	1464(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_38
# %bb.37:
	callq	_ZdlPv
.LBB23_38:
	leaq	1448(%rsp), %rbx
	movq	%rbx, 1432(%rsp)
	movl	$1684955506, 1448(%rsp)         # imm = 0x646E6172
	movq	$4, 1440(%rsp)
	movb	$0, 1452(%rsp)
	callq	rand
	movzwl	%ax, %ecx
	andl	$2147418112, %eax               # imm = 0x7FFF0000
	vcvtsi2ss	%eax, %xmm8, %xmm0
	vmovss	.LCPI23_1(%rip), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm6, %xmm1, %xmm2
	vsubss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm3, %xmm6, %xmm3
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm0, %xmm6, %xmm2
	vsubss	%xmm6, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm6, %xmm4
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm0, %xmm6, %xmm0
	vxorps	%xmm8, %xmm8, %xmm8
	vaddss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm3, %xmm1, %xmm2
	vsubss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vcvtsi2ss	%ecx, %xmm9, %xmm3
	vmulss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm1, %xmm8, %xmm3
	vsubss	%xmm1, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm6
	vsubss	%xmm6, %xmm1, %xmm1
	vsubss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm2, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	%xmm0, 80(%rsp)                 # 4-byte Spill
	vaddss	%xmm1, %xmm4, %xmm0
	vmovss	%xmm0, 64(%rsp)                 # 4-byte Spill
	vsubss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 32(%rsp)                 # 4-byte Spill
	callq	rand
	movl	%eax, %ecx
	andl	$2147418112, %ecx               # imm = 0x7FFF0000
	vcvtsi2ss	%ecx, %xmm9, %xmm0
	movzwl	%ax, %eax
	vmovss	.LCPI23_2(%rip), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm7, %xmm0, %xmm0
	vmovss	64(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm4, %xmm1, %xmm2
	vsubss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm3, %xmm4, %xmm3
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vxorps	%xmm8, %xmm8, %xmm8
	vmovss	32(%rsp), %xmm5                 # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vaddss	%xmm5, %xmm8, %xmm2
	vsubss	%xmm5, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm5, %xmm4
	vsubss	%xmm3, %xmm8, %xmm3
	vaddss	%xmm3, %xmm4, %xmm3
	vaddss	%xmm0, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vsubss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	80(%rsp), %xmm8, %xmm2          # 4-byte Folded Reload
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm0, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vcvtsi2ss	%eax, %xmm9, %xmm2
	vmulss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm3, %xmm3
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm1, %xmm8, %xmm3
	vsubss	%xmm1, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm6
	vsubss	%xmm6, %xmm1, %xmm1
	vsubss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm2, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	%xmm0, 80(%rsp)                 # 4-byte Spill
	vaddss	%xmm1, %xmm4, %xmm0
	vmovss	%xmm0, 64(%rsp)                 # 4-byte Spill
	vsubss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 32(%rsp)                 # 4-byte Spill
	callq	rand
	movl	%eax, %ecx
	andl	$2147418112, %ecx               # imm = 0x7FFF0000
	vcvtsi2ss	%ecx, %xmm9, %xmm0
	movzwl	%ax, %eax
	vmovss	.LCPI23_3(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm0, %xmm0
	vmovss	64(%rsp), %xmm5                 # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm5, %xmm4
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm0
	vxorps	%xmm8, %xmm8, %xmm8
	vmovss	32(%rsp), %xmm6                 # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vaddss	%xmm6, %xmm8, %xmm3
	vsubss	%xmm6, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm6, %xmm5
	vsubss	%xmm4, %xmm8, %xmm4
	vaddss	%xmm4, %xmm5, %xmm4
	vaddss	%xmm0, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	80(%rsp), %xmm8, %xmm3          # 4-byte Folded Reload
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm0, %xmm5, %xmm3
	vsubss	%xmm3, %xmm5, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm2
	vcvtsi2ss	%eax, %xmm9, %xmm5
	vaddss	%xmm3, %xmm2, %xmm2
	vmulss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm1, %xmm4, %xmm3
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm6
	vsubss	%xmm6, %xmm4, %xmm4
	vsubss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm2, %xmm8, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vsubss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm4, %xmm4
	vsubss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vinsertps	$16, %xmm1, %xmm2, %xmm1 # xmm1 = xmm2[0],xmm1[0],xmm2[2,3]
	vmovlps	%xmm1, 96(%rsp)
	vmovss	%xmm0, 104(%rsp)
.Ltmp950:
	leaq	1432(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp951:
# %bb.39:
	movq	1432(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_41
# %bb.40:
	callq	_ZdlPv
.LBB23_41:
	leaq	1416(%rsp), %rbx
	movq	%rbx, 1400(%rsp)
	movl	$1684955506, 1416(%rsp)         # imm = 0x646E6172
	movq	$4, 1408(%rsp)
	movb	$0, 1420(%rsp)
	vxorps	%xmm3, %xmm3, %xmm3
	movl	$-31, %ebp
	vmovss	.LCPI23_1(%rip), %xmm9          # xmm9 = mem[0],zero,zero,zero
	vxorps	%xmm0, %xmm0, %xmm0
	.p2align	4, 0x90
.LBB23_42:                              # =>This Inner Loop Header: Depth=1
	vmovups	%xmm0, 64(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm3, 32(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovss	%xmm9, 80(%rsp)                 # 4-byte Spill
	callq	rand
	vmovss	80(%rsp), %xmm9                 # 4-byte Reload
                                        # xmm9 = mem[0],zero,zero,zero
	vcvtsi2ss	%eax, %xmm10, %xmm0
	vmulss	%xmm0, %xmm9, %xmm1
	vmovups	64(%rsp), %xmm4                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm1, %xmm4, %xmm0
	vsubss	%xmm4, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm3, %xmm4, %xmm3
	vsubss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vmovshdup	%xmm4, %xmm2            # xmm2 = xmm4[1,1,3,3]
	vxorps	%xmm7, %xmm7, %xmm7
	vaddss	%xmm7, %xmm2, %xmm3
	vsubss	%xmm2, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm2, %xmm2
	vsubss	%xmm4, %xmm7, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vmovups	32(%rsp), %xmm8                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm7, %xmm8, %xmm4
	vsubss	%xmm8, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm8, %xmm6
	vsubss	%xmm5, %xmm7, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vmovshdup	%xmm8, %xmm6            # xmm6 = xmm8[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm3, %xmm1, %xmm6
	vsubss	%xmm1, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vsubss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm7
	vsubss	%xmm7, %xmm3, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vsubss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm4, %xmm3, %xmm2
	vsubss	%xmm3, %xmm2, %xmm7
	vsubss	%xmm7, %xmm2, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm3, %xmm6, %xmm2
	vsubss	%xmm2, %xmm6, %xmm4
	vaddss	%xmm3, %xmm4, %xmm3
	vinsertps	$16, %xmm1, %xmm3, %xmm3 # xmm3 = xmm3[0],xmm1[0],xmm3[2,3]
	vaddss	%xmm2, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	vmulss	.LCPI23_1(%rip), %xmm9, %xmm9
	addl	$31, %ebp
	cmpl	$181, %ebp
	jb	.LBB23_42
# %bb.43:
	vmovlhps	%xmm3, %xmm0, %xmm0             # xmm0 = xmm0[0],xmm3[0]
	vmovups	%xmm0, 96(%rsp)
.Ltmp953:
	leaq	1400(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp954:
# %bb.44:
	movq	1400(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_46
# %bb.45:
	callq	_ZdlPv
.LBB23_46:
	leaq	1384(%rsp), %rbx
	movq	%rbx, 1368(%rsp)
	movl	$1684955506, 1384(%rsp)         # imm = 0x646E6172
	movq	$4, 1376(%rsp)
	movb	$0, 1388(%rsp)
	callq	rand
	vcvtsi2sd	%eax, %xmm10, %xmm0
	vmulsd	.LCPI23_4(%rip), %xmm0, %xmm0
	vxorps	%xmm4, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm4, %xmm1
	vsubsd	%xmm4, %xmm1, %xmm2
	vsubsd	%xmm2, %xmm1, %xmm3
	vsubsd	%xmm3, %xmm4, %xmm3
	vsubsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm0
	vaddsd	%xmm4, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm2
	vsubsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm4, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm1
	vmovsd	%xmm1, 32(%rsp)                 # 8-byte Spill
	vsubsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovsd	%xmm0, 80(%rsp)                 # 8-byte Spill
	callq	rand
	vcvtsi2sd	%eax, %xmm10, %xmm0
	vmulsd	.LCPI23_5(%rip), %xmm0, %xmm0
	vmovsd	32(%rsp), %xmm3                 # 8-byte Reload
                                        # xmm3 = mem[0],zero
	vaddsd	%xmm0, %xmm3, %xmm1
	vsubsd	%xmm3, %xmm1, %xmm2
	vmovapd	%xmm3, %xmm4
	vsubsd	%xmm2, %xmm1, %xmm3
	vsubsd	%xmm3, %xmm4, %xmm3
	vsubsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmovsd	80(%rsp), %xmm4                 # 8-byte Reload
                                        # xmm4 = mem[0],zero
	vaddsd	%xmm5, %xmm4, %xmm2
	vsubsd	%xmm4, %xmm2, %xmm3
	vmovapd	%xmm4, %xmm6
	vsubsd	%xmm3, %xmm2, %xmm4
	vsubsd	%xmm4, %xmm6, %xmm4
	vsubsd	%xmm3, %xmm5, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm3
	vaddsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm2
	vsubsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm1
	vmovsd	%xmm1, 32(%rsp)                 # 8-byte Spill
	vsubsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovsd	%xmm0, 80(%rsp)                 # 8-byte Spill
	callq	rand
	vcvtsi2sd	%eax, %xmm10, %xmm0
	vmulsd	.LCPI23_6(%rip), %xmm0, %xmm0
	vmovsd	32(%rsp), %xmm3                 # 8-byte Reload
                                        # xmm3 = mem[0],zero
	vaddsd	%xmm0, %xmm3, %xmm1
	vsubsd	%xmm3, %xmm1, %xmm2
	vmovapd	%xmm3, %xmm4
	vsubsd	%xmm2, %xmm1, %xmm3
	vsubsd	%xmm3, %xmm4, %xmm3
	vsubsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vmovsd	80(%rsp), %xmm4                 # 8-byte Reload
                                        # xmm4 = mem[0],zero
	vaddsd	%xmm5, %xmm4, %xmm2
	vsubsd	%xmm4, %xmm2, %xmm3
	vmovapd	%xmm4, %xmm6
	vsubsd	%xmm3, %xmm2, %xmm4
	vsubsd	%xmm4, %xmm6, %xmm4
	vsubsd	%xmm3, %xmm5, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm3
	vaddsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm2
	vsubsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm1
	vmovsd	%xmm1, 32(%rsp)                 # 8-byte Spill
	vsubsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovsd	%xmm0, 80(%rsp)                 # 8-byte Spill
	callq	rand
	vcvtsi2sd	%eax, %xmm10, %xmm0
	vmulsd	.LCPI23_7(%rip), %xmm0, %xmm0
	vmovsd	32(%rsp), %xmm3                 # 8-byte Reload
                                        # xmm3 = mem[0],zero
	vaddsd	%xmm0, %xmm3, %xmm1
	vsubsd	%xmm3, %xmm1, %xmm2
	vmovapd	%xmm3, %xmm4
	vsubsd	%xmm2, %xmm1, %xmm3
	vsubsd	%xmm3, %xmm4, %xmm3
	vsubsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vmovsd	80(%rsp), %xmm4                 # 8-byte Reload
                                        # xmm4 = mem[0],zero
	vaddsd	%xmm5, %xmm4, %xmm2
	vsubsd	%xmm4, %xmm2, %xmm3
	vmovapd	%xmm4, %xmm6
	vsubsd	%xmm3, %xmm2, %xmm4
	vsubsd	%xmm4, %xmm6, %xmm4
	vsubsd	%xmm3, %xmm5, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm3
	vaddsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm2
	vsubsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm1
	vsubsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm0
	vmovsd	%xmm1, 96(%rsp)
	vmovsd	%xmm0, 104(%rsp)
.Ltmp956:
	leaq	1368(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp957:
# %bb.47:
	movq	1368(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_49
# %bb.48:
	callq	_ZdlPv
.LBB23_49:
	leaq	1352(%rsp), %rbx
	movq	%rbx, 1336(%rsp)
	movl	$1684955506, 1352(%rsp)         # imm = 0x646E6172
	movq	$4, 1344(%rsp)
	movb	$0, 1356(%rsp)
	callq	rand
	vcvtsi2sd	%eax, %xmm7, %xmm0
	vmulsd	.LCPI23_4(%rip), %xmm0, %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm0, %xmm5, %xmm1
	vsubsd	%xmm5, %xmm1, %xmm2
	vsubsd	%xmm2, %xmm1, %xmm3
	vsubsd	%xmm3, %xmm5, %xmm3
	vsubsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm0
	vaddsd	%xmm0, %xmm5, %xmm2
	vsubsd	%xmm5, %xmm2, %xmm3
	vsubsd	%xmm3, %xmm2, %xmm4
	vsubsd	%xmm4, %xmm5, %xmm4
	vsubsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm4, %xmm0
	vaddsd	%xmm0, %xmm5, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm3
	vsubsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm0
	vmovsd	%xmm0, 80(%rsp)                 # 8-byte Spill
	vaddsd	%xmm3, %xmm1, %xmm0
	vmovsd	%xmm0, 64(%rsp)                 # 8-byte Spill
	vsubsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm3, %xmm0, %xmm0
	vmovsd	%xmm0, 32(%rsp)                 # 8-byte Spill
	callq	rand
	vcvtsi2sd	%eax, %xmm7, %xmm0
	vmulsd	.LCPI23_5(%rip), %xmm0, %xmm0
	vmovsd	64(%rsp), %xmm3                 # 8-byte Reload
                                        # xmm3 = mem[0],zero
	vaddsd	%xmm0, %xmm3, %xmm1
	vsubsd	%xmm3, %xmm1, %xmm2
	vmovapd	%xmm3, %xmm4
	vsubsd	%xmm2, %xmm1, %xmm3
	vsubsd	%xmm3, %xmm4, %xmm3
	vsubsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vmovsd	32(%rsp), %xmm4                 # 8-byte Reload
                                        # xmm4 = mem[0],zero
	vaddsd	%xmm5, %xmm4, %xmm2
	vsubsd	%xmm4, %xmm2, %xmm3
	vmovapd	%xmm4, %xmm6
	vsubsd	%xmm3, %xmm2, %xmm4
	vsubsd	%xmm4, %xmm6, %xmm4
	vsubsd	%xmm3, %xmm5, %xmm3
	vxorps	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm3, %xmm4, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm4
	vsubsd	%xmm2, %xmm4, %xmm5
	vsubsd	%xmm5, %xmm4, %xmm6
	vsubsd	%xmm6, %xmm2, %xmm2
	vsubsd	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	80(%rsp), %xmm7, %xmm2          # 8-byte Folded Reload
	vaddsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm0, %xmm4, %xmm2
	vsubsd	%xmm2, %xmm4, %xmm3
	vaddsd	%xmm0, %xmm3, %xmm0
	vmovsd	%xmm0, 80(%rsp)                 # 8-byte Spill
	vaddsd	%xmm2, %xmm1, %xmm0
	vmovsd	%xmm0, 64(%rsp)                 # 8-byte Spill
	vsubsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vmovsd	%xmm0, 32(%rsp)                 # 8-byte Spill
	callq	rand
	vcvtsi2sd	%eax, %xmm8, %xmm0
	vmulsd	.LCPI23_6(%rip), %xmm0, %xmm0
	vmovsd	64(%rsp), %xmm3                 # 8-byte Reload
                                        # xmm3 = mem[0],zero
	vaddsd	%xmm0, %xmm3, %xmm1
	vsubsd	%xmm3, %xmm1, %xmm2
	vmovapd	%xmm3, %xmm4
	vsubsd	%xmm2, %xmm1, %xmm3
	vsubsd	%xmm3, %xmm4, %xmm3
	vsubsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vmovsd	32(%rsp), %xmm4                 # 8-byte Reload
                                        # xmm4 = mem[0],zero
	vaddsd	%xmm5, %xmm4, %xmm2
	vsubsd	%xmm4, %xmm2, %xmm3
	vmovapd	%xmm4, %xmm6
	vsubsd	%xmm3, %xmm2, %xmm4
	vsubsd	%xmm4, %xmm6, %xmm4
	vsubsd	%xmm3, %xmm5, %xmm3
	vxorpd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm3, %xmm4, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm4
	vsubsd	%xmm2, %xmm4, %xmm5
	vsubsd	%xmm5, %xmm4, %xmm6
	vsubsd	%xmm6, %xmm2, %xmm2
	vsubsd	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	80(%rsp), %xmm7, %xmm2          # 8-byte Folded Reload
	vaddsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm0, %xmm4, %xmm2
	vsubsd	%xmm2, %xmm4, %xmm3
	vaddsd	%xmm0, %xmm3, %xmm0
	vmovsd	%xmm0, 80(%rsp)                 # 8-byte Spill
	vaddsd	%xmm2, %xmm1, %xmm0
	vmovsd	%xmm0, 64(%rsp)                 # 8-byte Spill
	vsubsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vmovsd	%xmm0, 32(%rsp)                 # 8-byte Spill
	callq	rand
	vcvtsi2sd	%eax, %xmm8, %xmm0
	vmulsd	.LCPI23_7(%rip), %xmm0, %xmm0
	vmovsd	64(%rsp), %xmm3                 # 8-byte Reload
                                        # xmm3 = mem[0],zero
	vaddsd	%xmm0, %xmm3, %xmm1
	vsubsd	%xmm3, %xmm1, %xmm2
	vmovapd	%xmm3, %xmm4
	vsubsd	%xmm2, %xmm1, %xmm3
	vsubsd	%xmm3, %xmm4, %xmm3
	vsubsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vmovsd	32(%rsp), %xmm4                 # 8-byte Reload
                                        # xmm4 = mem[0],zero
	vaddsd	%xmm5, %xmm4, %xmm2
	vsubsd	%xmm4, %xmm2, %xmm3
	vmovapd	%xmm4, %xmm6
	vsubsd	%xmm3, %xmm2, %xmm4
	vsubsd	%xmm4, %xmm6, %xmm4
	vsubsd	%xmm3, %xmm5, %xmm3
	vxorpd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm3, %xmm4, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm4
	vsubsd	%xmm2, %xmm4, %xmm5
	vsubsd	%xmm5, %xmm4, %xmm6
	vsubsd	%xmm6, %xmm2, %xmm2
	vsubsd	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	80(%rsp), %xmm7, %xmm2          # 8-byte Folded Reload
	vaddsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm0, %xmm4, %xmm2
	vsubsd	%xmm2, %xmm4, %xmm3
	vaddsd	%xmm0, %xmm3, %xmm0
	vmovsd	%xmm0, 80(%rsp)                 # 8-byte Spill
	vaddsd	%xmm2, %xmm1, %xmm0
	vmovsd	%xmm0, 64(%rsp)                 # 8-byte Spill
	vsubsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vmovsd	%xmm0, 32(%rsp)                 # 8-byte Spill
	callq	rand
	vcvtsi2sd	%eax, %xmm8, %xmm0
	vmulsd	.LCPI23_8(%rip), %xmm0, %xmm0
	vmovsd	64(%rsp), %xmm3                 # 8-byte Reload
                                        # xmm3 = mem[0],zero
	vaddsd	%xmm0, %xmm3, %xmm1
	vsubsd	%xmm3, %xmm1, %xmm2
	vmovapd	%xmm3, %xmm4
	vsubsd	%xmm2, %xmm1, %xmm3
	vsubsd	%xmm3, %xmm4, %xmm3
	vsubsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vmovsd	32(%rsp), %xmm4                 # 8-byte Reload
                                        # xmm4 = mem[0],zero
	vaddsd	%xmm5, %xmm4, %xmm2
	vsubsd	%xmm4, %xmm2, %xmm3
	vmovapd	%xmm4, %xmm6
	vsubsd	%xmm3, %xmm2, %xmm4
	vsubsd	%xmm4, %xmm6, %xmm4
	vsubsd	%xmm3, %xmm5, %xmm3
	vxorpd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm3, %xmm4, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm4
	vsubsd	%xmm2, %xmm4, %xmm5
	vsubsd	%xmm5, %xmm4, %xmm6
	vsubsd	%xmm6, %xmm2, %xmm2
	vsubsd	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	80(%rsp), %xmm7, %xmm2          # 8-byte Folded Reload
	vaddsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm0, %xmm4, %xmm2
	vsubsd	%xmm2, %xmm4, %xmm3
	vaddsd	%xmm0, %xmm3, %xmm0
	vmovsd	%xmm0, 80(%rsp)                 # 8-byte Spill
	vaddsd	%xmm2, %xmm1, %xmm0
	vmovsd	%xmm0, 64(%rsp)                 # 8-byte Spill
	vsubsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vmovsd	%xmm0, 32(%rsp)                 # 8-byte Spill
	callq	rand
	vcvtsi2sd	%eax, %xmm8, %xmm0
	vmulsd	.LCPI23_9(%rip), %xmm0, %xmm0
	vmovsd	64(%rsp), %xmm3                 # 8-byte Reload
                                        # xmm3 = mem[0],zero
	vaddsd	%xmm0, %xmm3, %xmm1
	vsubsd	%xmm3, %xmm1, %xmm2
	vmovapd	%xmm3, %xmm4
	vsubsd	%xmm2, %xmm1, %xmm3
	vsubsd	%xmm3, %xmm4, %xmm3
	vsubsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vmovsd	32(%rsp), %xmm4                 # 8-byte Reload
                                        # xmm4 = mem[0],zero
	vaddsd	%xmm5, %xmm4, %xmm2
	vsubsd	%xmm4, %xmm2, %xmm3
	vmovapd	%xmm4, %xmm6
	vsubsd	%xmm3, %xmm2, %xmm4
	vsubsd	%xmm4, %xmm6, %xmm4
	vsubsd	%xmm3, %xmm5, %xmm3
	vxorpd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm3, %xmm4, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm4
	vsubsd	%xmm2, %xmm4, %xmm5
	vsubsd	%xmm5, %xmm4, %xmm6
	vsubsd	%xmm6, %xmm2, %xmm2
	vsubsd	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	80(%rsp), %xmm7, %xmm2          # 8-byte Folded Reload
	vaddsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm0, %xmm4, %xmm2
	vsubsd	%xmm2, %xmm4, %xmm3
	vaddsd	%xmm0, %xmm3, %xmm0
	vaddsd	%xmm2, %xmm1, %xmm3
	vsubsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm1, %xmm1
	vmovsd	%xmm3, 96(%rsp)
	vmovsd	%xmm1, 104(%rsp)
	vmovsd	%xmm0, 112(%rsp)
.Ltmp959:
	leaq	1336(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp960:
# %bb.50:
	movq	1336(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_52
# %bb.51:
	callq	_ZdlPv
.LBB23_52:
	leaq	1320(%rsp), %rbx
	movq	%rbx, 1304(%rsp)
	movl	$1684955506, 1320(%rsp)         # imm = 0x646E6172
	movq	$4, 1312(%rsp)
	movb	$0, 1324(%rsp)
	vmovsd	.LCPI23_4(%rip), %xmm10         # xmm10 = mem[0],zero
	movl	$-31, %ebp
	vxorpd	%xmm3, %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm4, %xmm4, %xmm4
	.p2align	4, 0x90
.LBB23_53:                              # =>This Inner Loop Header: Depth=1
	vmovsd	%xmm4, 136(%rsp)                # 8-byte Spill
	vmovsd	%xmm1, 160(%rsp)                # 8-byte Spill
	vmovsd	%xmm2, 64(%rsp)                 # 8-byte Spill
	vmovsd	%xmm3, 32(%rsp)                 # 8-byte Spill
	vmovsd	%xmm10, 80(%rsp)                # 8-byte Spill
	callq	rand
	vmovsd	80(%rsp), %xmm10                # 8-byte Reload
                                        # xmm10 = mem[0],zero
	movzwl	%ax, %ecx
	andl	$2147418112, %eax               # imm = 0x7FFF0000
	vcvtsi2sd	%eax, %xmm11, %xmm0
	vmulsd	%xmm0, %xmm10, %xmm0
	vmovsd	136(%rsp), %xmm4                # 8-byte Reload
                                        # xmm4 = mem[0],zero
	vaddsd	%xmm0, %xmm4, %xmm1
	vsubsd	%xmm4, %xmm1, %xmm2
	vsubsd	%xmm2, %xmm1, %xmm3
	vsubsd	%xmm3, %xmm4, %xmm3
	vsubsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm0
	vxorps	%xmm9, %xmm9, %xmm9
	vmovsd	160(%rsp), %xmm5                # 8-byte Reload
                                        # xmm5 = mem[0],zero
	vaddsd	%xmm5, %xmm9, %xmm2
	vsubsd	%xmm5, %xmm2, %xmm3
	vsubsd	%xmm3, %xmm2, %xmm4
	vsubsd	%xmm4, %xmm5, %xmm4
	vsubsd	%xmm3, %xmm9, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm3
	vmovsd	64(%rsp), %xmm7                 # 8-byte Reload
                                        # xmm7 = mem[0],zero
	vaddsd	%xmm7, %xmm9, %xmm4
	vsubsd	%xmm7, %xmm4, %xmm5
	vsubsd	%xmm5, %xmm4, %xmm6
	vsubsd	%xmm6, %xmm7, %xmm6
	vsubsd	%xmm5, %xmm9, %xmm5
	vaddsd	%xmm5, %xmm6, %xmm5
	vaddsd	32(%rsp), %xmm5, %xmm5          # 8-byte Folded Reload
	vaddsd	%xmm5, %xmm9, %xmm5
	vaddsd	%xmm2, %xmm0, %xmm6
	vsubsd	%xmm0, %xmm6, %xmm7
	vsubsd	%xmm7, %xmm6, %xmm8
	vsubsd	%xmm8, %xmm0, %xmm0
	vsubsd	%xmm7, %xmm2, %xmm2
	vaddsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm3, %xmm0, %xmm2
	vsubsd	%xmm0, %xmm2, %xmm7
	vsubsd	%xmm7, %xmm2, %xmm8
	vsubsd	%xmm8, %xmm0, %xmm0
	vsubsd	%xmm7, %xmm3, %xmm3
	vaddsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm4, %xmm2, %xmm3
	vsubsd	%xmm2, %xmm3, %xmm7
	vsubsd	%xmm7, %xmm3, %xmm8
	vsubsd	%xmm8, %xmm2, %xmm2
	vsubsd	%xmm7, %xmm4, %xmm4
	vaddsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm2
	vsubsd	%xmm2, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm3, %xmm0
	vaddsd	%xmm2, %xmm6, %xmm3
	vsubsd	%xmm3, %xmm6, %xmm4
	vaddsd	%xmm2, %xmm4, %xmm2
	vaddsd	%xmm3, %xmm1, %xmm4
	vsubsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm3, %xmm1, %xmm1
	vcvtsi2sd	%ecx, %xmm11, %xmm3
	vmulsd	%xmm3, %xmm10, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm5
	vsubsd	%xmm4, %xmm5, %xmm6
	vsubsd	%xmm6, %xmm5, %xmm7
	vsubsd	%xmm7, %xmm4, %xmm4
	vsubsd	%xmm6, %xmm3, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm3
	vaddsd	%xmm1, %xmm9, %xmm4
	vsubsd	%xmm1, %xmm4, %xmm6
	vsubsd	%xmm6, %xmm4, %xmm7
	vsubsd	%xmm7, %xmm1, %xmm1
	vsubsd	%xmm6, %xmm9, %xmm6
	vaddsd	%xmm6, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm9, %xmm6
	vsubsd	%xmm2, %xmm6, %xmm7
	vsubsd	%xmm7, %xmm6, %xmm8
	vsubsd	%xmm8, %xmm2, %xmm2
	vsubsd	%xmm7, %xmm9, %xmm7
	vaddsd	%xmm7, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm0, %xmm9, %xmm0
	vaddsd	%xmm4, %xmm3, %xmm2
	vsubsd	%xmm3, %xmm2, %xmm7
	vsubsd	%xmm7, %xmm2, %xmm8
	vsubsd	%xmm8, %xmm3, %xmm3
	vsubsd	%xmm7, %xmm4, %xmm4
	vaddsd	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm1, %xmm3, %xmm4
	vsubsd	%xmm3, %xmm4, %xmm7
	vsubsd	%xmm7, %xmm4, %xmm8
	vsubsd	%xmm8, %xmm3, %xmm3
	vsubsd	%xmm7, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm3, %xmm1
	vaddsd	%xmm6, %xmm4, %xmm3
	vsubsd	%xmm4, %xmm3, %xmm7
	vsubsd	%xmm7, %xmm3, %xmm8
	vsubsd	%xmm8, %xmm4, %xmm4
	vsubsd	%xmm7, %xmm6, %xmm6
	vaddsd	%xmm6, %xmm4, %xmm4
	vaddsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm1
	vsubsd	%xmm1, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm3, %xmm3
	vaddsd	%xmm1, %xmm2, %xmm0
	vsubsd	%xmm0, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm5, %xmm4
	vsubsd	%xmm4, %xmm5, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LCPI23_4(%rip), %xmm10, %xmm10
	addl	$31, %ebp
	cmpl	$65, %ebp
	jb	.LBB23_53
# %bb.54:
	vmovsd	%xmm4, 96(%rsp)
	vmovsd	%xmm1, 104(%rsp)
	vmovsd	%xmm2, 112(%rsp)
	vmovsd	%xmm3, 120(%rsp)
.Ltmp962:
	leaq	1304(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp963:
# %bb.55:
	movq	1304(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_57
# %bb.56:
	callq	_ZdlPv
.LBB23_57:
	leaq	2456(%rsp), %rbp
	movq	%rbp, 2440(%rsp)
	movl	$7565413, 2456(%rsp)            # imm = 0x737065
	movq	$3, 2448(%rsp)
	vmovsd	.LCPI23_10(%rip), %xmm0         # xmm0 = mem[0],zero
	vmovsd	%xmm0, 592(%rsp)
	fldl	592(%rsp)
	fstpt	80(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp965:
	movl	$_ZSt4cout, %edi
	movl	$3, %edx
	movq	%rbp, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp966:
# %bb.58:
	fldt	80(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movabsq	$4372995238176751616, %rsi      # imm = 0x3CB0000000000000
	movl	$.L.str.8, %edi
	xorl	%eax, %eax
	callq	printf
	movq	2440(%rsp), %rdi
	cmpq	%rbp, %rdi
	je	.LBB23_60
# %bb.59:
	callq	_ZdlPv
.LBB23_60:
	leaq	1288(%rsp), %rbx
	movq	%rbx, 1272(%rsp)
	movl	$7565413, 1288(%rsp)            # imm = 0x737065
	movq	$3, 1280(%rsp)
	movl	$2, %eax
	vcvtsi2sd	%eax, %xmm9, %xmm0
	movl	$52, %eax
	vcvtsi2sd	%eax, %xmm9, %xmm1
	vxorpd	.LCPI23_11(%rip), %xmm1, %xmm1
	callq	pow
	vmovsd	%xmm0, 32(%rsp)                 # 8-byte Spill
	vmovsd	%xmm0, 584(%rsp)
	fldl	584(%rsp)
	fstpt	80(%rsp)                        # 10-byte Folded Spill
	wait
	movq	1272(%rsp), %rsi
	movq	1280(%rsp), %rdx
.Ltmp968:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp969:
# %bb.61:
	vmovq	32(%rsp), %xmm0                 # 8-byte Folded Reload
                                        # xmm0 = mem[0],zero
	vmovq	%xmm0, %rsi
	fldt	80(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.8, %edi
	xorl	%eax, %eax
	callq	printf
	movq	1272(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_63
# %bb.62:
	callq	_ZdlPv
.LBB23_63:
	leaq	2264(%rsp), %rbx
	movq	%rbx, 2248(%rsp)
	movl	$7565413, 2264(%rsp)            # imm = 0x737065
	movq	$3, 2256(%rsp)
	movl	$2, %eax
	vcvtsi2sd	%eax, %xmm9, %xmm0
	vmulsd	.LCPI23_12(%rip), %xmm0, %xmm0
	vmovsd	%xmm0, 96(%rsp)
	movq	$0, 104(%rsp)
.Ltmp971:
	leaq	2248(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp972:
# %bb.64:
	movq	2248(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_66
# %bb.65:
	callq	_ZdlPv
.LBB23_66:
	leaq	1256(%rsp), %rbx
	movq	%rbx, 1240(%rsp)
	movl	$7565413, 1256(%rsp)            # imm = 0x737065
	movq	$3, 1248(%rsp)
	movl	$2, %eax
	vcvtsi2sd	%eax, %xmm5, %xmm0
	movl	$105, %eax
	vcvtsi2sd	%eax, %xmm5, %xmm1
	vxorpd	.LCPI23_11(%rip), %xmm1, %xmm1
	callq	pow
	vmovsd	%xmm0, 32(%rsp)                 # 8-byte Spill
	vmovsd	%xmm0, 576(%rsp)
	fldl	576(%rsp)
	fstpt	80(%rsp)                        # 10-byte Folded Spill
	wait
	movq	1240(%rsp), %rsi
	movq	1248(%rsp), %rdx
.Ltmp974:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp975:
# %bb.67:
	vmovq	32(%rsp), %xmm0                 # 8-byte Folded Reload
                                        # xmm0 = mem[0],zero
	vmovq	%xmm0, %rsi
	fldt	80(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.8, %edi
	xorl	%eax, %eax
	callq	printf
	movq	1240(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_69
# %bb.68:
	callq	_ZdlPv
.LBB23_69:
	leaq	2232(%rsp), %rbx
	movq	%rbx, 2216(%rsp)
	movl	$7565413, 2232(%rsp)            # imm = 0x737065
	movl	$2, %eax
	vcvtsi2sd	%eax, %xmm3, %xmm0
	vmulsd	.LCPI23_13(%rip), %xmm0, %xmm0
	movq	$3, 2224(%rsp)
	vmovsd	%xmm0, 96(%rsp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, 104(%rsp)
.Ltmp977:
	leaq	2216(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp978:
# %bb.70:
	movq	2216(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_72
# %bb.71:
	callq	_ZdlPv
.LBB23_72:
	leaq	1224(%rsp), %rbx
	movq	%rbx, 1208(%rsp)
	movl	$7565413, 1224(%rsp)            # imm = 0x737065
	movq	$3, 1216(%rsp)
	movl	$2, %eax
	vcvtsi2sd	%eax, %xmm2, %xmm0
	movl	$158, %eax
	vcvtsi2sd	%eax, %xmm2, %xmm1
	vxorpd	.LCPI23_11(%rip), %xmm1, %xmm1
	callq	pow
	vmovsd	%xmm0, 32(%rsp)                 # 8-byte Spill
	vmovsd	%xmm0, 568(%rsp)
	fldl	568(%rsp)
	fstpt	80(%rsp)                        # 10-byte Folded Spill
	wait
	movq	1208(%rsp), %rsi
	movq	1216(%rsp), %rdx
.Ltmp980:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp981:
# %bb.73:
	vmovq	32(%rsp), %xmm0                 # 8-byte Folded Reload
                                        # xmm0 = mem[0],zero
	vmovq	%xmm0, %rsi
	fldt	80(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.8, %edi
	xorl	%eax, %eax
	callq	printf
	movq	1208(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_75
# %bb.74:
	callq	_ZdlPv
.LBB23_75:
	leaq	2200(%rsp), %rbx
	movq	%rbx, 2184(%rsp)
	movl	$7565413, 2200(%rsp)            # imm = 0x737065
	movq	$3, 2192(%rsp)
	movl	$2, %eax
	vcvtsi2sd	%eax, %xmm2, %xmm0
	vmulsd	.LCPI23_14(%rip), %xmm0, %xmm0
	vmovsd	%xmm0, 96(%rsp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, 104(%rsp)
	movq	$0, 120(%rsp)
.Ltmp983:
	leaq	2184(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp984:
# %bb.76:
	movq	2184(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_78
# %bb.77:
	callq	_ZdlPv
.LBB23_78:
	leaq	1192(%rsp), %rbx
	movq	%rbx, 1176(%rsp)
	movl	$7565413, 1192(%rsp)            # imm = 0x737065
	movq	$3, 1184(%rsp)
	movl	$2, %eax
	vcvtsi2sd	%eax, %xmm2, %xmm0
	movl	$211, %eax
	vcvtsi2sd	%eax, %xmm2, %xmm1
	vxorpd	.LCPI23_11(%rip), %xmm1, %xmm1
	callq	pow
	vmovsd	%xmm0, 32(%rsp)                 # 8-byte Spill
	vmovsd	%xmm0, 560(%rsp)
	fldl	560(%rsp)
	fstpt	80(%rsp)                        # 10-byte Folded Spill
	wait
	movq	1176(%rsp), %rsi
	movq	1184(%rsp), %rdx
.Ltmp986:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp987:
# %bb.79:
	vmovq	32(%rsp), %xmm0                 # 8-byte Folded Reload
                                        # xmm0 = mem[0],zero
	vmovq	%xmm0, %rsi
	fldt	80(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.8, %edi
	xorl	%eax, %eax
	callq	printf
	movq	1176(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_81
# %bb.80:
	callq	_ZdlPv
.LBB23_81:
	leaq	2424(%rsp), %rbp
	movq	%rbp, 2408(%rsp)
	movl	$7565413, 2424(%rsp)            # imm = 0x737065
	movq	$3, 2416(%rsp)
	vmovss	.LCPI23_0(%rip), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 284(%rsp)
	flds	284(%rsp)
	fstpt	80(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp989:
	movl	$_ZSt4cout, %edi
	movl	$3, %edx
	movq	%rbp, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp990:
# %bb.82:
	fldt	80(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	movl	$872415232, %esi                # imm = 0x34000000
	xorl	%eax, %eax
	callq	printf
	movq	2408(%rsp), %rdi
	cmpq	%rbp, %rdi
	je	.LBB23_84
# %bb.83:
	callq	_ZdlPv
.LBB23_84:
	leaq	1160(%rsp), %rbx
	movq	%rbx, 1144(%rsp)
	movl	$7565413, 1160(%rsp)            # imm = 0x737065
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm2, %xmm0
	movl	$23, %eax
	vcvtsi2ss	%eax, %xmm2, %xmm1
	movq	$3, 1152(%rsp)
	vbroadcastss	.LCPI23_15(%rip), %xmm2 # xmm2 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vmovups	%xmm2, 80(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vxorps	%xmm2, %xmm1, %xmm1
	callq	powf
	vmovss	%xmm0, 64(%rsp)                 # 4-byte Spill
	vmovss	%xmm0, 280(%rsp)
	flds	280(%rsp)
	fstpt	32(%rsp)                        # 10-byte Folded Spill
	wait
	movq	1144(%rsp), %rsi
	movq	1152(%rsp), %rdx
.Ltmp992:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp993:
# %bb.85:
	vmovd	64(%rsp), %xmm0                 # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, %esi
	fldt	32(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	movq	1144(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_87
# %bb.86:
	callq	_ZdlPv
.LBB23_87:
	leaq	2168(%rsp), %rbx
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm3, %xmm0
	movq	%rbx, 2152(%rsp)
	movl	$7565413, 2168(%rsp)            # imm = 0x737065
	vmulss	.LCPI23_16(%rip), %xmm0, %xmm0
	movq	$3, 2160(%rsp)
	vxorps	%xmm1, %xmm1, %xmm1
	vblendps	$1, %xmm0, %xmm1, %xmm0         # xmm0 = xmm0[0],xmm1[1,2,3]
	vmovlps	%xmm0, 96(%rsp)
.Ltmp995:
	leaq	2152(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp996:
# %bb.88:
	movq	2152(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_90
# %bb.89:
	callq	_ZdlPv
.LBB23_90:
	leaq	1128(%rsp), %rbx
	movq	%rbx, 1112(%rsp)
	movl	$7565413, 1128(%rsp)            # imm = 0x737065
	movq	$3, 1120(%rsp)
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm3, %xmm0
	movl	$47, %eax
	vcvtsi2ss	%eax, %xmm3, %xmm1
	vxorps	80(%rsp), %xmm1, %xmm1          # 16-byte Folded Reload
	callq	powf
	vmovss	%xmm0, 64(%rsp)                 # 4-byte Spill
	vmovss	%xmm0, 276(%rsp)
	flds	276(%rsp)
	fstpt	32(%rsp)                        # 10-byte Folded Spill
	wait
	movq	1112(%rsp), %rsi
	movq	1120(%rsp), %rdx
.Ltmp998:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp999:
# %bb.91:
	vmovd	64(%rsp), %xmm0                 # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, %esi
	fldt	32(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	movq	1112(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_93
# %bb.92:
	callq	_ZdlPv
.LBB23_93:
	leaq	2136(%rsp), %rbx
	movq	%rbx, 2120(%rsp)
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm3, %xmm0
	movl	$7565413, 2136(%rsp)            # imm = 0x737065
	movq	$3, 2128(%rsp)
	vmulss	.LCPI23_17(%rip), %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vblendps	$1, %xmm0, %xmm1, %xmm0         # xmm0 = xmm0[0],xmm1[1,2,3]
	vmovlps	%xmm0, 96(%rsp)
	movl	$0, 104(%rsp)
.Ltmp1001:
	leaq	2120(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1002:
# %bb.94:
	movq	2120(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_96
# %bb.95:
	callq	_ZdlPv
.LBB23_96:
	leaq	1096(%rsp), %rbx
	movq	%rbx, 1080(%rsp)
	movl	$7565413, 1096(%rsp)            # imm = 0x737065
	movq	$3, 1088(%rsp)
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm3, %xmm0
	movl	$71, %eax
	vcvtsi2ss	%eax, %xmm3, %xmm1
	vxorps	80(%rsp), %xmm1, %xmm1          # 16-byte Folded Reload
	callq	powf
	vmovss	%xmm0, 64(%rsp)                 # 4-byte Spill
	vmovss	%xmm0, 272(%rsp)
	flds	272(%rsp)
	fstpt	32(%rsp)                        # 10-byte Folded Spill
	wait
	movq	1080(%rsp), %rsi
	movq	1088(%rsp), %rdx
.Ltmp1004:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1005:
# %bb.97:
	vmovd	64(%rsp), %xmm0                 # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, %esi
	fldt	32(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	movq	1080(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_99
# %bb.98:
	callq	_ZdlPv
.LBB23_99:
	leaq	2104(%rsp), %rbx
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm3, %xmm0
	movq	%rbx, 2088(%rsp)
	movl	$7565413, 2104(%rsp)            # imm = 0x737065
	vmulss	.LCPI23_18(%rip), %xmm0, %xmm0
	movq	$3, 2096(%rsp)
	vxorps	%xmm1, %xmm1, %xmm1
	vblendps	$1, %xmm0, %xmm1, %xmm0         # xmm0 = xmm0[0],xmm1[1,2,3]
	vmovups	%xmm0, 96(%rsp)
.Ltmp1007:
	leaq	2088(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1008:
# %bb.100:
	movq	2088(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_102
# %bb.101:
	callq	_ZdlPv
.LBB23_102:
	leaq	1064(%rsp), %rbx
	movq	%rbx, 1048(%rsp)
	movl	$7565413, 1064(%rsp)            # imm = 0x737065
	movq	$3, 1056(%rsp)
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm2, %xmm0
	movl	$95, %eax
	vcvtsi2ss	%eax, %xmm2, %xmm1
	vxorps	80(%rsp), %xmm1, %xmm1          # 16-byte Folded Reload
	callq	powf
	vmovss	%xmm0, 64(%rsp)                 # 4-byte Spill
	vmovss	%xmm0, 268(%rsp)
	flds	268(%rsp)
	fstpt	32(%rsp)                        # 10-byte Folded Spill
	wait
	movq	1048(%rsp), %rsi
	movq	1056(%rsp), %rdx
.Ltmp1010:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1011:
# %bb.103:
	vmovd	64(%rsp), %xmm0                 # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, %esi
	fldt	32(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	movq	1048(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_105
# %bb.104:
	callq	_ZdlPv
.LBB23_105:
	leaq	2392(%rsp), %rbp
	movq	%rbp, 2376(%rsp)
	movl	$7235949, 2392(%rsp)            # imm = 0x6E696D
	movq	$3, 2384(%rsp)
	vmovss	.LCPI23_19(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 264(%rsp)
	flds	264(%rsp)
	fstpt	32(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp1013:
	movl	$_ZSt4cout, %edi
	movl	$3, %edx
	movq	%rbp, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1014:
# %bb.106:
	fldt	32(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	movl	$8388608, %esi                  # imm = 0x800000
	xorl	%eax, %eax
	callq	printf
	movq	2376(%rsp), %rdi
	cmpq	%rbp, %rdi
	je	.LBB23_108
# %bb.107:
	callq	_ZdlPv
.LBB23_108:
	leaq	2072(%rsp), %rbx
	movq	%rbx, 2056(%rsp)
	movl	$7235949, 2072(%rsp)            # imm = 0x6E696D
	movq	$3, 2064(%rsp)
	movl	$2, %eax
	vcvtsi2sd	%eax, %xmm2, %xmm0
	vmulsd	.LCPI23_20(%rip), %xmm0, %xmm0
	vmulsd	.LCPI23_21(%rip), %xmm0, %xmm0
	vmovsd	%xmm0, 96(%rsp)
	movq	$0, 104(%rsp)
.Ltmp1016:
	leaq	2056(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1017:
# %bb.109:
	movq	2056(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_111
# %bb.110:
	callq	_ZdlPv
.LBB23_111:
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm2, %xmm0
	vmulss	.LCPI23_19(%rip), %xmm0, %xmm0
	vmulss	.LCPI23_22(%rip), %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vblendps	$1, %xmm0, %xmm1, %xmm0         # xmm0 = xmm0[0],xmm1[1,2,3]
	vmovlps	%xmm0, 24(%rsp)
	leaq	2040(%rsp), %rbx
	movq	%rbx, 2024(%rsp)
	movl	$7235949, 2040(%rsp)            # imm = 0x6E696D
	movq	$3, 2032(%rsp)
.Ltmp1019:
	leaq	2024(%rsp), %rdi
	leaq	24(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1020:
# %bb.112:
	movq	2024(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_114
# %bb.113:
	callq	_ZdlPv
.LBB23_114:
	vmovss	24(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vmovss	28(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm1, %xmm0, %xmm2
	vmovss	%xmm2, 24(%rsp)
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 28(%rsp)
	leaq	2008(%rsp), %rbx
	movq	%rbx, 1992(%rsp)
	movl	$7235949, 2008(%rsp)            # imm = 0x6E696D
	movq	$3, 2000(%rsp)
.Ltmp1022:
	leaq	1992(%rsp), %rdi
	leaq	24(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1023:
# %bb.115:
	movq	1992(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_117
# %bb.116:
	callq	_ZdlPv
.LBB23_117:
	leaq	2360(%rsp), %rbp
	movq	%rbp, 2344(%rsp)
	movl	$7235949, 2360(%rsp)            # imm = 0x6E696D
	movq	$3, 2352(%rsp)
	vmovsd	.LCPI23_20(%rip), %xmm0         # xmm0 = mem[0],zero
	vmovsd	%xmm0, 552(%rsp)
	fldl	552(%rsp)
	fstpt	32(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp1025:
	movl	$_ZSt4cout, %edi
	movl	$3, %edx
	movq	%rbp, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1026:
# %bb.118:
	fldt	32(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movabsq	$4503599627370496, %rsi         # imm = 0x10000000000000
	movl	$.L.str.8, %edi
	xorl	%eax, %eax
	callq	printf
	movq	2344(%rsp), %rdi
	cmpq	%rbp, %rdi
	je	.LBB23_120
# %bb.119:
	callq	_ZdlPv
.LBB23_120:
	leaq	1976(%rsp), %rbx
	movq	%rbx, 1960(%rsp)
	movl	$7235949, 1976(%rsp)            # imm = 0x6E696D
	movq	$3, 1968(%rsp)
	movl	$2, %eax
	vcvtsi2sd	%eax, %xmm3, %xmm0
	vmulsd	.LCPI23_20(%rip), %xmm0, %xmm0
	vmulsd	.LCPI23_21(%rip), %xmm0, %xmm0
	vmovsd	%xmm0, 96(%rsp)
	movq	$0, 104(%rsp)
.Ltmp1028:
	leaq	1960(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1029:
# %bb.121:
	movq	1960(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_123
# %bb.122:
	callq	_ZdlPv
.LBB23_123:
	movl	$2, %eax
	vcvtsi2sd	%eax, %xmm3, %xmm0
	vmulsd	.LCPI23_20(%rip), %xmm0, %xmm0
	vmulsd	.LCPI23_21(%rip), %xmm0, %xmm0
	vmovsd	%xmm0, 96(%rsp)
	movq	$0, 104(%rsp)
	leaq	1944(%rsp), %rbx
	movq	%rbx, 1928(%rsp)
	movl	$7235949, 1944(%rsp)            # imm = 0x6E696D
	movq	$3, 1936(%rsp)
.Ltmp1031:
	leaq	1928(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1032:
# %bb.124:
	movq	1928(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_126
# %bb.125:
	callq	_ZdlPv
.LBB23_126:
	vmovsd	96(%rsp), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	104(%rsp), %xmm1                # xmm1 = mem[0],zero
	vaddsd	%xmm1, %xmm0, %xmm2
	vmovsd	%xmm2, 96(%rsp)
	vsubsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovsd	%xmm0, 104(%rsp)
	leaq	1912(%rsp), %rbx
	movq	%rbx, 1896(%rsp)
	movl	$7235949, 1912(%rsp)            # imm = 0x6E696D
	movq	$3, 1904(%rsp)
.Ltmp1034:
	leaq	1896(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1035:
# %bb.127:
	movq	1896(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_129
# %bb.128:
	callq	_ZdlPv
.LBB23_129:
	leaq	2328(%rsp), %rbp
	movq	%rbp, 2312(%rsp)
	movl	$7889261, 2328(%rsp)            # imm = 0x78616D
	movq	$3, 2320(%rsp)
	vmovss	.LCPI23_23(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 260(%rsp)
	flds	260(%rsp)
	fstpt	32(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp1037:
	movl	$_ZSt4cout, %edi
	movl	$3, %edx
	movq	%rbp, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1038:
# %bb.130:
	fldt	32(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	movl	$2139095039, %esi               # imm = 0x7F7FFFFF
	xorl	%eax, %eax
	callq	printf
	movq	2312(%rsp), %rdi
	cmpq	%rbp, %rdi
	je	.LBB23_132
# %bb.131:
	callq	_ZdlPv
.LBB23_132:
	leaq	1880(%rsp), %rbx
	movq	%rbx, 1864(%rsp)
	movl	$7889261, 1880(%rsp)            # imm = 0x78616D
	movq	$3, 1872(%rsp)
	vmovsd	.LCPI23_24(%rip), %xmm0         # xmm0 = mem[0],zero
	vmovsd	%xmm0, 96(%rsp)
.Ltmp1040:
	leaq	1864(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1041:
# %bb.133:
	movq	1864(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_135
# %bb.134:
	callq	_ZdlPv
.LBB23_135:
	vmovsd	.LCPI23_24(%rip), %xmm0         # xmm0 = mem[0],zero
	vmovsd	%xmm0, 24(%rsp)
	leaq	1848(%rsp), %rbx
	movq	%rbx, 1832(%rsp)
	movl	$7889261, 1848(%rsp)            # imm = 0x78616D
	movq	$3, 1840(%rsp)
.Ltmp1043:
	leaq	1832(%rsp), %rdi
	leaq	24(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1044:
# %bb.136:
	movq	1832(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_138
# %bb.137:
	callq	_ZdlPv
.LBB23_138:
	vmovss	24(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vmovss	28(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm1, %xmm0, %xmm2
	vmovss	%xmm2, 24(%rsp)
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 28(%rsp)
	leaq	1816(%rsp), %rbx
	movq	%rbx, 1800(%rsp)
	movl	$7889261, 1816(%rsp)            # imm = 0x78616D
	movq	$3, 1808(%rsp)
.Ltmp1046:
	leaq	1800(%rsp), %rdi
	leaq	24(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1047:
# %bb.139:
	movq	1800(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_141
# %bb.140:
	callq	_ZdlPv
.LBB23_141:
	leaq	2296(%rsp), %rbp
	movq	%rbp, 2280(%rsp)
	movl	$7889261, 2296(%rsp)            # imm = 0x78616D
	movq	$3, 2288(%rsp)
	vmovsd	.LCPI23_25(%rip), %xmm0         # xmm0 = mem[0],zero
	vmovsd	%xmm0, 544(%rsp)
	fldl	544(%rsp)
	fstpt	32(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp1049:
	movl	$_ZSt4cout, %edi
	movl	$3, %edx
	movq	%rbp, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1050:
# %bb.142:
	fldt	32(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movabsq	$9218868437227405311, %rsi      # imm = 0x7FEFFFFFFFFFFFFF
	movl	$.L.str.8, %edi
	xorl	%eax, %eax
	callq	printf
	movq	2280(%rsp), %rdi
	cmpq	%rbp, %rdi
	je	.LBB23_144
# %bb.143:
	callq	_ZdlPv
.LBB23_144:
	leaq	1784(%rsp), %rbx
	movq	%rbx, 1768(%rsp)
	movl	$7889261, 1784(%rsp)            # imm = 0x78616D
	movq	$3, 1776(%rsp)
	vmovups	.LCPI23_26(%rip), %xmm0         # xmm0 = [1.7976931348623157E+308,1.9958403095347196E+292]
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm0, 96(%rsp)                 # AlignMOV convert to UnAlignMOV 
.Ltmp1052:
	leaq	1768(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1053:
# %bb.145:
	movq	1768(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_147
# %bb.146:
	callq	_ZdlPv
.LBB23_147:
	vmovups	.LCPI23_26(%rip), %xmm0         # xmm0 = [1.7976931348623157E+308,1.9958403095347196E+292]
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm0, 96(%rsp)                 # AlignMOV convert to UnAlignMOV 
	leaq	1752(%rsp), %rbx
	movq	%rbx, 1736(%rsp)
	movl	$7889261, 1752(%rsp)            # imm = 0x78616D
	movq	$3, 1744(%rsp)
.Ltmp1055:
	leaq	1736(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1056:
# %bb.148:
	movq	1736(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_150
# %bb.149:
	callq	_ZdlPv
.LBB23_150:
	vmovsd	96(%rsp), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	104(%rsp), %xmm1                # xmm1 = mem[0],zero
	vaddsd	%xmm1, %xmm0, %xmm2
	vmovsd	%xmm2, 96(%rsp)
	vsubsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovsd	%xmm0, 104(%rsp)
	leaq	1720(%rsp), %rbx
	movq	%rbx, 1704(%rsp)
	movl	$7889261, 1720(%rsp)            # imm = 0x78616D
	movq	$3, 1712(%rsp)
.Ltmp1058:
	leaq	1704(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1059:
# %bb.151:
	movq	1704(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_153
# %bb.152:
	callq	_ZdlPv
.LBB23_153:
	movl	$8388608, %eax                  # imm = 0x800000
	vcvtsi2sd	%eax, %xmm3, %xmm0
	vmovsd	.LCPI23_27(%rip), %xmm1         # xmm1 = mem[0],zero
	vdivsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	movl	$16777216, %eax                 # imm = 0x1000000
	vcvtsi2sd	%eax, %xmm3, %xmm1
	vmovsd	.LCPI23_28(%rip), %xmm2         # xmm2 = mem[0],zero
	vdivsd	%xmm1, %xmm2, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vmovss	%xmm0, 24(%rsp)
	vmovss	%xmm1, 28(%rsp)
	leaq	512(%rsp), %rbp
	movq	%rbp, 496(%rsp)
	movabsq	$7450482244226082164, %rbx      # imm = 0x67656E2074736574
	movq	%rbx, 512(%rsp)
	movabsq	$7311146993653867886, %r15      # imm = 0x657669746167656E
	movq	%r15, 517(%rsp)
	movq	$13, 504(%rsp)
	movb	$0, 525(%rsp)
.Ltmp1061:
	leaq	496(%rsp), %rdi
	leaq	24(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1062:
# %bb.154:
	movq	496(%rsp), %rdi
	cmpq	%rbp, %rdi
	je	.LBB23_156
# %bb.155:
	callq	_ZdlPv
.LBB23_156:
	vmovss	24(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vmovss	28(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm1, %xmm0, %xmm2
	vmovss	%xmm2, 24(%rsp)
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 28(%rsp)
	leaq	480(%rsp), %rbp
	movq	%rbp, 464(%rsp)
	movq	%rbx, 480(%rsp)
	movq	%r15, 485(%rsp)
	movq	$13, 472(%rsp)
	movb	$0, 493(%rsp)
.Ltmp1064:
	leaq	464(%rsp), %rdi
	leaq	24(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1065:
# %bb.157:
	movq	464(%rsp), %rdi
	cmpq	%rbp, %rdi
	je	.LBB23_159
# %bb.158:
	callq	_ZdlPv
.LBB23_159:
	vmovss	24(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vmovss	28(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm1, %xmm0, %xmm2
	vmovss	%xmm2, 24(%rsp)
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 28(%rsp)
	leaq	448(%rsp), %rbp
	movq	%rbp, 432(%rsp)
	movq	%rbx, 448(%rsp)
	movq	%r15, 453(%rsp)
	movq	$13, 440(%rsp)
	movb	$0, 461(%rsp)
.Ltmp1067:
	leaq	432(%rsp), %rdi
	leaq	24(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1068:
# %bb.160:
	movq	432(%rsp), %rdi
	cmpq	%rbp, %rdi
	je	.LBB23_162
# %bb.161:
	callq	_ZdlPv
.LBB23_162:
	leaq	1032(%rsp), %rbx
	movq	%rbx, 1016(%rsp)
	movl	$1953657203, 1032(%rsp)         # imm = 0x74727173
	movq	$4, 1024(%rsp)
	vmovss	24(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI23_29(%rip), %xmm4         # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm2
	vsubss	%xmm2, %xmm5, %xmm3
	vsubss	%xmm3, %xmm4, %xmm3
	vsubss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vxorps	%xmm2, %xmm2, %xmm2
	vaddss	28(%rsp), %xmm2, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm1, %xmm5, %xmm3
	vucomiss	%xmm2, %xmm3
	movb	$0, 1036(%rsp)
	setnp	%al
	sete	%cl
	testb	%al, %cl
	je	.LBB23_164
# %bb.163:
	vinsertps	$16, %xmm1, %xmm5, %xmm0 # xmm0 = xmm5[0],xmm1[0],xmm5[2,3]
	jmp	.LBB23_168
.LBB23_164:
	vucomiss	%xmm2, %xmm5
	vinsertps	$16, %xmm1, %xmm5, %xmm0 # xmm0 = xmm5[0],xmm1[0],xmm5[2,3]
	jne	.LBB23_167
# %bb.165:
	jp	.LBB23_167
# %bb.166:
	vinsertps	$16, %xmm5, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm5[0],xmm1[2,3]
.LBB23_167:
	vmovups	%xmm0, 32(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovaps	%xmm0, %xmm1
	vmovups	32(%rsp), %xmm2                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vfnmadd213ss	%xmm2, %xmm0, %xmm1     # xmm1 = -(xmm0 * xmm1) + xmm2
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm0, %xmm2
	vdivss	%xmm2, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
.LBB23_168:
	vmovlps	%xmm0, 96(%rsp)
.Ltmp1070:
	leaq	1016(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1071:
# %bb.169:
	movq	1016(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_171
# %bb.170:
	callq	_ZdlPv
.LBB23_171:
	leaq	1000(%rsp), %rbx
	movq	%rbx, 984(%rsp)
	movl	$1953657203, 1000(%rsp)         # imm = 0x74727173
	movq	$4, 992(%rsp)
	movb	$0, 1004(%rsp)
	vmovss	24(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vmovss	.LCPI23_29(%rip), %xmm4         # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm4, %xmm1, %xmm2
	vsubss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm3, %xmm4, %xmm3
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vaddss	28(%rsp), %xmm2, %xmm3
	vaddss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm3, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm1
	vucomiss	%xmm2, %xmm0
	vaddss	%xmm3, %xmm1, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	je	.LBB23_173
# %bb.172:
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	jmp	.LBB23_174
.LBB23_173:
	vmovups	%xmm0, 64(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm1, 32(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovups	64(%rsp), %xmm1                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vfnmadd231ss	%xmm0, %xmm0, %xmm1     # xmm1 = -(xmm0 * xmm0) + xmm1
	vaddss	32(%rsp), %xmm1, %xmm1          # 16-byte Folded Reload
	vaddss	%xmm0, %xmm0, %xmm2
	vdivss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
.LBB23_174:
	vmovlps	%xmm0, 96(%rsp)
.Ltmp1073:
	leaq	984(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1074:
# %bb.175:
	movq	984(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_177
# %bb.176:
	callq	_ZdlPv
.LBB23_177:
	leaq	968(%rsp), %rbx
	movq	%rbx, 952(%rsp)
	movl	$1953657203, 968(%rsp)          # imm = 0x74727173
	movq	$4, 960(%rsp)
	movb	$0, 972(%rsp)
	vmovss	24(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vmovss	28(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI23_29(%rip), %xmm5         # xmm5 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm5, %xmm4
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm0
	vxorps	%xmm3, %xmm3, %xmm3
	vaddss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm3, %xmm6
	vsubss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm6, %xmm1
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm0, %xmm2
	vaddss	%xmm2, %xmm4, %xmm0
	vsubss	%xmm0, %xmm4, %xmm1
	vucomiss	%xmm3, %xmm0
	vaddss	%xmm2, %xmm1, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	je	.LBB23_179
# %bb.178:
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	jmp	.LBB23_180
.LBB23_179:
	vmovups	%xmm1, 32(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm0, 64(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovss	.LCPI23_30(%rip), %xmm1         # xmm1 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm1, %xmm0
	vmovups	64(%rsp), %xmm7                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmulss	%xmm0, %xmm7, %xmm1
	vmulss	%xmm1, %xmm1, %xmm2
	vbroadcastss	.LCPI23_15(%rip), %xmm3 # xmm3 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm3, %xmm2, %xmm4
	vmovaps	%xmm1, %xmm5
	vfmsub213ss	%xmm2, %xmm1, %xmm5     # xmm5 = (xmm1 * xmm5) - xmm2
	vxorps	%xmm3, %xmm5, %xmm3
	vsubss	%xmm2, %xmm7, %xmm2
	vsubss	%xmm7, %xmm2, %xmm6
	vmovaps	%xmm7, %xmm8
	vsubss	%xmm6, %xmm2, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vmovups	32(%rsp), %xmm7                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm5, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm6
	vmovaps	%xmm7, %xmm8
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm7, %xmm3
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vmulss	.LCPI23_31(%rip), %xmm0, %xmm0
	vmulss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm0, %xmm1, %xmm2
	vsubss	%xmm1, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm1, %xmm1
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
.LBB23_180:
	vmovlps	%xmm0, 96(%rsp)
.Ltmp1076:
	leaq	952(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1077:
# %bb.181:
	movq	952(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_183
# %bb.182:
	callq	_ZdlPv
.LBB23_183:
	leaq	936(%rsp), %rbx
	movq	%rbx, 920(%rsp)
	movl	$1953657203, 936(%rsp)          # imm = 0x74727173
	movq	$4, 928(%rsp)
	movb	$0, 940(%rsp)
	vmovss	.LCPI23_30(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vdivss	.LCPI23_32(%rip), %xmm0, %xmm0
	vmovss	.LCPI23_29(%rip), %xmm7         # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm7, %xmm1
	vmulss	%xmm1, %xmm1, %xmm2
	vxorps	80(%rsp), %xmm2, %xmm3          # 16-byte Folded Reload
	vmovaps	%xmm1, %xmm4
	vfmsub213ss	%xmm2, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm4) - xmm2
	vsubss	%xmm2, %xmm7, %xmm2
	vsubss	%xmm7, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmulss	.LCPI23_31(%rip), %xmm0, %xmm0
	vmulss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm0, %xmm1, %xmm2
	vsubss	%xmm1, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm1, %xmm1
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
	vmovlps	%xmm0, 96(%rsp)
.Ltmp1079:
	leaq	920(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1080:
# %bb.184:
	movq	920(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_186
# %bb.185:
	callq	_ZdlPv
.LBB23_186:
	vmovss	.LCPI23_33(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vdivss	.LCPI23_32(%rip), %xmm0, %xmm4
	movl	$3, %eax
	vcvtsi2ss	%eax, %xmm9, %xmm1
	vmovss	.LCPI23_31(%rip), %xmm3         # xmm3 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm1, %xmm2
	vfmsub213ss	%xmm2, %xmm1, %xmm3     # xmm3 = (xmm1 * xmm3) - xmm2
	vxorps	%xmm0, %xmm0, %xmm0
	vmulss	%xmm0, %xmm1, %xmm5
	vxorps	%xmm6, %xmm6, %xmm6
	vfmsub213ss	%xmm5, %xmm1, %xmm6     # xmm6 = (xmm1 * xmm6) - xmm5
	vaddss	%xmm5, %xmm3, %xmm7
	vsubss	%xmm3, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm6, %xmm3, %xmm3
	vfmadd231ss	%xmm1, %xmm0, %xmm3     # xmm3 = (xmm0 * xmm1) + xmm3
	vaddss	%xmm3, %xmm7, %xmm5
	vsubss	%xmm5, %xmm7, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vmovss	%xmm1, 32(%rsp)                 # 4-byte Spill
	vaddss	%xmm5, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm5, %xmm2, %xmm1
	vmovss	%xmm1, 64(%rsp)                 # 4-byte Spill
	vmulss	%xmm4, %xmm4, %xmm6
	vmovaps	%xmm4, %xmm5
	vfmsub213ss	%xmm6, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm5) - xmm6
	vmulss	%xmm0, %xmm4, %xmm7
	vxorps	%xmm8, %xmm8, %xmm8
	vfmsub213ss	%xmm7, %xmm4, %xmm8     # xmm8 = (xmm4 * xmm8) - xmm7
	vmulss	%xmm4, %xmm0, %xmm9
	vmovaps	%xmm4, %xmm10
	vfmsub213ss	%xmm9, %xmm0, %xmm10    # xmm10 = (xmm0 * xmm10) - xmm9
	vaddss	%xmm7, %xmm5, %xmm11
	vsubss	%xmm5, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm5, %xmm5
	vsubss	%xmm12, %xmm7, %xmm7
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm9, %xmm11, %xmm7
	vsubss	%xmm11, %xmm7, %xmm12
	vsubss	%xmm12, %xmm7, %xmm13
	vsubss	%xmm13, %xmm11, %xmm11
	vsubss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm9, %xmm11, %xmm9
	vfmadd231ss	%xmm0, %xmm0, %xmm9     # xmm9 = (xmm0 * xmm0) + xmm9
	vaddss	%xmm5, %xmm9, %xmm5
	vfmadd231ss	%xmm0, %xmm4, %xmm8     # xmm8 = (xmm4 * xmm0) + xmm8
	vaddss	%xmm5, %xmm8, %xmm5
	vfmadd231ss	%xmm4, %xmm0, %xmm10    # xmm10 = (xmm0 * xmm4) + xmm10
	vxorps	%xmm2, %xmm2, %xmm2
	vaddss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm5, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm7
	vaddss	%xmm5, %xmm7, %xmm5
	vaddss	%xmm6, %xmm8, %xmm9
	vsubss	%xmm9, %xmm6, %xmm6
	vaddss	%xmm6, %xmm8, %xmm7
	vmovss	.LCPI23_34(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm9, %xmm6
	vmulss	%xmm2, %xmm9, %xmm8
	vmovaps	%xmm9, %xmm10
	vfmsub213ss	%xmm8, %xmm2, %xmm10    # xmm10 = (xmm2 * xmm10) - xmm8
	vfmadd231ss	%xmm9, %xmm2, %xmm10    # xmm10 = (xmm2 * xmm9) + xmm10
	vfmsub213ss	%xmm6, %xmm0, %xmm9     # xmm9 = (xmm0 * xmm9) - xmm6
	vmulss	%xmm7, %xmm0, %xmm11
	vaddss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm9, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm9, %xmm9
	vaddss	%xmm8, %xmm12, %xmm14
	vsubss	%xmm12, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm1
	vsubss	%xmm1, %xmm12, %xmm1
	vsubss	%xmm15, %xmm8, %xmm8
	vaddss	%xmm1, %xmm8, %xmm1
	vfmadd231ss	%xmm7, %xmm2, %xmm1     # xmm1 = (xmm2 * xmm7) + xmm1
	vfmsub213ss	%xmm11, %xmm0, %xmm7    # xmm7 = (xmm0 * xmm7) - xmm11
	vsubss	%xmm13, %xmm11, %xmm8
	vaddss	%xmm8, %xmm9, %xmm8
	vaddss	%xmm1, %xmm8, %xmm1
	vfmadd231ss	%xmm5, %xmm0, %xmm7     # xmm7 = (xmm0 * xmm5) + xmm7
	vaddss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm10, %xmm1
	vaddss	%xmm1, %xmm14, %xmm5
	vsubss	%xmm5, %xmm14, %xmm7
	vaddss	%xmm1, %xmm7, %xmm1
	vaddss	%xmm5, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm5, %xmm6, %xmm5
	vbroadcastss	.LCPI23_15(%rip), %xmm8 # xmm8 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm7, %xmm8, %xmm6
	vxorps	%xmm5, %xmm8, %xmm8
	vsubss	%xmm7, %xmm3, %xmm7
	vsubss	%xmm3, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm10
	vsubss	%xmm10, %xmm3, %xmm10
	vsubss	%xmm9, %xmm6, %xmm6
	vaddss	%xmm6, %xmm10, %xmm6
	vmovss	64(%rsp), %xmm11                # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm11, %xmm5
	vsubss	%xmm11, %xmm5, %xmm9
	vsubss	%xmm9, %xmm5, %xmm10
	vsubss	%xmm10, %xmm11, %xmm10
	vsubss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm8, %xmm10, %xmm8
	vaddss	%xmm6, %xmm5, %xmm9
	vsubss	%xmm5, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm5, %xmm5
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	32(%rsp), %xmm6                 # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm6, %xmm1
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm9, %xmm5
	vsubss	%xmm5, %xmm9, %xmm6
	vaddss	%xmm1, %xmm6, %xmm1
	vaddss	%xmm5, %xmm7, %xmm6
	vsubss	%xmm6, %xmm7, %xmm7
	vaddss	%xmm5, %xmm7, %xmm7
	vmulss	%xmm4, %xmm6, %xmm5
	vmulss	%xmm4, %xmm7, %xmm8
	vmovaps	%xmm4, %xmm9
	vfmsub213ss	%xmm8, %xmm7, %xmm9     # xmm9 = (xmm7 * xmm9) - xmm8
	vfmadd231ss	%xmm1, %xmm4, %xmm9     # xmm9 = (xmm4 * xmm1) + xmm9
	vfmsub213ss	%xmm5, %xmm6, %xmm4     # xmm4 = (xmm6 * xmm4) - xmm5
	vmulss	%xmm2, %xmm6, %xmm1
	vaddss	%xmm1, %xmm4, %xmm10
	vsubss	%xmm4, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm4, %xmm4
	vxorps	%xmm12, %xmm12, %xmm12
	vfmsub213ss	%xmm1, %xmm6, %xmm12    # xmm12 = (xmm6 * xmm12) - xmm1
	vsubss	%xmm11, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm8, %xmm10, %xmm4
	vsubss	%xmm10, %xmm4, %xmm11
	vsubss	%xmm11, %xmm4, %xmm13
	vsubss	%xmm13, %xmm10, %xmm10
	vsubss	%xmm11, %xmm8, %xmm8
	vaddss	%xmm8, %xmm10, %xmm8
	vfmadd231ss	%xmm7, %xmm2, %xmm8     # xmm8 = (xmm2 * xmm7) + xmm8
	vaddss	%xmm1, %xmm8, %xmm1
	vfmadd231ss	%xmm6, %xmm2, %xmm12    # xmm12 = (xmm2 * xmm6) + xmm12
	vaddss	%xmm1, %xmm12, %xmm1
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm1, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm1, %xmm4, %xmm4
	vaddss	%xmm7, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm1
	vaddss	%xmm7, %xmm1, %xmm5
	vmulss	%xmm6, %xmm6, %xmm1
	vmovaps	%xmm6, %xmm7
	vfmsub213ss	%xmm1, %xmm6, %xmm7     # xmm7 = (xmm6 * xmm7) - xmm1
	vmulss	%xmm5, %xmm6, %xmm8
	vaddss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm7, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vmovaps	%xmm5, %xmm11
	vfmsub213ss	%xmm8, %xmm6, %xmm11    # xmm11 = (xmm6 * xmm11) - xmm8
	vsubss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vmulss	%xmm6, %xmm5, %xmm8
	vaddss	%xmm8, %xmm9, %xmm10
	vsubss	%xmm9, %xmm10, %xmm12
	vsubss	%xmm12, %xmm10, %xmm13
	vsubss	%xmm13, %xmm9, %xmm9
	vmovaps	%xmm6, %xmm13
	vfmsub213ss	%xmm8, %xmm5, %xmm13    # xmm13 = (xmm5 * xmm13) - xmm8
	vsubss	%xmm12, %xmm8, %xmm8
	vaddss	%xmm8, %xmm9, %xmm8
	vfmadd231ss	%xmm5, %xmm5, %xmm8     # xmm8 = (xmm5 * xmm5) + xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vmovss	%xmm4, 160(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm4, %xmm6, %xmm11    # xmm11 = (xmm6 * xmm4) + xmm11
	vaddss	%xmm7, %xmm11, %xmm7
	vfmadd231ss	%xmm6, %xmm4, %xmm13    # xmm13 = (xmm4 * xmm6) + xmm13
	vaddss	%xmm7, %xmm13, %xmm7
	vaddss	%xmm7, %xmm10, %xmm8
	vsubss	%xmm8, %xmm10, %xmm9
	vaddss	%xmm7, %xmm9, %xmm2
	vmovss	%xmm2, 136(%rsp)                # 4-byte Spill
	vaddss	%xmm1, %xmm8, %xmm9
	vsubss	%xmm9, %xmm1, %xmm1
	vaddss	%xmm1, %xmm8, %xmm8
	vxorps	%xmm2, %xmm2, %xmm2
	vmulss	%xmm2, %xmm9, %xmm1
	vmovaps	%xmm9, %xmm10
	vfmsub213ss	%xmm1, %xmm2, %xmm10    # xmm10 = (xmm2 * xmm10) - xmm1
	vfmadd231ss	%xmm9, %xmm2, %xmm10    # xmm10 = (xmm2 * xmm9) + xmm10
	vxorps	%xmm7, %xmm7, %xmm7
	vmovaps	%xmm0, %xmm2
	vmulss	%xmm0, %xmm9, %xmm11
	vfmsub213ss	%xmm11, %xmm0, %xmm9    # xmm9 = (xmm0 * xmm9) - xmm11
	vmulss	%xmm0, %xmm8, %xmm12
	vaddss	%xmm12, %xmm9, %xmm13
	vsubss	%xmm9, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm9, %xmm9
	vaddss	%xmm1, %xmm13, %xmm15
	vsubss	%xmm13, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm4
	vsubss	%xmm4, %xmm13, %xmm4
	vsubss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm4, %xmm0
	vfmadd231ss	%xmm8, %xmm7, %xmm0     # xmm0 = (xmm7 * xmm8) + xmm0
	vfmsub213ss	%xmm12, %xmm2, %xmm8    # xmm8 = (xmm2 * xmm8) - xmm12
	vsubss	%xmm14, %xmm12, %xmm1
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vfmadd231ss	136(%rsp), %xmm2, %xmm8 # 4-byte Folded Reload
                                        # xmm8 = (xmm2 * mem) + xmm8
	vmovaps	%xmm2, %xmm13
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm0, %xmm15, %xmm1
	vsubss	%xmm1, %xmm15, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm1, %xmm11, %xmm4
	vsubss	%xmm4, %xmm11, %xmm7
	vaddss	%xmm1, %xmm7, %xmm1
	vsubss	%xmm4, %xmm3, %xmm7
	vsubss	%xmm3, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vbroadcastss	.LCPI23_15(%rip), %xmm10 # xmm10 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm4, %xmm10, %xmm4
	vsubss	%xmm8, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vmovss	64(%rsp), %xmm2                 # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm8
	vsubss	%xmm8, %xmm4, %xmm9
	vsubss	%xmm9, %xmm2, %xmm2
	vxorps	%xmm1, %xmm10, %xmm1
	vsubss	%xmm8, %xmm1, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm3, %xmm4, %xmm2
	vsubss	%xmm4, %xmm2, %xmm8
	vsubss	%xmm8, %xmm2, %xmm9
	vsubss	%xmm9, %xmm4, %xmm4
	vsubss	%xmm8, %xmm3, %xmm3
	vaddss	%xmm3, %xmm4, %xmm3
	vmovss	32(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm7, %xmm2
	vsubss	%xmm2, %xmm7, %xmm3
	vaddss	%xmm1, %xmm3, %xmm3
	vmulss	%xmm6, %xmm2, %xmm1
	vmulss	%xmm6, %xmm3, %xmm4
	vmovaps	%xmm6, %xmm7
	vfmsub213ss	%xmm4, %xmm3, %xmm7     # xmm7 = (xmm3 * xmm7) - xmm4
	vfmadd231ss	%xmm0, %xmm6, %xmm7     # xmm7 = (xmm6 * xmm0) + xmm7
	vfmsub213ss	%xmm1, %xmm2, %xmm6     # xmm6 = (xmm2 * xmm6) - xmm1
	vmulss	%xmm5, %xmm2, %xmm0
	vaddss	%xmm0, %xmm6, %xmm8
	vaddss	%xmm4, %xmm8, %xmm9
	vsubss	%xmm8, %xmm9, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vsubss	%xmm10, %xmm9, %xmm10
	vsubss	%xmm10, %xmm8, %xmm10
	vaddss	%xmm4, %xmm10, %xmm4
	vfmadd231ss	%xmm5, %xmm3, %xmm4     # xmm4 = (xmm3 * xmm5) + xmm4
	vfmsub213ss	%xmm0, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm5) - xmm0
	vsubss	%xmm6, %xmm8, %xmm3
	vsubss	%xmm3, %xmm8, %xmm8
	vsubss	%xmm8, %xmm6, %xmm6
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vfmadd231ss	160(%rsp), %xmm2, %xmm5 # 4-byte Folded Reload
                                        # xmm5 = (xmm2 * mem) + xmm5
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm3
	vsubss	%xmm3, %xmm9, %xmm2
	vaddss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm3, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm3, %xmm1, %xmm3
	vmulss	%xmm0, %xmm13, %xmm1
	vxorps	%xmm6, %xmm6, %xmm6
	vmulss	%xmm0, %xmm6, %xmm4
	vmovaps	%xmm0, %xmm5
	vfmsub213ss	%xmm4, %xmm6, %xmm5     # xmm5 = (xmm6 * xmm5) - xmm4
	vxorps	%xmm12, %xmm12, %xmm12
	vfmadd231ss	%xmm0, %xmm12, %xmm5    # xmm5 = (xmm12 * xmm0) + xmm5
	vfmsub213ss	%xmm1, %xmm13, %xmm0    # xmm0 = (xmm13 * xmm0) - xmm1
	vmulss	%xmm3, %xmm13, %xmm6
	vaddss	%xmm6, %xmm0, %xmm7
	vsubss	%xmm0, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm0, %xmm0
	vaddss	%xmm4, %xmm7, %xmm9
	vsubss	%xmm7, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vfmadd231ss	%xmm3, %xmm12, %xmm4    # xmm4 = (xmm12 * xmm3) + xmm4
	vfmsub213ss	%xmm6, %xmm13, %xmm3    # xmm3 = (xmm13 * xmm3) - xmm6
	vsubss	%xmm8, %xmm6, %xmm6
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vfmadd231ss	%xmm2, %xmm13, %xmm3    # xmm3 = (xmm13 * xmm2) + xmm3
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm2
	vsubss	%xmm2, %xmm9, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	movl	$2, %eax
	vxorps	%xmm14, %xmm14, %xmm14
	vcvtsi2ss	%eax, %xmm14, %xmm2
	vmulss	.LCPI23_33(%rip), %xmm2, %xmm2
	vmulss	%xmm2, %xmm3, %xmm3
	vmulss	%xmm2, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm3, %xmm1 # xmm1 = xmm3[0],xmm1[0],xmm3[2,3]
	vmulss	%xmm2, %xmm0, %xmm0
	vmovlps	%xmm1, 96(%rsp)
	vmovss	%xmm0, 104(%rsp)
	leaq	904(%rsp), %rbx
	movq	%rbx, 888(%rsp)
	movl	$1953657203, 904(%rsp)          # imm = 0x74727173
	movq	$4, 896(%rsp)
	movb	$0, 908(%rsp)
.Ltmp1082:
	leaq	888(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1083:
# %bb.187:
	movq	888(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_189
# %bb.188:
	callq	_ZdlPv
.LBB23_189:
	leaq	872(%rsp), %rbx
	movq	%rbx, 856(%rsp)
	movl	$1953657203, 872(%rsp)          # imm = 0x74727173
	movq	$4, 864(%rsp)
	vmovss	.LCPI23_33(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vdivss	.LCPI23_32(%rip), %xmm0, %xmm5
	movl	$3, %eax
	vcvtsi2ss	%eax, %xmm13, %xmm1
	movb	$0, 876(%rsp)
	vmovss	.LCPI23_31(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm1, %xmm3
	vmovaps	%xmm0, %xmm2
	vfmsub213ss	%xmm3, %xmm1, %xmm2     # xmm2 = (xmm1 * xmm2) - xmm3
	vxorps	%xmm14, %xmm14, %xmm14
	vmulss	%xmm1, %xmm14, %xmm4
	vxorps	%xmm6, %xmm6, %xmm6
	vfmsub213ss	%xmm4, %xmm1, %xmm6     # xmm6 = (xmm1 * xmm6) - xmm4
	vaddss	%xmm4, %xmm2, %xmm7
	vsubss	%xmm2, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm2, %xmm2
	vsubss	%xmm8, %xmm4, %xmm8
	vaddss	%xmm2, %xmm8, %xmm8
	vaddss	%xmm7, %xmm14, %xmm2
	vsubss	%xmm7, %xmm2, %xmm9
	vsubss	%xmm9, %xmm2, %xmm10
	vsubss	%xmm10, %xmm7, %xmm7
	vsubss	%xmm9, %xmm14, %xmm9
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm8, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm8, %xmm8
	vsubss	%xmm10, %xmm7, %xmm7
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm6, %xmm7, %xmm7
	vaddss	%xmm7, %xmm14, %xmm7
	vaddss	%xmm7, %xmm14, %xmm7
	vaddss	%xmm6, %xmm14, %xmm8
	vsubss	%xmm6, %xmm8, %xmm10
	vsubss	%xmm10, %xmm8, %xmm11
	vsubss	%xmm11, %xmm6, %xmm6
	vsubss	%xmm10, %xmm14, %xmm10
	vaddss	%xmm6, %xmm10, %xmm6
	vaddss	%xmm4, %xmm8, %xmm10
	vsubss	%xmm8, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vsubss	%xmm11, %xmm4, %xmm4
	vaddss	%xmm4, %xmm8, %xmm4
	vaddss	%xmm9, %xmm14, %xmm8
	vsubss	%xmm14, %xmm8, %xmm11
	vsubss	%xmm11, %xmm8, %xmm12
	vsubss	%xmm12, %xmm14, %xmm12
	vsubss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm9, %xmm12, %xmm9
	vaddss	%xmm8, %xmm10, %xmm11
	vsubss	%xmm10, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm10, %xmm10
	vsubss	%xmm12, %xmm8, %xmm8
	vaddss	%xmm8, %xmm10, %xmm8
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm6, %xmm14, %xmm6
	vaddss	%xmm4, %xmm6, %xmm4
	vaddss	%xmm4, %xmm9, %xmm4
	vaddss	%xmm4, %xmm8, %xmm4
	vfmadd231ss	%xmm0, %xmm14, %xmm4    # xmm4 = (xmm14 * xmm0) + xmm4
	vfmadd231ss	%xmm14, %xmm14, %xmm4   # xmm4 = (xmm14 * xmm14) + xmm4
	vfmadd231ss	%xmm14, %xmm14, %xmm4   # xmm4 = (xmm14 * xmm14) + xmm4
	vfmadd231ss	%xmm1, %xmm14, %xmm4    # xmm4 = (xmm14 * xmm1) + xmm4
	vxorps	%xmm0, %xmm0, %xmm0
	vaddss	%xmm4, %xmm11, %xmm6
	vsubss	%xmm6, %xmm11, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vmovss	%xmm1, 32(%rsp)                 # 4-byte Spill
	vaddss	%xmm6, %xmm2, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm6, %xmm2, %xmm1
	vmovss	%xmm1, 64(%rsp)                 # 4-byte Spill
	vaddss	%xmm7, %xmm3, %xmm1
	vmovss	%xmm1, 136(%rsp)                # 4-byte Spill
	vsubss	%xmm1, %xmm3, %xmm3
	vaddss	%xmm7, %xmm3, %xmm1
	vmovss	%xmm1, 160(%rsp)                # 4-byte Spill
	vmulss	%xmm5, %xmm5, %xmm6
	vmovaps	%xmm5, %xmm8
	vfmsub213ss	%xmm6, %xmm5, %xmm8     # xmm8 = (xmm5 * xmm8) - xmm6
	vmulss	%xmm0, %xmm5, %xmm7
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm7, %xmm5, %xmm10    # xmm10 = (xmm5 * xmm10) - xmm7
	vmulss	%xmm5, %xmm0, %xmm9
	vaddss	%xmm7, %xmm8, %xmm11
	vsubss	%xmm8, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm8, %xmm8
	vmovaps	%xmm5, %xmm13
	vfmsub213ss	%xmm9, %xmm0, %xmm13    # xmm13 = (xmm0 * xmm13) - xmm9
	vsubss	%xmm12, %xmm7, %xmm12
	vaddss	%xmm12, %xmm8, %xmm12
	vaddss	%xmm9, %xmm11, %xmm8
	vsubss	%xmm11, %xmm8, %xmm14
	vsubss	%xmm14, %xmm8, %xmm15
	vsubss	%xmm15, %xmm11, %xmm11
	vsubss	%xmm14, %xmm9, %xmm14
	vaddss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm11, %xmm12, %xmm14
	vsubss	%xmm12, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm1
	vsubss	%xmm1, %xmm12, %xmm1
	vsubss	%xmm15, %xmm11, %xmm11
	vaddss	%xmm1, %xmm11, %xmm1
	vaddss	%xmm1, %xmm10, %xmm1
	vaddss	%xmm13, %xmm10, %xmm11
	vsubss	%xmm10, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vaddss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm1, %xmm13, %xmm1
	vsubss	%xmm12, %xmm13, %xmm12
	vaddss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm0, %xmm9, %xmm12
	vsubss	%xmm0, %xmm12, %xmm13
	vsubss	%xmm13, %xmm9, %xmm9
	vsubss	%xmm13, %xmm12, %xmm13
	vsubss	%xmm13, %xmm0, %xmm13
	vaddss	%xmm9, %xmm13, %xmm9
	vaddss	%xmm7, %xmm11, %xmm13
	vsubss	%xmm11, %xmm13, %xmm15
	vsubss	%xmm15, %xmm13, %xmm2
	vsubss	%xmm2, %xmm11, %xmm2
	vsubss	%xmm15, %xmm7, %xmm7
	vaddss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm14, %xmm12, %xmm7
	vsubss	%xmm12, %xmm7, %xmm11
	vsubss	%xmm11, %xmm7, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm11, %xmm14, %xmm11
	vaddss	%xmm11, %xmm12, %xmm11
	vaddss	%xmm7, %xmm13, %xmm12
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm12, %xmm15
	vsubss	%xmm15, %xmm13, %xmm13
	vsubss	%xmm14, %xmm7, %xmm7
	vaddss	%xmm7, %xmm13, %xmm7
	vaddss	%xmm1, %xmm10, %xmm1
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm11, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vfmadd231ss	%xmm5, %xmm0, %xmm1     # xmm1 = (xmm0 * xmm5) + xmm1
	vfmadd231ss	%xmm0, %xmm0, %xmm1     # xmm1 = (xmm0 * xmm0) + xmm1
	vfmadd231ss	%xmm0, %xmm0, %xmm1     # xmm1 = (xmm0 * xmm0) + xmm1
	vfmadd231ss	%xmm0, %xmm5, %xmm1     # xmm1 = (xmm5 * xmm0) + xmm1
	vaddss	%xmm1, %xmm12, %xmm2
	vsubss	%xmm2, %xmm12, %xmm7
	vaddss	%xmm1, %xmm7, %xmm0
	vmovss	%xmm0, 56(%rsp)                 # 4-byte Spill
	vaddss	%xmm2, %xmm8, %xmm1
	vsubss	%xmm1, %xmm8, %xmm8
	vaddss	%xmm2, %xmm8, %xmm8
	vaddss	%xmm1, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm2
	vaddss	%xmm1, %xmm2, %xmm10
	vmovss	.LCPI23_34(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm9, %xmm6
	vmovaps	%xmm9, %xmm1
	vfmsub213ss	%xmm6, %xmm0, %xmm1     # xmm1 = (xmm0 * xmm1) - xmm6
	vmulss	%xmm0, %xmm10, %xmm2
	vaddss	%xmm2, %xmm1, %xmm12
	vsubss	%xmm1, %xmm12, %xmm11
	vsubss	%xmm11, %xmm12, %xmm13
	vsubss	%xmm13, %xmm1, %xmm1
	vmovaps	%xmm10, %xmm13
	vfmsub213ss	%xmm2, %xmm0, %xmm13    # xmm13 = (xmm0 * xmm13) - xmm2
	vsubss	%xmm11, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vxorps	%xmm7, %xmm7, %xmm7
	vmulss	%xmm7, %xmm9, %xmm2
	vaddss	%xmm2, %xmm12, %xmm11
	vsubss	%xmm12, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm2, %xmm14
	vaddss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm1, %xmm12, %xmm14
	vsubss	%xmm1, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm3
	vsubss	%xmm3, %xmm1, %xmm1
	vsubss	%xmm15, %xmm12, %xmm3
	vaddss	%xmm3, %xmm1, %xmm1
	vmulss	%xmm0, %xmm8, %xmm3
	vmovaps	%xmm8, %xmm12
	vfmsub213ss	%xmm3, %xmm0, %xmm12    # xmm12 = (xmm0 * xmm12) - xmm3
	vaddss	%xmm1, %xmm12, %xmm1
	vmulss	%xmm7, %xmm10, %xmm12
	vmovaps	%xmm10, %xmm15
	vfmsub213ss	%xmm12, %xmm7, %xmm15   # xmm15 = (xmm7 * xmm15) - xmm12
	vaddss	%xmm1, %xmm15, %xmm4
	vmovaps	%xmm9, %xmm15
	vfmsub213ss	%xmm2, %xmm7, %xmm15    # xmm15 = (xmm7 * xmm15) - xmm2
	vaddss	%xmm15, %xmm13, %xmm0
	vsubss	%xmm13, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm7
	vsubss	%xmm7, %xmm13, %xmm7
	vaddss	%xmm4, %xmm15, %xmm4
	vsubss	%xmm1, %xmm15, %xmm1
	vaddss	%xmm1, %xmm7, %xmm1
	vaddss	%xmm2, %xmm12, %xmm7
	vsubss	%xmm12, %xmm7, %xmm13
	vsubss	%xmm13, %xmm7, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm13, %xmm2, %xmm2
	vaddss	%xmm2, %xmm12, %xmm2
	vaddss	%xmm3, %xmm0, %xmm12
	vsubss	%xmm0, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm15
	vsubss	%xmm15, %xmm0, %xmm0
	vsubss	%xmm13, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm7, %xmm14, %xmm3
	vsubss	%xmm7, %xmm3, %xmm13
	vsubss	%xmm13, %xmm3, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm13, %xmm14, %xmm13
	vaddss	%xmm7, %xmm13, %xmm7
	vaddss	%xmm3, %xmm12, %xmm13
	vsubss	%xmm12, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm3, %xmm3
	vaddss	%xmm3, %xmm12, %xmm3
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vfmadd231ss	%xmm9, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm9) + xmm0
	vfmadd231ss	%xmm10, %xmm1, %xmm0    # xmm0 = (xmm1 * xmm10) + xmm0
	vfmadd231ss	%xmm8, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm8) + xmm0
	vxorps	%xmm14, %xmm14, %xmm14
	vmovss	56(%rsp), %xmm1                 # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vfmadd231ss	.LCPI23_34(%rip), %xmm1, %xmm0 # xmm0 = (xmm1 * mem) + xmm0
	vaddss	%xmm0, %xmm13, %xmm1
	vsubss	%xmm1, %xmm13, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm11, %xmm2
	vsubss	%xmm2, %xmm11, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm6, %xmm3
	vsubss	%xmm3, %xmm6, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vbroadcastss	.LCPI23_15(%rip), %xmm7 # xmm7 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm7, %xmm3, %xmm4
	vxorps	%xmm7, %xmm2, %xmm6
	vxorps	%xmm7, %xmm1, %xmm7
	vmovss	136(%rsp), %xmm9                # 4-byte Reload
                                        # xmm9 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm9, %xmm8
	vsubss	%xmm9, %xmm8, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vsubss	%xmm3, %xmm8, %xmm3
	vsubss	%xmm3, %xmm9, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vmovss	160(%rsp), %xmm9                # 4-byte Reload
                                        # xmm9 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm9, %xmm2
	vsubss	%xmm9, %xmm2, %xmm4
	vsubss	%xmm4, %xmm6, %xmm6
	vsubss	%xmm4, %xmm2, %xmm4
	vsubss	%xmm4, %xmm9, %xmm4
	vaddss	%xmm6, %xmm4, %xmm4
	vmovss	64(%rsp), %xmm9                 # 4-byte Reload
                                        # xmm9 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm9, %xmm1
	vsubss	%xmm9, %xmm1, %xmm6
	vsubss	%xmm6, %xmm7, %xmm7
	vsubss	%xmm6, %xmm1, %xmm6
	vsubss	%xmm6, %xmm9, %xmm6
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	32(%rsp), %xmm6, %xmm6          # 4-byte Folded Reload
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm2, %xmm3, %xmm6
	vsubss	%xmm3, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vsubss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm4, %xmm2, %xmm3
	vsubss	%xmm2, %xmm3, %xmm7
	vsubss	%xmm7, %xmm3, %xmm9
	vsubss	%xmm9, %xmm2, %xmm2
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	%xmm0, 56(%rsp)                 # 4-byte Spill
	vaddss	%xmm1, %xmm6, %xmm0
	vsubss	%xmm0, %xmm6, %xmm2
	vaddss	%xmm1, %xmm2, %xmm9
	vaddss	%xmm0, %xmm8, %xmm6
	vsubss	%xmm6, %xmm8, %xmm1
	vaddss	%xmm0, %xmm1, %xmm10
	vmulss	%xmm5, %xmm6, %xmm8
	vmovaps	%xmm5, %xmm0
	vfmsub213ss	%xmm8, %xmm6, %xmm0     # xmm0 = (xmm6 * xmm0) - xmm8
	vmulss	%xmm6, %xmm14, %xmm12
	vaddss	%xmm0, %xmm12, %xmm1
	vsubss	%xmm0, %xmm1, %xmm2
	vsubss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm2, %xmm12, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vmulss	%xmm5, %xmm10, %xmm2
	vaddss	%xmm2, %xmm1, %xmm11
	vsubss	%xmm1, %xmm11, %xmm3
	vsubss	%xmm3, %xmm11, %xmm4
	vsubss	%xmm4, %xmm1, %xmm1
	vmovaps	%xmm5, %xmm4
	vfmsub213ss	%xmm2, %xmm10, %xmm4    # xmm4 = (xmm10 * xmm4) - xmm2
	vsubss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm13
	vsubss	%xmm13, %xmm0, %xmm0
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vfmsub213ss	%xmm12, %xmm6, %xmm1    # xmm1 = (xmm6 * xmm1) - xmm12
	vmulss	%xmm14, %xmm10, %xmm3
	vxorps	%xmm13, %xmm13, %xmm13
	vfmsub213ss	%xmm3, %xmm10, %xmm13   # xmm13 = (xmm10 * xmm13) - xmm3
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm13, %xmm0
	vmulss	%xmm5, %xmm9, %xmm13
	vmovaps	%xmm5, %xmm14
	vfmsub213ss	%xmm13, %xmm9, %xmm14   # xmm14 = (xmm9 * xmm14) - xmm13
	vaddss	%xmm0, %xmm14, %xmm0
	vaddss	%xmm4, %xmm1, %xmm14
	vsubss	%xmm1, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm15, %xmm4, %xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm3, %xmm13, %xmm4
	vsubss	%xmm3, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm15
	vsubss	%xmm15, %xmm3, %xmm3
	vsubss	%xmm7, %xmm13, %xmm7
	vaddss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm12, %xmm14, %xmm7
	vsubss	%xmm14, %xmm7, %xmm13
	vsubss	%xmm13, %xmm7, %xmm15
	vsubss	%xmm15, %xmm14, %xmm14
	vsubss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm12, %xmm14, %xmm12
	vaddss	%xmm2, %xmm4, %xmm13
	vsubss	%xmm4, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm2
	vaddss	%xmm7, %xmm13, %xmm4
	vsubss	%xmm7, %xmm4, %xmm14
	vsubss	%xmm14, %xmm4, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm7, %xmm13, %xmm7
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vfmadd231ss	56(%rsp), %xmm5, %xmm0  # 4-byte Folded Reload
                                        # xmm0 = (xmm5 * mem) + xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vfmadd231ss	%xmm9, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm9) + xmm0
	vfmadd231ss	%xmm10, %xmm1, %xmm0    # xmm0 = (xmm1 * xmm10) + xmm0
	vfmadd231ss	%xmm6, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm6) + xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm2
	vaddss	%xmm0, %xmm2, %xmm5
	vaddss	%xmm1, %xmm11, %xmm0
	vsubss	%xmm0, %xmm11, %xmm2
	vaddss	%xmm1, %xmm2, %xmm7
	vmovaps	%xmm8, %xmm1
	vaddss	%xmm0, %xmm8, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm6
	vmulss	%xmm8, %xmm8, %xmm9
	vmovaps	%xmm8, %xmm0
	vfmsub213ss	%xmm9, %xmm8, %xmm0     # xmm0 = (xmm8 * xmm0) - xmm9
	vmulss	%xmm6, %xmm8, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm6, %xmm4
	vfmsub213ss	%xmm1, %xmm8, %xmm4     # xmm4 = (xmm8 * xmm4) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmulss	%xmm6, %xmm8, %xmm1
	vaddss	%xmm1, %xmm2, %xmm11
	vsubss	%xmm2, %xmm11, %xmm3
	vsubss	%xmm3, %xmm11, %xmm10
	vsubss	%xmm10, %xmm2, %xmm2
	vmovaps	%xmm8, %xmm10
	vfmsub213ss	%xmm1, %xmm6, %xmm10    # xmm10 = (xmm6 * xmm10) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm12
	vsubss	%xmm12, %xmm0, %xmm0
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmulss	%xmm7, %xmm8, %xmm1
	vmovaps	%xmm7, %xmm3
	vfmsub213ss	%xmm1, %xmm8, %xmm3     # xmm3 = (xmm8 * xmm3) - xmm1
	vaddss	%xmm3, %xmm0, %xmm0
	vmulss	%xmm6, %xmm6, %xmm3
	vmovaps	%xmm6, %xmm12
	vfmsub213ss	%xmm3, %xmm6, %xmm12    # xmm12 = (xmm6 * xmm12) - xmm3
	vaddss	%xmm0, %xmm12, %xmm0
	vmulss	%xmm7, %xmm8, %xmm12
	vmovaps	%xmm8, %xmm13
	vfmsub213ss	%xmm12, %xmm7, %xmm13   # xmm13 = (xmm7 * xmm13) - xmm12
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm4, %xmm10, %xmm13
	vsubss	%xmm4, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm14, %xmm10, %xmm10
	vaddss	%xmm4, %xmm10, %xmm4
	vaddss	%xmm3, %xmm12, %xmm10
	vsubss	%xmm3, %xmm10, %xmm14
	vsubss	%xmm14, %xmm10, %xmm15
	vsubss	%xmm15, %xmm3, %xmm3
	vsubss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm3, %xmm12, %xmm3
	vaddss	%xmm1, %xmm13, %xmm12
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm12, %xmm15
	vsubss	%xmm15, %xmm13, %xmm13
	vsubss	%xmm14, %xmm1, %xmm1
	vaddss	%xmm1, %xmm13, %xmm1
	vaddss	%xmm2, %xmm10, %xmm13
	vsubss	%xmm10, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm2, %xmm10, %xmm2
	vaddss	%xmm13, %xmm12, %xmm10
	vsubss	%xmm12, %xmm10, %xmm14
	vsubss	%xmm14, %xmm10, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm12, %xmm0
	vmovss	%xmm5, 60(%rsp)                 # 4-byte Spill
	vfmadd231ss	%xmm8, %xmm5, %xmm0     # xmm0 = (xmm5 * xmm8) + xmm0
	vmovss	%xmm6, 156(%rsp)                # 4-byte Spill
	vmovss	%xmm7, 56(%rsp)                 # 4-byte Spill
	vfmadd231ss	%xmm6, %xmm7, %xmm0     # xmm0 = (xmm7 * xmm6) + xmm0
	vfmadd231ss	%xmm7, %xmm6, %xmm0     # xmm0 = (xmm6 * xmm7) + xmm0
	vfmadd231ss	%xmm5, %xmm8, %xmm0     # xmm0 = (xmm8 * xmm5) + xmm0
	vaddss	%xmm0, %xmm10, %xmm1
	vsubss	%xmm1, %xmm10, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	%xmm0, 172(%rsp)                # 4-byte Spill
	vaddss	%xmm1, %xmm11, %xmm0
	vsubss	%xmm0, %xmm11, %xmm2
	vaddss	%xmm1, %xmm2, %xmm11
	vaddss	%xmm0, %xmm9, %xmm12
	vsubss	%xmm12, %xmm9, %xmm1
	vaddss	%xmm0, %xmm1, %xmm13
	vmovss	.LCPI23_34(%rip), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm12, %xmm9
	vmovaps	%xmm12, %xmm0
	vfmsub213ss	%xmm9, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm0) - xmm9
	vmulss	%xmm2, %xmm13, %xmm1
	vmovaps	%xmm2, %xmm7
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm13, %xmm4
	vfmsub213ss	%xmm1, %xmm7, %xmm4     # xmm4 = (xmm7 * xmm4) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmulss	%xmm1, %xmm12, %xmm6
	vaddss	%xmm6, %xmm2, %xmm14
	vsubss	%xmm2, %xmm14, %xmm3
	vsubss	%xmm3, %xmm14, %xmm15
	vsubss	%xmm15, %xmm2, %xmm2
	vsubss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm0, %xmm3, %xmm15
	vsubss	%xmm15, %xmm3, %xmm5
	vsubss	%xmm5, %xmm0, %xmm0
	vsubss	%xmm15, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vmulss	%xmm7, %xmm11, %xmm2
	vmovaps	%xmm11, %xmm5
	vfmsub213ss	%xmm2, %xmm7, %xmm5     # xmm5 = (xmm7 * xmm5) - xmm2
	vaddss	%xmm5, %xmm0, %xmm0
	vmulss	%xmm1, %xmm13, %xmm5
	vmovaps	%xmm13, %xmm15
	vfmsub213ss	%xmm5, %xmm1, %xmm15    # xmm15 = (xmm1 * xmm15) - xmm5
	vaddss	%xmm0, %xmm15, %xmm7
	vmovaps	%xmm12, %xmm15
	vfmsub213ss	%xmm6, %xmm1, %xmm15    # xmm15 = (xmm1 * xmm15) - xmm6
	vaddss	%xmm4, %xmm15, %xmm0
	vsubss	%xmm4, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm7, %xmm15, %xmm7
	vsubss	%xmm1, %xmm15, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm6, %xmm5, %xmm4
	vsubss	%xmm5, %xmm4, %xmm10
	vsubss	%xmm10, %xmm4, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm2, %xmm0, %xmm6
	vsubss	%xmm0, %xmm6, %xmm10
	vsubss	%xmm10, %xmm6, %xmm15
	vsubss	%xmm15, %xmm0, %xmm0
	vsubss	%xmm10, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm3, %xmm4, %xmm2
	vsubss	%xmm4, %xmm2, %xmm10
	vsubss	%xmm10, %xmm2, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm10, %xmm3, %xmm3
	vaddss	%xmm3, %xmm4, %xmm3
	vaddss	%xmm2, %xmm6, %xmm4
	vsubss	%xmm6, %xmm4, %xmm10
	vsubss	%xmm10, %xmm4, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm10, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vaddss	%xmm1, %xmm7, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vfmadd231ss	%xmm12, %xmm1, %xmm0    # xmm0 = (xmm1 * xmm12) + xmm0
	vfmadd231ss	%xmm13, %xmm1, %xmm0    # xmm0 = (xmm1 * xmm13) + xmm0
	vfmadd231ss	%xmm11, %xmm1, %xmm0    # xmm0 = (xmm1 * xmm11) + xmm0
	vmovss	172(%rsp), %xmm1                # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vfmadd231ss	.LCPI23_34(%rip), %xmm1, %xmm0 # xmm0 = (xmm1 * mem) + xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm14, %xmm2
	vsubss	%xmm2, %xmm14, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm9, %xmm3
	vsubss	%xmm3, %xmm9, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vbroadcastss	.LCPI23_15(%rip), %xmm7 # xmm7 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm7, %xmm3, %xmm4
	vmovss	136(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm5, %xmm9
	vsubss	%xmm5, %xmm9, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vsubss	%xmm3, %xmm9, %xmm3
	vsubss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vxorps	%xmm7, %xmm2, %xmm4
	vmovss	160(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm6, %xmm2
	vsubss	%xmm6, %xmm2, %xmm5
	vsubss	%xmm5, %xmm4, %xmm4
	vsubss	%xmm5, %xmm2, %xmm5
	vsubss	%xmm5, %xmm6, %xmm5
	vaddss	%xmm4, %xmm5, %xmm4
	vxorps	%xmm7, %xmm1, %xmm5
	vmovss	64(%rsp), %xmm7                 # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm7, %xmm1
	vsubss	%xmm7, %xmm1, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm6, %xmm1, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm5, %xmm6, %xmm5
	vaddss	32(%rsp), %xmm5, %xmm5          # 4-byte Folded Reload
	vsubss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm2, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm4, %xmm2, %xmm3
	vsubss	%xmm2, %xmm3, %xmm6
	vsubss	%xmm6, %xmm3, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	%xmm0, 132(%rsp)                # 4-byte Spill
	vaddss	%xmm1, %xmm5, %xmm0
	vsubss	%xmm0, %xmm5, %xmm2
	vaddss	%xmm1, %xmm2, %xmm12
	vaddss	%xmm0, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm1
	vaddss	%xmm0, %xmm1, %xmm13
	vmulss	%xmm8, %xmm10, %xmm1
	vmovss	%xmm1, 172(%rsp)                # 4-byte Spill
	vmovaps	%xmm8, %xmm0
	vfmsub213ss	%xmm1, %xmm10, %xmm0    # xmm0 = (xmm10 * xmm0) - xmm1
	vmovss	156(%rsp), %xmm9                # 4-byte Reload
                                        # xmm9 = mem[0],zero,zero,zero
	vmulss	%xmm9, %xmm10, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm9, %xmm4
	vfmsub213ss	%xmm1, %xmm10, %xmm4    # xmm4 = (xmm10 * xmm4) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmulss	%xmm8, %xmm13, %xmm1
	vaddss	%xmm1, %xmm2, %xmm14
	vsubss	%xmm2, %xmm14, %xmm3
	vsubss	%xmm3, %xmm14, %xmm5
	vsubss	%xmm5, %xmm2, %xmm2
	vmovaps	%xmm8, %xmm5
	vfmsub213ss	%xmm1, %xmm13, %xmm5    # xmm5 = (xmm13 * xmm5) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm6
	vsubss	%xmm6, %xmm0, %xmm0
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	56(%rsp), %xmm3                 # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm10, %xmm1
	vfmsub213ss	%xmm1, %xmm10, %xmm3    # xmm3 = (xmm10 * xmm3) - xmm1
	vaddss	%xmm3, %xmm0, %xmm0
	vmulss	%xmm9, %xmm13, %xmm3
	vmovaps	%xmm9, %xmm6
	vfmsub213ss	%xmm3, %xmm13, %xmm6    # xmm6 = (xmm13 * xmm6) - xmm3
	vaddss	%xmm6, %xmm0, %xmm0
	vmulss	%xmm8, %xmm12, %xmm6
	vmovaps	%xmm8, %xmm7
	vfmsub213ss	%xmm6, %xmm12, %xmm7    # xmm7 = (xmm12 * xmm7) - xmm6
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm5, %xmm4, %xmm7
	vsubss	%xmm4, %xmm7, %xmm15
	vsubss	%xmm15, %xmm7, %xmm11
	vsubss	%xmm11, %xmm4, %xmm4
	vsubss	%xmm15, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm6, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm11
	vsubss	%xmm11, %xmm5, %xmm15
	vsubss	%xmm15, %xmm3, %xmm3
	vsubss	%xmm11, %xmm6, %xmm6
	vaddss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm1, %xmm7, %xmm6
	vsubss	%xmm7, %xmm6, %xmm11
	vsubss	%xmm11, %xmm6, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm11, %xmm1, %xmm1
	vaddss	%xmm1, %xmm7, %xmm1
	vaddss	%xmm2, %xmm5, %xmm7
	vsubss	%xmm5, %xmm7, %xmm11
	vsubss	%xmm11, %xmm7, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm11, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vaddss	%xmm7, %xmm6, %xmm5
	vsubss	%xmm6, %xmm5, %xmm11
	vsubss	%xmm11, %xmm5, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm11, %xmm7, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vfmadd231ss	132(%rsp), %xmm8, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm8 * mem) + xmm0
	vfmadd231ss	%xmm9, %xmm12, %xmm0    # xmm0 = (xmm12 * xmm9) + xmm0
	vfmadd231ss	56(%rsp), %xmm13, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm13 * mem) + xmm0
	vfmadd231ss	60(%rsp), %xmm10, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm10 * mem) + xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm2
	vaddss	%xmm0, %xmm2, %xmm6
	vaddss	%xmm1, %xmm14, %xmm0
	vsubss	%xmm0, %xmm14, %xmm2
	vaddss	%xmm1, %xmm2, %xmm7
	vmovss	172(%rsp), %xmm1                # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm9
	vmulss	%xmm8, %xmm8, %xmm5
	vmovaps	%xmm8, %xmm0
	vfmsub213ss	%xmm5, %xmm8, %xmm0     # xmm0 = (xmm8 * xmm0) - xmm5
	vmulss	%xmm9, %xmm8, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm9, %xmm4
	vfmsub213ss	%xmm1, %xmm8, %xmm4     # xmm4 = (xmm8 * xmm4) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmulss	%xmm8, %xmm9, %xmm1
	vaddss	%xmm1, %xmm2, %xmm11
	vsubss	%xmm2, %xmm11, %xmm3
	vsubss	%xmm3, %xmm11, %xmm10
	vsubss	%xmm10, %xmm2, %xmm2
	vmovaps	%xmm8, %xmm10
	vfmsub213ss	%xmm1, %xmm9, %xmm10    # xmm10 = (xmm9 * xmm10) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm12
	vsubss	%xmm12, %xmm0, %xmm0
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmulss	%xmm7, %xmm8, %xmm1
	vmovaps	%xmm7, %xmm3
	vfmsub213ss	%xmm1, %xmm8, %xmm3     # xmm3 = (xmm8 * xmm3) - xmm1
	vaddss	%xmm3, %xmm0, %xmm0
	vmulss	%xmm9, %xmm9, %xmm3
	vmovaps	%xmm9, %xmm12
	vfmsub213ss	%xmm3, %xmm9, %xmm12    # xmm12 = (xmm9 * xmm12) - xmm3
	vaddss	%xmm0, %xmm12, %xmm0
	vmulss	%xmm7, %xmm8, %xmm12
	vmovaps	%xmm8, %xmm13
	vfmsub213ss	%xmm12, %xmm7, %xmm13   # xmm13 = (xmm7 * xmm13) - xmm12
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm4, %xmm10, %xmm13
	vsubss	%xmm4, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm14, %xmm10, %xmm10
	vaddss	%xmm4, %xmm10, %xmm4
	vaddss	%xmm3, %xmm12, %xmm10
	vsubss	%xmm3, %xmm10, %xmm14
	vsubss	%xmm14, %xmm10, %xmm15
	vsubss	%xmm15, %xmm3, %xmm3
	vsubss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm3, %xmm12, %xmm3
	vaddss	%xmm1, %xmm13, %xmm12
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm12, %xmm15
	vsubss	%xmm15, %xmm13, %xmm13
	vsubss	%xmm14, %xmm1, %xmm1
	vaddss	%xmm1, %xmm13, %xmm1
	vaddss	%xmm2, %xmm10, %xmm13
	vsubss	%xmm10, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm2, %xmm10, %xmm2
	vaddss	%xmm13, %xmm12, %xmm10
	vsubss	%xmm12, %xmm10, %xmm14
	vsubss	%xmm14, %xmm10, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm12, %xmm0
	vmovss	%xmm6, 60(%rsp)                 # 4-byte Spill
	vfmadd231ss	%xmm8, %xmm6, %xmm0     # xmm0 = (xmm6 * xmm8) + xmm0
	vmovss	%xmm7, 56(%rsp)                 # 4-byte Spill
	vfmadd231ss	%xmm9, %xmm7, %xmm0     # xmm0 = (xmm7 * xmm9) + xmm0
	vfmadd231ss	%xmm7, %xmm9, %xmm0     # xmm0 = (xmm9 * xmm7) + xmm0
	vfmadd231ss	%xmm6, %xmm8, %xmm0     # xmm0 = (xmm8 * xmm6) + xmm0
	vaddss	%xmm0, %xmm10, %xmm1
	vsubss	%xmm1, %xmm10, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	%xmm0, 156(%rsp)                # 4-byte Spill
	vaddss	%xmm1, %xmm11, %xmm0
	vsubss	%xmm0, %xmm11, %xmm2
	vaddss	%xmm1, %xmm2, %xmm11
	vaddss	%xmm0, %xmm5, %xmm12
	vsubss	%xmm12, %xmm5, %xmm1
	vaddss	%xmm0, %xmm1, %xmm13
	vmovss	.LCPI23_34(%rip), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm12, %xmm1
	vmovss	%xmm1, 172(%rsp)                # 4-byte Spill
	vmovaps	%xmm12, %xmm0
	vfmsub213ss	%xmm1, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm0) - xmm1
	vmulss	%xmm2, %xmm13, %xmm1
	vmovaps	%xmm2, %xmm7
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm13, %xmm4
	vfmsub213ss	%xmm1, %xmm7, %xmm4     # xmm4 = (xmm7 * xmm4) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmulss	%xmm1, %xmm12, %xmm6
	vaddss	%xmm6, %xmm2, %xmm14
	vsubss	%xmm2, %xmm14, %xmm3
	vsubss	%xmm3, %xmm14, %xmm15
	vsubss	%xmm15, %xmm2, %xmm2
	vsubss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm0, %xmm3, %xmm15
	vsubss	%xmm15, %xmm3, %xmm5
	vsubss	%xmm5, %xmm0, %xmm0
	vsubss	%xmm15, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vmulss	%xmm7, %xmm11, %xmm2
	vmovaps	%xmm11, %xmm5
	vfmsub213ss	%xmm2, %xmm7, %xmm5     # xmm5 = (xmm7 * xmm5) - xmm2
	vaddss	%xmm5, %xmm0, %xmm0
	vmulss	%xmm1, %xmm13, %xmm5
	vmovaps	%xmm13, %xmm15
	vfmsub213ss	%xmm5, %xmm1, %xmm15    # xmm15 = (xmm1 * xmm15) - xmm5
	vaddss	%xmm0, %xmm15, %xmm7
	vmovaps	%xmm12, %xmm15
	vfmsub213ss	%xmm6, %xmm1, %xmm15    # xmm15 = (xmm1 * xmm15) - xmm6
	vaddss	%xmm4, %xmm15, %xmm0
	vsubss	%xmm4, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm7, %xmm15, %xmm7
	vsubss	%xmm1, %xmm15, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm6, %xmm5, %xmm4
	vsubss	%xmm5, %xmm4, %xmm10
	vsubss	%xmm10, %xmm4, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm2, %xmm0, %xmm6
	vsubss	%xmm0, %xmm6, %xmm10
	vsubss	%xmm10, %xmm6, %xmm15
	vsubss	%xmm15, %xmm0, %xmm0
	vsubss	%xmm10, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm3, %xmm4, %xmm2
	vsubss	%xmm4, %xmm2, %xmm10
	vsubss	%xmm10, %xmm2, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm10, %xmm3, %xmm3
	vaddss	%xmm3, %xmm4, %xmm3
	vaddss	%xmm2, %xmm6, %xmm4
	vsubss	%xmm6, %xmm4, %xmm10
	vsubss	%xmm10, %xmm4, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm10, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vaddss	%xmm1, %xmm7, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vfmadd231ss	%xmm12, %xmm1, %xmm0    # xmm0 = (xmm1 * xmm12) + xmm0
	vfmadd231ss	%xmm13, %xmm1, %xmm0    # xmm0 = (xmm1 * xmm13) + xmm0
	vfmadd231ss	%xmm11, %xmm1, %xmm0    # xmm0 = (xmm1 * xmm11) + xmm0
	vmovss	156(%rsp), %xmm1                # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vfmadd231ss	.LCPI23_34(%rip), %xmm1, %xmm0 # xmm0 = (xmm1 * mem) + xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm14, %xmm2
	vsubss	%xmm2, %xmm14, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	172(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vmovss	136(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm5, %xmm11
	vsubss	%xmm5, %xmm11, %xmm4
	vmovaps	%xmm5, %xmm6
	vsubss	%xmm4, %xmm11, %xmm5
	vsubss	%xmm5, %xmm6, %xmm5
	vbroadcastss	.LCPI23_15(%rip), %xmm7 # xmm7 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm7, %xmm3, %xmm3
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm5, %xmm3
	vmovss	160(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm6, %xmm4
	vsubss	%xmm6, %xmm4, %xmm5
	vmovaps	%xmm6, %xmm10
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm10, %xmm6
	vxorps	%xmm7, %xmm2, %xmm2
	vmovaps	%xmm7, %xmm10
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vmovss	64(%rsp), %xmm7                 # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm6
	vmovaps	%xmm7, %xmm12
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm12, %xmm7
	vxorps	%xmm1, %xmm10, %xmm1
	vsubss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm1, %xmm7, %xmm1
	vaddss	32(%rsp), %xmm1, %xmm1          # 4-byte Folded Reload
	vsubss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm4, %xmm3, %xmm1
	vsubss	%xmm3, %xmm1, %xmm6
	vsubss	%xmm6, %xmm1, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm2, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm5, %xmm4, %xmm3
	vsubss	%xmm4, %xmm3, %xmm6
	vsubss	%xmm6, %xmm3, %xmm7
	vsubss	%xmm7, %xmm4, %xmm4
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm0, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm0, 160(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm10
	vaddss	%xmm3, %xmm11, %xmm4
	vsubss	%xmm4, %xmm11, %xmm1
	vaddss	%xmm3, %xmm1, %xmm11
	vmulss	%xmm4, %xmm8, %xmm0
	vmovss	%xmm0, 32(%rsp)                 # 4-byte Spill
	vmovaps	%xmm8, %xmm1
	vfmsub213ss	%xmm0, %xmm4, %xmm1     # xmm1 = (xmm4 * xmm1) - xmm0
	vmulss	%xmm4, %xmm9, %xmm2
	vaddss	%xmm2, %xmm1, %xmm5
	vsubss	%xmm1, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vmovaps	%xmm9, %xmm7
	vfmsub213ss	%xmm2, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm7) - xmm2
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vmulss	%xmm8, %xmm11, %xmm2
	vaddss	%xmm2, %xmm5, %xmm12
	vsubss	%xmm5, %xmm12, %xmm6
	vsubss	%xmm6, %xmm12, %xmm13
	vsubss	%xmm13, %xmm5, %xmm5
	vmovaps	%xmm8, %xmm13
	vfmsub213ss	%xmm2, %xmm11, %xmm13   # xmm13 = (xmm11 * xmm13) - xmm2
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vaddss	%xmm2, %xmm1, %xmm5
	vsubss	%xmm1, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm14
	vsubss	%xmm14, %xmm1, %xmm1
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vmovss	56(%rsp), %xmm6                 # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmulss	%xmm6, %xmm4, %xmm2
	vfmsub213ss	%xmm2, %xmm4, %xmm6     # xmm6 = (xmm4 * xmm6) - xmm2
	vaddss	%xmm6, %xmm1, %xmm1
	vmulss	%xmm9, %xmm11, %xmm6
	vmovaps	%xmm9, %xmm14
	vfmsub213ss	%xmm6, %xmm11, %xmm14   # xmm14 = (xmm11 * xmm14) - xmm6
	vaddss	%xmm1, %xmm14, %xmm1
	vmulss	%xmm8, %xmm10, %xmm14
	vmovaps	%xmm8, %xmm15
	vfmsub213ss	%xmm14, %xmm10, %xmm15  # xmm15 = (xmm10 * xmm15) - xmm14
	vaddss	%xmm1, %xmm15, %xmm15
	vaddss	%xmm7, %xmm13, %xmm1
	vsubss	%xmm7, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm3
	vsubss	%xmm3, %xmm7, %xmm3
	vsubss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm6, %xmm14, %xmm3
	vsubss	%xmm6, %xmm3, %xmm7
	vsubss	%xmm7, %xmm3, %xmm13
	vsubss	%xmm13, %xmm6, %xmm6
	vsubss	%xmm7, %xmm14, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm2, %xmm1, %xmm7
	vsubss	%xmm1, %xmm7, %xmm13
	vsubss	%xmm13, %xmm7, %xmm14
	vsubss	%xmm14, %xmm1, %xmm1
	vsubss	%xmm13, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm2
	vaddss	%xmm5, %xmm3, %xmm13
	vsubss	%xmm3, %xmm13, %xmm1
	vsubss	%xmm1, %xmm13, %xmm14
	vsubss	%xmm14, %xmm3, %xmm3
	vsubss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm1, %xmm3, %xmm3
	vaddss	%xmm7, %xmm13, %xmm1
	vsubss	%xmm7, %xmm1, %xmm5
	vsubss	%xmm5, %xmm1, %xmm14
	vmovss	%xmm1, 64(%rsp)                 # 4-byte Spill
	vsubss	%xmm14, %xmm7, %xmm7
	vsubss	%xmm5, %xmm13, %xmm5
	vaddss	%xmm5, %xmm7, %xmm5
	vaddss	%xmm0, %xmm15, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm2
	vfmadd231ss	160(%rsp), %xmm8, %xmm2 # 4-byte Folded Reload
                                        # xmm2 = (xmm8 * mem) + xmm2
	vfmadd231ss	%xmm9, %xmm10, %xmm2    # xmm2 = (xmm10 * xmm9) + xmm2
	vfmadd231ss	56(%rsp), %xmm11, %xmm2 # 4-byte Folded Reload
                                        # xmm2 = (xmm11 * mem) + xmm2
	vfmadd231ss	60(%rsp), %xmm4, %xmm2  # 4-byte Folded Reload
                                        # xmm2 = (xmm4 * mem) + xmm2
	vaddss	%xmm2, %xmm1, %xmm4
	vaddss	%xmm4, %xmm12, %xmm0
	vsubss	%xmm0, %xmm12, %xmm8
	vmovss	32(%rsp), %xmm1                 # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm5
	vsubss	%xmm5, %xmm1, %xmm3
	vaddss	%xmm0, %xmm3, %xmm6
	vmovss	.LCPI23_34(%rip), %xmm1         # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm1, %xmm3
	vmovss	%xmm3, 32(%rsp)                 # 4-byte Spill
	vmovaps	%xmm5, %xmm0
	vfmsub213ss	%xmm3, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm0) - xmm3
	vmulss	%xmm6, %xmm1, %xmm7
	vmovaps	%xmm6, %xmm9
	vfmsub213ss	%xmm7, %xmm1, %xmm9     # xmm9 = (xmm1 * xmm9) - xmm7
	vaddss	%xmm7, %xmm0, %xmm11
	vsubss	%xmm0, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm10
	vsubss	%xmm10, %xmm0, %xmm0
	vxorps	%xmm3, %xmm3, %xmm3
	vmulss	%xmm5, %xmm3, %xmm10
	vsubss	%xmm12, %xmm7, %xmm7
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm10, %xmm11, %xmm7
	vsubss	%xmm11, %xmm7, %xmm12
	vsubss	%xmm12, %xmm7, %xmm13
	vsubss	%xmm13, %xmm11, %xmm11
	vsubss	%xmm12, %xmm10, %xmm12
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm0, %xmm11, %xmm12
	vsubss	%xmm0, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm0, %xmm0
	vsubss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm4, %xmm8, %xmm8
	vaddss	%xmm0, %xmm11, %xmm0
	vmulss	%xmm1, %xmm8, %xmm11
	vmovaps	%xmm8, %xmm13
	vfmsub213ss	%xmm11, %xmm1, %xmm13   # xmm13 = (xmm1 * xmm13) - xmm11
	vaddss	%xmm0, %xmm13, %xmm0
	vmulss	%xmm6, %xmm3, %xmm13
	vmovaps	%xmm6, %xmm14
	vfmsub213ss	%xmm13, %xmm3, %xmm14   # xmm14 = (xmm3 * xmm14) - xmm13
	vaddss	%xmm0, %xmm14, %xmm1
	vmovaps	%xmm5, %xmm14
	vfmsub213ss	%xmm10, %xmm3, %xmm14   # xmm14 = (xmm3 * xmm14) - xmm10
	vaddss	%xmm14, %xmm9, %xmm15
	vsubss	%xmm9, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm3
	vsubss	%xmm3, %xmm9, %xmm3
	vaddss	%xmm1, %xmm14, %xmm1
	vsubss	%xmm0, %xmm14, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm10, %xmm13, %xmm3
	vsubss	%xmm13, %xmm3, %xmm9
	vsubss	%xmm9, %xmm3, %xmm14
	vsubss	%xmm14, %xmm13, %xmm13
	vsubss	%xmm9, %xmm10, %xmm9
	vaddss	%xmm9, %xmm13, %xmm9
	vaddss	%xmm11, %xmm15, %xmm10
	vsubss	%xmm15, %xmm10, %xmm13
	vsubss	%xmm13, %xmm10, %xmm14
	vsubss	%xmm14, %xmm15, %xmm14
	vsubss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm11, %xmm14, %xmm11
	vaddss	%xmm3, %xmm12, %xmm13
	vsubss	%xmm3, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm3, %xmm3
	vsubss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm3, %xmm12, %xmm3
	vaddss	%xmm13, %xmm10, %xmm12
	vsubss	%xmm10, %xmm12, %xmm14
	vsubss	%xmm14, %xmm12, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vsubss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm13, %xmm10, %xmm10
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vfmadd231ss	%xmm5, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm5) + xmm0
	vfmadd231ss	%xmm6, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm6) + xmm0
	vfmadd231ss	%xmm8, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm8) + xmm0
	vmovss	64(%rsp), %xmm1                 # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vfmadd231ss	.LCPI23_34(%rip), %xmm1, %xmm0 # xmm0 = (xmm1 * mem) + xmm0
	vaddss	%xmm0, %xmm12, %xmm1
	vsubss	%xmm1, %xmm12, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm7, %xmm2
	vsubss	%xmm2, %xmm7, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	32(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	movl	$2, %eax
	vxorps	%xmm5, %xmm5, %xmm5
	vcvtsi2ss	%eax, %xmm5, %xmm4
	vmulss	.LCPI23_33(%rip), %xmm4, %xmm4
	vmulss	%xmm4, %xmm3, %xmm3
	vmulss	%xmm4, %xmm2, %xmm2
	vinsertps	$16, %xmm2, %xmm3, %xmm2 # xmm2 = xmm3[0],xmm2[0],xmm3[2,3]
	vmulss	%xmm4, %xmm1, %xmm1
	vmulss	%xmm4, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	vmovlps	%xmm2, 96(%rsp)
	vmovlps	%xmm0, 104(%rsp)
.Ltmp1085:
	leaq	856(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1086:
# %bb.190:
	movq	856(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_192
# %bb.191:
	callq	_ZdlPv
.LBB23_192:
	leaq	840(%rsp), %rbx
	movq	%rbx, 824(%rsp)
	movl	$1953657203, 840(%rsp)          # imm = 0x74727173
	movq	$4, 832(%rsp)
	vmovss	24(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm0, %xmm1
	movb	$0, 844(%rsp)
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB23_194
# %bb.193:
	vmovsd	24(%rsp), %xmm0                 # xmm0 = mem[0],zero
	jmp	.LBB23_195
.LBB23_194:
	callq	sqrtf
	vmovss	.LCPI23_30(%rip), %xmm1         # xmm1 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm1, %xmm0
	vmovss	24(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	vmovss	28(%rsp), %xmm2                 # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm1, %xmm3
	vmulss	%xmm3, %xmm3, %xmm4
	vbroadcastss	.LCPI23_15(%rip), %xmm7 # xmm7 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm7, %xmm4, %xmm5
	vmovaps	%xmm3, %xmm6
	vfmsub213ss	%xmm4, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm6) - xmm4
	vxorps	%xmm7, %xmm6, %xmm7
	vsubss	%xmm4, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm8
	vsubss	%xmm8, %xmm4, %xmm9
	vsubss	%xmm9, %xmm1, %xmm1
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm1, %xmm1
	vsubss	%xmm6, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm1
	vmulss	.LCPI23_31(%rip), %xmm0, %xmm0
	vmulss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm3, %xmm1
	vsubss	%xmm3, %xmm1, %xmm2
	vsubss	%xmm2, %xmm1, %xmm4
	vsubss	%xmm4, %xmm3, %xmm3
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
.LBB23_195:
	vmovlps	%xmm0, 96(%rsp)
.Ltmp1088:
	leaq	824(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1089:
# %bb.196:
	movq	824(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_198
# %bb.197:
	callq	_ZdlPv
.LBB23_198:
	leaq	808(%rsp), %rbx
	movq	%rbx, 792(%rsp)
	movl	$1953657203, 808(%rsp)          # imm = 0x74727173
	movq	$4, 800(%rsp)
	movb	$0, 812(%rsp)
	vmovss	24(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vxorps	%xmm2, %xmm2, %xmm2
	vucomiss	%xmm0, %xmm2
	vmovss	28(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB23_200
# %bb.199:
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	jmp	.LBB23_209
.LBB23_200:
	vmovups	%xmm1, 64(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm0, 32(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vucomiss	.LCPI23_40(%rip), %xmm0
	setnp	%al
	sete	%cl
	vmovd	.LCPI23_30(%rip), %xmm5         # xmm5 = mem[0],zero,zero,zero
	vmovdqa	%xmm5, %xmm8
	testb	%al, %cl
	jne	.LBB23_204
# %bb.201:
	vmovd	%xmm0, %eax
	andl	$2139095040, %eax               # imm = 0x7F800000
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB23_203
# %bb.202:
	vmovd	%eax, %xmm8
	jmp	.LBB23_204
.LBB23_203:
	vmovd	.LCPI23_30(%rip), %xmm8         # xmm8 = mem[0],zero,zero,zero
.LBB23_204:
	vxorps	%xmm6, %xmm6, %xmm6
	vucomiss	%xmm6, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB23_208
# %bb.205:
	vmovd	%xmm0, %eax
	andl	$2139095040, %eax               # imm = 0x7F800000
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB23_207
# %bb.206:
	movl	$2130706432, %ecx               # imm = 0x7F000000
	subl	%eax, %ecx
	vmovd	%ecx, %xmm5
	jmp	.LBB23_208
.LBB23_207:
	vmovd	.LCPI23_30(%rip), %xmm5         # xmm5 = mem[0],zero,zero,zero
.LBB23_208:
	vmovss	.LCPI23_31(%rip), %xmm1         # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm5, %xmm4
	vmulss	32(%rsp), %xmm5, %xmm2          # 16-byte Folded Reload
	vmulss	%xmm4, %xmm2, %xmm2
	vmulss	64(%rsp), %xmm5, %xmm3          # 16-byte Folded Reload
	vmulss	%xmm4, %xmm3, %xmm3
	vmovss	%xmm3, 136(%rsp)                # 4-byte Spill
	vmulss	%xmm5, %xmm6, %xmm5
	movl	$3, %eax
	vxorps	%xmm7, %xmm7, %xmm7
	vxorps	%xmm11, %xmm11, %xmm11
	vcvtsi2ss	%eax, %xmm11, %xmm6
	vmulss	%xmm4, %xmm5, %xmm3
	vmovss	%xmm3, 32(%rsp)                 # 4-byte Spill
	vmovd	%xmm8, 56(%rsp)                 # 4-byte Folded Spill
	vdivss	%xmm0, %xmm8, %xmm8
	vmulss	%xmm1, %xmm6, %xmm5
	vmovaps	%xmm1, %xmm0
	vfmsub213ss	%xmm5, %xmm6, %xmm0     # xmm0 = (xmm6 * xmm0) - xmm5
	vmulss	%xmm7, %xmm6, %xmm9
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm9, %xmm6, %xmm10    # xmm10 = (xmm6 * xmm10) - xmm9
	vaddss	%xmm0, %xmm9, %xmm11
	vsubss	%xmm0, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm0, %xmm0
	vsubss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vfmadd231ss	%xmm6, %xmm7, %xmm0     # xmm0 = (xmm7 * xmm6) + xmm0
	vaddss	%xmm0, %xmm11, %xmm9
	vsubss	%xmm9, %xmm11, %xmm6
	vaddss	%xmm0, %xmm6, %xmm0
	vmovss	%xmm0, 64(%rsp)                 # 4-byte Spill
	vaddss	%xmm5, %xmm9, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm9, %xmm0
	vmovss	%xmm0, 160(%rsp)                # 4-byte Spill
	vmulss	%xmm8, %xmm8, %xmm10
	vmovaps	%xmm8, %xmm9
	vfmsub213ss	%xmm10, %xmm8, %xmm9    # xmm9 = (xmm8 * xmm9) - xmm10
	vmulss	%xmm7, %xmm8, %xmm11
	vxorps	%xmm12, %xmm12, %xmm12
	vfmsub213ss	%xmm11, %xmm8, %xmm12   # xmm12 = (xmm8 * xmm12) - xmm11
	vaddss	%xmm11, %xmm9, %xmm13
	vsubss	%xmm9, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm9, %xmm9
	vmulss	%xmm7, %xmm8, %xmm15
	vsubss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm15, %xmm13, %xmm11
	vsubss	%xmm13, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm1
	vsubss	%xmm1, %xmm13, %xmm1
	vmovaps	%xmm8, %xmm13
	vfmsub213ss	%xmm15, %xmm7, %xmm13   # xmm13 = (xmm7 * xmm13) - xmm15
	vsubss	%xmm14, %xmm15, %xmm14
	vaddss	%xmm1, %xmm14, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vfmadd231ss	%xmm0, %xmm0, %xmm1     # xmm1 = (xmm0 * xmm0) + xmm1
	vaddss	%xmm1, %xmm9, %xmm1
	vfmadd231ss	%xmm0, %xmm8, %xmm12    # xmm12 = (xmm8 * xmm0) + xmm12
	vaddss	%xmm1, %xmm12, %xmm1
	vfmadd231ss	%xmm8, %xmm0, %xmm13    # xmm13 = (xmm0 * xmm8) + xmm13
	vaddss	%xmm1, %xmm13, %xmm1
	vaddss	%xmm1, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm9
	vaddss	%xmm1, %xmm9, %xmm9
	vaddss	%xmm12, %xmm10, %xmm1
	vsubss	%xmm1, %xmm10, %xmm10
	vaddss	%xmm12, %xmm10, %xmm11
	vmulss	%xmm1, %xmm2, %xmm10
	vmovss	136(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm5, %xmm12
	vmovaps	%xmm1, %xmm13
	vfmsub213ss	%xmm12, %xmm5, %xmm13   # xmm13 = (xmm5 * xmm13) - xmm12
	vfmadd231ss	%xmm1, %xmm3, %xmm13    # xmm13 = (xmm3 * xmm1) + xmm13
	vfmsub213ss	%xmm10, %xmm2, %xmm1    # xmm1 = (xmm2 * xmm1) - xmm10
	vmulss	%xmm2, %xmm11, %xmm14
	vaddss	%xmm1, %xmm14, %xmm15
	vsubss	%xmm1, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm4
	vsubss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm12, %xmm15, %xmm4
	vsubss	%xmm15, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm3
	vsubss	%xmm3, %xmm15, %xmm3
	vsubss	%xmm7, %xmm12, %xmm7
	vaddss	%xmm7, %xmm3, %xmm3
	vfmadd231ss	%xmm11, %xmm5, %xmm3    # xmm3 = (xmm5 * xmm11) + xmm3
	vmovaps	%xmm5, %xmm15
	vfmsub213ss	%xmm14, %xmm2, %xmm11   # xmm11 = (xmm2 * xmm11) - xmm14
	vsubss	%xmm0, %xmm14, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vfmadd231ss	%xmm9, %xmm2, %xmm11    # xmm11 = (xmm2 * xmm9) + xmm11
	vmovaps	%xmm2, %xmm14
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm1, %xmm10, %xmm3
	vsubss	%xmm3, %xmm10, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vbroadcastss	.LCPI23_15(%rip), %xmm2 # xmm2 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm3, %xmm4
	vxorps	%xmm2, %xmm1, %xmm7
	vmovss	%xmm6, 156(%rsp)                # 4-byte Spill
	vsubss	%xmm3, %xmm6, %xmm3
	vsubss	%xmm6, %xmm3, %xmm9
	vsubss	%xmm9, %xmm3, %xmm10
	vsubss	%xmm10, %xmm6, %xmm10
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm4, %xmm10, %xmm4
	vmovss	160(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm9
	vsubss	%xmm9, %xmm1, %xmm10
	vsubss	%xmm10, %xmm5, %xmm10
	vsubss	%xmm9, %xmm7, %xmm7
	vaddss	%xmm7, %xmm10, %xmm7
	vaddss	%xmm4, %xmm1, %xmm9
	vsubss	%xmm1, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vmovss	64(%rsp), %xmm2                 # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm1
	vsubss	%xmm1, %xmm9, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm1, %xmm3, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vaddss	%xmm1, %xmm3, %xmm11
	vmulss	%xmm8, %xmm9, %xmm10
	vmulss	%xmm8, %xmm11, %xmm1
	vmovaps	%xmm8, %xmm3
	vfmsub213ss	%xmm1, %xmm11, %xmm3    # xmm3 = (xmm11 * xmm3) - xmm1
	vfmadd231ss	%xmm0, %xmm8, %xmm3     # xmm3 = (xmm8 * xmm0) + xmm3
	vfmsub213ss	%xmm10, %xmm9, %xmm8    # xmm8 = (xmm9 * xmm8) - xmm10
	vxorps	%xmm2, %xmm2, %xmm2
	vmulss	%xmm2, %xmm9, %xmm0
	vaddss	%xmm0, %xmm8, %xmm4
	vsubss	%xmm8, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vxorps	%xmm12, %xmm12, %xmm12
	vfmsub213ss	%xmm0, %xmm9, %xmm12    # xmm12 = (xmm9 * xmm12) - xmm0
	vsubss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm1, %xmm4, %xmm7
	vsubss	%xmm4, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm13
	vsubss	%xmm13, %xmm4, %xmm4
	vsubss	%xmm8, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vfmadd231ss	%xmm11, %xmm2, %xmm1    # xmm1 = (xmm2 * xmm11) + xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vfmadd231ss	%xmm2, %xmm9, %xmm12    # xmm12 = (xmm9 * xmm2) + xmm12
	vaddss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm3
	vaddss	%xmm0, %xmm3, %xmm2
	vaddss	%xmm1, %xmm10, %xmm9
	vsubss	%xmm9, %xmm10, %xmm0
	vaddss	%xmm1, %xmm0, %xmm8
	vmulss	%xmm9, %xmm9, %xmm0
	vmovaps	%xmm9, %xmm1
	vfmsub213ss	%xmm0, %xmm9, %xmm1     # xmm1 = (xmm9 * xmm1) - xmm0
	vmulss	%xmm8, %xmm9, %xmm3
	vaddss	%xmm3, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm10
	vsubss	%xmm10, %xmm4, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vmovaps	%xmm8, %xmm11
	vfmsub213ss	%xmm3, %xmm9, %xmm11    # xmm11 = (xmm9 * xmm11) - xmm3
	vsubss	%xmm10, %xmm3, %xmm3
	vmulss	%xmm9, %xmm8, %xmm10
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm4, %xmm10, %xmm3
	vsubss	%xmm4, %xmm3, %xmm12
	vsubss	%xmm12, %xmm3, %xmm13
	vsubss	%xmm13, %xmm4, %xmm4
	vmovaps	%xmm9, %xmm13
	vfmsub213ss	%xmm10, %xmm8, %xmm13   # xmm13 = (xmm8 * xmm13) - xmm10
	vsubss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm4, %xmm10, %xmm4
	vfmadd231ss	%xmm8, %xmm8, %xmm4     # xmm4 = (xmm8 * xmm8) + xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vmovss	%xmm2, 60(%rsp)                 # 4-byte Spill
	vfmadd231ss	%xmm2, %xmm9, %xmm11    # xmm11 = (xmm9 * xmm2) + xmm11
	vaddss	%xmm1, %xmm11, %xmm1
	vfmadd231ss	%xmm9, %xmm2, %xmm13    # xmm13 = (xmm2 * xmm9) + xmm13
	vaddss	%xmm1, %xmm13, %xmm1
	vaddss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm1, %xmm3, %xmm10
	vaddss	%xmm4, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm12
	vmulss	%xmm1, %xmm14, %xmm11
	vmovaps	%xmm15, %xmm6
	vmulss	%xmm1, %xmm15, %xmm2
	vmovaps	%xmm1, %xmm3
	vfmsub213ss	%xmm2, %xmm15, %xmm3    # xmm3 = (xmm15 * xmm3) - xmm2
	vfmadd231ss	32(%rsp), %xmm1, %xmm3  # 4-byte Folded Reload
                                        # xmm3 = (xmm1 * mem) + xmm3
	vmovaps	%xmm14, %xmm5
	vfmsub213ss	%xmm11, %xmm14, %xmm1   # xmm1 = (xmm14 * xmm1) - xmm11
	vmulss	%xmm12, %xmm14, %xmm4
	vaddss	%xmm4, %xmm1, %xmm13
	vsubss	%xmm1, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm1, %xmm1
	vaddss	%xmm2, %xmm13, %xmm15
	vsubss	%xmm13, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm7
	vsubss	%xmm7, %xmm13, %xmm7
	vsubss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vfmadd231ss	%xmm12, %xmm6, %xmm0    # xmm0 = (xmm6 * xmm12) + xmm0
	vfmsub213ss	%xmm4, %xmm5, %xmm12    # xmm12 = (xmm5 * xmm12) - xmm4
	vsubss	%xmm14, %xmm4, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vfmadd231ss	%xmm10, %xmm5, %xmm12   # xmm12 = (xmm5 * xmm10) + xmm12
	vmovaps	%xmm5, %xmm13
	vaddss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm15, %xmm1
	vsubss	%xmm1, %xmm15, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm11, %xmm2
	vsubss	%xmm2, %xmm11, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vbroadcastss	.LCPI23_15(%rip), %xmm10 # xmm10 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm10, %xmm3
	vmovss	156(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm7
	vsubss	%xmm7, %xmm5, %xmm6
	vxorps	%xmm1, %xmm10, %xmm7
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vmovss	160(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm4
	vsubss	%xmm4, %xmm1, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm4, %xmm7, %xmm4
	vaddss	%xmm4, %xmm5, %xmm4
	vaddss	%xmm3, %xmm1, %xmm5
	vsubss	%xmm1, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm1, %xmm1
	vmovss	64(%rsp), %xmm3                 # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm3
	vaddss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm1, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm0
	vaddss	%xmm1, %xmm0, %xmm6
	vmulss	%xmm5, %xmm9, %xmm0
	vmulss	%xmm6, %xmm9, %xmm1
	vmovaps	%xmm9, %xmm2
	vfmsub213ss	%xmm1, %xmm6, %xmm2     # xmm2 = (xmm6 * xmm2) - xmm1
	vfmadd231ss	%xmm3, %xmm9, %xmm2     # xmm2 = (xmm9 * xmm3) + xmm2
	vfmsub213ss	%xmm0, %xmm5, %xmm9     # xmm9 = (xmm5 * xmm9) - xmm0
	vmulss	%xmm5, %xmm8, %xmm3
	vaddss	%xmm3, %xmm9, %xmm4
	vsubss	%xmm9, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm10
	vsubss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm1, %xmm4, %xmm10
	vsubss	%xmm4, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm4, %xmm4
	vsubss	%xmm11, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vfmadd231ss	%xmm8, %xmm6, %xmm1     # xmm1 = (xmm6 * xmm8) + xmm1
	vfmsub213ss	%xmm3, %xmm5, %xmm8     # xmm8 = (xmm5 * xmm8) - xmm3
	vsubss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm9, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vfmadd231ss	60(%rsp), %xmm5, %xmm8  # 4-byte Folded Reload
                                        # xmm8 = (xmm5 * mem) + xmm8
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm10, %xmm2
	vsubss	%xmm2, %xmm10, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm2
	vmulss	%xmm3, %xmm13, %xmm0
	vmovss	136(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm6, %xmm4
	vmovaps	%xmm3, %xmm5
	vfmsub213ss	%xmm4, %xmm6, %xmm5     # xmm5 = (xmm6 * xmm5) - xmm4
	vmovaps	%xmm6, %xmm10
	vfmadd231ss	32(%rsp), %xmm3, %xmm5  # 4-byte Folded Reload
                                        # xmm5 = (xmm3 * mem) + xmm5
	vfmsub213ss	%xmm0, %xmm13, %xmm3    # xmm3 = (xmm13 * xmm3) - xmm0
	vmulss	%xmm2, %xmm13, %xmm6
	vaddss	%xmm6, %xmm3, %xmm7
	vaddss	%xmm4, %xmm7, %xmm8
	vsubss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm9, %xmm4, %xmm4
	vsubss	%xmm9, %xmm8, %xmm9
	vsubss	%xmm9, %xmm7, %xmm9
	vaddss	%xmm4, %xmm9, %xmm4
	vfmadd231ss	%xmm2, %xmm10, %xmm4    # xmm4 = (xmm10 * xmm2) + xmm4
	vfmsub213ss	%xmm6, %xmm13, %xmm2    # xmm2 = (xmm13 * xmm2) - xmm6
	vfmadd231ss	%xmm1, %xmm13, %xmm2    # xmm2 = (xmm13 * xmm1) + xmm2
	vsubss	%xmm3, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm1, %xmm6, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm8, %xmm2
	vsubss	%xmm2, %xmm8, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	movl	$2, %eax
	vxorps	%xmm14, %xmm14, %xmm14
	vcvtsi2ss	%eax, %xmm14, %xmm2
	vmulss	56(%rsp), %xmm2, %xmm2          # 4-byte Folded Reload
	vmulss	%xmm2, %xmm3, %xmm3
	vmulss	%xmm2, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0],xmm0[0],xmm3[2,3]
	vmulss	%xmm2, %xmm1, %xmm2
.LBB23_209:
	vmovlps	%xmm0, 96(%rsp)
	vmovss	%xmm2, 104(%rsp)
.Ltmp1091:
	leaq	792(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1092:
# %bb.210:
	movq	792(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_212
# %bb.211:
	callq	_ZdlPv
.LBB23_212:
	leaq	776(%rsp), %rbx
	movq	%rbx, 760(%rsp)
	movl	$1953657203, 776(%rsp)          # imm = 0x74727173
	movq	$4, 768(%rsp)
	vmovss	24(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm0, %xmm1
	movb	$0, 780(%rsp)
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB23_214
# %bb.213:
	vxorps	%xmm1, %xmm1, %xmm1
	vblendps	$1, %xmm0, %xmm1, %xmm0         # xmm0 = xmm0[0],xmm1[1,2,3]
	jmp	.LBB23_225
.LBB23_214:
	vcomiss	%xmm0, %xmm1
	jbe	.LBB23_216
# %bb.215:
	vbroadcastss	.LCPI23_35(%rip), %xmm0 # xmm0 = [NaN,NaN,NaN,NaN]
	vmovaps	%xmm0, %xmm1
	jmp	.LBB23_225
.LBB23_216:
	vmovss	28(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	vmovss	%xmm1, 160(%rsp)                # 4-byte Spill
	vmovups	%xmm0, 32(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vxorps	%xmm2, %xmm2, %xmm2
	vucomiss	%xmm2, %xmm0
	setnp	%cl
	sete	%dl
	vmovd	.LCPI23_30(%rip), %xmm1         # xmm1 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	vmovdqa	%xmm1, %xmm3
	testb	%cl, %dl
	jne	.LBB23_220
# %bb.217:
	movl	%eax, %ecx
	andl	$2139095040, %ecx               # imm = 0x7F800000
	cmpl	$2139095040, %ecx               # imm = 0x7F800000
	je	.LBB23_219
# %bb.218:
	vmovd	%ecx, %xmm3
	jmp	.LBB23_220
.LBB23_219:
	vmovd	.LCPI23_30(%rip), %xmm3         # xmm3 = mem[0],zero,zero,zero
.LBB23_220:
	vmovd	%xmm3, 172(%rsp)                # 4-byte Folded Spill
	vucomiss	%xmm2, %xmm0
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB23_224
# %bb.221:
	andl	$2139095040, %eax               # imm = 0x7F800000
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB23_223
# %bb.222:
	movl	$2130706432, %ecx               # imm = 0x7F000000
	subl	%eax, %ecx
	vmovd	%ecx, %xmm1
	jmp	.LBB23_224
.LBB23_223:
	vmovd	.LCPI23_30(%rip), %xmm1         # xmm1 = mem[0],zero,zero,zero
.LBB23_224:
	vmovss	.LCPI23_31(%rip), %xmm6         # xmm6 = mem[0],zero,zero,zero
	vmulss	%xmm6, %xmm1, %xmm4
	vmulss	32(%rsp), %xmm1, %xmm2          # 16-byte Folded Reload
	vmulss	%xmm4, %xmm2, %xmm2
	vmovss	%xmm2, 64(%rsp)                 # 4-byte Spill
	vmulss	160(%rsp), %xmm1, %xmm3         # 4-byte Folded Reload
	vmulss	%xmm4, %xmm3, %xmm8
	vxorps	%xmm2, %xmm2, %xmm2
	vmulss	%xmm1, %xmm2, %xmm1
	vmulss	%xmm4, %xmm1, %xmm1
	vmovss	%xmm1, 32(%rsp)                 # 4-byte Spill
	movl	$3, %eax
	vcvtsi2ss	%eax, %xmm13, %xmm5
	vmovaps	%xmm6, %xmm3
	vmulss	%xmm6, %xmm5, %xmm1
	vfmsub213ss	%xmm1, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm6) - xmm1
	vmulss	%xmm2, %xmm5, %xmm7
	vxorps	%xmm9, %xmm9, %xmm9
	vfmsub213ss	%xmm7, %xmm5, %xmm9     # xmm9 = (xmm5 * xmm9) - xmm7
	vaddss	%xmm7, %xmm6, %xmm10
	vsubss	%xmm6, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm6, %xmm6
	vsubss	%xmm11, %xmm7, %xmm11
	vaddss	%xmm6, %xmm11, %xmm11
	vaddss	%xmm2, %xmm10, %xmm6
	vsubss	%xmm10, %xmm6, %xmm12
	vsubss	%xmm12, %xmm6, %xmm13
	vsubss	%xmm13, %xmm10, %xmm10
	vsubss	%xmm12, %xmm2, %xmm12
	vaddss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm10, %xmm11, %xmm12
	vsubss	%xmm11, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm11, %xmm11
	vsubss	%xmm13, %xmm10, %xmm10
	vaddss	%xmm10, %xmm11, %xmm10
	vaddss	%xmm9, %xmm10, %xmm10
	vaddss	%xmm2, %xmm9, %xmm11
	vsubss	%xmm9, %xmm11, %xmm13
	vsubss	%xmm13, %xmm11, %xmm14
	vsubss	%xmm14, %xmm9, %xmm9
	vsubss	%xmm13, %xmm2, %xmm13
	vaddss	%xmm13, %xmm9, %xmm13
	vaddss	%xmm7, %xmm11, %xmm9
	vsubss	%xmm11, %xmm9, %xmm14
	vsubss	%xmm14, %xmm9, %xmm15
	vsubss	%xmm15, %xmm11, %xmm11
	vsubss	%xmm14, %xmm7, %xmm7
	vaddss	%xmm7, %xmm11, %xmm7
	vaddss	%xmm2, %xmm12, %xmm11
	vsubss	%xmm2, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm15
	vsubss	%xmm15, %xmm2, %xmm15
	vsubss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm12, %xmm15, %xmm12
	vaddss	%xmm11, %xmm9, %xmm14
	vsubss	%xmm9, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm4
	vsubss	%xmm4, %xmm9, %xmm4
	vmovss	172(%rsp), %xmm2                # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm2, %xmm9
	vxorps	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm10, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vsubss	%xmm15, %xmm11, %xmm10
	vaddss	%xmm4, %xmm10, %xmm4
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vfmadd231ss	%xmm3, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm3) + xmm0
	vfmadd231ss	%xmm2, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm2) + xmm0
	vfmadd231ss	%xmm2, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm2) + xmm0
	vfmadd231ss	%xmm5, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm5) + xmm0
	vxorps	%xmm3, %xmm3, %xmm3
	vaddss	%xmm0, %xmm14, %xmm4
	vsubss	%xmm4, %xmm14, %xmm5
	vaddss	%xmm0, %xmm5, %xmm0
	vmovss	%xmm0, 136(%rsp)                # 4-byte Spill
	vaddss	%xmm4, %xmm6, %xmm0
	vsubss	%xmm0, %xmm6, %xmm5
	vaddss	%xmm4, %xmm5, %xmm2
	vmovss	%xmm2, 56(%rsp)                 # 4-byte Spill
	vaddss	%xmm0, %xmm1, %xmm2
	vmovss	%xmm2, 156(%rsp)                # 4-byte Spill
	vsubss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, 60(%rsp)                 # 4-byte Spill
	vmulss	%xmm9, %xmm9, %xmm10
	vmovaps	%xmm9, %xmm0
	vfmsub213ss	%xmm10, %xmm9, %xmm0    # xmm0 = (xmm9 * xmm0) - xmm10
	vmulss	%xmm3, %xmm9, %xmm1
	vmulss	%xmm3, %xmm9, %xmm11
	vaddss	%xmm1, %xmm0, %xmm4
	vsubss	%xmm0, %xmm4, %xmm12
	vsubss	%xmm12, %xmm4, %xmm13
	vsubss	%xmm13, %xmm0, %xmm0
	vsubss	%xmm12, %xmm1, %xmm12
	vaddss	%xmm0, %xmm12, %xmm12
	vaddss	%xmm4, %xmm11, %xmm2
	vsubss	%xmm4, %xmm2, %xmm13
	vsubss	%xmm13, %xmm2, %xmm14
	vsubss	%xmm14, %xmm4, %xmm4
	vsubss	%xmm13, %xmm11, %xmm13
	vaddss	%xmm4, %xmm13, %xmm4
	vxorps	%xmm13, %xmm13, %xmm13
	vfmsub213ss	%xmm1, %xmm9, %xmm13    # xmm13 = (xmm9 * xmm13) - xmm1
	vmovaps	%xmm9, %xmm14
	vfmsub213ss	%xmm11, %xmm3, %xmm14   # xmm14 = (xmm3 * xmm14) - xmm11
	vaddss	%xmm4, %xmm12, %xmm15
	vsubss	%xmm12, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm5
	vsubss	%xmm5, %xmm12, %xmm5
	vsubss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm14, %xmm13, %xmm4
	vsubss	%xmm13, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm12
	vsubss	%xmm12, %xmm13, %xmm12
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm14, %xmm0
	vsubss	%xmm5, %xmm14, %xmm5
	vaddss	%xmm5, %xmm12, %xmm5
	vaddss	%xmm3, %xmm11, %xmm12
	vsubss	%xmm3, %xmm12, %xmm13
	vsubss	%xmm13, %xmm11, %xmm11
	vsubss	%xmm13, %xmm12, %xmm13
	vsubss	%xmm13, %xmm3, %xmm13
	vaddss	%xmm11, %xmm13, %xmm11
	vaddss	%xmm1, %xmm4, %xmm13
	vsubss	%xmm4, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm6
	vsubss	%xmm6, %xmm4, %xmm4
	vsubss	%xmm14, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm15, %xmm12, %xmm4
	vsubss	%xmm12, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm14
	vsubss	%xmm14, %xmm12, %xmm12
	vsubss	%xmm6, %xmm15, %xmm6
	vaddss	%xmm6, %xmm12, %xmm6
	vaddss	%xmm4, %xmm13, %xmm12
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm12, %xmm15
	vsubss	%xmm15, %xmm13, %xmm13
	vsubss	%xmm14, %xmm4, %xmm4
	vaddss	%xmm4, %xmm13, %xmm4
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vfmadd231ss	%xmm9, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm9) + xmm0
	vfmadd231ss	%xmm1, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm1) + xmm0
	vfmadd231ss	%xmm1, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm1) + xmm0
	vfmadd231ss	%xmm1, %xmm9, %xmm0     # xmm0 = (xmm9 * xmm1) + xmm0
	vaddss	%xmm0, %xmm12, %xmm1
	vsubss	%xmm1, %xmm12, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vmovss	%xmm0, 132(%rsp)                # 4-byte Spill
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm11
	vaddss	%xmm0, %xmm10, %xmm3
	vsubss	%xmm3, %xmm10, %xmm2
	vaddss	%xmm0, %xmm2, %xmm13
	vmovss	64(%rsp), %xmm12                # 4-byte Reload
                                        # xmm12 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm12, %xmm2
	vmovss	%xmm2, 188(%rsp)                # 4-byte Spill
	vmovaps	%xmm3, %xmm0
	vfmsub213ss	%xmm2, %xmm12, %xmm0    # xmm0 = (xmm12 * xmm0) - xmm2
	vmulss	%xmm13, %xmm12, %xmm2
	vaddss	%xmm2, %xmm0, %xmm4
	vsubss	%xmm0, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm13, %xmm6
	vfmsub213ss	%xmm2, %xmm12, %xmm6    # xmm6 = (xmm12 * xmm6) - xmm2
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vmulss	%xmm3, %xmm8, %xmm2
	vaddss	%xmm2, %xmm4, %xmm14
	vsubss	%xmm4, %xmm14, %xmm5
	vsubss	%xmm5, %xmm14, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vmovaps	%xmm3, %xmm15
	vfmsub213ss	%xmm2, %xmm8, %xmm15    # xmm15 = (xmm8 * xmm15) - xmm2
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm2
	vaddss	%xmm2, %xmm0, %xmm4
	vsubss	%xmm0, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm7
	vsubss	%xmm7, %xmm0, %xmm0
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm11, %xmm5
	vmovss	%xmm11, 184(%rsp)               # 4-byte Spill
	vmulss	%xmm11, %xmm12, %xmm2
	vfmsub213ss	%xmm2, %xmm12, %xmm5    # xmm5 = (xmm12 * xmm5) - xmm2
	vaddss	%xmm5, %xmm0, %xmm0
	vmulss	%xmm13, %xmm8, %xmm5
	vmovaps	%xmm13, %xmm7
	vfmsub213ss	%xmm5, %xmm8, %xmm7     # xmm7 = (xmm8 * xmm7) - xmm5
	vmovss	%xmm8, 160(%rsp)                # 4-byte Spill
	vaddss	%xmm7, %xmm0, %xmm0
	vmovss	32(%rsp), %xmm10                # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm10, %xmm7
	vmovaps	%xmm3, %xmm1
	vfmsub213ss	%xmm7, %xmm10, %xmm1    # xmm1 = (xmm10 * xmm1) - xmm7
	vaddss	%xmm1, %xmm0, %xmm10
	vaddss	%xmm6, %xmm15, %xmm1
	vsubss	%xmm6, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm11
	vsubss	%xmm11, %xmm6, %xmm6
	vsubss	%xmm0, %xmm15, %xmm0
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm7, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm11
	vsubss	%xmm11, %xmm6, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm11, %xmm7, %xmm7
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm2, %xmm1, %xmm7
	vsubss	%xmm1, %xmm7, %xmm11
	vsubss	%xmm11, %xmm7, %xmm15
	vsubss	%xmm15, %xmm1, %xmm1
	vsubss	%xmm11, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm4, %xmm6, %xmm2
	vsubss	%xmm6, %xmm2, %xmm11
	vsubss	%xmm11, %xmm2, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm11, %xmm4, %xmm4
	vaddss	%xmm4, %xmm6, %xmm4
	vaddss	%xmm2, %xmm7, %xmm6
	vsubss	%xmm7, %xmm6, %xmm11
	vsubss	%xmm11, %xmm6, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm11, %xmm2, %xmm2
	vaddss	%xmm2, %xmm7, %xmm2
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vmovss	32(%rsp), %xmm1                 # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm3, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm3) + xmm0
	vfmadd231ss	%xmm13, %xmm1, %xmm0    # xmm0 = (xmm1 * xmm13) + xmm0
	vfmadd231ss	184(%rsp), %xmm8, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm8 * mem) + xmm0
	vfmadd231ss	132(%rsp), %xmm12, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm12 * mem) + xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm2
	vaddss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm14, %xmm0
	vsubss	%xmm0, %xmm14, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	188(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm0, %xmm4, %xmm4
	vbroadcastss	.LCPI23_15(%rip), %xmm10 # xmm10 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm3, %xmm10, %xmm5
	vmovss	156(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm6, %xmm0
	vsubss	%xmm6, %xmm0, %xmm3
	vsubss	%xmm3, %xmm5, %xmm5
	vsubss	%xmm3, %xmm0, %xmm3
	vsubss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm5, %xmm3, %xmm3
	vxorps	%xmm4, %xmm10, %xmm5
	vmovss	60(%rsp), %xmm7                 # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm7, %xmm4
	vsubss	%xmm7, %xmm4, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm6, %xmm4, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm5, %xmm6, %xmm5
	vxorps	%xmm1, %xmm10, %xmm6
	vmovss	56(%rsp), %xmm10                # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm10, %xmm1
	vsubss	%xmm10, %xmm1, %xmm7
	vsubss	%xmm7, %xmm6, %xmm6
	vsubss	%xmm7, %xmm1, %xmm7
	vsubss	%xmm7, %xmm10, %xmm7
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	136(%rsp), %xmm6, %xmm6         # 4-byte Folded Reload
	vsubss	%xmm2, %xmm6, %xmm2
	vaddss	%xmm4, %xmm3, %xmm6
	vsubss	%xmm3, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm10
	vsubss	%xmm10, %xmm3, %xmm3
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm5, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm10
	vsubss	%xmm10, %xmm3, %xmm3
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm2
	vsubss	%xmm2, %xmm5, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm1, 132(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm6, %xmm3
	vsubss	%xmm3, %xmm6, %xmm4
	vaddss	%xmm2, %xmm4, %xmm12
	vaddss	%xmm3, %xmm0, %xmm10
	vsubss	%xmm10, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm13
	vmulss	%xmm9, %xmm10, %xmm8
	vmovaps	%xmm9, %xmm0
	vfmsub213ss	%xmm8, %xmm10, %xmm0    # xmm0 = (xmm10 * xmm0) - xmm8
	vxorps	%xmm11, %xmm11, %xmm11
	vmulss	%xmm11, %xmm10, %xmm14
	vaddss	%xmm0, %xmm14, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vsubss	%xmm3, %xmm14, %xmm3
	vaddss	%xmm3, %xmm0, %xmm3
	vmulss	%xmm9, %xmm13, %xmm4
	vaddss	%xmm4, %xmm2, %xmm1
	vsubss	%xmm2, %xmm1, %xmm5
	vsubss	%xmm5, %xmm1, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm9, %xmm6
	vfmsub213ss	%xmm4, %xmm13, %xmm6    # xmm6 = (xmm13 * xmm6) - xmm4
	vsubss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	vfmsub213ss	%xmm14, %xmm10, %xmm3   # xmm3 = (xmm10 * xmm3) - xmm14
	vmulss	%xmm11, %xmm13, %xmm5
	vxorps	%xmm7, %xmm7, %xmm7
	vfmsub213ss	%xmm5, %xmm13, %xmm7    # xmm7 = (xmm13 * xmm7) - xmm5
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm7, %xmm2, %xmm2
	vmulss	%xmm9, %xmm12, %xmm7
	vmovaps	%xmm9, %xmm15
	vfmsub213ss	%xmm7, %xmm12, %xmm15   # xmm15 = (xmm12 * xmm15) - xmm7
	vaddss	%xmm2, %xmm15, %xmm2
	vaddss	%xmm6, %xmm3, %xmm15
	vsubss	%xmm3, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm11
	vsubss	%xmm11, %xmm3, %xmm3
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm7, %xmm5, %xmm3
	vsubss	%xmm5, %xmm3, %xmm6
	vsubss	%xmm6, %xmm3, %xmm11
	vsubss	%xmm11, %xmm5, %xmm5
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm14, %xmm15, %xmm6
	vsubss	%xmm15, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm11
	vsubss	%xmm11, %xmm15, %xmm11
	vsubss	%xmm7, %xmm14, %xmm7
	vaddss	%xmm7, %xmm11, %xmm7
	vaddss	%xmm4, %xmm3, %xmm11
	vsubss	%xmm3, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm15
	vsubss	%xmm15, %xmm3, %xmm3
	vsubss	%xmm14, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm6, %xmm11, %xmm4
	vsubss	%xmm6, %xmm4, %xmm14
	vsubss	%xmm14, %xmm4, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm6, %xmm11, %xmm6
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vfmadd231ss	132(%rsp), %xmm9, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm9 * mem) + xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vfmadd231ss	%xmm12, %xmm2, %xmm0    # xmm0 = (xmm2 * xmm12) + xmm0
	vfmadd231ss	%xmm13, %xmm2, %xmm0    # xmm0 = (xmm2 * xmm13) + xmm0
	vfmadd231ss	%xmm2, %xmm10, %xmm0    # xmm0 = (xmm10 * xmm2) + xmm0
	vaddss	%xmm0, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm0, %xmm3, %xmm9
	vaddss	%xmm2, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm11
	vaddss	%xmm0, %xmm8, %xmm10
	vsubss	%xmm10, %xmm8, %xmm1
	vaddss	%xmm0, %xmm1, %xmm12
	vmulss	%xmm10, %xmm10, %xmm8
	vmovaps	%xmm10, %xmm0
	vfmsub213ss	%xmm8, %xmm10, %xmm0    # xmm0 = (xmm10 * xmm0) - xmm8
	vmulss	%xmm12, %xmm10, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm12, %xmm4
	vfmsub213ss	%xmm1, %xmm10, %xmm4    # xmm4 = (xmm10 * xmm4) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmulss	%xmm10, %xmm12, %xmm3
	vaddss	%xmm3, %xmm2, %xmm1
	vsubss	%xmm2, %xmm1, %xmm5
	vsubss	%xmm5, %xmm1, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm10, %xmm6
	vfmsub213ss	%xmm3, %xmm12, %xmm6    # xmm6 = (xmm12 * xmm6) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm0, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm7
	vsubss	%xmm7, %xmm0, %xmm0
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm11, %xmm7
	vmulss	%xmm11, %xmm10, %xmm2
	vmovaps	%xmm11, %xmm5
	vfmsub213ss	%xmm2, %xmm10, %xmm5    # xmm5 = (xmm10 * xmm5) - xmm2
	vaddss	%xmm5, %xmm0, %xmm0
	vmulss	%xmm12, %xmm12, %xmm5
	vmovaps	%xmm12, %xmm7
	vfmsub213ss	%xmm5, %xmm12, %xmm7    # xmm7 = (xmm12 * xmm7) - xmm5
	vaddss	%xmm7, %xmm0, %xmm0
	vmulss	%xmm10, %xmm11, %xmm7
	vmovaps	%xmm10, %xmm13
	vfmsub213ss	%xmm7, %xmm11, %xmm13   # xmm13 = (xmm11 * xmm13) - xmm7
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm6, %xmm4, %xmm13
	vsubss	%xmm4, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm14, %xmm6, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm7, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm14, %xmm7, %xmm7
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm2, %xmm13, %xmm7
	vsubss	%xmm13, %xmm7, %xmm14
	vsubss	%xmm14, %xmm7, %xmm15
	vsubss	%xmm15, %xmm13, %xmm13
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm2, %xmm13, %xmm2
	vaddss	%xmm3, %xmm6, %xmm13
	vsubss	%xmm6, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm14, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm7, %xmm13, %xmm6
	vsubss	%xmm7, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm7, %xmm13, %xmm7
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vmovss	%xmm9, 188(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm10, %xmm9, %xmm0    # xmm0 = (xmm9 * xmm10) + xmm0
	vmovss	%xmm12, 184(%rsp)               # 4-byte Spill
	vmovss	%xmm11, 132(%rsp)               # 4-byte Spill
	vfmadd231ss	%xmm12, %xmm11, %xmm0   # xmm0 = (xmm11 * xmm12) + xmm0
	vfmadd231ss	%xmm11, %xmm12, %xmm0   # xmm0 = (xmm12 * xmm11) + xmm0
	vfmadd231ss	%xmm9, %xmm10, %xmm0    # xmm0 = (xmm10 * xmm9) + xmm0
	vaddss	%xmm0, %xmm6, %xmm2
	vsubss	%xmm2, %xmm6, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm0, 180(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm12
	vaddss	%xmm0, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm2
	vaddss	%xmm0, %xmm2, %xmm15
	vmovss	64(%rsp), %xmm14                # 4-byte Reload
                                        # xmm14 = mem[0],zero,zero,zero
	vmulss	%xmm9, %xmm14, %xmm2
	vmovss	%xmm2, 176(%rsp)                # 4-byte Spill
	vmovaps	%xmm9, %xmm0
	vfmsub213ss	%xmm2, %xmm14, %xmm0    # xmm0 = (xmm14 * xmm0) - xmm2
	vmulss	%xmm15, %xmm14, %xmm2
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm0, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm15, %xmm5
	vfmsub213ss	%xmm2, %xmm14, %xmm5    # xmm5 = (xmm14 * xmm5) - xmm2
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm2
	vmovss	160(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm9, %xmm4
	vaddss	%xmm4, %xmm3, %xmm11
	vsubss	%xmm3, %xmm11, %xmm6
	vsubss	%xmm6, %xmm11, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vmovaps	%xmm9, %xmm7
	vfmsub213ss	%xmm4, %xmm0, %xmm7     # xmm7 = (xmm0 * xmm7) - xmm4
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmovaps	%xmm12, %xmm6
	vmovss	%xmm12, 192(%rsp)               # 4-byte Spill
	vmulss	%xmm12, %xmm14, %xmm3
	vfmsub213ss	%xmm3, %xmm14, %xmm6    # xmm6 = (xmm14 * xmm6) - xmm3
	vaddss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm0, %xmm12
	vmulss	%xmm0, %xmm15, %xmm6
	vmovaps	%xmm15, %xmm8
	vfmsub213ss	%xmm6, %xmm0, %xmm8     # xmm8 = (xmm0 * xmm8) - xmm6
	vaddss	%xmm2, %xmm8, %xmm2
	vmovss	32(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm9, %xmm8
	vmovaps	%xmm9, %xmm1
	vfmsub213ss	%xmm8, %xmm0, %xmm1     # xmm1 = (xmm0 * xmm1) - xmm8
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm7, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm13
	vsubss	%xmm13, %xmm5, %xmm5
	vsubss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm6, %xmm8, %xmm5
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm13
	vsubss	%xmm13, %xmm6, %xmm6
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm3, %xmm2, %xmm7
	vsubss	%xmm2, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm13
	vsubss	%xmm13, %xmm2, %xmm2
	vsubss	%xmm8, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm4, %xmm5, %xmm3
	vsubss	%xmm5, %xmm3, %xmm8
	vsubss	%xmm8, %xmm3, %xmm13
	vsubss	%xmm13, %xmm5, %xmm5
	vsubss	%xmm8, %xmm4, %xmm4
	vaddss	%xmm4, %xmm5, %xmm4
	vaddss	%xmm3, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm5, %xmm13
	vsubss	%xmm13, %xmm7, %xmm7
	vsubss	%xmm8, %xmm3, %xmm3
	vaddss	%xmm3, %xmm7, %xmm3
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vmovss	32(%rsp), %xmm1                 # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm9, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm9) + xmm0
	vfmadd231ss	%xmm15, %xmm1, %xmm0    # xmm0 = (xmm1 * xmm15) + xmm0
	vfmadd231ss	192(%rsp), %xmm12, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm12 * mem) + xmm0
	vfmadd231ss	180(%rsp), %xmm14, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm14 * mem) + xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm2
	vaddss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm11, %xmm0
	vsubss	%xmm0, %xmm11, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	176(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm0, %xmm4, %xmm4
	vbroadcastss	.LCPI23_15(%rip), %xmm8 # xmm8 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm3, %xmm8, %xmm5
	vmovss	156(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm6, %xmm0
	vsubss	%xmm6, %xmm0, %xmm3
	vsubss	%xmm3, %xmm5, %xmm5
	vsubss	%xmm3, %xmm0, %xmm3
	vsubss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm5, %xmm3, %xmm3
	vxorps	%xmm4, %xmm8, %xmm5
	vmovss	60(%rsp), %xmm7                 # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm7, %xmm4
	vsubss	%xmm7, %xmm4, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm6, %xmm4, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm5, %xmm6, %xmm5
	vxorps	%xmm1, %xmm8, %xmm6
	vmovss	56(%rsp), %xmm8                 # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm8, %xmm1
	vsubss	%xmm8, %xmm1, %xmm7
	vsubss	%xmm7, %xmm6, %xmm6
	vsubss	%xmm7, %xmm1, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	136(%rsp), %xmm6, %xmm6         # 4-byte Folded Reload
	vsubss	%xmm2, %xmm6, %xmm2
	vaddss	%xmm4, %xmm3, %xmm6
	vsubss	%xmm3, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm5, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm2
	vsubss	%xmm2, %xmm5, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm1, 176(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm3
	vaddss	%xmm2, %xmm3, %xmm15
	vaddss	%xmm1, %xmm0, %xmm13
	vsubss	%xmm13, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm10, %xmm13, %xmm2
	vmovss	%xmm2, 180(%rsp)                # 4-byte Spill
	vmovaps	%xmm10, %xmm0
	vfmsub213ss	%xmm2, %xmm13, %xmm0    # xmm0 = (xmm13 * xmm0) - xmm2
	vmovss	184(%rsp), %xmm2                # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm13, %xmm3
	vaddss	%xmm3, %xmm0, %xmm4
	vsubss	%xmm0, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm2, %xmm6
	vmovaps	%xmm2, %xmm11
	vfmsub213ss	%xmm3, %xmm13, %xmm6    # xmm6 = (xmm13 * xmm6) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm3
	vmulss	%xmm1, %xmm10, %xmm5
	vaddss	%xmm5, %xmm4, %xmm2
	vsubss	%xmm4, %xmm2, %xmm7
	vsubss	%xmm7, %xmm2, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vmovaps	%xmm10, %xmm8
	vfmsub213ss	%xmm5, %xmm1, %xmm8     # xmm8 = (xmm1 * xmm8) - xmm5
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vmovss	132(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm7, %xmm13, %xmm4
	vfmsub213ss	%xmm4, %xmm13, %xmm7    # xmm7 = (xmm13 * xmm7) - xmm4
	vaddss	%xmm7, %xmm3, %xmm3
	vmovaps	%xmm11, %xmm0
	vmulss	%xmm1, %xmm11, %xmm7
	vmovaps	%xmm11, %xmm9
	vmovaps	%xmm11, %xmm12
	vfmsub213ss	%xmm7, %xmm1, %xmm9     # xmm9 = (xmm1 * xmm9) - xmm7
	vaddss	%xmm3, %xmm9, %xmm3
	vmulss	%xmm10, %xmm15, %xmm9
	vmovaps	%xmm10, %xmm11
	vfmsub213ss	%xmm9, %xmm15, %xmm11   # xmm11 = (xmm15 * xmm11) - xmm9
	vaddss	%xmm3, %xmm11, %xmm3
	vaddss	%xmm6, %xmm8, %xmm11
	vsubss	%xmm6, %xmm11, %xmm0
	vsubss	%xmm0, %xmm11, %xmm14
	vsubss	%xmm14, %xmm6, %xmm6
	vsubss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm7, %xmm9, %xmm6
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm14
	vsubss	%xmm14, %xmm7, %xmm7
	vsubss	%xmm8, %xmm9, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm11, %xmm8
	vsubss	%xmm11, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm14
	vsubss	%xmm14, %xmm11, %xmm11
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm4, %xmm11, %xmm4
	vaddss	%xmm5, %xmm6, %xmm9
	vsubss	%xmm6, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm14
	vsubss	%xmm14, %xmm6, %xmm6
	vsubss	%xmm11, %xmm5, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vaddss	%xmm9, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm11
	vsubss	%xmm11, %xmm6, %xmm14
	vsubss	%xmm14, %xmm8, %xmm8
	vsubss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vfmadd231ss	176(%rsp), %xmm10, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm10 * mem) + xmm0
	vfmadd231ss	%xmm12, %xmm15, %xmm0   # xmm0 = (xmm15 * xmm12) + xmm0
	vfmadd231ss	132(%rsp), %xmm1, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm1 * mem) + xmm0
	vfmadd231ss	188(%rsp), %xmm13, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm13 * mem) + xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm3
	vaddss	%xmm0, %xmm3, %xmm9
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm11
	vmovss	180(%rsp), %xmm1                # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm10
	vsubss	%xmm10, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm12
	vmulss	%xmm10, %xmm10, %xmm8
	vmovaps	%xmm10, %xmm0
	vfmsub213ss	%xmm8, %xmm10, %xmm0    # xmm0 = (xmm10 * xmm0) - xmm8
	vmulss	%xmm12, %xmm10, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm12, %xmm4
	vfmsub213ss	%xmm1, %xmm10, %xmm4    # xmm4 = (xmm10 * xmm4) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm10, %xmm12, %xmm3
	vaddss	%xmm3, %xmm2, %xmm0
	vsubss	%xmm2, %xmm0, %xmm5
	vsubss	%xmm5, %xmm0, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm10, %xmm6
	vfmsub213ss	%xmm3, %xmm12, %xmm6    # xmm6 = (xmm12 * xmm6) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vmovaps	%xmm11, %xmm7
	vmulss	%xmm11, %xmm10, %xmm2
	vmovaps	%xmm11, %xmm5
	vfmsub213ss	%xmm2, %xmm10, %xmm5    # xmm5 = (xmm10 * xmm5) - xmm2
	vaddss	%xmm5, %xmm1, %xmm1
	vmulss	%xmm12, %xmm12, %xmm5
	vmovaps	%xmm12, %xmm7
	vfmsub213ss	%xmm5, %xmm12, %xmm7    # xmm7 = (xmm12 * xmm7) - xmm5
	vaddss	%xmm7, %xmm1, %xmm1
	vmulss	%xmm10, %xmm11, %xmm7
	vmovaps	%xmm10, %xmm13
	vfmsub213ss	%xmm7, %xmm11, %xmm13   # xmm13 = (xmm11 * xmm13) - xmm7
	vaddss	%xmm1, %xmm13, %xmm1
	vaddss	%xmm6, %xmm4, %xmm13
	vsubss	%xmm4, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm14, %xmm6, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm7, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm14, %xmm7, %xmm7
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm2, %xmm13, %xmm7
	vsubss	%xmm13, %xmm7, %xmm14
	vsubss	%xmm14, %xmm7, %xmm15
	vsubss	%xmm15, %xmm13, %xmm13
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm2, %xmm13, %xmm2
	vaddss	%xmm3, %xmm6, %xmm13
	vsubss	%xmm6, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm14, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm7, %xmm13, %xmm6
	vsubss	%xmm7, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm7, %xmm13, %xmm7
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vmovss	%xmm9, 188(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm10, %xmm9, %xmm1    # xmm1 = (xmm9 * xmm10) + xmm1
	vmovss	%xmm11, 132(%rsp)               # 4-byte Spill
	vmovss	%xmm12, 184(%rsp)               # 4-byte Spill
	vfmadd231ss	%xmm12, %xmm11, %xmm1   # xmm1 = (xmm11 * xmm12) + xmm1
	vfmadd231ss	%xmm11, %xmm12, %xmm1   # xmm1 = (xmm12 * xmm11) + xmm1
	vfmadd231ss	%xmm9, %xmm10, %xmm1    # xmm1 = (xmm10 * xmm9) + xmm1
	vaddss	%xmm1, %xmm6, %xmm2
	vsubss	%xmm2, %xmm6, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm1, 180(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm12
	vaddss	%xmm3, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm0
	vaddss	%xmm3, %xmm0, %xmm15
	vmovss	64(%rsp), %xmm14                # 4-byte Reload
                                        # xmm14 = mem[0],zero,zero,zero
	vmulss	%xmm9, %xmm14, %xmm2
	vmovss	%xmm2, 176(%rsp)                # 4-byte Spill
	vmovaps	%xmm9, %xmm0
	vfmsub213ss	%xmm2, %xmm14, %xmm0    # xmm0 = (xmm14 * xmm0) - xmm2
	vmulss	%xmm15, %xmm14, %xmm2
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm0, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm15, %xmm5
	vfmsub213ss	%xmm2, %xmm14, %xmm5    # xmm5 = (xmm14 * xmm5) - xmm2
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm2
	vmovss	160(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm9, %xmm4
	vaddss	%xmm4, %xmm3, %xmm11
	vsubss	%xmm3, %xmm11, %xmm6
	vsubss	%xmm6, %xmm11, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vmovaps	%xmm9, %xmm7
	vfmsub213ss	%xmm4, %xmm0, %xmm7     # xmm7 = (xmm0 * xmm7) - xmm4
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmovaps	%xmm12, %xmm6
	vmovss	%xmm12, 192(%rsp)               # 4-byte Spill
	vmulss	%xmm12, %xmm14, %xmm3
	vfmsub213ss	%xmm3, %xmm14, %xmm6    # xmm6 = (xmm14 * xmm6) - xmm3
	vaddss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm0, %xmm12
	vmulss	%xmm0, %xmm15, %xmm6
	vmovaps	%xmm15, %xmm8
	vfmsub213ss	%xmm6, %xmm0, %xmm8     # xmm8 = (xmm0 * xmm8) - xmm6
	vaddss	%xmm2, %xmm8, %xmm2
	vmovss	32(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm9, %xmm8
	vmovaps	%xmm9, %xmm1
	vfmsub213ss	%xmm8, %xmm0, %xmm1     # xmm1 = (xmm0 * xmm1) - xmm8
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm7, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm13
	vsubss	%xmm13, %xmm5, %xmm5
	vsubss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm6, %xmm8, %xmm5
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm13
	vsubss	%xmm13, %xmm6, %xmm6
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm3, %xmm2, %xmm7
	vsubss	%xmm2, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm13
	vsubss	%xmm13, %xmm2, %xmm2
	vsubss	%xmm8, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm4, %xmm5, %xmm3
	vsubss	%xmm5, %xmm3, %xmm8
	vsubss	%xmm8, %xmm3, %xmm13
	vsubss	%xmm13, %xmm5, %xmm5
	vsubss	%xmm8, %xmm4, %xmm4
	vaddss	%xmm4, %xmm5, %xmm4
	vaddss	%xmm3, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm5, %xmm13
	vsubss	%xmm13, %xmm7, %xmm7
	vsubss	%xmm8, %xmm3, %xmm3
	vaddss	%xmm3, %xmm7, %xmm3
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vmovss	32(%rsp), %xmm1                 # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm9, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm9) + xmm0
	vfmadd231ss	%xmm15, %xmm1, %xmm0    # xmm0 = (xmm1 * xmm15) + xmm0
	vfmadd231ss	192(%rsp), %xmm12, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm12 * mem) + xmm0
	vfmadd231ss	180(%rsp), %xmm14, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm14 * mem) + xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm2
	vaddss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm11, %xmm0
	vsubss	%xmm0, %xmm11, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	176(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm0, %xmm4, %xmm4
	vmovss	156(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm6, %xmm0
	vsubss	%xmm6, %xmm0, %xmm5
	vmovaps	%xmm6, %xmm7
	vsubss	%xmm5, %xmm0, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vbroadcastss	.LCPI23_15(%rip), %xmm8 # xmm8 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm3, %xmm8, %xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vmovss	60(%rsp), %xmm7                 # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm6
	vmovaps	%xmm7, %xmm9
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm9, %xmm7
	vxorps	%xmm4, %xmm8, %xmm4
	vmovaps	%xmm8, %xmm9
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vmovss	56(%rsp), %xmm8                 # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm7
	vmovaps	%xmm8, %xmm11
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm11, %xmm8
	vxorps	%xmm1, %xmm9, %xmm1
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	136(%rsp), %xmm1, %xmm1         # 4-byte Folded Reload
	vsubss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm5, %xmm3, %xmm7
	vsubss	%xmm3, %xmm7, %xmm2
	vsubss	%xmm2, %xmm7, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm2, %xmm5, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm4, %xmm2, %xmm3
	vsubss	%xmm2, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm6, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm5, %xmm6, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmovss	%xmm1, 60(%rsp)                 # 4-byte Spill
	vaddss	%xmm3, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm4
	vaddss	%xmm3, %xmm4, %xmm13
	vaddss	%xmm1, %xmm0, %xmm7
	vsubss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm2
	vmulss	%xmm7, %xmm10, %xmm1
	vmovss	%xmm1, 56(%rsp)                 # 4-byte Spill
	vmovaps	%xmm10, %xmm0
	vfmsub213ss	%xmm1, %xmm7, %xmm0     # xmm0 = (xmm7 * xmm0) - xmm1
	vmovss	184(%rsp), %xmm1                # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm7, %xmm3
	vaddss	%xmm3, %xmm0, %xmm4
	vsubss	%xmm0, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm8
	vsubss	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm1, %xmm8
	vmovaps	%xmm1, %xmm6
	vfmsub213ss	%xmm3, %xmm7, %xmm8     # xmm8 = (xmm7 * xmm8) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm0
	vmulss	%xmm2, %xmm10, %xmm3
	vaddss	%xmm3, %xmm4, %xmm14
	vsubss	%xmm4, %xmm14, %xmm5
	vsubss	%xmm5, %xmm14, %xmm9
	vsubss	%xmm9, %xmm4, %xmm4
	vmovaps	%xmm10, %xmm9
	vfmsub213ss	%xmm3, %xmm2, %xmm9     # xmm9 = (xmm2 * xmm9) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm4, %xmm3
	vaddss	%xmm3, %xmm0, %xmm4
	vsubss	%xmm0, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm11
	vsubss	%xmm11, %xmm0, %xmm0
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm0
	vmovss	132(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm7, %xmm3
	vfmsub213ss	%xmm3, %xmm7, %xmm5     # xmm5 = (xmm7 * xmm5) - xmm3
	vaddss	%xmm5, %xmm0, %xmm0
	vmulss	%xmm1, %xmm2, %xmm5
	vmovaps	%xmm1, %xmm11
	vmovaps	%xmm1, %xmm12
	vfmsub213ss	%xmm5, %xmm2, %xmm11    # xmm11 = (xmm2 * xmm11) - xmm5
	vaddss	%xmm0, %xmm11, %xmm0
	vmulss	%xmm10, %xmm13, %xmm11
	vmovaps	%xmm10, %xmm15
	vfmsub213ss	%xmm11, %xmm13, %xmm15  # xmm15 = (xmm13 * xmm15) - xmm11
	vaddss	%xmm0, %xmm15, %xmm15
	vaddss	%xmm9, %xmm8, %xmm0
	vsubss	%xmm8, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm6
	vsubss	%xmm6, %xmm8, %xmm6
	vsubss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm1, %xmm6, %xmm1
	vaddss	%xmm5, %xmm11, %xmm6
	vsubss	%xmm5, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vsubss	%xmm8, %xmm11, %xmm8
	vaddss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm3, %xmm0, %xmm8
	vsubss	%xmm0, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm11
	vsubss	%xmm11, %xmm0, %xmm0
	vsubss	%xmm9, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm3
	vaddss	%xmm4, %xmm6, %xmm9
	vsubss	%xmm6, %xmm9, %xmm0
	vsubss	%xmm0, %xmm9, %xmm11
	vsubss	%xmm11, %xmm6, %xmm6
	vsubss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm0, %xmm6, %xmm4
	vaddss	%xmm9, %xmm8, %xmm0
	vsubss	%xmm8, %xmm0, %xmm6
	vsubss	%xmm6, %xmm0, %xmm11
	vmovss	%xmm0, 136(%rsp)                # 4-byte Spill
	vsubss	%xmm11, %xmm8, %xmm8
	vsubss	%xmm6, %xmm9, %xmm6
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm1, %xmm15, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm6, %xmm1, %xmm5
	vfmadd231ss	60(%rsp), %xmm10, %xmm5 # 4-byte Folded Reload
                                        # xmm5 = (xmm10 * mem) + xmm5
	vfmadd231ss	%xmm12, %xmm13, %xmm5   # xmm5 = (xmm13 * xmm12) + xmm5
	vfmadd231ss	132(%rsp), %xmm2, %xmm5 # 4-byte Folded Reload
                                        # xmm5 = (xmm2 * mem) + xmm5
	vfmadd231ss	188(%rsp), %xmm7, %xmm5 # 4-byte Folded Reload
                                        # xmm5 = (xmm7 * mem) + xmm5
	vaddss	%xmm5, %xmm0, %xmm1
	vaddss	%xmm1, %xmm14, %xmm2
	vmovaps	%xmm1, %xmm6
	vmovss	%xmm1, 60(%rsp)                 # 4-byte Spill
	vsubss	%xmm2, %xmm14, %xmm10
	vmovss	56(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm3
	vaddss	%xmm2, %xmm3, %xmm8
	vmovss	64(%rsp), %xmm7                 # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm7, %xmm0
	vmovss	%xmm0, 56(%rsp)                 # 4-byte Spill
	vmovaps	%xmm1, %xmm2
	vfmsub213ss	%xmm0, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm0
	vmulss	%xmm7, %xmm8, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm9
	vsubss	%xmm9, %xmm4, %xmm11
	vsubss	%xmm11, %xmm2, %xmm2
	vmovaps	%xmm8, %xmm11
	vfmsub213ss	%xmm3, %xmm7, %xmm11    # xmm11 = (xmm7 * xmm11) - xmm3
	vsubss	%xmm9, %xmm3, %xmm3
	vmovss	160(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm0, %xmm12
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm4, %xmm12, %xmm9
	vsubss	%xmm4, %xmm9, %xmm3
	vsubss	%xmm3, %xmm9, %xmm13
	vsubss	%xmm13, %xmm4, %xmm4
	vmovaps	%xmm1, %xmm13
	vfmsub213ss	%xmm12, %xmm0, %xmm13   # xmm13 = (xmm0 * xmm13) - xmm12
	vsubss	%xmm3, %xmm12, %xmm3
	vaddss	%xmm3, %xmm4, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm12
	vsubss	%xmm12, %xmm4, %xmm14
	vsubss	%xmm14, %xmm2, %xmm2
	vsubss	%xmm12, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm6, %xmm10, %xmm10
	vmulss	%xmm7, %xmm10, %xmm3
	vmovaps	%xmm10, %xmm12
	vfmsub213ss	%xmm3, %xmm7, %xmm12    # xmm12 = (xmm7 * xmm12) - xmm3
	vaddss	%xmm2, %xmm12, %xmm2
	vmulss	%xmm0, %xmm8, %xmm12
	vmovaps	%xmm8, %xmm14
	vfmsub213ss	%xmm12, %xmm0, %xmm14   # xmm14 = (xmm0 * xmm14) - xmm12
	vaddss	%xmm2, %xmm14, %xmm2
	vmovss	32(%rsp), %xmm7                 # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm7, %xmm14
	vmovaps	%xmm1, %xmm15
	vfmsub213ss	%xmm14, %xmm7, %xmm15   # xmm15 = (xmm7 * xmm15) - xmm14
	vaddss	%xmm2, %xmm15, %xmm2
	vaddss	%xmm13, %xmm11, %xmm15
	vsubss	%xmm11, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm6
	vsubss	%xmm6, %xmm11, %xmm6
	vsubss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm14, %xmm12, %xmm6
	vsubss	%xmm12, %xmm6, %xmm11
	vsubss	%xmm11, %xmm6, %xmm13
	vsubss	%xmm13, %xmm12, %xmm12
	vsubss	%xmm11, %xmm14, %xmm11
	vaddss	%xmm11, %xmm12, %xmm11
	vaddss	%xmm3, %xmm15, %xmm12
	vsubss	%xmm15, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm15, %xmm14
	vsubss	%xmm13, %xmm3, %xmm3
	vaddss	%xmm3, %xmm14, %xmm3
	vaddss	%xmm4, %xmm6, %xmm13
	vsubss	%xmm6, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm14, %xmm4, %xmm4
	vaddss	%xmm4, %xmm6, %xmm4
	vaddss	%xmm13, %xmm12, %xmm6
	vsubss	%xmm12, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm12, %xmm0
	vfmadd231ss	%xmm1, %xmm7, %xmm0     # xmm0 = (xmm7 * xmm1) + xmm0
	vfmadd231ss	%xmm8, %xmm7, %xmm0     # xmm0 = (xmm7 * xmm8) + xmm0
	vfmadd231ss	160(%rsp), %xmm10, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm10 * mem) + xmm0
	vmovss	136(%rsp), %xmm1                # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	60(%rsp), %xmm1, %xmm1          # 4-byte Folded Reload
	vaddss	%xmm5, %xmm1, %xmm1
	vfmadd231ss	64(%rsp), %xmm1, %xmm0  # 4-byte Folded Reload
                                        # xmm0 = (xmm1 * mem) + xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm2
	vaddss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm9, %xmm0
	vsubss	%xmm0, %xmm9, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	56(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	movl	$2, %eax
	vxorps	%xmm5, %xmm5, %xmm5
	vcvtsi2ss	%eax, %xmm5, %xmm5
	vaddss	%xmm0, %xmm4, %xmm0
	vmulss	172(%rsp), %xmm5, %xmm4         # 4-byte Folded Reload
	vmulss	%xmm4, %xmm3, %xmm3
	vmulss	%xmm4, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0],xmm0[0],xmm3[2,3]
	vmulss	%xmm4, %xmm1, %xmm1
	vmulss	%xmm4, %xmm2, %xmm2
	vinsertps	$16, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[2,3]
.LBB23_225:
	vmovlhps	%xmm1, %xmm0, %xmm0             # xmm0 = xmm0[0],xmm1[0]
	vmovups	%xmm0, 96(%rsp)
.Ltmp1094:
	leaq	760(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1095:
# %bb.226:
	movq	760(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_228
# %bb.227:
	callq	_ZdlPv
.LBB23_228:
	vmovsd	.LCPI23_27(%rip), %xmm0         # xmm0 = mem[0],zero
	vcvtsd2ss	%xmm0, %xmm0, %xmm1
	vmovsd	.LCPI23_36(%rip), %xmm0         # xmm0 = mem[0],zero
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	leaq	1688(%rsp), %rbp
	movq	%rbp, 1672(%rsp)
	movl	$543978861, 1688(%rsp)          # imm = 0x206C756D
	movq	$4, 1680(%rsp)
	movb	$0, 1692(%rsp)
	vmovss	%xmm1, 32(%rsp)                 # 4-byte Spill
	vmovss	%xmm0, 64(%rsp)                 # 4-byte Spill
	vmulss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, 136(%rsp)                # 4-byte Spill
	vmovss	%xmm0, 256(%rsp)
	flds	256(%rsp)
	fstpt	160(%rsp)                       # 10-byte Folded Spill
	wait
.Ltmp1097:
	movl	$_ZSt4cout, %edi
	movl	$4, %edx
	movq	%rbp, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1098:
# %bb.229:
	vmovd	136(%rsp), %xmm0                # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, %esi
	fldt	160(%rsp)                       # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	movq	1672(%rsp), %rdi
	cmpq	%rbp, %rdi
	je	.LBB23_231
# %bb.230:
	callq	_ZdlPv
.LBB23_231:
	leaq	744(%rsp), %rbx
	movq	%rbx, 728(%rsp)
	movl	$2020373869, 744(%rsp)          # imm = 0x786C756D
	movq	$4, 736(%rsp)
	movb	$0, 748(%rsp)
	vmovss	32(%rsp), %xmm2                 # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vmovss	64(%rsp), %xmm1                 # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm2, %xmm0
	vfmsub213ss	%xmm0, %xmm2, %xmm1     # xmm1 = (xmm2 * xmm1) - xmm0
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
	vmovlps	%xmm0, 96(%rsp)
.Ltmp1100:
	leaq	728(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1101:
# %bb.232:
	movq	728(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_234
# %bb.233:
	callq	_ZdlPv
.LBB23_234:
	leaq	1656(%rsp), %rbp
	movq	%rbp, 1640(%rsp)
	movl	$543450209, 1656(%rsp)          # imm = 0x20646461
	movq	$4, 1648(%rsp)
	movb	$0, 1660(%rsp)
	vmovss	64(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vaddss	32(%rsp), %xmm0, %xmm0          # 4-byte Folded Reload
	vmovss	%xmm0, 136(%rsp)                # 4-byte Spill
	vmovss	%xmm0, 252(%rsp)
	flds	252(%rsp)
	fstpt	160(%rsp)                       # 10-byte Folded Spill
	wait
.Ltmp1103:
	movl	$_ZSt4cout, %edi
	movl	$4, %edx
	movq	%rbp, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1104:
# %bb.235:
	vmovd	136(%rsp), %xmm0                # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, %esi
	fldt	160(%rsp)                       # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	movq	1640(%rsp), %rdi
	cmpq	%rbp, %rdi
	je	.LBB23_237
# %bb.236:
	callq	_ZdlPv
.LBB23_237:
	leaq	712(%rsp), %rbx
	movq	%rbx, 696(%rsp)
	movl	$2019845217, 712(%rsp)          # imm = 0x78646461
	movq	$4, 704(%rsp)
	movb	$0, 716(%rsp)
	vmovss	32(%rsp), %xmm3                 # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vmovss	64(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm4, %xmm3, %xmm0
	vsubss	%xmm3, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm2, %xmm3, %xmm2
	vsubss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	vmovlps	%xmm0, 96(%rsp)
.Ltmp1106:
	leaq	696(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1107:
# %bb.238:
	movq	696(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_240
# %bb.239:
	callq	_ZdlPv
.LBB23_240:
.Ltmp1109:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.32, %esi
	movl	$4, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1110:
# %bb.241:
	vmovss	.LCPI23_35(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
.Ltmp1111:
	movl	$_ZSt4cout, %edi
	callq	_ZNSo9_M_insertIdEERSoT_
.Ltmp1112:
# %bb.242:
.Ltmp1113:
	movq	%rax, %rbp
	movl	$.L.str.33, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1114:
# %bb.243:
.Ltmp1115:
	movl	$.L.str.34, %esi
	movl	$5, %edx
	movq	%rbp, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1116:
# %bb.244:
	vmovss	.LCPI23_37(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
.Ltmp1117:
	movq	%rbp, %rdi
	callq	_ZNSo9_M_insertIdEERSoT_
.Ltmp1118:
# %bb.245:
.Ltmp1119:
	movl	$.L.str.33, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1120:
# %bb.246:
.Ltmp1121:
	movl	$_ZSt4cout, %edi
	xorl	%esi, %esi
	callq	_ZNSo9_M_insertIbEERSoT_
.Ltmp1122:
# %bb.247:
.Ltmp1123:
	movq	%rax, %r15
	movl	$.L.str.35, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1124:
# %bb.248:
.Ltmp1125:
	movq	%r15, %rdi
	xorl	%esi, %esi
	callq	_ZNSo9_M_insertIbEERSoT_
.Ltmp1126:
# %bb.249:
.Ltmp1127:
	movq	%rax, %r15
	movl	$.L.str.35, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1128:
# %bb.250:
.Ltmp1129:
	movq	%r15, %rdi
	xorl	%esi, %esi
	callq	_ZNSo9_M_insertIbEERSoT_
.Ltmp1130:
# %bb.251:
.Ltmp1131:
	movl	$.L.str.33, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1132:
# %bb.252:
.Ltmp1134:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.36, %esi
	movl	$4, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1135:
# %bb.253:
	vmovss	.LCPI23_38(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
.Ltmp1136:
	movl	$_ZSt4cout, %edi
	callq	_ZNSo9_M_insertIdEERSoT_
.Ltmp1137:
# %bb.254:
.Ltmp1138:
	movq	%rax, %rbp
	movl	$.L.str.33, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1139:
# %bb.255:
.Ltmp1140:
	movl	$.L.str.37, %esi
	movl	$5, %edx
	movq	%rbp, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1141:
# %bb.256:
	vmovss	.LCPI23_39(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
.Ltmp1142:
	movq	%rbp, %rdi
	callq	_ZNSo9_M_insertIdEERSoT_
.Ltmp1143:
# %bb.257:
.Ltmp1144:
	movl	$.L.str.33, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1145:
# %bb.258:
.Ltmp1146:
	movl	$_ZSt4cout, %edi
	xorl	%esi, %esi
	callq	_ZNSo9_M_insertIbEERSoT_
.Ltmp1147:
# %bb.259:
.Ltmp1148:
	movq	%rax, %r15
	movl	$.L.str.35, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1149:
# %bb.260:
.Ltmp1150:
	movq	%r15, %rdi
	movl	$1, %esi
	callq	_ZNSo9_M_insertIbEERSoT_
.Ltmp1151:
# %bb.261:
.Ltmp1152:
	movq	%rax, %r15
	movl	$.L.str.35, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1153:
# %bb.262:
.Ltmp1154:
	movq	%r15, %rdi
	movl	$1, %esi
	callq	_ZNSo9_M_insertIbEERSoT_
.Ltmp1155:
# %bb.263:
.Ltmp1156:
	movl	$.L.str.33, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1157:
# %bb.264:
	vmovss	.LCPI23_38(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
.Ltmp1158:
	movl	$_ZSt4cout, %edi
	callq	_ZNSo9_M_insertIdEERSoT_
.Ltmp1159:
# %bb.265:
.Ltmp1160:
	movq	%rax, %r15
	movl	$.L.str.35, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1161:
# %bb.266:
	vmovss	.LCPI23_38(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
.Ltmp1162:
	movq	%r15, %rdi
	callq	_ZNSo9_M_insertIdEERSoT_
.Ltmp1163:
# %bb.267:
.Ltmp1164:
	movq	%rax, %r15
	movl	$.L.str.35, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1165:
# %bb.268:
	vmovss	.LCPI23_38(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vaddss	.LCPI23_39(%rip), %xmm0, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
.Ltmp1166:
	movq	%r15, %rdi
	callq	_ZNSo9_M_insertIdEERSoT_
.Ltmp1167:
# %bb.269:
.Ltmp1168:
	movl	$.L.str.33, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1169:
# %bb.270:
	vmovss	.LCPI23_39(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
.Ltmp1170:
	movl	$_ZSt4cout, %edi
	callq	_ZNSo9_M_insertIdEERSoT_
.Ltmp1171:
# %bb.271:
.Ltmp1172:
	movq	%rax, %r15
	movl	$.L.str.35, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1173:
# %bb.272:
	vmovss	.LCPI23_39(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
.Ltmp1174:
	movq	%r15, %rdi
	callq	_ZNSo9_M_insertIdEERSoT_
.Ltmp1175:
# %bb.273:
.Ltmp1176:
	movq	%rax, %r15
	movl	$.L.str.35, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1177:
# %bb.274:
	vmovss	.LCPI23_39(%rip), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vaddss	.LCPI23_38(%rip), %xmm0, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
.Ltmp1178:
	movq	%r15, %rdi
	callq	_ZNSo9_M_insertIdEERSoT_
.Ltmp1179:
# %bb.275:
.Ltmp1180:
	movl	$.L.str.33, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1181:
# %bb.276:
	xorl	%eax, %eax
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovups	%xmm0, 32(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
.Ltmp1183:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.38, %esi
	movl	$3, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1184:
# %bb.277:
	vmovups	32(%rsp), %xmm0                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vcvtss2sd	%xmm0, %xmm0, %xmm0
.Ltmp1185:
	movl	$_ZSt4cout, %edi
	callq	_ZNSo9_M_insertIdEERSoT_
.Ltmp1186:
# %bb.278:
.Ltmp1187:
	movq	%rax, %rbp
	movl	$.L.str.33, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1188:
# %bb.279:
.Ltmp1189:
	movl	$.L.str.39, %esi
	movl	$3, %edx
	movq	%rbp, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1190:
# %bb.280:
	vmovups	32(%rsp), %xmm0                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vxorps	80(%rsp), %xmm0, %xmm0          # 16-byte Folded Reload
	vmovups	%xmm0, 80(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vcvtss2sd	%xmm0, %xmm0, %xmm0
.Ltmp1191:
	movq	%rbp, %rdi
	callq	_ZNSo9_M_insertIdEERSoT_
.Ltmp1192:
# %bb.281:
.Ltmp1193:
	movl	$.L.str.33, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1194:
# %bb.282:
	vmovups	32(%rsp), %xmm0                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vucomiss	80(%rsp), %xmm0                 # 16-byte Folded Reload
	setnp	%al
	sete	%cl
	andb	%al, %cl
	movzbl	%cl, %esi
.Ltmp1195:
	movl	$_ZSt4cout, %edi
	callq	_ZNSo9_M_insertIbEERSoT_
.Ltmp1196:
# %bb.283:
.Ltmp1197:
	movq	%rax, %rbp
	movl	$.L.str.35, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1198:
# %bb.284:
	vmovups	80(%rsp), %xmm0                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vucomiss	%xmm0, %xmm0
	setnp	%al
	sete	%cl
	andb	%al, %cl
	movzbl	%cl, %esi
.Ltmp1199:
	movq	%rbp, %rdi
	callq	_ZNSo9_M_insertIbEERSoT_
.Ltmp1200:
# %bb.285:
.Ltmp1201:
	movq	%rax, %rbp
	movl	$.L.str.35, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1202:
# %bb.286:
	vmovups	32(%rsp), %xmm0                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vucomiss	%xmm0, %xmm0
	setnp	%al
	sete	%cl
	andb	%al, %cl
	movzbl	%cl, %esi
.Ltmp1203:
	movq	%rbp, %rdi
	callq	_ZNSo9_M_insertIbEERSoT_
.Ltmp1204:
# %bb.287:
.Ltmp1205:
	movl	$.L.str.33, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1206:
# %bb.288:
	movl	$1, %eax
	vcvtsi2ss	%eax, %xmm1, %xmm0
	leaq	1624(%rsp), %rbp
	movq	%rbp, 1608(%rsp)
	movl	$828730991, 1624(%rsp)          # imm = 0x31656E6F
	movw	$61, 1628(%rsp)
	movq	$5, 1616(%rsp)
	vmovss	%xmm0, 80(%rsp)                 # 4-byte Spill
	vmovss	%xmm0, 248(%rsp)
	flds	248(%rsp)
	fstpt	32(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp1208:
	movl	$_ZSt4cout, %edi
	movl	$5, %edx
	movq	%rbp, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1209:
# %bb.289:
	vmovd	80(%rsp), %xmm0                 # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, %esi
	fldt	32(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	movq	1608(%rsp), %rdi
	cmpq	%rbp, %rdi
	je	.LBB23_291
# %bb.290:
	callq	_ZdlPv
.LBB23_291:
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	.LCPI23_0(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm1, %xmm0
	vmovss	80(%rsp), %xmm1                 # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm1, %xmm0
	leaq	1592(%rsp), %rbp
	movq	%rbp, 1576(%rsp)
	movabsq	$4409434347135135343, %rax      # imm = 0x3D31752D31656E6F
	movq	%rax, 1592(%rsp)
	movq	$8, 1584(%rsp)
	movb	$0, 1600(%rsp)
	vmovss	%xmm0, 80(%rsp)                 # 4-byte Spill
	vmovss	%xmm0, 244(%rsp)
	flds	244(%rsp)
	fstpt	32(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp1211:
	movl	$_ZSt4cout, %edi
	movl	$8, %edx
	movq	%rbp, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1212:
# %bb.292:
	vmovd	80(%rsp), %xmm0                 # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, %esi
	fldt	32(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	movq	1576(%rsp), %rdi
	cmpq	%rbp, %rdi
	je	.LBB23_294
# %bb.293:
	callq	_ZdlPv
.LBB23_294:
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm2, %xmm0
	vmovss	80(%rsp), %xmm2                 # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vmulss	.LCPI23_0(%rip), %xmm2, %xmm1
	vdivss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm2, 24(%rsp)
	vmovss	%xmm0, 28(%rsp)
	leaq	320(%rsp), %rbx
	movq	%rbx, 304(%rsp)
	movq	$23, 96(%rsp)
.Ltmp1214:
	leaq	304(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp1215:
	movq	528(%rsp), %rbp                 # 8-byte Reload
# %bb.295:
	movq	%rax, 304(%rsp)
	movq	96(%rsp), %rcx
	movq	%rcx, 320(%rsp)
	vmovups	.L.str.42(%rip), %xmm0
	vmovups	%xmm0, (%rax)
	movabsq	$4407108089647018357, %rdx      # imm = 0x3D2931752A293175
	movq	%rdx, 15(%rax)
	movq	%rcx, 312(%rsp)
	movq	304(%rsp), %rax
	movb	$0, (%rax,%rcx)
.Ltmp1217:
	leaq	304(%rsp), %rdi
	leaq	24(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1218:
# %bb.296:
	movq	304(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_298
# %bb.297:
	callq	_ZdlPv
.LBB23_298:
	movl	$1, %eax
	vcvtsi2ss	%eax, %xmm3, %xmm0
	vmovss	%xmm0, 144(%rsp)
	movl	$0, 148(%rsp)
	leaq	680(%rsp), %rbx
	movq	%rbx, 664(%rsp)
	movl	$845508207, 680(%rsp)           # imm = 0x32656E6F
	movw	$61, 684(%rsp)
	movq	$5, 672(%rsp)
.Ltmp1220:
	leaq	664(%rsp), %rdi
	leaq	144(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1221:
# %bb.299:
	movq	664(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_301
# %bb.300:
	callq	_ZdlPv
.LBB23_301:
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm3, %xmm0
	vmulss	.LCPI23_16(%rip), %xmm0, %xmm0
	vmovss	.LCPI23_31(%rip), %xmm4         # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm0, %xmm1
	vmovaps	%xmm4, %xmm2
	vfmsub213ss	%xmm1, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm2) - xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vfmadd231ss	%xmm0, %xmm3, %xmm2     # xmm2 = (xmm3 * xmm0) + xmm2
	vfmadd231ss	%xmm3, %xmm4, %xmm2     # xmm2 = (xmm4 * xmm3) + xmm2
	vaddss	%xmm2, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vbroadcastss	.LCPI23_15(%rip), %xmm3 # xmm3 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm3, %xmm0, %xmm2
	vxorps	%xmm3, %xmm1, %xmm3
	vmovss	144(%rsp), %xmm4                # xmm4 = mem[0],zero,zero,zero
	vmovss	148(%rsp), %xmm5                # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm4, %xmm0
	vsubss	%xmm4, %xmm0, %xmm6
	vsubss	%xmm6, %xmm0, %xmm7
	vsubss	%xmm7, %xmm4, %xmm4
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm2
	vsubss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm4
	vsubss	%xmm4, %xmm1, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	vmovlps	%xmm0, 144(%rsp)
	leaq	648(%rsp), %r15
	movq	%r15, 632(%rsp)
	movabsq	$4409715822128623215, %rbx      # imm = 0x3D32752D32656E6F
	movq	%rbx, 648(%rsp)
	movq	$8, 640(%rsp)
	movb	$0, 656(%rsp)
.Ltmp1223:
	leaq	632(%rsp), %rdi
	leaq	144(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1224:
# %bb.302:
	movq	632(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB23_304
# %bb.303:
	callq	_ZdlPv
.LBB23_304:
	vmovss	24(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vmovss	28(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI23_15(%rip), %xmm3 # xmm3 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm3, %xmm0, %xmm2
	vxorps	%xmm3, %xmm1, %xmm3
	vmovss	144(%rsp), %xmm4                # xmm4 = mem[0],zero,zero,zero
	vmovss	148(%rsp), %xmm5                # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm4, %xmm0
	vsubss	%xmm4, %xmm0, %xmm6
	vsubss	%xmm6, %xmm0, %xmm7
	vsubss	%xmm7, %xmm4, %xmm4
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm2
	vsubss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm4
	vsubss	%xmm4, %xmm1, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	vmovlps	%xmm0, 144(%rsp)
	leaq	416(%rsp), %r15
	movq	%r15, 400(%rsp)
	movq	$33, 96(%rsp)
.Ltmp1226:
	leaq	400(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp1227:
# %bb.305:
	movq	%rax, 400(%rsp)
	movq	96(%rsp), %rcx
	movq	%rcx, 416(%rsp)
	vmovups	.L.str.45(%rip), %ymm0
	vmovups	%ymm0, (%rax)
	movb	$61, 32(%rax)
	movq	%rcx, 408(%rsp)
	movb	$0, (%rax,%rcx)
.Ltmp1229:
	leaq	400(%rsp), %rdi
	leaq	144(%rsp), %rsi
	xorl	%edx, %edx
	vzeroupper
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1230:
# %bb.306:
	movq	400(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB23_308
# %bb.307:
	callq	_ZdlPv
.LBB23_308:
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm8, %xmm0
	vmulss	.LCPI23_16(%rip), %xmm0, %xmm0
	vmovss	.LCPI23_31(%rip), %xmm4         # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm0, %xmm1
	vmovaps	%xmm4, %xmm2
	vfmsub213ss	%xmm1, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm2) - xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vfmadd231ss	%xmm0, %xmm3, %xmm2     # xmm2 = (xmm3 * xmm0) + xmm2
	vfmadd231ss	%xmm4, %xmm3, %xmm2     # xmm2 = (xmm3 * xmm4) + xmm2
	vaddss	%xmm2, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vmovss	24(%rsp), %xmm2                 # xmm2 = mem[0],zero,zero,zero
	vmovss	28(%rsp), %xmm3                 # xmm3 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vsubss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm3, %xmm2
	vsubss	%xmm3, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm6
	vsubss	%xmm6, %xmm3, %xmm3
	vsubss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	vmovlps	%xmm0, 144(%rsp)
	leaq	616(%rsp), %r15
	movq	%r15, 600(%rsp)
	movq	%rbx, 616(%rsp)
	movq	$8, 608(%rsp)
	movb	$0, 624(%rsp)
.Ltmp1232:
	leaq	600(%rsp), %rdi
	leaq	144(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1233:
# %bb.309:
	movq	600(%rsp), %rdi
	cmpq	%r15, %rdi
	movq	536(%rsp), %rbx                 # 8-byte Reload
	je	.LBB23_311
# %bb.310:
	callq	_ZdlPv
.LBB23_311:
.Ltmp1235:
	leaq	200(%rsp), %rsi
	movl	$1000000, %edi                  # imm = 0xF4240
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	296(%rsp), %r13                 # 8-byte Reload
	movq	%r13, %r8
	callq	_Z4initiRKN4mpfr6mprealEPS0_S3_S3_
.Ltmp1236:
# %bb.312:
.Ltmp1237:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.46, %esi
	movl	$29, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1238:
# %bb.313:
.Ltmp1239:
	movl	$_ZSt4cout, %edi
	movl	$1000000, %esi                  # imm = 0xF4240
	callq	_ZNSolsEi
.Ltmp1240:
# %bb.314:
.Ltmp1241:
	movl	$.L.str.33, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1242:
# %bb.315:
.Ltmp1243:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.47, %esi
	movl	$29, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1244:
# %bb.316:
.Ltmp1245:
	movl	$_ZSt4cout, %edi
	movl	196(%rsp), %esi                 # 4-byte Reload
	callq	_ZNSolsEi
.Ltmp1246:
# %bb.317:
.Ltmp1247:
	movq	%rax, %r15
	movl	$.L.str.48, %esi
	movl	$3, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1248:
# %bb.318:
.Ltmp1249:
	movq	%r15, %rdi
	movl	196(%rsp), %esi                 # 4-byte Reload
	callq	_ZNSolsEi
.Ltmp1250:
# %bb.319:
.Ltmp1251:
	movl	$.L.str.33, %esi
	movl	$1, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1252:
# %bb.320:
.Ltmp1253:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.49, %esi
	movl	$91, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1254:
# %bb.321:
.Ltmp1255:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIfEvRKiRKN4mpfr6mprealEPS3_S6_S6_
.Ltmp1256:
# %bb.322:
.Ltmp1257:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1258:
# %bb.323:
.Ltmp1259:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1260:
# %bb.324:
.Ltmp1261:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1262:
# %bb.325:
.Ltmp1263:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIdEvRKiRKN4mpfr6mprealEPS3_S6_S6_
.Ltmp1264:
# %bb.326:
.Ltmp1265:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1266:
# %bb.327:
.Ltmp1267:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1268:
# %bb.328:
.Ltmp1269:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1270:
# %bb.329:
.Ltmp1271:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1272:
# %bb.330:
.Ltmp1273:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1274:
# %bb.331:
.Ltmp1275:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1276:
# %bb.332:
.Ltmp1277:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1278:
# %bb.333:
.Ltmp1279:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1280:
# %bb.334:
.Ltmp1281:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyI7dd_realEvRKiRKN4mpfr6mprealEPS4_S7_S7_
.Ltmp1282:
# %bb.335:
.Ltmp1283:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1284:
# %bb.336:
.Ltmp1285:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1286:
# %bb.337:
.Ltmp1287:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1288:
# %bb.338:
.Ltmp1289:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1290:
# %bb.339:
.Ltmp1291:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1292:
# %bb.340:
.Ltmp1293:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1294:
# %bb.341:
.Ltmp1295:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyI7qd_realEvRKiRKN4mpfr6mprealEPS4_S7_S7_
.Ltmp1296:
# %bb.342:
.Ltmp1297:
	leaq	52(%rsp), %rdi
	leaq	200(%rsp), %rsi
	movq	%rbp, %rdx
	movq	%rbx, %rcx
	movq	%r13, %r8
	callq	_Z6verifyIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.Ltmp1298:
# %bb.343:
	movq	_ZSt4cout(%rip), %rax
	movq	-24(%rax), %rax
	movq	_ZSt4cout+240(%rax), %r15
	testq	%r15, %r15
	je	.LBB23_368
# %bb.344:
	cmpb	$0, 56(%r15)
	je	.LBB23_346
# %bb.345:
	movzbl	67(%r15), %eax
	jmp	.LBB23_348
.LBB23_346:
.Ltmp1299:
	movq	%r15, %rdi
	callq	_ZNKSt5ctypeIcE13_M_widen_initEv
.Ltmp1300:
# %bb.347:
	movq	(%r15), %rax
.Ltmp1301:
	movq	%r15, %rdi
	movl	$10, %esi
	callq	*48(%rax)
.Ltmp1302:
.LBB23_348:
.Ltmp1303:
	movsbl	%al, %esi
	movl	$_ZSt4cout, %edi
	callq	_ZNSo3putEc
.Ltmp1304:
# %bb.349:
.Ltmp1305:
	movq	%rax, %rdi
	callq	_ZNSo5flushEv
.Ltmp1306:
# %bb.350:
	movq	288(%rsp), %rax                 # 8-byte Reload
	movq	(%rax), %rbx
	shlq	$5, %rbx
	je	.LBB23_355
# %bb.351:
	addq	%rbx, %r12
	addq	$-24, %r12
	negq	%rbx
	jmp	.LBB23_353
	.p2align	4, 0x90
.LBB23_352:                             #   in Loop: Header=BB23_353 Depth=1
	addq	$-32, %r12
	addq	$32, %rbx
	je	.LBB23_355
.LBB23_353:                             # =>This Inner Loop Header: Depth=1
	cmpq	$0, 24(%r12)
	je	.LBB23_352
# %bb.354:                              #   in Loop: Header=BB23_353 Depth=1
.Ltmp1307:
	movq	%r12, %rdi
	callq	mpfr_clear
.Ltmp1308:
	jmp	.LBB23_352
.LBB23_355:
	movq	288(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	movq	(%r14), %r15
	shlq	$5, %r15
	je	.LBB23_360
# %bb.356:
	leaq	(%r15,%r14), %rbx
	addq	$-24, %rbx
	negq	%r15
	jmp	.LBB23_358
	.p2align	4, 0x90
.LBB23_357:                             #   in Loop: Header=BB23_358 Depth=1
	addq	$-32, %rbx
	addq	$32, %r15
	je	.LBB23_360
.LBB23_358:                             # =>This Inner Loop Header: Depth=1
	cmpq	$0, 24(%rbx)
	je	.LBB23_357
# %bb.359:                              #   in Loop: Header=BB23_358 Depth=1
.Ltmp1310:
	movq	%rbx, %rdi
	callq	mpfr_clear
.Ltmp1311:
	jmp	.LBB23_357
.LBB23_360:
	movq	%r14, %rdi
	callq	_ZdaPv
	movq	232(%rsp), %rax                 # 8-byte Reload
	movq	(%rax), %r14
	shlq	$5, %r14
	je	.LBB23_365
# %bb.361:
	movq	232(%rsp), %rax                 # 8-byte Reload
	leaq	(%r14,%rax), %rbx
	addq	$-24, %rbx
	negq	%r14
	jmp	.LBB23_363
	.p2align	4, 0x90
.LBB23_362:                             #   in Loop: Header=BB23_363 Depth=1
	addq	$-32, %rbx
	addq	$32, %r14
	je	.LBB23_365
.LBB23_363:                             # =>This Inner Loop Header: Depth=1
	cmpq	$0, 24(%rbx)
	je	.LBB23_362
# %bb.364:                              #   in Loop: Header=BB23_363 Depth=1
.Ltmp1313:
	movq	%rbx, %rdi
	callq	mpfr_clear
.Ltmp1314:
	jmp	.LBB23_362
.LBB23_365:
	movq	232(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	cmpq	$0, 224(%rsp)
	je	.LBB23_367
# %bb.366:
.Ltmp1316:
	leaq	200(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1317:
.LBB23_367:
	xorl	%eax, %eax
	addq	$2472, %rsp                     # imm = 0x9A8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB23_368:
	.cfi_def_cfa_offset 2528
.Ltmp1319:
	callq	_ZSt16__throw_bad_castv
.Ltmp1320:
# %bb.369:
.LBB23_370:
.Ltmp1318:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB23_371:
.Ltmp928:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB23_372:
.Ltmp1234:
	movq	%rax, %r13
	movq	600(%rsp), %rdi
	jmp	.LBB23_376
.LBB23_373:
.Ltmp1231:
	movq	%rax, %r13
	movq	400(%rsp), %rdi
	jmp	.LBB23_376
.LBB23_374:
.Ltmp1228:
	jmp	.LBB23_456
.LBB23_375:
.Ltmp1225:
	movq	%rax, %r13
	movq	632(%rsp), %rdi
.LBB23_376:
	cmpq	%r15, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_377:
.Ltmp1222:
	movq	%rax, %r13
	movq	664(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_378:
.Ltmp1219:
	movq	%rax, %r13
	movq	304(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_379:
.Ltmp1216:
	jmp	.LBB23_456
.LBB23_380:
.Ltmp1213:
	movq	%rax, %r13
	movq	1576(%rsp), %rdi
	cmpq	%rbp, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_381:
.Ltmp1210:
	movq	%rax, %r13
	movq	1608(%rsp), %rdi
	cmpq	%rbp, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_382:
.Ltmp1108:
	movq	%rax, %r13
	movq	696(%rsp), %rdi
	jmp	.LBB23_395
.LBB23_383:
.Ltmp1105:
	movq	%rax, %r13
	movq	1640(%rsp), %rdi
	jmp	.LBB23_399
.LBB23_384:
.Ltmp1102:
	movq	%rax, %r13
	movq	728(%rsp), %rdi
	jmp	.LBB23_395
.LBB23_385:
.Ltmp1099:
	movq	%rax, %r13
	movq	1672(%rsp), %rdi
	jmp	.LBB23_399
.LBB23_386:
.Ltmp1096:
	movq	%rax, %r13
	movq	760(%rsp), %rdi
	jmp	.LBB23_395
.LBB23_387:
.Ltmp1093:
	movq	%rax, %r13
	movq	792(%rsp), %rdi
	jmp	.LBB23_395
.LBB23_388:
.Ltmp1090:
	movq	%rax, %r13
	movq	824(%rsp), %rdi
	jmp	.LBB23_395
.LBB23_389:
.Ltmp1087:
	movq	%rax, %r13
	movq	856(%rsp), %rdi
	jmp	.LBB23_395
.LBB23_390:
.Ltmp1084:
	movq	%rax, %r13
	movq	888(%rsp), %rdi
	jmp	.LBB23_395
.LBB23_391:
.Ltmp1081:
	movq	%rax, %r13
	movq	920(%rsp), %rdi
	jmp	.LBB23_395
.LBB23_392:
.Ltmp1078:
	movq	%rax, %r13
	movq	952(%rsp), %rdi
	jmp	.LBB23_395
.LBB23_393:
.Ltmp1075:
	movq	%rax, %r13
	movq	984(%rsp), %rdi
	jmp	.LBB23_395
.LBB23_394:
.Ltmp1072:
	movq	%rax, %r13
	movq	1016(%rsp), %rdi
	jmp	.LBB23_395
.LBB23_396:
.Ltmp1069:
	movq	%rax, %r13
	movq	432(%rsp), %rdi
	jmp	.LBB23_399
.LBB23_397:
.Ltmp1066:
	movq	%rax, %r13
	movq	464(%rsp), %rdi
	jmp	.LBB23_399
.LBB23_398:
.Ltmp1063:
	movq	%rax, %r13
	movq	496(%rsp), %rdi
.LBB23_399:
	cmpq	%rbp, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_400:
.Ltmp1060:
	movq	%rax, %r13
	movq	1704(%rsp), %rdi
	jmp	.LBB23_395
.LBB23_401:
.Ltmp1057:
	movq	%rax, %r13
	movq	1736(%rsp), %rdi
	jmp	.LBB23_395
.LBB23_403:
.Ltmp1054:
	movq	%rax, %r13
	movq	1768(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_404:
.Ltmp1051:
	movq	%rax, %r13
	movq	2280(%rsp), %rdi
	cmpq	%rbp, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_405:
.Ltmp1048:
	movq	%rax, %r13
	movq	1800(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_406:
.Ltmp1045:
	movq	%rax, %r13
	movq	1832(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_407:
.Ltmp1042:
	movq	%rax, %r13
	movq	1864(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_408:
.Ltmp1039:
	movq	%rax, %r13
	movq	2312(%rsp), %rdi
	cmpq	%rbp, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_409:
.Ltmp1036:
	movq	%rax, %r13
	movq	1896(%rsp), %rdi
	jmp	.LBB23_395
.LBB23_410:
.Ltmp1033:
	movq	%rax, %r13
	movq	1928(%rsp), %rdi
.LBB23_395:
	cmpq	%rbx, %rdi
	je	.LBB23_457
.LBB23_446:
	callq	_ZdlPv
	jmp	.LBB23_457
.LBB23_412:
.Ltmp1030:
	movq	%rax, %r13
	movq	1960(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_413:
.Ltmp1027:
	movq	%rax, %r13
	movq	2344(%rsp), %rdi
	cmpq	%rbp, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_414:
.Ltmp1024:
	movq	%rax, %r13
	movq	1992(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_415:
.Ltmp1021:
	movq	%rax, %r13
	movq	2024(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_416:
.Ltmp1018:
	movq	%rax, %r13
	movq	2056(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_417:
.Ltmp1015:
	movq	%rax, %r13
	movq	2376(%rsp), %rdi
	cmpq	%rbp, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_418:
.Ltmp1012:
	movq	%rax, %r13
	movq	1048(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_419:
.Ltmp1009:
	movq	%rax, %r13
	movq	2088(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_420:
.Ltmp1006:
	movq	%rax, %r13
	movq	1080(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_421:
.Ltmp1003:
	movq	%rax, %r13
	movq	2120(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_422:
.Ltmp1000:
	movq	%rax, %r13
	movq	1112(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_423:
.Ltmp997:
	movq	%rax, %r13
	movq	2152(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_424:
.Ltmp994:
	movq	%rax, %r13
	movq	1144(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_425:
.Ltmp991:
	movq	%rax, %r13
	movq	2408(%rsp), %rdi
	cmpq	%rbp, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_426:
.Ltmp988:
	movq	%rax, %r13
	movq	1176(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_427:
.Ltmp985:
	movq	%rax, %r13
	movq	2184(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_428:
.Ltmp982:
	movq	%rax, %r13
	movq	1208(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_429:
.Ltmp979:
	movq	%rax, %r13
	movq	2216(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_430:
.Ltmp976:
	movq	%rax, %r13
	movq	1240(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_431:
.Ltmp973:
	movq	%rax, %r13
	movq	2248(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_432:
.Ltmp970:
	movq	%rax, %r13
	movq	1272(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_433:
.Ltmp967:
	movq	%rax, %r13
	movq	2440(%rsp), %rdi
	cmpq	%rbp, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_434:
.Ltmp964:
	movq	%rax, %r13
	movq	1304(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_435:
.Ltmp961:
	movq	%rax, %r13
	movq	1336(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_436:
.Ltmp958:
	movq	%rax, %r13
	movq	1368(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_437:
.Ltmp955:
	movq	%rax, %r13
	movq	1400(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_438:
.Ltmp952:
	movq	%rax, %r13
	movq	1432(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_439:
.Ltmp949:
	movq	%rax, %r13
	movq	1464(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_440:
.Ltmp946:
	movq	%rax, %r13
	movq	336(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_441:
.Ltmp943:
	jmp	.LBB23_456
.LBB23_442:
.Ltmp940:
	movq	%rax, %r13
	movq	368(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_443:
.Ltmp937:
	jmp	.LBB23_456
.LBB23_444:
.Ltmp934:
	movq	%rax, %r13
	movq	1512(%rsp), %rdi
	cmpq	%rbx, %rdi
	jne	.LBB23_446
	jmp	.LBB23_457
.LBB23_445:
.Ltmp931:
	movq	%rax, %r13
	movq	1544(%rsp), %rdi
	cmpq	%rbx, %rdi
	je	.LBB23_457
	jmp	.LBB23_446
.LBB23_447:
.Ltmp925:
	movq	%rax, %r13
	leaq	200(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	leaq	96(%rsp), %rdi
	jmp	.LBB23_458
.LBB23_448:
.Ltmp922:
	movq	%rax, %r13
	leaq	96(%rsp), %rdi
	jmp	.LBB23_458
.LBB23_449:
.Ltmp1207:
	jmp	.LBB23_456
.LBB23_450:
.Ltmp1133:
	jmp	.LBB23_456
.LBB23_451:
.Ltmp1315:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB23_452:
.Ltmp1312:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB23_453:
.Ltmp1309:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB23_454:
.Ltmp1182:
	jmp	.LBB23_456
.LBB23_455:
.Ltmp1321:
.LBB23_456:
	movq	%rax, %r13
.LBB23_457:
	leaq	200(%rsp), %rdi
.LBB23_458:
	callq	_ZN4mpfr6mprealD2Ev
	movq	%r13, %rdi
	callq	_Unwind_Resume@PLT
.LBB23_459:
.Ltmp911:
	movq	%rax, %r13
	testq	%r15, %r15
	je	.LBB23_462
# %bb.460:
	movq	232(%rsp), %rbx                 # 8-byte Reload
	addq	$-24, %rbx
	.p2align	4, 0x90
.LBB23_461:                             # =>This Inner Loop Header: Depth=1
	leaq	(%rbx,%r15), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	addq	$-32, %r15
	jne	.LBB23_461
.LBB23_462:
	movq	232(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	movq	%r13, %rdi
	callq	_Unwind_Resume@PLT
.LBB23_463:
.Ltmp906:
	movq	%rax, %r13
	testq	%r15, %r15
	je	.LBB23_466
# %bb.464:
	movq	%r14, %rbx
	addq	$-24, %rbx
	.p2align	4, 0x90
.LBB23_465:                             # =>This Inner Loop Header: Depth=1
	leaq	(%rbx,%r15), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	addq	$-32, %r15
	jne	.LBB23_465
.LBB23_466:
	movq	%r14, %rdi
	callq	_ZdaPv
	movq	%r13, %rdi
	callq	_Unwind_Resume@PLT
.LBB23_467:
.Ltmp901:
	movq	%r13, %rbx
	movq	%rax, %r13
	testq	%r14, %r14
	je	.LBB23_470
# %bb.468:
	addq	$-24, %r12
	.p2align	4, 0x90
.LBB23_469:                             # =>This Inner Loop Header: Depth=1
	leaq	(%r12,%r14), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	addq	$-32, %r14
	jne	.LBB23_469
.LBB23_470:
	movq	%rbx, %rdi
	callq	_ZdaPv
	movq	%r13, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end23:
	.size	main, .Lfunc_end23-main
	.cfi_endproc
	.section	.gcc_except_table,"a",@progbits
	.p2align	2, 0x0
GCC_except_table23:
.Lexception16:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase13-.Lttbaseref13
.Lttbaseref13:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end16-.Lcst_begin16
.Lcst_begin16:
	.uleb128 .Lfunc_begin16-.Lfunc_begin16  # >> Call Site 1 <<
	.uleb128 .Ltmp897-.Lfunc_begin16        #   Call between .Lfunc_begin16 and .Ltmp897
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp897-.Lfunc_begin16        # >> Call Site 2 <<
	.uleb128 .Ltmp900-.Ltmp897              #   Call between .Ltmp897 and .Ltmp900
	.uleb128 .Ltmp901-.Lfunc_begin16        #     jumps to .Ltmp901
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp900-.Lfunc_begin16        # >> Call Site 3 <<
	.uleb128 .Ltmp902-.Ltmp900              #   Call between .Ltmp900 and .Ltmp902
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp902-.Lfunc_begin16        # >> Call Site 4 <<
	.uleb128 .Ltmp905-.Ltmp902              #   Call between .Ltmp902 and .Ltmp905
	.uleb128 .Ltmp906-.Lfunc_begin16        #     jumps to .Ltmp906
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp905-.Lfunc_begin16        # >> Call Site 5 <<
	.uleb128 .Ltmp907-.Ltmp905              #   Call between .Ltmp905 and .Ltmp907
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp907-.Lfunc_begin16        # >> Call Site 6 <<
	.uleb128 .Ltmp910-.Ltmp907              #   Call between .Ltmp907 and .Ltmp910
	.uleb128 .Ltmp911-.Lfunc_begin16        #     jumps to .Ltmp911
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp910-.Lfunc_begin16        # >> Call Site 7 <<
	.uleb128 .Ltmp912-.Ltmp910              #   Call between .Ltmp910 and .Ltmp912
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp912-.Lfunc_begin16        # >> Call Site 8 <<
	.uleb128 .Ltmp921-.Ltmp912              #   Call between .Ltmp912 and .Ltmp921
	.uleb128 .Ltmp922-.Lfunc_begin16        #     jumps to .Ltmp922
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp923-.Lfunc_begin16        # >> Call Site 9 <<
	.uleb128 .Ltmp924-.Ltmp923              #   Call between .Ltmp923 and .Ltmp924
	.uleb128 .Ltmp925-.Lfunc_begin16        #     jumps to .Ltmp925
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp926-.Lfunc_begin16        # >> Call Site 10 <<
	.uleb128 .Ltmp927-.Ltmp926              #   Call between .Ltmp926 and .Ltmp927
	.uleb128 .Ltmp928-.Lfunc_begin16        #     jumps to .Ltmp928
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp929-.Lfunc_begin16        # >> Call Site 11 <<
	.uleb128 .Ltmp930-.Ltmp929              #   Call between .Ltmp929 and .Ltmp930
	.uleb128 .Ltmp931-.Lfunc_begin16        #     jumps to .Ltmp931
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp932-.Lfunc_begin16        # >> Call Site 12 <<
	.uleb128 .Ltmp933-.Ltmp932              #   Call between .Ltmp932 and .Ltmp933
	.uleb128 .Ltmp934-.Lfunc_begin16        #     jumps to .Ltmp934
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp935-.Lfunc_begin16        # >> Call Site 13 <<
	.uleb128 .Ltmp936-.Ltmp935              #   Call between .Ltmp935 and .Ltmp936
	.uleb128 .Ltmp937-.Lfunc_begin16        #     jumps to .Ltmp937
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp938-.Lfunc_begin16        # >> Call Site 14 <<
	.uleb128 .Ltmp939-.Ltmp938              #   Call between .Ltmp938 and .Ltmp939
	.uleb128 .Ltmp940-.Lfunc_begin16        #     jumps to .Ltmp940
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp941-.Lfunc_begin16        # >> Call Site 15 <<
	.uleb128 .Ltmp942-.Ltmp941              #   Call between .Ltmp941 and .Ltmp942
	.uleb128 .Ltmp943-.Lfunc_begin16        #     jumps to .Ltmp943
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp944-.Lfunc_begin16        # >> Call Site 16 <<
	.uleb128 .Ltmp945-.Ltmp944              #   Call between .Ltmp944 and .Ltmp945
	.uleb128 .Ltmp946-.Lfunc_begin16        #     jumps to .Ltmp946
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp947-.Lfunc_begin16        # >> Call Site 17 <<
	.uleb128 .Ltmp948-.Ltmp947              #   Call between .Ltmp947 and .Ltmp948
	.uleb128 .Ltmp949-.Lfunc_begin16        #     jumps to .Ltmp949
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp950-.Lfunc_begin16        # >> Call Site 18 <<
	.uleb128 .Ltmp951-.Ltmp950              #   Call between .Ltmp950 and .Ltmp951
	.uleb128 .Ltmp952-.Lfunc_begin16        #     jumps to .Ltmp952
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp953-.Lfunc_begin16        # >> Call Site 19 <<
	.uleb128 .Ltmp954-.Ltmp953              #   Call between .Ltmp953 and .Ltmp954
	.uleb128 .Ltmp955-.Lfunc_begin16        #     jumps to .Ltmp955
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp956-.Lfunc_begin16        # >> Call Site 20 <<
	.uleb128 .Ltmp957-.Ltmp956              #   Call between .Ltmp956 and .Ltmp957
	.uleb128 .Ltmp958-.Lfunc_begin16        #     jumps to .Ltmp958
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp959-.Lfunc_begin16        # >> Call Site 21 <<
	.uleb128 .Ltmp960-.Ltmp959              #   Call between .Ltmp959 and .Ltmp960
	.uleb128 .Ltmp961-.Lfunc_begin16        #     jumps to .Ltmp961
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp962-.Lfunc_begin16        # >> Call Site 22 <<
	.uleb128 .Ltmp963-.Ltmp962              #   Call between .Ltmp962 and .Ltmp963
	.uleb128 .Ltmp964-.Lfunc_begin16        #     jumps to .Ltmp964
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp965-.Lfunc_begin16        # >> Call Site 23 <<
	.uleb128 .Ltmp966-.Ltmp965              #   Call between .Ltmp965 and .Ltmp966
	.uleb128 .Ltmp967-.Lfunc_begin16        #     jumps to .Ltmp967
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp968-.Lfunc_begin16        # >> Call Site 24 <<
	.uleb128 .Ltmp969-.Ltmp968              #   Call between .Ltmp968 and .Ltmp969
	.uleb128 .Ltmp970-.Lfunc_begin16        #     jumps to .Ltmp970
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp971-.Lfunc_begin16        # >> Call Site 25 <<
	.uleb128 .Ltmp972-.Ltmp971              #   Call between .Ltmp971 and .Ltmp972
	.uleb128 .Ltmp973-.Lfunc_begin16        #     jumps to .Ltmp973
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp974-.Lfunc_begin16        # >> Call Site 26 <<
	.uleb128 .Ltmp975-.Ltmp974              #   Call between .Ltmp974 and .Ltmp975
	.uleb128 .Ltmp976-.Lfunc_begin16        #     jumps to .Ltmp976
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp977-.Lfunc_begin16        # >> Call Site 27 <<
	.uleb128 .Ltmp978-.Ltmp977              #   Call between .Ltmp977 and .Ltmp978
	.uleb128 .Ltmp979-.Lfunc_begin16        #     jumps to .Ltmp979
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp980-.Lfunc_begin16        # >> Call Site 28 <<
	.uleb128 .Ltmp981-.Ltmp980              #   Call between .Ltmp980 and .Ltmp981
	.uleb128 .Ltmp982-.Lfunc_begin16        #     jumps to .Ltmp982
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp983-.Lfunc_begin16        # >> Call Site 29 <<
	.uleb128 .Ltmp984-.Ltmp983              #   Call between .Ltmp983 and .Ltmp984
	.uleb128 .Ltmp985-.Lfunc_begin16        #     jumps to .Ltmp985
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp986-.Lfunc_begin16        # >> Call Site 30 <<
	.uleb128 .Ltmp987-.Ltmp986              #   Call between .Ltmp986 and .Ltmp987
	.uleb128 .Ltmp988-.Lfunc_begin16        #     jumps to .Ltmp988
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp989-.Lfunc_begin16        # >> Call Site 31 <<
	.uleb128 .Ltmp990-.Ltmp989              #   Call between .Ltmp989 and .Ltmp990
	.uleb128 .Ltmp991-.Lfunc_begin16        #     jumps to .Ltmp991
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp992-.Lfunc_begin16        # >> Call Site 32 <<
	.uleb128 .Ltmp993-.Ltmp992              #   Call between .Ltmp992 and .Ltmp993
	.uleb128 .Ltmp994-.Lfunc_begin16        #     jumps to .Ltmp994
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp995-.Lfunc_begin16        # >> Call Site 33 <<
	.uleb128 .Ltmp996-.Ltmp995              #   Call between .Ltmp995 and .Ltmp996
	.uleb128 .Ltmp997-.Lfunc_begin16        #     jumps to .Ltmp997
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp998-.Lfunc_begin16        # >> Call Site 34 <<
	.uleb128 .Ltmp999-.Ltmp998              #   Call between .Ltmp998 and .Ltmp999
	.uleb128 .Ltmp1000-.Lfunc_begin16       #     jumps to .Ltmp1000
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1001-.Lfunc_begin16       # >> Call Site 35 <<
	.uleb128 .Ltmp1002-.Ltmp1001            #   Call between .Ltmp1001 and .Ltmp1002
	.uleb128 .Ltmp1003-.Lfunc_begin16       #     jumps to .Ltmp1003
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1004-.Lfunc_begin16       # >> Call Site 36 <<
	.uleb128 .Ltmp1005-.Ltmp1004            #   Call between .Ltmp1004 and .Ltmp1005
	.uleb128 .Ltmp1006-.Lfunc_begin16       #     jumps to .Ltmp1006
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1007-.Lfunc_begin16       # >> Call Site 37 <<
	.uleb128 .Ltmp1008-.Ltmp1007            #   Call between .Ltmp1007 and .Ltmp1008
	.uleb128 .Ltmp1009-.Lfunc_begin16       #     jumps to .Ltmp1009
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1010-.Lfunc_begin16       # >> Call Site 38 <<
	.uleb128 .Ltmp1011-.Ltmp1010            #   Call between .Ltmp1010 and .Ltmp1011
	.uleb128 .Ltmp1012-.Lfunc_begin16       #     jumps to .Ltmp1012
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1013-.Lfunc_begin16       # >> Call Site 39 <<
	.uleb128 .Ltmp1014-.Ltmp1013            #   Call between .Ltmp1013 and .Ltmp1014
	.uleb128 .Ltmp1015-.Lfunc_begin16       #     jumps to .Ltmp1015
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1016-.Lfunc_begin16       # >> Call Site 40 <<
	.uleb128 .Ltmp1017-.Ltmp1016            #   Call between .Ltmp1016 and .Ltmp1017
	.uleb128 .Ltmp1018-.Lfunc_begin16       #     jumps to .Ltmp1018
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1019-.Lfunc_begin16       # >> Call Site 41 <<
	.uleb128 .Ltmp1020-.Ltmp1019            #   Call between .Ltmp1019 and .Ltmp1020
	.uleb128 .Ltmp1021-.Lfunc_begin16       #     jumps to .Ltmp1021
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1022-.Lfunc_begin16       # >> Call Site 42 <<
	.uleb128 .Ltmp1023-.Ltmp1022            #   Call between .Ltmp1022 and .Ltmp1023
	.uleb128 .Ltmp1024-.Lfunc_begin16       #     jumps to .Ltmp1024
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1025-.Lfunc_begin16       # >> Call Site 43 <<
	.uleb128 .Ltmp1026-.Ltmp1025            #   Call between .Ltmp1025 and .Ltmp1026
	.uleb128 .Ltmp1027-.Lfunc_begin16       #     jumps to .Ltmp1027
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1028-.Lfunc_begin16       # >> Call Site 44 <<
	.uleb128 .Ltmp1029-.Ltmp1028            #   Call between .Ltmp1028 and .Ltmp1029
	.uleb128 .Ltmp1030-.Lfunc_begin16       #     jumps to .Ltmp1030
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1031-.Lfunc_begin16       # >> Call Site 45 <<
	.uleb128 .Ltmp1032-.Ltmp1031            #   Call between .Ltmp1031 and .Ltmp1032
	.uleb128 .Ltmp1033-.Lfunc_begin16       #     jumps to .Ltmp1033
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1034-.Lfunc_begin16       # >> Call Site 46 <<
	.uleb128 .Ltmp1035-.Ltmp1034            #   Call between .Ltmp1034 and .Ltmp1035
	.uleb128 .Ltmp1036-.Lfunc_begin16       #     jumps to .Ltmp1036
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1037-.Lfunc_begin16       # >> Call Site 47 <<
	.uleb128 .Ltmp1038-.Ltmp1037            #   Call between .Ltmp1037 and .Ltmp1038
	.uleb128 .Ltmp1039-.Lfunc_begin16       #     jumps to .Ltmp1039
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1040-.Lfunc_begin16       # >> Call Site 48 <<
	.uleb128 .Ltmp1041-.Ltmp1040            #   Call between .Ltmp1040 and .Ltmp1041
	.uleb128 .Ltmp1042-.Lfunc_begin16       #     jumps to .Ltmp1042
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1043-.Lfunc_begin16       # >> Call Site 49 <<
	.uleb128 .Ltmp1044-.Ltmp1043            #   Call between .Ltmp1043 and .Ltmp1044
	.uleb128 .Ltmp1045-.Lfunc_begin16       #     jumps to .Ltmp1045
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1046-.Lfunc_begin16       # >> Call Site 50 <<
	.uleb128 .Ltmp1047-.Ltmp1046            #   Call between .Ltmp1046 and .Ltmp1047
	.uleb128 .Ltmp1048-.Lfunc_begin16       #     jumps to .Ltmp1048
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1049-.Lfunc_begin16       # >> Call Site 51 <<
	.uleb128 .Ltmp1050-.Ltmp1049            #   Call between .Ltmp1049 and .Ltmp1050
	.uleb128 .Ltmp1051-.Lfunc_begin16       #     jumps to .Ltmp1051
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1052-.Lfunc_begin16       # >> Call Site 52 <<
	.uleb128 .Ltmp1053-.Ltmp1052            #   Call between .Ltmp1052 and .Ltmp1053
	.uleb128 .Ltmp1054-.Lfunc_begin16       #     jumps to .Ltmp1054
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1055-.Lfunc_begin16       # >> Call Site 53 <<
	.uleb128 .Ltmp1056-.Ltmp1055            #   Call between .Ltmp1055 and .Ltmp1056
	.uleb128 .Ltmp1057-.Lfunc_begin16       #     jumps to .Ltmp1057
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1058-.Lfunc_begin16       # >> Call Site 54 <<
	.uleb128 .Ltmp1059-.Ltmp1058            #   Call between .Ltmp1058 and .Ltmp1059
	.uleb128 .Ltmp1060-.Lfunc_begin16       #     jumps to .Ltmp1060
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1061-.Lfunc_begin16       # >> Call Site 55 <<
	.uleb128 .Ltmp1062-.Ltmp1061            #   Call between .Ltmp1061 and .Ltmp1062
	.uleb128 .Ltmp1063-.Lfunc_begin16       #     jumps to .Ltmp1063
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1064-.Lfunc_begin16       # >> Call Site 56 <<
	.uleb128 .Ltmp1065-.Ltmp1064            #   Call between .Ltmp1064 and .Ltmp1065
	.uleb128 .Ltmp1066-.Lfunc_begin16       #     jumps to .Ltmp1066
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1067-.Lfunc_begin16       # >> Call Site 57 <<
	.uleb128 .Ltmp1068-.Ltmp1067            #   Call between .Ltmp1067 and .Ltmp1068
	.uleb128 .Ltmp1069-.Lfunc_begin16       #     jumps to .Ltmp1069
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1070-.Lfunc_begin16       # >> Call Site 58 <<
	.uleb128 .Ltmp1071-.Ltmp1070            #   Call between .Ltmp1070 and .Ltmp1071
	.uleb128 .Ltmp1072-.Lfunc_begin16       #     jumps to .Ltmp1072
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1073-.Lfunc_begin16       # >> Call Site 59 <<
	.uleb128 .Ltmp1074-.Ltmp1073            #   Call between .Ltmp1073 and .Ltmp1074
	.uleb128 .Ltmp1075-.Lfunc_begin16       #     jumps to .Ltmp1075
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1076-.Lfunc_begin16       # >> Call Site 60 <<
	.uleb128 .Ltmp1077-.Ltmp1076            #   Call between .Ltmp1076 and .Ltmp1077
	.uleb128 .Ltmp1078-.Lfunc_begin16       #     jumps to .Ltmp1078
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1079-.Lfunc_begin16       # >> Call Site 61 <<
	.uleb128 .Ltmp1080-.Ltmp1079            #   Call between .Ltmp1079 and .Ltmp1080
	.uleb128 .Ltmp1081-.Lfunc_begin16       #     jumps to .Ltmp1081
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1082-.Lfunc_begin16       # >> Call Site 62 <<
	.uleb128 .Ltmp1083-.Ltmp1082            #   Call between .Ltmp1082 and .Ltmp1083
	.uleb128 .Ltmp1084-.Lfunc_begin16       #     jumps to .Ltmp1084
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1085-.Lfunc_begin16       # >> Call Site 63 <<
	.uleb128 .Ltmp1086-.Ltmp1085            #   Call between .Ltmp1085 and .Ltmp1086
	.uleb128 .Ltmp1087-.Lfunc_begin16       #     jumps to .Ltmp1087
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1088-.Lfunc_begin16       # >> Call Site 64 <<
	.uleb128 .Ltmp1089-.Ltmp1088            #   Call between .Ltmp1088 and .Ltmp1089
	.uleb128 .Ltmp1090-.Lfunc_begin16       #     jumps to .Ltmp1090
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1091-.Lfunc_begin16       # >> Call Site 65 <<
	.uleb128 .Ltmp1092-.Ltmp1091            #   Call between .Ltmp1091 and .Ltmp1092
	.uleb128 .Ltmp1093-.Lfunc_begin16       #     jumps to .Ltmp1093
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1094-.Lfunc_begin16       # >> Call Site 66 <<
	.uleb128 .Ltmp1095-.Ltmp1094            #   Call between .Ltmp1094 and .Ltmp1095
	.uleb128 .Ltmp1096-.Lfunc_begin16       #     jumps to .Ltmp1096
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1097-.Lfunc_begin16       # >> Call Site 67 <<
	.uleb128 .Ltmp1098-.Ltmp1097            #   Call between .Ltmp1097 and .Ltmp1098
	.uleb128 .Ltmp1099-.Lfunc_begin16       #     jumps to .Ltmp1099
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1100-.Lfunc_begin16       # >> Call Site 68 <<
	.uleb128 .Ltmp1101-.Ltmp1100            #   Call between .Ltmp1100 and .Ltmp1101
	.uleb128 .Ltmp1102-.Lfunc_begin16       #     jumps to .Ltmp1102
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1103-.Lfunc_begin16       # >> Call Site 69 <<
	.uleb128 .Ltmp1104-.Ltmp1103            #   Call between .Ltmp1103 and .Ltmp1104
	.uleb128 .Ltmp1105-.Lfunc_begin16       #     jumps to .Ltmp1105
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1106-.Lfunc_begin16       # >> Call Site 70 <<
	.uleb128 .Ltmp1107-.Ltmp1106            #   Call between .Ltmp1106 and .Ltmp1107
	.uleb128 .Ltmp1108-.Lfunc_begin16       #     jumps to .Ltmp1108
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1109-.Lfunc_begin16       # >> Call Site 71 <<
	.uleb128 .Ltmp1132-.Ltmp1109            #   Call between .Ltmp1109 and .Ltmp1132
	.uleb128 .Ltmp1133-.Lfunc_begin16       #     jumps to .Ltmp1133
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1134-.Lfunc_begin16       # >> Call Site 72 <<
	.uleb128 .Ltmp1181-.Ltmp1134            #   Call between .Ltmp1134 and .Ltmp1181
	.uleb128 .Ltmp1182-.Lfunc_begin16       #     jumps to .Ltmp1182
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1183-.Lfunc_begin16       # >> Call Site 73 <<
	.uleb128 .Ltmp1206-.Ltmp1183            #   Call between .Ltmp1183 and .Ltmp1206
	.uleb128 .Ltmp1207-.Lfunc_begin16       #     jumps to .Ltmp1207
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1208-.Lfunc_begin16       # >> Call Site 74 <<
	.uleb128 .Ltmp1209-.Ltmp1208            #   Call between .Ltmp1208 and .Ltmp1209
	.uleb128 .Ltmp1210-.Lfunc_begin16       #     jumps to .Ltmp1210
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1211-.Lfunc_begin16       # >> Call Site 75 <<
	.uleb128 .Ltmp1212-.Ltmp1211            #   Call between .Ltmp1211 and .Ltmp1212
	.uleb128 .Ltmp1213-.Lfunc_begin16       #     jumps to .Ltmp1213
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1214-.Lfunc_begin16       # >> Call Site 76 <<
	.uleb128 .Ltmp1215-.Ltmp1214            #   Call between .Ltmp1214 and .Ltmp1215
	.uleb128 .Ltmp1216-.Lfunc_begin16       #     jumps to .Ltmp1216
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1217-.Lfunc_begin16       # >> Call Site 77 <<
	.uleb128 .Ltmp1218-.Ltmp1217            #   Call between .Ltmp1217 and .Ltmp1218
	.uleb128 .Ltmp1219-.Lfunc_begin16       #     jumps to .Ltmp1219
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1220-.Lfunc_begin16       # >> Call Site 78 <<
	.uleb128 .Ltmp1221-.Ltmp1220            #   Call between .Ltmp1220 and .Ltmp1221
	.uleb128 .Ltmp1222-.Lfunc_begin16       #     jumps to .Ltmp1222
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1223-.Lfunc_begin16       # >> Call Site 79 <<
	.uleb128 .Ltmp1224-.Ltmp1223            #   Call between .Ltmp1223 and .Ltmp1224
	.uleb128 .Ltmp1225-.Lfunc_begin16       #     jumps to .Ltmp1225
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1226-.Lfunc_begin16       # >> Call Site 80 <<
	.uleb128 .Ltmp1227-.Ltmp1226            #   Call between .Ltmp1226 and .Ltmp1227
	.uleb128 .Ltmp1228-.Lfunc_begin16       #     jumps to .Ltmp1228
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1229-.Lfunc_begin16       # >> Call Site 81 <<
	.uleb128 .Ltmp1230-.Ltmp1229            #   Call between .Ltmp1229 and .Ltmp1230
	.uleb128 .Ltmp1231-.Lfunc_begin16       #     jumps to .Ltmp1231
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1232-.Lfunc_begin16       # >> Call Site 82 <<
	.uleb128 .Ltmp1233-.Ltmp1232            #   Call between .Ltmp1232 and .Ltmp1233
	.uleb128 .Ltmp1234-.Lfunc_begin16       #     jumps to .Ltmp1234
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1235-.Lfunc_begin16       # >> Call Site 83 <<
	.uleb128 .Ltmp1306-.Ltmp1235            #   Call between .Ltmp1235 and .Ltmp1306
	.uleb128 .Ltmp1321-.Lfunc_begin16       #     jumps to .Ltmp1321
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1307-.Lfunc_begin16       # >> Call Site 84 <<
	.uleb128 .Ltmp1308-.Ltmp1307            #   Call between .Ltmp1307 and .Ltmp1308
	.uleb128 .Ltmp1309-.Lfunc_begin16       #     jumps to .Ltmp1309
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1310-.Lfunc_begin16       # >> Call Site 85 <<
	.uleb128 .Ltmp1311-.Ltmp1310            #   Call between .Ltmp1310 and .Ltmp1311
	.uleb128 .Ltmp1312-.Lfunc_begin16       #     jumps to .Ltmp1312
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1313-.Lfunc_begin16       # >> Call Site 86 <<
	.uleb128 .Ltmp1314-.Ltmp1313            #   Call between .Ltmp1313 and .Ltmp1314
	.uleb128 .Ltmp1315-.Lfunc_begin16       #     jumps to .Ltmp1315
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1316-.Lfunc_begin16       # >> Call Site 87 <<
	.uleb128 .Ltmp1317-.Ltmp1316            #   Call between .Ltmp1316 and .Ltmp1317
	.uleb128 .Ltmp1318-.Lfunc_begin16       #     jumps to .Ltmp1318
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1319-.Lfunc_begin16       # >> Call Site 88 <<
	.uleb128 .Ltmp1320-.Ltmp1319            #   Call between .Ltmp1319 and .Ltmp1320
	.uleb128 .Ltmp1321-.Lfunc_begin16       #     jumps to .Ltmp1321
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1320-.Lfunc_begin16       # >> Call Site 89 <<
	.uleb128 .Lfunc_end23-.Ltmp1320         #   Call between .Ltmp1320 and .Lfunc_end23
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end16:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase13:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"axG",@progbits,_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.weak	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b # -- Begin function _Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.p2align	4, 0x90
	.type	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,@function
_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b: # 
.Lfunc_begin17:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception17
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r12
	.cfi_def_cfa_offset 40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	subq	$112, %rsp
	.cfi_def_cfa_offset 160
	.cfi_offset %rbx, -48
	.cfi_offset %r12, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, %ebp
	movq	%rsi, %rbx
	leaq	40(%rsp), %r12
	movq	%r12, 24(%rsp)
	movq	(%rdi), %r14
	movq	8(%rdi), %r15
	movq	%r15, 56(%rsp)
	movq	%r12, %rax
	cmpq	$16, %r15
	jb	.LBB24_2
# %bb.1:
	leaq	24(%rsp), %rdi
	leaq	56(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
	movq	%rax, 24(%rsp)
	movq	56(%rsp), %rcx
	movq	%rcx, 40(%rsp)
.LBB24_2:
	testq	%r15, %r15
	je	.LBB24_6
# %bb.3:
	cmpq	$1, %r15
	jne	.LBB24_5
# %bb.4:
	movzbl	(%r14), %ecx
	movb	%cl, (%rax)
	jmp	.LBB24_6
.LBB24_5:
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r15, %rdx
	callq	_intel_fast_memcpy@PLT
.LBB24_6:
	movq	56(%rsp), %rax
	movq	%rax, 32(%rsp)
	movq	24(%rsp), %rcx
	movb	$0, (%rcx,%rax)
	vmovss	(%rbx), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	4(%rbx), %xmm1                  # xmm1 = mem[0],zero,zero,zero
	vmovss	%xmm1, 16(%rsp)
	flds	16(%rsp)
	wait
	vmovss	%xmm0, 20(%rsp)
	flds	20(%rsp)
	faddp	%st, %st(1)
	fstpt	92(%rsp)                        # 10-byte Folded Spill
	wait
	movq	24(%rsp), %rsi
	movq	32(%rsp), %rdx
.Ltmp1322:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1323:
# %bb.7:
	leaq	72(%rsp), %rax
	movq	%rax, 56(%rsp)
	movl	$1935766865, 72(%rsp)           # imm = 0x73617551
	movw	$105, 76(%rsp)
	movq	$5, 64(%rsp)
	movl	(%rbx), %edx
	movl	4(%rbx), %ecx
	fldt	92(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.50, %edi
	movl	$81, %esi
	xorl	%eax, %eax
	callq	printf
	testb	%bpl, %bpl
	je	.LBB24_13
# %bb.8:
.Ltmp1324:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.6, %esi
	movl	$4, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1325:
# %bb.9:
	movl	(%rbx), %eax
	movq	%rax, 56(%rsp)
.Ltmp1326:
	leaq	56(%rsp), %rsi
	movl	$_ZSt4cout, %edi
	callq	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1327:
# %bb.10:
.Ltmp1328:
	movq	%rax, %r14
	movl	$.L.str.51, %esi
	movl	$5, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1329:
# %bb.11:
	movl	4(%rbx), %eax
	movq	%rax, 104(%rsp)
.Ltmp1330:
	leaq	104(%rsp), %rsi
	movq	%r14, %rdi
	callq	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1331:
# %bb.12:
	fldt	92(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.7, %edi
	xorl	%eax, %eax
	callq	printf
.LBB24_13:
	movq	24(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB24_15
# %bb.14:
	callq	_ZdlPv
.LBB24_15:
	addq	$112, %rsp
	.cfi_def_cfa_offset 48
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB24_16:
	.cfi_def_cfa_offset 160
.Ltmp1332:
	movq	%rax, %rbx
	movq	24(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB24_18
# %bb.17:
	callq	_ZdlPv
.LBB24_18:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end24:
	.size	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b, .Lfunc_end24-_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.cfi_endproc
	.section	.gcc_except_table._Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"aG",@progbits,_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.p2align	2, 0x0
GCC_except_table24:
.Lexception17:
	.byte	255                             # @LPStart Encoding = omit
	.byte	255                             # @TType Encoding = omit
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end17-.Lcst_begin17
.Lcst_begin17:
	.uleb128 .Lfunc_begin17-.Lfunc_begin17  # >> Call Site 1 <<
	.uleb128 .Ltmp1322-.Lfunc_begin17       #   Call between .Lfunc_begin17 and .Ltmp1322
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1322-.Lfunc_begin17       # >> Call Site 2 <<
	.uleb128 .Ltmp1331-.Ltmp1322            #   Call between .Ltmp1322 and .Ltmp1331
	.uleb128 .Ltmp1332-.Lfunc_begin17       #     jumps to .Ltmp1332
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1331-.Lfunc_begin17       # >> Call Site 3 <<
	.uleb128 .Lfunc_end24-.Ltmp1331         #   Call between .Ltmp1331 and .Lfunc_end24
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end17:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"axG",@progbits,_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.weak	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b # -- Begin function _Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.p2align	4, 0x90
	.type	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,@function
_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b: # 
.Lfunc_begin18:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception18
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r12
	.cfi_def_cfa_offset 40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	subq	$112, %rsp
	.cfi_def_cfa_offset 160
	.cfi_offset %rbx, -48
	.cfi_offset %r12, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, %ebp
	movq	%rsi, %rbx
	leaq	40(%rsp), %r12
	movq	%r12, 24(%rsp)
	movq	(%rdi), %r14
	movq	8(%rdi), %r15
	movq	%r15, 56(%rsp)
	movq	%r12, %rax
	cmpq	$16, %r15
	jb	.LBB25_2
# %bb.1:
	leaq	24(%rsp), %rdi
	leaq	56(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
	movq	%rax, 24(%rsp)
	movq	56(%rsp), %rcx
	movq	%rcx, 40(%rsp)
.LBB25_2:
	testq	%r15, %r15
	je	.LBB25_6
# %bb.3:
	cmpq	$1, %r15
	jne	.LBB25_5
# %bb.4:
	movzbl	(%r14), %ecx
	movb	%cl, (%rax)
	jmp	.LBB25_6
.LBB25_5:
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r15, %rdx
	callq	_intel_fast_memcpy@PLT
.LBB25_6:
	movq	56(%rsp), %rax
	movq	%rax, 32(%rsp)
	movq	24(%rsp), %rcx
	movb	$0, (%rcx,%rax)
	vmovss	(%rbx), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	4(%rbx), %xmm1                  # xmm1 = mem[0],zero,zero,zero
	vmovss	%xmm1, 16(%rsp)
	flds	16(%rsp)
	wait
	vmovss	%xmm0, 20(%rsp)
	flds	20(%rsp)
	faddp	%st, %st(1)
	fstpt	92(%rsp)                        # 10-byte Folded Spill
	wait
	movq	24(%rsp), %rsi
	movq	32(%rsp), %rdx
.Ltmp1333:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1334:
# %bb.7:
	leaq	72(%rsp), %rax
	movq	%rax, 56(%rsp)
	movabsq	$7310575239352771393, %rax      # imm = 0x6574617275636341
	movq	%rax, 72(%rsp)
	movq	$8, 64(%rsp)
	movb	$0, 80(%rsp)
	movl	(%rbx), %edx
	movl	4(%rbx), %ecx
	fldt	92(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.50, %edi
	movl	$65, %esi
	xorl	%eax, %eax
	callq	printf
	testb	%bpl, %bpl
	je	.LBB25_13
# %bb.8:
.Ltmp1335:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.6, %esi
	movl	$4, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1336:
# %bb.9:
	movl	(%rbx), %eax
	movq	%rax, 56(%rsp)
.Ltmp1337:
	leaq	56(%rsp), %rsi
	movl	$_ZSt4cout, %edi
	callq	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1338:
# %bb.10:
.Ltmp1339:
	movq	%rax, %r14
	movl	$.L.str.51, %esi
	movl	$5, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1340:
# %bb.11:
	movl	4(%rbx), %eax
	movq	%rax, 104(%rsp)
.Ltmp1341:
	leaq	104(%rsp), %rsi
	movq	%r14, %rdi
	callq	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1342:
# %bb.12:
	fldt	92(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.7, %edi
	xorl	%eax, %eax
	callq	printf
.LBB25_13:
	movq	24(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB25_15
# %bb.14:
	callq	_ZdlPv
.LBB25_15:
	addq	$112, %rsp
	.cfi_def_cfa_offset 48
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB25_16:
	.cfi_def_cfa_offset 160
.Ltmp1343:
	movq	%rax, %rbx
	movq	24(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB25_18
# %bb.17:
	callq	_ZdlPv
.LBB25_18:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end25:
	.size	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b, .Lfunc_end25-_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.cfi_endproc
	.section	.gcc_except_table._Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"aG",@progbits,_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.p2align	2, 0x0
GCC_except_table25:
.Lexception18:
	.byte	255                             # @LPStart Encoding = omit
	.byte	255                             # @TType Encoding = omit
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end18-.Lcst_begin18
.Lcst_begin18:
	.uleb128 .Lfunc_begin18-.Lfunc_begin18  # >> Call Site 1 <<
	.uleb128 .Ltmp1333-.Lfunc_begin18       #   Call between .Lfunc_begin18 and .Ltmp1333
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1333-.Lfunc_begin18       # >> Call Site 2 <<
	.uleb128 .Ltmp1342-.Ltmp1333            #   Call between .Ltmp1333 and .Ltmp1342
	.uleb128 .Ltmp1343-.Lfunc_begin18       #     jumps to .Ltmp1343
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1342-.Lfunc_begin18       # >> Call Site 3 <<
	.uleb128 .Lfunc_end25-.Ltmp1342         #   Call between .Ltmp1342 and .Lfunc_end25
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end18:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"axG",@progbits,_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.weak	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b # -- Begin function _Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.p2align	4, 0x90
	.type	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,@function
_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b: # 
.Lfunc_begin19:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception19
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r12
	.cfi_def_cfa_offset 40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	subq	$128, %rsp
	.cfi_def_cfa_offset 176
	.cfi_offset %rbx, -48
	.cfi_offset %r12, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, %ebp
	movq	%rsi, %rbx
	leaq	48(%rsp), %r12
	movq	%r12, 32(%rsp)
	movq	(%rdi), %r14
	movq	8(%rdi), %r15
	movq	%r15, 64(%rsp)
	movq	%r12, %rax
	cmpq	$16, %r15
	jb	.LBB26_2
# %bb.1:
	leaq	32(%rsp), %rdi
	leaq	64(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
	movq	%rax, 32(%rsp)
	movq	64(%rsp), %rcx
	movq	%rcx, 48(%rsp)
.LBB26_2:
	testq	%r15, %r15
	je	.LBB26_6
# %bb.3:
	cmpq	$1, %r15
	jne	.LBB26_5
# %bb.4:
	movzbl	(%r14), %ecx
	movb	%cl, (%rax)
	jmp	.LBB26_6
.LBB26_5:
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r15, %rdx
	callq	_intel_fast_memcpy@PLT
.LBB26_6:
	movq	64(%rsp), %rax
	movq	%rax, 40(%rsp)
	movq	32(%rsp), %rcx
	movb	$0, (%rcx,%rax)
	vmovss	8(%rbx), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 20(%rsp)
	flds	20(%rsp)
	wait
	vmovss	(%rbx), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	4(%rbx), %xmm1                  # xmm1 = mem[0],zero,zero,zero
	vmovss	%xmm1, 24(%rsp)
	flds	24(%rsp)
	faddp	%st, %st(1)
	wait
	vmovss	%xmm0, 28(%rsp)
	flds	28(%rsp)
	faddp	%st, %st(1)
	fstpt	100(%rsp)                       # 10-byte Folded Spill
	wait
	movq	32(%rsp), %rsi
	movq	40(%rsp), %rdx
.Ltmp1344:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1345:
# %bb.7:
	leaq	80(%rsp), %rax
	movq	%rax, 64(%rsp)
	movabsq	$7310575239352771393, %rax      # imm = 0x6574617275636341
	movq	%rax, 80(%rsp)
	movq	$8, 72(%rsp)
	movb	$0, 88(%rsp)
	movl	(%rbx), %edx
	movl	4(%rbx), %ecx
	movl	8(%rbx), %r8d
	fldt	100(%rsp)                       # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.52, %edi
	movl	$65, %esi
	xorl	%eax, %eax
	callq	printf
	testb	%bpl, %bpl
	je	.LBB26_15
# %bb.8:
.Ltmp1346:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.6, %esi
	movl	$4, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1347:
# %bb.9:
	movl	(%rbx), %eax
	movq	%rax, 64(%rsp)
.Ltmp1348:
	leaq	64(%rsp), %rsi
	movl	$_ZSt4cout, %edi
	callq	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1349:
# %bb.10:
.Ltmp1350:
	movq	%rax, %r14
	movl	$.L.str.51, %esi
	movl	$5, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1351:
# %bb.11:
	movl	4(%rbx), %eax
	movq	%rax, 120(%rsp)
.Ltmp1352:
	leaq	120(%rsp), %rsi
	movq	%r14, %rdi
	callq	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1353:
# %bb.12:
.Ltmp1354:
	movq	%rax, %r14
	movl	$.L.str.51, %esi
	movl	$5, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1355:
# %bb.13:
	movl	8(%rbx), %eax
	movq	%rax, 112(%rsp)
.Ltmp1356:
	leaq	112(%rsp), %rsi
	movq	%r14, %rdi
	callq	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1357:
# %bb.14:
	fldt	100(%rsp)                       # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.7, %edi
	xorl	%eax, %eax
	callq	printf
.LBB26_15:
	movq	32(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB26_17
# %bb.16:
	callq	_ZdlPv
.LBB26_17:
	addq	$128, %rsp
	.cfi_def_cfa_offset 48
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB26_18:
	.cfi_def_cfa_offset 176
.Ltmp1358:
	movq	%rax, %rbx
	movq	32(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB26_20
# %bb.19:
	callq	_ZdlPv
.LBB26_20:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end26:
	.size	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b, .Lfunc_end26-_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.cfi_endproc
	.section	.gcc_except_table._Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"aG",@progbits,_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.p2align	2, 0x0
GCC_except_table26:
.Lexception19:
	.byte	255                             # @LPStart Encoding = omit
	.byte	255                             # @TType Encoding = omit
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end19-.Lcst_begin19
.Lcst_begin19:
	.uleb128 .Lfunc_begin19-.Lfunc_begin19  # >> Call Site 1 <<
	.uleb128 .Ltmp1344-.Lfunc_begin19       #   Call between .Lfunc_begin19 and .Ltmp1344
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1344-.Lfunc_begin19       # >> Call Site 2 <<
	.uleb128 .Ltmp1357-.Ltmp1344            #   Call between .Ltmp1344 and .Ltmp1357
	.uleb128 .Ltmp1358-.Lfunc_begin19       #     jumps to .Ltmp1358
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1357-.Lfunc_begin19       # >> Call Site 3 <<
	.uleb128 .Lfunc_end26-.Ltmp1357         #   Call between .Ltmp1357 and .Lfunc_end26
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end19:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"axG",@progbits,_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.weak	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b # -- Begin function _Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.p2align	4, 0x90
	.type	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,@function
_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b: # 
.Lfunc_begin20:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception20
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r12
	.cfi_def_cfa_offset 40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	subq	$144, %rsp
	.cfi_def_cfa_offset 192
	.cfi_offset %rbx, -48
	.cfi_offset %r12, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, %ebp
	movq	%rsi, %rbx
	leaq	56(%rsp), %r12
	movq	%r12, 40(%rsp)
	movq	(%rdi), %r14
	movq	8(%rdi), %r15
	movq	%r15, 72(%rsp)
	movq	%r12, %rax
	cmpq	$16, %r15
	jb	.LBB27_2
# %bb.1:
	leaq	40(%rsp), %rdi
	leaq	72(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
	movq	%rax, 40(%rsp)
	movq	72(%rsp), %rcx
	movq	%rcx, 56(%rsp)
.LBB27_2:
	testq	%r15, %r15
	je	.LBB27_6
# %bb.3:
	cmpq	$1, %r15
	jne	.LBB27_5
# %bb.4:
	movzbl	(%r14), %ecx
	movb	%cl, (%rax)
	jmp	.LBB27_6
.LBB27_5:
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r15, %rdx
	callq	_intel_fast_memcpy@PLT
.LBB27_6:
	movq	72(%rsp), %rax
	movq	%rax, 48(%rsp)
	movq	40(%rsp), %rcx
	movb	$0, (%rcx,%rax)
	vmovss	12(%rbx), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24(%rsp)
	flds	24(%rsp)
	wait
	vmovss	8(%rbx), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 28(%rsp)
	flds	28(%rsp)
	faddp	%st, %st(1)
	wait
	vmovss	(%rbx), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	4(%rbx), %xmm1                  # xmm1 = mem[0],zero,zero,zero
	vmovss	%xmm1, 32(%rsp)
	flds	32(%rsp)
	faddp	%st, %st(1)
	wait
	vmovss	%xmm0, 36(%rsp)
	flds	36(%rsp)
	faddp	%st, %st(1)
	fstpt	108(%rsp)                       # 10-byte Folded Spill
	wait
	movq	40(%rsp), %rsi
	movq	48(%rsp), %rdx
.Ltmp1359:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1360:
# %bb.7:
	leaq	88(%rsp), %rax
	movq	%rax, 72(%rsp)
	movabsq	$7310575239352771393, %rax      # imm = 0x6574617275636341
	movq	%rax, 88(%rsp)
	movq	$8, 80(%rsp)
	movb	$0, 96(%rsp)
	movl	(%rbx), %edx
	movl	4(%rbx), %ecx
	movl	8(%rbx), %r8d
	movl	12(%rbx), %r9d
	fldt	108(%rsp)                       # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.53, %edi
	movl	$65, %esi
	xorl	%eax, %eax
	callq	printf
	testb	%bpl, %bpl
	je	.LBB27_17
# %bb.8:
.Ltmp1361:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.6, %esi
	movl	$4, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1362:
# %bb.9:
	movl	(%rbx), %eax
	movq	%rax, 72(%rsp)
.Ltmp1363:
	leaq	72(%rsp), %rsi
	movl	$_ZSt4cout, %edi
	callq	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1364:
# %bb.10:
.Ltmp1365:
	movq	%rax, %r14
	movl	$.L.str.51, %esi
	movl	$5, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1366:
# %bb.11:
	movl	4(%rbx), %eax
	movq	%rax, 136(%rsp)
.Ltmp1367:
	leaq	136(%rsp), %rsi
	movq	%r14, %rdi
	callq	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1368:
# %bb.12:
.Ltmp1369:
	movq	%rax, %r14
	movl	$.L.str.51, %esi
	movl	$5, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1370:
# %bb.13:
	movl	8(%rbx), %eax
	movq	%rax, 128(%rsp)
.Ltmp1371:
	leaq	128(%rsp), %rsi
	movq	%r14, %rdi
	callq	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1372:
# %bb.14:
.Ltmp1373:
	movq	%rax, %r14
	movl	$.L.str.51, %esi
	movl	$5, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1374:
# %bb.15:
	movl	12(%rbx), %eax
	movq	%rax, 120(%rsp)
.Ltmp1375:
	leaq	120(%rsp), %rsi
	movq	%r14, %rdi
	callq	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1376:
# %bb.16:
	fldt	108(%rsp)                       # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.7, %edi
	xorl	%eax, %eax
	callq	printf
.LBB27_17:
	movq	40(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB27_19
# %bb.18:
	callq	_ZdlPv
.LBB27_19:
	addq	$144, %rsp
	.cfi_def_cfa_offset 48
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB27_20:
	.cfi_def_cfa_offset 192
.Ltmp1377:
	movq	%rax, %rbx
	movq	40(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB27_22
# %bb.21:
	callq	_ZdlPv
.LBB27_22:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end27:
	.size	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b, .Lfunc_end27-_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.cfi_endproc
	.section	.gcc_except_table._Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"aG",@progbits,_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.p2align	2, 0x0
GCC_except_table27:
.Lexception20:
	.byte	255                             # @LPStart Encoding = omit
	.byte	255                             # @TType Encoding = omit
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end20-.Lcst_begin20
.Lcst_begin20:
	.uleb128 .Lfunc_begin20-.Lfunc_begin20  # >> Call Site 1 <<
	.uleb128 .Ltmp1359-.Lfunc_begin20       #   Call between .Lfunc_begin20 and .Ltmp1359
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1359-.Lfunc_begin20       # >> Call Site 2 <<
	.uleb128 .Ltmp1376-.Ltmp1359            #   Call between .Ltmp1359 and .Ltmp1376
	.uleb128 .Ltmp1377-.Lfunc_begin20       #     jumps to .Ltmp1377
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1376-.Lfunc_begin20       # >> Call Site 3 <<
	.uleb128 .Lfunc_end27-.Ltmp1376         #   Call between .Ltmp1376 and .Lfunc_end27
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end20:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"axG",@progbits,_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.weak	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b # -- Begin function _Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.p2align	4, 0x90
	.type	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,@function
_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b: # 
.Lfunc_begin21:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception21
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r12
	.cfi_def_cfa_offset 40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	subq	$128, %rsp
	.cfi_def_cfa_offset 176
	.cfi_offset %rbx, -48
	.cfi_offset %r12, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, %ebp
	movq	%rsi, %rbx
	leaq	40(%rsp), %r12
	movq	%r12, 24(%rsp)
	movq	(%rdi), %r14
	movq	8(%rdi), %r15
	movq	%r15, 56(%rsp)
	movq	%r12, %rax
	cmpq	$16, %r15
	jb	.LBB28_2
# %bb.1:
	leaq	24(%rsp), %rdi
	leaq	56(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
	movq	%rax, 24(%rsp)
	movq	56(%rsp), %rcx
	movq	%rcx, 40(%rsp)
.LBB28_2:
	testq	%r15, %r15
	je	.LBB28_6
# %bb.3:
	cmpq	$1, %r15
	jne	.LBB28_5
# %bb.4:
	movzbl	(%r14), %ecx
	movb	%cl, (%rax)
	jmp	.LBB28_6
.LBB28_5:
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r15, %rdx
	callq	_intel_fast_memcpy@PLT
.LBB28_6:
	movq	56(%rsp), %rax
	movq	%rax, 32(%rsp)
	movq	24(%rsp), %rcx
	movb	$0, (%rcx,%rax)
	vmovsd	(%rbx), %xmm0                   # xmm0 = mem[0],zero
	vmovsd	8(%rbx), %xmm1                  # xmm1 = mem[0],zero
	vmovsd	%xmm1, 104(%rsp)
	fldl	104(%rsp)
	wait
	vmovsd	%xmm0, 112(%rsp)
	fldl	112(%rsp)
	faddp	%st, %st(1)
	fstpt	92(%rsp)                        # 10-byte Folded Spill
	wait
	movq	24(%rsp), %rsi
	movq	32(%rsp), %rdx
.Ltmp1378:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1379:
# %bb.7:
	leaq	72(%rsp), %rax
	movq	%rax, 56(%rsp)
	movabsq	$7310575239352771393, %rax      # imm = 0x6574617275636341
	movq	%rax, 72(%rsp)
	movq	$8, 64(%rsp)
	movb	$0, 80(%rsp)
	movq	(%rbx), %rdx
	movq	8(%rbx), %rcx
	fldt	92(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.54, %edi
	movl	$65, %esi
	xorl	%eax, %eax
	callq	printf
	testb	%bpl, %bpl
	je	.LBB28_13
# %bb.8:
.Ltmp1380:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.6, %esi
	movl	$4, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1381:
# %bb.9:
	movq	(%rbx), %rax
	movq	%rax, 56(%rsp)
.Ltmp1382:
	leaq	56(%rsp), %rsi
	movl	$_ZSt4cout, %edi
	callq	_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1383:
# %bb.10:
.Ltmp1384:
	movq	%rax, %r14
	movl	$.L.str.51, %esi
	movl	$5, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1385:
# %bb.11:
	movq	8(%rbx), %rax
	movq	%rax, 120(%rsp)
.Ltmp1386:
	leaq	120(%rsp), %rsi
	movq	%r14, %rdi
	callq	_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1387:
# %bb.12:
	fldt	92(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.7, %edi
	xorl	%eax, %eax
	callq	printf
.LBB28_13:
	movq	24(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB28_15
# %bb.14:
	callq	_ZdlPv
.LBB28_15:
	addq	$128, %rsp
	.cfi_def_cfa_offset 48
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB28_16:
	.cfi_def_cfa_offset 176
.Ltmp1388:
	movq	%rax, %rbx
	movq	24(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB28_18
# %bb.17:
	callq	_ZdlPv
.LBB28_18:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end28:
	.size	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b, .Lfunc_end28-_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.cfi_endproc
	.section	.gcc_except_table._Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"aG",@progbits,_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.p2align	2, 0x0
GCC_except_table28:
.Lexception21:
	.byte	255                             # @LPStart Encoding = omit
	.byte	255                             # @TType Encoding = omit
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end21-.Lcst_begin21
.Lcst_begin21:
	.uleb128 .Lfunc_begin21-.Lfunc_begin21  # >> Call Site 1 <<
	.uleb128 .Ltmp1378-.Lfunc_begin21       #   Call between .Lfunc_begin21 and .Ltmp1378
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1378-.Lfunc_begin21       # >> Call Site 2 <<
	.uleb128 .Ltmp1387-.Ltmp1378            #   Call between .Ltmp1378 and .Ltmp1387
	.uleb128 .Ltmp1388-.Lfunc_begin21       #     jumps to .Ltmp1388
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1387-.Lfunc_begin21       # >> Call Site 3 <<
	.uleb128 .Lfunc_end28-.Ltmp1387         #   Call between .Ltmp1387 and .Lfunc_end28
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end21:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._Z5printIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"axG",@progbits,_Z5printIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.weak	_Z5printIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b # -- Begin function _Z5printIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.p2align	4, 0x90
	.type	_Z5printIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,@function
_Z5printIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b: # 
.Lfunc_begin22:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception22
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r12
	.cfi_def_cfa_offset 40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	subq	$144, %rsp
	.cfi_def_cfa_offset 192
	.cfi_offset %rbx, -48
	.cfi_offset %r12, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, %ebp
	movq	%rsi, %rbx
	leaq	40(%rsp), %r12
	movq	%r12, 24(%rsp)
	movq	(%rdi), %r14
	movq	8(%rdi), %r15
	movq	%r15, 56(%rsp)
	movq	%r12, %rax
	cmpq	$16, %r15
	jb	.LBB29_2
# %bb.1:
	leaq	24(%rsp), %rdi
	leaq	56(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
	movq	%rax, 24(%rsp)
	movq	56(%rsp), %rcx
	movq	%rcx, 40(%rsp)
.LBB29_2:
	testq	%r15, %r15
	je	.LBB29_6
# %bb.3:
	cmpq	$1, %r15
	jne	.LBB29_5
# %bb.4:
	movzbl	(%r14), %ecx
	movb	%cl, (%rax)
	jmp	.LBB29_6
.LBB29_5:
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r15, %rdx
	callq	_intel_fast_memcpy@PLT
.LBB29_6:
	movq	56(%rsp), %rax
	movq	%rax, 32(%rsp)
	movq	24(%rsp), %rcx
	movb	$0, (%rcx,%rax)
	vmovsd	16(%rbx), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 104(%rsp)
	fldl	104(%rsp)
	wait
	vmovsd	(%rbx), %xmm0                   # xmm0 = mem[0],zero
	vmovsd	8(%rbx), %xmm1                  # xmm1 = mem[0],zero
	vmovsd	%xmm1, 112(%rsp)
	fldl	112(%rsp)
	faddp	%st, %st(1)
	wait
	vmovsd	%xmm0, 120(%rsp)
	fldl	120(%rsp)
	faddp	%st, %st(1)
	fstpt	92(%rsp)                        # 10-byte Folded Spill
	wait
	movq	24(%rsp), %rsi
	movq	32(%rsp), %rdx
.Ltmp1389:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1390:
# %bb.7:
	leaq	72(%rsp), %rax
	movq	%rax, 56(%rsp)
	movabsq	$7310575239352771393, %rax      # imm = 0x6574617275636341
	movq	%rax, 72(%rsp)
	movq	$8, 64(%rsp)
	movb	$0, 80(%rsp)
	movq	(%rbx), %rdx
	movq	8(%rbx), %rcx
	movq	16(%rbx), %r8
	fldt	92(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.55, %edi
	movl	$65, %esi
	xorl	%eax, %eax
	callq	printf
	testb	%bpl, %bpl
	je	.LBB29_15
# %bb.8:
.Ltmp1391:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.6, %esi
	movl	$4, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1392:
# %bb.9:
	movq	(%rbx), %rax
	movq	%rax, 56(%rsp)
.Ltmp1393:
	leaq	56(%rsp), %rsi
	movl	$_ZSt4cout, %edi
	callq	_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1394:
# %bb.10:
.Ltmp1395:
	movq	%rax, %r14
	movl	$.L.str.51, %esi
	movl	$5, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1396:
# %bb.11:
	movq	8(%rbx), %rax
	movq	%rax, 136(%rsp)
.Ltmp1397:
	leaq	136(%rsp), %rsi
	movq	%r14, %rdi
	callq	_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1398:
# %bb.12:
.Ltmp1399:
	movq	%rax, %r14
	movl	$.L.str.51, %esi
	movl	$5, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1400:
# %bb.13:
	movq	16(%rbx), %rax
	movq	%rax, 128(%rsp)
.Ltmp1401:
	leaq	128(%rsp), %rsi
	movq	%r14, %rdi
	callq	_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1402:
# %bb.14:
	fldt	92(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.7, %edi
	xorl	%eax, %eax
	callq	printf
.LBB29_15:
	movq	24(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB29_17
# %bb.16:
	callq	_ZdlPv
.LBB29_17:
	addq	$144, %rsp
	.cfi_def_cfa_offset 48
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB29_18:
	.cfi_def_cfa_offset 192
.Ltmp1403:
	movq	%rax, %rbx
	movq	24(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB29_20
# %bb.19:
	callq	_ZdlPv
.LBB29_20:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end29:
	.size	_Z5printIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b, .Lfunc_end29-_Z5printIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.cfi_endproc
	.section	.gcc_except_table._Z5printIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"aG",@progbits,_Z5printIN7mX_real7tX_real7tx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.p2align	2, 0x0
GCC_except_table29:
.Lexception22:
	.byte	255                             # @LPStart Encoding = omit
	.byte	255                             # @TType Encoding = omit
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end22-.Lcst_begin22
.Lcst_begin22:
	.uleb128 .Lfunc_begin22-.Lfunc_begin22  # >> Call Site 1 <<
	.uleb128 .Ltmp1389-.Lfunc_begin22       #   Call between .Lfunc_begin22 and .Ltmp1389
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1389-.Lfunc_begin22       # >> Call Site 2 <<
	.uleb128 .Ltmp1402-.Ltmp1389            #   Call between .Ltmp1389 and .Ltmp1402
	.uleb128 .Ltmp1403-.Lfunc_begin22       #     jumps to .Ltmp1403
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1402-.Lfunc_begin22       # >> Call Site 3 <<
	.uleb128 .Lfunc_end29-.Ltmp1402         #   Call between .Ltmp1402 and .Lfunc_end29
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end22:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._Z5printIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"axG",@progbits,_Z5printIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.weak	_Z5printIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b # -- Begin function _Z5printIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.p2align	4, 0x90
	.type	_Z5printIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,@function
_Z5printIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b: # 
.Lfunc_begin23:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception23
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r12
	.cfi_def_cfa_offset 40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	subq	$160, %rsp
	.cfi_def_cfa_offset 208
	.cfi_offset %rbx, -48
	.cfi_offset %r12, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, %ebp
	movq	%rsi, %rbx
	leaq	40(%rsp), %r12
	movq	%r12, 24(%rsp)
	movq	(%rdi), %r14
	movq	8(%rdi), %r15
	movq	%r15, 56(%rsp)
	movq	%r12, %rax
	cmpq	$16, %r15
	jb	.LBB30_2
# %bb.1:
	leaq	24(%rsp), %rdi
	leaq	56(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
	movq	%rax, 24(%rsp)
	movq	56(%rsp), %rcx
	movq	%rcx, 40(%rsp)
.LBB30_2:
	testq	%r15, %r15
	je	.LBB30_6
# %bb.3:
	cmpq	$1, %r15
	jne	.LBB30_5
# %bb.4:
	movzbl	(%r14), %ecx
	movb	%cl, (%rax)
	jmp	.LBB30_6
.LBB30_5:
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r15, %rdx
	callq	_intel_fast_memcpy@PLT
.LBB30_6:
	movq	56(%rsp), %rax
	movq	%rax, 32(%rsp)
	movq	24(%rsp), %rcx
	movb	$0, (%rcx,%rax)
	vmovsd	24(%rbx), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 104(%rsp)
	fldl	104(%rsp)
	wait
	vmovsd	16(%rbx), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 112(%rsp)
	fldl	112(%rsp)
	faddp	%st, %st(1)
	wait
	vmovsd	(%rbx), %xmm0                   # xmm0 = mem[0],zero
	vmovsd	8(%rbx), %xmm1                  # xmm1 = mem[0],zero
	vmovsd	%xmm1, 120(%rsp)
	fldl	120(%rsp)
	faddp	%st, %st(1)
	wait
	vmovsd	%xmm0, 128(%rsp)
	fldl	128(%rsp)
	faddp	%st, %st(1)
	fstpt	92(%rsp)                        # 10-byte Folded Spill
	wait
	movq	24(%rsp), %rsi
	movq	32(%rsp), %rdx
.Ltmp1404:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1405:
# %bb.7:
	leaq	72(%rsp), %rax
	movq	%rax, 56(%rsp)
	movabsq	$7310575239352771393, %rax      # imm = 0x6574617275636341
	movq	%rax, 72(%rsp)
	movq	$8, 64(%rsp)
	movb	$0, 80(%rsp)
	movq	(%rbx), %rdx
	movq	8(%rbx), %rcx
	movq	16(%rbx), %r8
	movq	24(%rbx), %r9
	fldt	92(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.56, %edi
	movl	$65, %esi
	xorl	%eax, %eax
	callq	printf
	testb	%bpl, %bpl
	je	.LBB30_17
# %bb.8:
.Ltmp1406:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.6, %esi
	movl	$4, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1407:
# %bb.9:
	movq	(%rbx), %rax
	movq	%rax, 56(%rsp)
.Ltmp1408:
	leaq	56(%rsp), %rsi
	movl	$_ZSt4cout, %edi
	callq	_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1409:
# %bb.10:
.Ltmp1410:
	movq	%rax, %r14
	movl	$.L.str.51, %esi
	movl	$5, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1411:
# %bb.11:
	movq	8(%rbx), %rax
	movq	%rax, 152(%rsp)
.Ltmp1412:
	leaq	152(%rsp), %rsi
	movq	%r14, %rdi
	callq	_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1413:
# %bb.12:
.Ltmp1414:
	movq	%rax, %r14
	movl	$.L.str.51, %esi
	movl	$5, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1415:
# %bb.13:
	movq	16(%rbx), %rax
	movq	%rax, 144(%rsp)
.Ltmp1416:
	leaq	144(%rsp), %rsi
	movq	%r14, %rdi
	callq	_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1417:
# %bb.14:
.Ltmp1418:
	movq	%rax, %r14
	movl	$.L.str.51, %esi
	movl	$5, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1419:
# %bb.15:
	movq	24(%rbx), %rax
	movq	%rax, 136(%rsp)
.Ltmp1420:
	leaq	136(%rsp), %rsi
	movq	%r14, %rdi
	callq	_ZStlsIcSt11char_traitsIcELm64EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1421:
# %bb.16:
	fldt	92(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.7, %edi
	xorl	%eax, %eax
	callq	printf
.LBB30_17:
	movq	24(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB30_19
# %bb.18:
	callq	_ZdlPv
.LBB30_19:
	addq	$160, %rsp
	.cfi_def_cfa_offset 48
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB30_20:
	.cfi_def_cfa_offset 208
.Ltmp1422:
	movq	%rax, %rbx
	movq	24(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB30_22
# %bb.21:
	callq	_ZdlPv
.LBB30_22:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end30:
	.size	_Z5printIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b, .Lfunc_end30-_Z5printIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.cfi_endproc
	.section	.gcc_except_table._Z5printIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"aG",@progbits,_Z5printIN7mX_real7qX_real7qx_realIdLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.p2align	2, 0x0
GCC_except_table30:
.Lexception23:
	.byte	255                             # @LPStart Encoding = omit
	.byte	255                             # @TType Encoding = omit
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end23-.Lcst_begin23
.Lcst_begin23:
	.uleb128 .Lfunc_begin23-.Lfunc_begin23  # >> Call Site 1 <<
	.uleb128 .Ltmp1404-.Lfunc_begin23       #   Call between .Lfunc_begin23 and .Ltmp1404
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1404-.Lfunc_begin23       # >> Call Site 2 <<
	.uleb128 .Ltmp1421-.Ltmp1404            #   Call between .Ltmp1404 and .Ltmp1421
	.uleb128 .Ltmp1422-.Lfunc_begin23       #     jumps to .Ltmp1422
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1421-.Lfunc_begin23       # >> Call Site 3 <<
	.uleb128 .Lfunc_end30-.Ltmp1421         #   Call between .Ltmp1421 and .Lfunc_end30
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end23:
	.p2align	2, 0x0
                                        # -- End function
	.section	.text._Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"axG",@progbits,_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.weak	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b # -- Begin function _Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.p2align	4, 0x90
	.type	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,@function
_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b: # 
.Lfunc_begin24:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception24
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r12
	.cfi_def_cfa_offset 40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	subq	$112, %rsp
	.cfi_def_cfa_offset 160
	.cfi_offset %rbx, -48
	.cfi_offset %r12, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, %ebp
	movq	%rsi, %rbx
	leaq	72(%rsp), %r12
	movq	%r12, 56(%rsp)
	movq	(%rdi), %r14
	movq	8(%rdi), %r15
	movq	%r15, 24(%rsp)
	movq	%r12, %rax
	cmpq	$16, %r15
	jb	.LBB31_2
# %bb.1:
	leaq	56(%rsp), %rdi
	leaq	24(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
	movq	%rax, 56(%rsp)
	movq	24(%rsp), %rcx
	movq	%rcx, 72(%rsp)
.LBB31_2:
	testq	%r15, %r15
	je	.LBB31_6
# %bb.3:
	cmpq	$1, %r15
	jne	.LBB31_5
# %bb.4:
	movzbl	(%r14), %ecx
	movb	%cl, (%rax)
	jmp	.LBB31_6
.LBB31_5:
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r15, %rdx
	callq	_intel_fast_memcpy@PLT
.LBB31_6:
	movq	24(%rsp), %rax
	movq	%rax, 64(%rsp)
	movq	56(%rsp), %rcx
	movb	$0, (%rcx,%rax)
	vmovss	(%rbx), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	4(%rbx), %xmm1                  # xmm1 = mem[0],zero,zero,zero
	vmovss	%xmm1, 16(%rsp)
	flds	16(%rsp)
	wait
	vmovss	%xmm0, 20(%rsp)
	flds	20(%rsp)
	faddp	%st, %st(1)
	fstpt	92(%rsp)                        # 10-byte Folded Spill
	wait
	movq	56(%rsp), %rsi
	movq	64(%rsp), %rdx
.Ltmp1423:
	movl	$_ZSt4cout, %edi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1424:
# %bb.7:
	leaq	40(%rsp), %rax
	movq	%rax, 24(%rsp)
	movl	$1886350419, 40(%rsp)           # imm = 0x706F6C53
	movw	$31088, 44(%rsp)                # imm = 0x7970
	movq	$6, 32(%rsp)
	movb	$0, 46(%rsp)
	movl	(%rbx), %edx
	movl	4(%rbx), %ecx
	fldt	92(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.50, %edi
	movl	$83, %esi
	xorl	%eax, %eax
	callq	printf
	testb	%bpl, %bpl
	je	.LBB31_13
# %bb.8:
.Ltmp1425:
	movl	$_ZSt4cout, %edi
	movl	$.L.str.6, %esi
	movl	$4, %edx
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1426:
# %bb.9:
	movl	(%rbx), %eax
	movq	%rax, 24(%rsp)
.Ltmp1427:
	leaq	24(%rsp), %rsi
	movl	$_ZSt4cout, %edi
	callq	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1428:
# %bb.10:
.Ltmp1429:
	movq	%rax, %r14
	movl	$.L.str.51, %esi
	movl	$5, %edx
	movq	%rax, %rdi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1430:
# %bb.11:
	movl	4(%rbx), %eax
	movq	%rax, 104(%rsp)
.Ltmp1431:
	leaq	104(%rsp), %rsi
	movq	%r14, %rdi
	callq	_ZStlsIcSt11char_traitsIcELm32EERSt13basic_ostreamIT_T0_ES6_RKSt6bitsetIXT1_EE
.Ltmp1432:
# %bb.12:
	fldt	92(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.7, %edi
	xorl	%eax, %eax
	callq	printf
.LBB31_13:
	movq	56(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB31_15
# %bb.14:
	callq	_ZdlPv
.LBB31_15:
	addq	$112, %rsp
	.cfi_def_cfa_offset 48
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB31_16:
	.cfi_def_cfa_offset 160
.Ltmp1433:
	movq	%rax, %rbx
	movq	56(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB31_18
# %bb.17:
	callq	_ZdlPv
.LBB31_18:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end31:
	.size	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b, .Lfunc_end31-_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
	.cfi_endproc
	.section	.gcc_except_table._Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,"aG",@progbits,_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b,comdat
	.p2align	2, 0x0
GCC_except_table31:
.Lexception24:
	.byte	255                             # @LPStart Encoding = omit
	.byte	255                             # @TType Encoding = omit
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end24-.Lcst_begin24
.Lcst_begin24:
	.uleb128 .Lfunc_begin24-.Lfunc_begin24  # >> Call Site 1 <<
	.uleb128 .Ltmp1423-.Lfunc_begin24       #   Call between .Lfunc_begin24 and .Ltmp1423
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1423-.Lfunc_begin24       # >> Call Site 2 <<
	.uleb128 .Ltmp1432-.Ltmp1423            #   Call between .Ltmp1423 and .Ltmp1432
	.uleb128 .Ltmp1433-.Lfunc_begin24       #     jumps to .Ltmp1433
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1432-.Lfunc_begin24       # >> Call Site 3 <<
	.uleb128 .Lfunc_end31-.Ltmp1432         #   Call between .Ltmp1432 and .Lfunc_end31
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end24:
	.p2align	2, 0x0
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2, 0x0                          # -- Begin function _Z6verifyIfEvRKiRKN4mpfr6mprealEPS3_S6_S6_
.LCPI32_0:
	.long	0x7fffffff                      #  NaN
	.section	.text._Z6verifyIfEvRKiRKN4mpfr6mprealEPS3_S6_S6_,"axG",@progbits,_Z6verifyIfEvRKiRKN4mpfr6mprealEPS3_S6_S6_,comdat
	.weak	_Z6verifyIfEvRKiRKN4mpfr6mprealEPS3_S6_S6_
	.p2align	4, 0x90
	.type	_Z6verifyIfEvRKiRKN4mpfr6mprealEPS3_S6_S6_,@function
_Z6verifyIfEvRKiRKN4mpfr6mprealEPS3_S6_S6_: # 
.Lfunc_begin25:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception25
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$440, %rsp                      # imm = 0x1B8
	.cfi_def_cfa_offset 496
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, 160(%rsp)                  # 8-byte Spill
	movq	%rcx, 88(%rsp)                  # 8-byte Spill
	movq	%rdx, %r12
	movq	%rsi, 24(%rsp)                  # 8-byte Spill
	movq	%rdi, %r15
	vcvtsi2sdl	(%rdi), %xmm0, %xmm0
	callq	sqrt
	vcvttsd2si	%xmm0, %eax
	movl	%eax, 36(%rsp)                  # 4-byte Spill
	movl	(%r15), %r13d
	movq	%r13, %rax
	shlq	$2, %rax
	testl	%r13d, %r13d
	movq	$-1, %r14
	cmovnsq	%rax, %r14
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %rbx
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %r14
	testl	%r13d, %r13d
	jle	.LBB32_3
# %bb.1:
	xorl	%ebp, %ebp
	movq	88(%rsp), %r13                  # 8-byte Reload
	.p2align	4, 0x90
.LBB32_2:                               # =>This Inner Loop Header: Depth=1
	movq	%r12, %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovss	%xmm0, (%rbx,%rbp,4)
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovss	%xmm0, (%r14,%rbp,4)
	incq	%rbp
	movslq	(%r15), %rax
	addq	$32, %r13
	addq	$32, %r12
	cmpq	%rax, %rbp
	jl	.LBB32_2
.LBB32_3:
	movq	24(%rsp), %rdi                  # 8-byte Reload
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovss	%xmm0, 172(%rsp)                # 4-byte Spill
	movl	$40, %edi
	callq	_Znam
	movq	%rax, %rbp
	callq	omp_get_wtime
	movq	%r15, 216(%rsp)                 # 8-byte Spill
	movl	(%r15), %eax
	testl	%eax, %eax
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
	jle	.LBB32_12
# %bb.4:
	movl	%eax, %ecx
	andl	$-8, %ecx
	leaq	(,%rax,4), %rdx
	andq	$-32, %rdx
	xorl	%esi, %esi
	xorl	%edi, %edi
	jmp	.LBB32_6
	.p2align	4, 0x90
.LBB32_5:                               #   in Loop: Header=BB32_6 Depth=1
	vmovss	%xmm0, (%rbp,%rdi,4)
	leaq	1(%rdi), %r8
	cmpq	$9, %rdi
	movq	%r8, %rdi
	je	.LBB32_13
.LBB32_6:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB32_8 Depth 2
                                        #     Child Loop BB32_11 Depth 2
	vcvtsi2ss	%esi, %xmm3, %xmm0
	cmpl	$8, %eax
	jb	.LBB32_9
# %bb.7:                                #   in Loop: Header=BB32_6 Depth=1
	xorl	%r8d, %r8d
	.p2align	4, 0x90
.LBB32_8:                               #   Parent Loop BB32_6 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%r8), %xmm1               # xmm1 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%r8), %xmm2              # xmm2 = mem[0],zero,zero,zero
	vmulss	(%r14,%r8), %xmm1, %xmm1
	vmulss	4(%r14,%r8), %xmm2, %xmm2
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	8(%rbx,%r8), %xmm1              # xmm1 = mem[0],zero,zero,zero
	vmulss	8(%r14,%r8), %xmm1, %xmm1
	vaddss	%xmm2, %xmm0, %xmm0
	vmovss	12(%rbx,%r8), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vmulss	12(%r14,%r8), %xmm2, %xmm2
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	16(%rbx,%r8), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vmulss	16(%r14,%r8), %xmm1, %xmm1
	vaddss	%xmm2, %xmm0, %xmm0
	vmovss	20(%rbx,%r8), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vmulss	20(%r14,%r8), %xmm2, %xmm2
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	24(%rbx,%r8), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vmulss	24(%r14,%r8), %xmm1, %xmm1
	vaddss	%xmm2, %xmm0, %xmm0
	vmovss	28(%rbx,%r8), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vmulss	28(%r14,%r8), %xmm2, %xmm2
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	addq	$32, %r8
	cmpq	%r8, %rdx
	jne	.LBB32_8
.LBB32_9:                               #   in Loop: Header=BB32_6 Depth=1
	cmpq	%rax, %rcx
	jae	.LBB32_5
# %bb.10:                               #   in Loop: Header=BB32_6 Depth=1
	movq	%rcx, %r8
	.p2align	4, 0x90
.LBB32_11:                              #   Parent Loop BB32_6 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%r8,4), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vmulss	(%r14,%r8,4), %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	incq	%r8
	cmpq	%r8, %rax
	jne	.LBB32_11
	jmp	.LBB32_5
.LBB32_12:
	xorl	%eax, %eax
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, (%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 4(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 8(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 12(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 16(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 20(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 24(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 28(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 32(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 36(%rbp)
.LBB32_13:
	callq	omp_get_wtime
	vsubsd	24(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm3, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	vmovss	(%rbp), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %r12d
	leaq	176(%rsp), %r13
	movq	%r13, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r13, %rdi
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%r12d, %esi
	callq	mpfr_set_d
.Ltmp1434:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp1435:
# %bb.14:
.Ltmp1436:
	movq	%rax, %r12
	movq	160(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp1437:
# %bb.15:
.Ltmp1438:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp1439:
# %bb.16:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp1440:
	leaq	128(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1441:
# %bb.17:
.Ltmp1442:
	leaq	128(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp1443:
# %bb.18:
.Ltmp1445:
	callq	mpfr_get_default_rounding_mode
.Ltmp1446:
# %bb.19:
.Ltmp1447:
	leaq	128(%rsp), %rdi
	leaq	176(%rsp), %rsi
	movq	160(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp1448:
# %bb.20:
.Ltmp1450:
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1451:
# %bb.21:
.Ltmp1452:
	movq	%rax, %r12
	movq	160(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp1453:
# %bb.22:
.Ltmp1454:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp1455:
# %bb.23:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp1456:
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1457:
# %bb.24:
.Ltmp1458:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp1459:
# %bb.25:
.Ltmp1461:
	callq	mpfr_get_default_rounding_mode
.Ltmp1462:
# %bb.26:
.Ltmp1463:
	leaq	40(%rsp), %rdi
	leaq	128(%rsp), %rsi
	movq	160(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp1464:
# %bb.27:
.Ltmp1466:
	callq	mpfr_get_default_rounding_mode
.Ltmp1467:
# %bb.28:
.Ltmp1468:
	movl	%eax, %r15d
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1469:
# %bb.29:
.Ltmp1470:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp1471:
# %bb.30:
.Ltmp1472:
	movl	%eax, %r12d
	leaq	96(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1473:
# %bb.31:
.Ltmp1474:
	leaq	96(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp1475:
# %bb.32:
.Ltmp1477:
	leaq	96(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp1478:
# %bb.33:
.Ltmp1480:
	leaq	96(%rsp), %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovss	%xmm0, 24(%rsp)                 # 4-byte Spill
.Ltmp1481:
# %bb.34:
	cmpq	$0, 120(%rsp)
	je	.LBB32_36
# %bb.35:
.Ltmp1483:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1484:
.LBB32_36:
	cmpq	$0, 64(%rsp)
	je	.LBB32_38
# %bb.37:
.Ltmp1486:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1487:
.LBB32_38:
	cmpq	$0, 152(%rsp)
	je	.LBB32_40
# %bb.39:
.Ltmp1489:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1490:
.LBB32_40:
	cmpq	$0, 200(%rsp)
	je	.LBB32_42
# %bb.41:
.Ltmp1492:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1493:
.LBB32_42:
	leaq	424(%rsp), %r15
	movq	%r15, 408(%rsp)
	movl	$544501604, 424(%rsp)           # imm = 0x20746F64
	movw	$32, 428(%rsp)
	movq	$5, 416(%rsp)
	vmovss	24(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 244(%rsp)
	flds	244(%rsp)
	fstpt	72(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp1495:
	movl	$_ZSt4cout, %edi
	movl	$5, %edx
	movq	%r15, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1496:
# %bb.43:
	vmovd	24(%rsp), %xmm0                 # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, %esi
	fldt	72(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	xorl	%r12d, %r12d
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	movq	408(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB32_45
# %bb.44:
	callq	_ZdlPv
.LBB32_45:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$40, %edi
	callq	_Znam
	movq	%rax, %rbp
	callq	omp_get_wtime
	vmovq	%xmm0, 24(%rsp)                 # 8-byte Folded Spill
	xorl	%r15d, %r15d
	jmp	.LBB32_47
	.p2align	4, 0x90
.LBB32_46:                              #   in Loop: Header=BB32_47 Depth=1
	callq	sqrtf
	vmovss	%xmm0, (%rbp,%r15,4)
	leaq	1(%r15), %rax
	cmpq	$9, %r15
	movq	%rax, %r15
	je	.LBB32_53
.LBB32_47:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB32_50 Depth 2
                                        #     Child Loop BB32_52 Depth 2
	vcvtsi2ss	%r12d, %xmm3, %xmm0
	movq	216(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	jle	.LBB32_46
# %bb.48:                               #   in Loop: Header=BB32_47 Depth=1
	cmpl	$8, %eax
	jb	.LBB32_51
# %bb.49:                               #   in Loop: Header=BB32_47 Depth=1
	leaq	(,%rax,4), %rcx
	andq	$-32, %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB32_50:                              #   Parent Loop BB32_47 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rdx), %xmm1              # xmm1 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%rdx), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmulss	%xmm2, %xmm2, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	8(%rbx,%rdx), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	12(%rbx,%rdx), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	16(%rbx,%rdx), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	20(%rbx,%rdx), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	24(%rbx,%rdx), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	28(%rbx,%rdx), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	addq	$32, %rdx
	cmpq	%rdx, %rcx
	jne	.LBB32_50
.LBB32_51:                              #   in Loop: Header=BB32_47 Depth=1
	movl	%eax, %ecx
	andl	$-8, %ecx
	cmpq	%rax, %rcx
	jae	.LBB32_46
	.p2align	4, 0x90
.LBB32_52:                              #   Parent Loop BB32_47 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rcx,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	incq	%rcx
	cmpq	%rcx, %rax
	jne	.LBB32_52
	jmp	.LBB32_46
.LBB32_53:
	callq	omp_get_wtime
	vsubsd	24(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm3, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	vmovss	(%rbp), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %r12d
	leaq	176(%rsp), %r13
	movq	%r13, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r13, %rdi
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%r12d, %esi
	callq	mpfr_set_d
.Ltmp1498:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp1499:
# %bb.54:
	movq	%rax, %r13
	movq	160(%rsp), %rax                 # 8-byte Reload
	leaq	32(%rax), %rdi
.Ltmp1500:
	movq	%rdi, 24(%rsp)                  # 8-byte Spill
	callq	mpfr_get_prec
.Ltmp1501:
# %bb.55:
.Ltmp1502:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp1503:
# %bb.56:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp1504:
	leaq	128(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp1505:
# %bb.57:
.Ltmp1506:
	leaq	128(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp1507:
# %bb.58:
.Ltmp1509:
	callq	mpfr_get_default_rounding_mode
.Ltmp1510:
# %bb.59:
.Ltmp1511:
	leaq	128(%rsp), %rdi
	leaq	176(%rsp), %rsi
	movq	24(%rsp), %rdx                  # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp1512:
# %bb.60:
.Ltmp1514:
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1515:
# %bb.61:
.Ltmp1516:
	movq	%rax, %r13
	movq	24(%rsp), %rdi                  # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp1517:
# %bb.62:
.Ltmp1518:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp1519:
# %bb.63:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp1520:
	leaq	40(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp1521:
# %bb.64:
.Ltmp1522:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp1523:
# %bb.65:
.Ltmp1525:
	callq	mpfr_get_default_rounding_mode
.Ltmp1526:
# %bb.66:
.Ltmp1527:
	leaq	40(%rsp), %rdi
	leaq	128(%rsp), %rsi
	movq	24(%rsp), %rdx                  # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp1528:
# %bb.67:
.Ltmp1530:
	callq	mpfr_get_default_rounding_mode
.Ltmp1531:
# %bb.68:
.Ltmp1532:
	movl	%eax, %r15d
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1533:
# %bb.69:
.Ltmp1534:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp1535:
# %bb.70:
.Ltmp1536:
	movl	%eax, %r12d
	leaq	96(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1537:
# %bb.71:
.Ltmp1538:
	leaq	96(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp1539:
# %bb.72:
.Ltmp1541:
	leaq	96(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp1542:
# %bb.73:
.Ltmp1544:
	leaq	96(%rsp), %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovss	%xmm0, 24(%rsp)                 # 4-byte Spill
.Ltmp1545:
# %bb.74:
	cmpq	$0, 120(%rsp)
	je	.LBB32_76
# %bb.75:
.Ltmp1547:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1548:
.LBB32_76:
	cmpq	$0, 64(%rsp)
	je	.LBB32_78
# %bb.77:
.Ltmp1550:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1551:
.LBB32_78:
	cmpq	$0, 152(%rsp)
	je	.LBB32_80
# %bb.79:
.Ltmp1553:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1554:
.LBB32_80:
	cmpq	$0, 200(%rsp)
	je	.LBB32_82
# %bb.81:
.Ltmp1556:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1557:
.LBB32_82:
	leaq	392(%rsp), %r15
	movq	%r15, 376(%rsp)
	movl	$846033518, 392(%rsp)           # imm = 0x326D726E
	movw	$32, 396(%rsp)
	movq	$5, 384(%rsp)
	vmovss	24(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 240(%rsp)
	flds	240(%rsp)
	fstpt	72(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp1559:
	movl	$_ZSt4cout, %edi
	movl	$5, %edx
	movq	%r15, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1560:
# %bb.83:
	vmovd	24(%rsp), %xmm0                 # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, %esi
	fldt	72(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	movq	376(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB32_85
# %bb.84:
	callq	_ZdlPv
.LBB32_85:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$40, %edi
	callq	_Znam
	movq	%rax, %rbp
	callq	omp_get_wtime
	movq	216(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	vmovq	%xmm0, 24(%rsp)                 # 8-byte Folded Spill
	jle	.LBB32_94
# %bb.86:
	movl	%eax, %ecx
	andl	$-8, %ecx
	leaq	(,%rax,4), %rdx
	andq	$-32, %rdx
	xorl	%esi, %esi
	vbroadcastss	.LCPI32_0(%rip), %xmm0  # xmm0 = [NaN,NaN,NaN,NaN]
	vbroadcastss	.LCPI32_0(%rip), %xmm1  # xmm1 = [NaN,NaN,NaN,NaN]
	xorl	%edi, %edi
	jmp	.LBB32_88
	.p2align	4, 0x90
.LBB32_87:                              #   in Loop: Header=BB32_88 Depth=1
	vmovss	%xmm2, (%rbp,%rdi,4)
	leaq	1(%rdi), %r8
	cmpq	$9, %rdi
	movq	%r8, %rdi
	je	.LBB32_95
.LBB32_88:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB32_90 Depth 2
                                        #     Child Loop BB32_93 Depth 2
	vcvtsi2ss	%esi, %xmm7, %xmm2
	cmpl	$8, %eax
	jb	.LBB32_91
# %bb.89:                               #   in Loop: Header=BB32_88 Depth=1
	xorl	%r8d, %r8d
	.p2align	4, 0x90
.LBB32_90:                              #   Parent Loop BB32_88 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%r8), %xmm3               # xmm3 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%r8), %xmm4              # xmm4 = mem[0],zero,zero,zero
	vmovss	8(%rbx,%r8), %xmm5              # xmm5 = mem[0],zero,zero,zero
	vmovss	12(%rbx,%r8), %xmm6             # xmm6 = mem[0],zero,zero,zero
	vandps	%xmm1, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vandps	%xmm1, %xmm4, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vandps	%xmm1, %xmm5, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vandps	%xmm1, %xmm6, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	16(%rbx,%r8), %xmm3             # xmm3 = mem[0],zero,zero,zero
	vandps	%xmm1, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	20(%rbx,%r8), %xmm3             # xmm3 = mem[0],zero,zero,zero
	vandps	%xmm1, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	24(%rbx,%r8), %xmm3             # xmm3 = mem[0],zero,zero,zero
	vandps	%xmm1, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	28(%rbx,%r8), %xmm3             # xmm3 = mem[0],zero,zero,zero
	vandps	%xmm1, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	addq	$32, %r8
	cmpq	%r8, %rdx
	jne	.LBB32_90
.LBB32_91:                              #   in Loop: Header=BB32_88 Depth=1
	cmpq	%rax, %rcx
	jae	.LBB32_87
# %bb.92:                               #   in Loop: Header=BB32_88 Depth=1
	movq	%rcx, %r8
	.p2align	4, 0x90
.LBB32_93:                              #   Parent Loop BB32_88 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%r8,4), %xmm3             # xmm3 = mem[0],zero,zero,zero
	vandps	%xmm0, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	incq	%r8
	cmpq	%r8, %rax
	jne	.LBB32_93
	jmp	.LBB32_87
.LBB32_94:
	xorl	%eax, %eax
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, (%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 4(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 8(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 12(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 16(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 20(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 24(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 28(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 32(%rbp)
	vcvtsi2ss	%eax, %xmm1, %xmm0
	vmovss	%xmm0, 36(%rbp)
.LBB32_95:
	callq	omp_get_wtime
	vsubsd	24(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm7, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	vmovss	(%rbp), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %r12d
	leaq	176(%rsp), %r13
	movq	%r13, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r13, %rdi
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%r12d, %esi
	callq	mpfr_set_d
.Ltmp1562:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp1563:
# %bb.96:
	movq	%rax, %r13
	movq	160(%rsp), %rax                 # 8-byte Reload
	leaq	64(%rax), %rdi
.Ltmp1564:
	movq	%rdi, 24(%rsp)                  # 8-byte Spill
	callq	mpfr_get_prec
.Ltmp1565:
# %bb.97:
.Ltmp1566:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp1567:
# %bb.98:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp1568:
	leaq	128(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp1569:
# %bb.99:
.Ltmp1570:
	leaq	128(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp1571:
# %bb.100:
.Ltmp1573:
	callq	mpfr_get_default_rounding_mode
.Ltmp1574:
# %bb.101:
.Ltmp1575:
	leaq	128(%rsp), %rdi
	leaq	176(%rsp), %rsi
	movq	24(%rsp), %rdx                  # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp1576:
# %bb.102:
.Ltmp1578:
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1579:
# %bb.103:
.Ltmp1580:
	movq	%rax, %r13
	movq	24(%rsp), %rdi                  # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp1581:
# %bb.104:
.Ltmp1582:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp1583:
# %bb.105:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp1584:
	leaq	40(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp1585:
# %bb.106:
.Ltmp1586:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp1587:
# %bb.107:
.Ltmp1589:
	callq	mpfr_get_default_rounding_mode
.Ltmp1590:
# %bb.108:
.Ltmp1591:
	leaq	40(%rsp), %rdi
	leaq	128(%rsp), %rsi
	movq	24(%rsp), %rdx                  # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp1592:
# %bb.109:
.Ltmp1594:
	callq	mpfr_get_default_rounding_mode
.Ltmp1595:
# %bb.110:
.Ltmp1596:
	movl	%eax, %r15d
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1597:
# %bb.111:
.Ltmp1598:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp1599:
# %bb.112:
.Ltmp1600:
	movl	%eax, %r12d
	leaq	96(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1601:
# %bb.113:
.Ltmp1602:
	leaq	96(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp1603:
# %bb.114:
.Ltmp1605:
	leaq	96(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp1606:
# %bb.115:
.Ltmp1608:
	leaq	96(%rsp), %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovss	%xmm0, 24(%rsp)                 # 4-byte Spill
.Ltmp1609:
# %bb.116:
	cmpq	$0, 120(%rsp)
	je	.LBB32_118
# %bb.117:
.Ltmp1611:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1612:
.LBB32_118:
	cmpq	$0, 64(%rsp)
	je	.LBB32_120
# %bb.119:
.Ltmp1614:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1615:
.LBB32_120:
	cmpq	$0, 152(%rsp)
	je	.LBB32_122
# %bb.121:
.Ltmp1617:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1618:
.LBB32_122:
	cmpq	$0, 200(%rsp)
	je	.LBB32_124
# %bb.123:
.Ltmp1620:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1621:
.LBB32_124:
	leaq	360(%rsp), %r15
	movq	%r15, 344(%rsp)
	movl	$1836413793, 360(%rsp)          # imm = 0x6D757361
	movw	$32, 364(%rsp)
	movq	$5, 352(%rsp)
	vmovss	24(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 236(%rsp)
	flds	236(%rsp)
	fstpt	72(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp1623:
	movl	$_ZSt4cout, %edi
	movl	$5, %edx
	movq	%r15, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1624:
# %bb.125:
	vmovd	24(%rsp), %xmm0                 # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, %esi
	fldt	72(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	movq	344(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB32_127
# %bb.126:
	callq	_ZdlPv
.LBB32_127:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movq	216(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	vmovss	172(%rsp), %xmm2                # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	jle	.LBB32_133
# %bb.128:
	cmpl	$8, %eax
	jb	.LBB32_131
# %bb.129:
	leaq	(,%rax,4), %rcx
	andq	$-32, %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB32_130:                             # =>This Inner Loop Header: Depth=1
	vmulss	(%rbx,%rdx), %xmm2, %xmm0
	vaddss	(%r14,%rdx), %xmm0, %xmm0
	vmovss	%xmm0, (%r14,%rdx)
	vmulss	4(%rbx,%rdx), %xmm2, %xmm0
	vaddss	4(%r14,%rdx), %xmm0, %xmm0
	vmovss	%xmm0, 4(%r14,%rdx)
	vmulss	8(%rbx,%rdx), %xmm2, %xmm0
	vaddss	8(%r14,%rdx), %xmm0, %xmm0
	vmulss	12(%rbx,%rdx), %xmm2, %xmm1
	vmovss	%xmm0, 8(%r14,%rdx)
	vaddss	12(%r14,%rdx), %xmm1, %xmm0
	vmovss	%xmm0, 12(%r14,%rdx)
	vmulss	16(%rbx,%rdx), %xmm2, %xmm0
	vaddss	16(%r14,%rdx), %xmm0, %xmm0
	vmovss	%xmm0, 16(%r14,%rdx)
	vmulss	20(%rbx,%rdx), %xmm2, %xmm0
	vaddss	20(%r14,%rdx), %xmm0, %xmm0
	vmulss	24(%rbx,%rdx), %xmm2, %xmm1
	vmovss	%xmm0, 20(%r14,%rdx)
	vaddss	24(%r14,%rdx), %xmm1, %xmm0
	vmovss	%xmm0, 24(%r14,%rdx)
	vmulss	28(%rbx,%rdx), %xmm2, %xmm0
	vaddss	28(%r14,%rdx), %xmm0, %xmm0
	vmovss	%xmm0, 28(%r14,%rdx)
	addq	$32, %rdx
	cmpq	%rdx, %rcx
	jne	.LBB32_130
.LBB32_131:
	movl	%eax, %ecx
	andl	$-8, %ecx
	cmpq	%rax, %rcx
	jae	.LBB32_133
	.p2align	4, 0x90
.LBB32_132:                             # =>This Inner Loop Header: Depth=1
	vmulss	(%rbx,%rcx,4), %xmm2, %xmm0
	vaddss	(%r14,%rcx,4), %xmm0, %xmm0
	vmovss	%xmm0, (%r14,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %rax
	jne	.LBB32_132
.LBB32_133:
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	96(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	movq	216(%rsp), %rax                 # 8-byte Reload
	cmpl	$0, (%rax)
	jle	.LBB32_177
# %bb.134:
	movq	160(%rsp), %rax                 # 8-byte Reload
	leaq	256(%rax), %r15
	xorl	%ecx, %ecx
	jmp	.LBB32_136
	.p2align	4, 0x90
.LBB32_135:                             #   in Loop: Header=BB32_136 Depth=1
	movq	72(%rsp), %rcx                  # 8-byte Reload
	incq	%rcx
	movq	216(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r15
	cmpq	%rax, %rcx
	jge	.LBB32_177
.LBB32_136:                             # =>This Inner Loop Header: Depth=1
	movq	%rcx, 72(%rsp)                  # 8-byte Spill
	vmovss	(%r14,%rcx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
.Ltmp1626:
	callq	mpfr_get_default_prec
.Ltmp1627:
# %bb.137:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1628:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp1629:
# %bb.138:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1630:
	movl	%eax, %ebp
	leaq	128(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp1631:
# %bb.139:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1632:
	leaq	128(%rsp), %rdi
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%ebp, %esi
	callq	mpfr_set_d
.Ltmp1633:
# %bb.140:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1635:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp1636:
# %bb.141:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1637:
	movq	%rax, %r13
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1638:
# %bb.142:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1639:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp1640:
# %bb.143:                              #   in Loop: Header=BB32_136 Depth=1
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp1641:
	leaq	40(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp1642:
# %bb.144:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1643:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp1644:
# %bb.145:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1646:
	callq	mpfr_get_default_rounding_mode
.Ltmp1647:
# %bb.146:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1648:
	leaq	40(%rsp), %rdi
	movq	%r15, %rsi
	leaq	128(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp1649:
# %bb.147:                              #   in Loop: Header=BB32_136 Depth=1
	cmpq	$0, 152(%rsp)
	je	.LBB32_149
# %bb.148:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1651:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1652:
.LBB32_149:                             #   in Loop: Header=BB32_136 Depth=1
.Ltmp1654:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1655:
# %bb.150:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1656:
	movq	%rax, %r13
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1657:
# %bb.151:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1658:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp1659:
# %bb.152:                              #   in Loop: Header=BB32_136 Depth=1
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp1660:
	leaq	176(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp1661:
# %bb.153:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1662:
	leaq	176(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp1663:
# %bb.154:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1665:
	callq	mpfr_get_default_rounding_mode
.Ltmp1666:
# %bb.155:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1667:
	leaq	176(%rsp), %rdi
	leaq	40(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp1668:
# %bb.156:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1670:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1671:
# %bb.157:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1672:
	movq	%rax, %r13
	leaq	176(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1673:
# %bb.158:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1674:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp1675:
# %bb.159:                              #   in Loop: Header=BB32_136 Depth=1
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp1676:
	leaq	128(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp1677:
# %bb.160:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1678:
	leaq	128(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp1679:
# %bb.161:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1681:
	callq	mpfr_get_default_rounding_mode
.Ltmp1682:
	leaq	96(%rsp), %r12
# %bb.162:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1683:
	leaq	128(%rsp), %rdi
	movq	%r12, %rsi
	leaq	176(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp1684:
# %bb.163:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1686:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp1687:
# %bb.164:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1688:
	movq	%rax, %r12
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1689:
# %bb.165:                              #   in Loop: Header=BB32_136 Depth=1
	movq	%rax, %r13
	cmpq	%rax, %r12
	je	.LBB32_169
# %bb.166:                              #   in Loop: Header=BB32_136 Depth=1
	cmpq	$0, 120(%rsp)
	je	.LBB32_168
# %bb.167:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1690:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1691:
.LBB32_168:                             #   in Loop: Header=BB32_136 Depth=1
.Ltmp1692:
	leaq	96(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1693:
.LBB32_169:                             #   in Loop: Header=BB32_136 Depth=1
.Ltmp1694:
	callq	mpfr_get_default_rounding_mode
.Ltmp1695:
# %bb.170:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1696:
	leaq	96(%rsp), %rdi
	leaq	128(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp1697:
# %bb.171:                              #   in Loop: Header=BB32_136 Depth=1
	cmpq	$0, 152(%rsp)
	je	.LBB32_173
# %bb.172:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1699:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1700:
.LBB32_173:                             #   in Loop: Header=BB32_136 Depth=1
	cmpq	$0, 200(%rsp)
	je	.LBB32_175
# %bb.174:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1702:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1703:
.LBB32_175:                             #   in Loop: Header=BB32_136 Depth=1
	cmpq	$0, 64(%rsp)
	je	.LBB32_135
# %bb.176:                              #   in Loop: Header=BB32_136 Depth=1
.Ltmp1705:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1706:
	jmp	.LBB32_135
.LBB32_177:
.Ltmp1708:
	callq	mpfr_get_default_rounding_mode
.Ltmp1709:
# %bb.178:
.Ltmp1710:
	movl	%eax, %ebp
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1711:
# %bb.179:
.Ltmp1712:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp1713:
# %bb.180:
.Ltmp1714:
	movl	%eax, %r15d
	leaq	40(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp1715:
# %bb.181:
.Ltmp1716:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp1717:
# %bb.182:
.Ltmp1719:
	leaq	40(%rsp), %rdi
	leaq	96(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp1720:
# %bb.183:
.Ltmp1722:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1723:
# %bb.184:
.Ltmp1724:
	movq	%rax, %r12
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1725:
# %bb.185:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB32_189
# %bb.186:
	cmpq	$0, 120(%rsp)
	je	.LBB32_188
# %bb.187:
.Ltmp1726:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1727:
.LBB32_188:
.Ltmp1728:
	leaq	96(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp1729:
.LBB32_189:
.Ltmp1730:
	callq	mpfr_get_default_rounding_mode
.Ltmp1731:
# %bb.190:
.Ltmp1732:
	leaq	96(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp1733:
# %bb.191:
	cmpq	$0, 64(%rsp)
	je	.LBB32_193
# %bb.192:
.Ltmp1735:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1736:
.LBB32_193:
	callq	omp_get_wtime
	vmovq	%xmm0, 24(%rsp)                 # 8-byte Folded Spill
	movq	216(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	vmovss	172(%rsp), %xmm2                # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	jle	.LBB32_202
# %bb.194:
	movl	%eax, %ecx
	andl	$-8, %ecx
	leaq	(,%rax,4), %rdx
	andq	$-32, %rdx
	xorl	%esi, %esi
	jmp	.LBB32_196
	.p2align	4, 0x90
.LBB32_195:                             #   in Loop: Header=BB32_196 Depth=1
	leal	1(%rsi), %edi
	cmpl	$9, %esi
	movl	%edi, %esi
	je	.LBB32_202
.LBB32_196:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB32_198 Depth 2
                                        #     Child Loop BB32_201 Depth 2
	cmpl	$8, %eax
	jb	.LBB32_199
# %bb.197:                              #   in Loop: Header=BB32_196 Depth=1
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB32_198:                             #   Parent Loop BB32_196 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmulss	(%rbx,%rdi), %xmm2, %xmm0
	vaddss	(%r14,%rdi), %xmm0, %xmm0
	vmovss	%xmm0, (%r14,%rdi)
	vmulss	4(%rbx,%rdi), %xmm2, %xmm0
	vaddss	4(%r14,%rdi), %xmm0, %xmm0
	vmovss	%xmm0, 4(%r14,%rdi)
	vmulss	8(%rbx,%rdi), %xmm2, %xmm0
	vaddss	8(%r14,%rdi), %xmm0, %xmm0
	vmulss	12(%rbx,%rdi), %xmm2, %xmm1
	vmovss	%xmm0, 8(%r14,%rdi)
	vaddss	12(%r14,%rdi), %xmm1, %xmm0
	vmovss	%xmm0, 12(%r14,%rdi)
	vmulss	16(%rbx,%rdi), %xmm2, %xmm0
	vaddss	16(%r14,%rdi), %xmm0, %xmm0
	vmovss	%xmm0, 16(%r14,%rdi)
	vmulss	20(%rbx,%rdi), %xmm2, %xmm0
	vaddss	20(%r14,%rdi), %xmm0, %xmm0
	vmulss	24(%rbx,%rdi), %xmm2, %xmm1
	vmovss	%xmm0, 20(%r14,%rdi)
	vaddss	24(%r14,%rdi), %xmm1, %xmm0
	vmovss	%xmm0, 24(%r14,%rdi)
	vmulss	28(%rbx,%rdi), %xmm2, %xmm0
	vaddss	28(%r14,%rdi), %xmm0, %xmm0
	vmovss	%xmm0, 28(%r14,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %rdx
	jne	.LBB32_198
.LBB32_199:                             #   in Loop: Header=BB32_196 Depth=1
	cmpq	%rax, %rcx
	jae	.LBB32_195
# %bb.200:                              #   in Loop: Header=BB32_196 Depth=1
	movq	%rcx, %rdi
	.p2align	4, 0x90
.LBB32_201:                             #   Parent Loop BB32_196 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmulss	(%rbx,%rdi,4), %xmm2, %xmm0
	vaddss	(%r14,%rdi,4), %xmm0, %xmm0
	vmovss	%xmm0, (%r14,%rdi,4)
	incq	%rdi
	cmpq	%rdi, %rax
	jne	.LBB32_201
	jmp	.LBB32_195
.LBB32_202:
	callq	omp_get_wtime
	vsubsd	24(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm3, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp1738:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1739:
# %bb.203:
	movq	%rax, %r12
	movq	160(%rsp), %rax                 # 8-byte Reload
	leaq	96(%rax), %r15
.Ltmp1740:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp1741:
# %bb.204:
.Ltmp1742:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp1743:
# %bb.205:
	movl	%eax, %ebp
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp1744:
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1745:
# %bb.206:
.Ltmp1746:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp1747:
# %bb.207:
.Ltmp1749:
	callq	mpfr_get_default_rounding_mode
.Ltmp1750:
# %bb.208:
.Ltmp1751:
	leaq	40(%rsp), %rdi
	leaq	96(%rsp), %rsi
	movq	%r15, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp1752:
# %bb.209:
.Ltmp1754:
	leaq	40(%rsp), %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovss	%xmm0, 24(%rsp)                 # 4-byte Spill
.Ltmp1755:
# %bb.210:
	cmpq	$0, 64(%rsp)
	je	.LBB32_212
# %bb.211:
.Ltmp1757:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1758:
.LBB32_212:
	leaq	328(%rsp), %r15
	movq	%r15, 312(%rsp)
	movl	$2037413985, 328(%rsp)          # imm = 0x79707861
	movw	$32, 332(%rsp)
	movq	$5, 320(%rsp)
	vmovss	24(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 232(%rsp)
	flds	232(%rsp)
	fstpt	72(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp1760:
	movl	$_ZSt4cout, %edi
	movl	$5, %edx
	movq	%r15, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1761:
# %bb.213:
	vmovd	24(%rsp), %xmm0                 # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, %esi
	fldt	72(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	movq	312(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB32_215
# %bb.214:
	callq	_ZdlPv
.LBB32_215:
	cmpq	$0, 120(%rsp)
	je	.LBB32_217
# %bb.216:
.Ltmp1763:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1764:
.LBB32_217:
	movl	36(%rsp), %r12d                 # 4-byte Reload
	movl	%r12d, %r15d
	leaq	(,%r15,4), %rax
	testl	%r12d, %r12d
	movq	$-1, %rdi
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	cmovnsq	%rax, %rdi
	callq	_Znam
	movq	%rax, %rbp
	testl	%r12d, %r12d
	jle	.LBB32_232
# %bb.218:
	movq	%r15, %rax
	shrq	$3, %rax
	movq	%rax, 208(%rsp)                 # 8-byte Spill
	cmpl	$8, 36(%rsp)                    # 4-byte Folded Reload
	movq	88(%rsp), %rcx                  # 8-byte Reload
	jb	.LBB32_221
# %bb.219:
	movq	208(%rsp), %rax                 # 8-byte Reload
	shlq	$8, %rax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	movl	$28, %r13d
	xorl	%r12d, %r12d
	.p2align	4, 0x90
.LBB32_220:                             # =>This Inner Loop Header: Depth=1
	leaq	(%rcx,%r12), %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovd	%xmm0, -28(%r14,%r13)
	vmovd	%xmm0, -28(%rbp,%r13)
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r12), %rdi
	addq	$32, %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovd	%xmm0, -24(%r14,%r13)
	vmovd	%xmm0, -24(%rbp,%r13)
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r12), %rdi
	addq	$64, %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovd	%xmm0, -20(%r14,%r13)
	vmovd	%xmm0, -20(%rbp,%r13)
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r12), %rdi
	addq	$96, %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovd	%xmm0, -16(%r14,%r13)
	vmovd	%xmm0, -16(%rbp,%r13)
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r12), %rdi
	addq	$128, %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovd	%xmm0, -12(%r14,%r13)
	vmovd	%xmm0, -12(%rbp,%r13)
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r12), %rdi
	addq	$160, %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovd	%xmm0, -8(%r14,%r13)
	vmovd	%xmm0, -8(%rbp,%r13)
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r12), %rdi
	addq	$192, %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovd	%xmm0, -4(%r14,%r13)
	vmovd	%xmm0, -4(%rbp,%r13)
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r12), %rdi
	addq	$224, %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	movq	88(%rsp), %rcx                  # 8-byte Reload
	vmovd	%xmm0, (%r14,%r13)
	vmovd	%xmm0, (%rbp,%r13)
	addq	$256, %r12                      # imm = 0x100
	addq	$32, %r13
	cmpq	%r12, 72(%rsp)                  # 8-byte Folded Reload
	jne	.LBB32_220
.LBB32_221:
	movl	%r15d, %eax
	andl	$-8, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	cmpq	%r15, %rax
	jae	.LBB32_224
# %bb.222:
	movq	%rcx, %r13
	movq	208(%rsp), %rax                 # 8-byte Reload
	shlq	$8, %rax
	addq	%rax, %r13
	movq	72(%rsp), %r12                  # 8-byte Reload
	.p2align	4, 0x90
.LBB32_223:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovd	%xmm0, (%r14,%r12,4)
	vmovd	%xmm0, (%rbp,%r12,4)
	incq	%r12
	addq	$32, %r13
	cmpq	%r12, %r15
	jne	.LBB32_223
.LBB32_224:
	leaq	-1(%r15), %rax
	leaq	28(%rbx), %rcx
	movq	208(%rsp), %r9                  # 8-byte Reload
	shlq	$5, %r9
	xorl	%esi, %esi
	movq	%rbx, %rdx
	vmovss	172(%rsp), %xmm3                # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	movq	72(%rsp), %r10                  # 8-byte Reload
	movq	24(%rsp), %r8                   # 8-byte Reload
	jmp	.LBB32_226
	.p2align	4, 0x90
.LBB32_225:                             #   in Loop: Header=BB32_226 Depth=1
	leaq	1(%rsi), %rdi
	addq	%r8, %rcx
	addq	%r8, %rdx
	cmpq	%rax, %rsi
	movq	%rdi, %rsi
	je	.LBB32_232
.LBB32_226:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB32_228 Depth 2
                                        #     Child Loop BB32_231 Depth 2
	vmulss	(%r14,%rsi,4), %xmm3, %xmm0
	cmpl	$8, 36(%rsp)                    # 4-byte Folded Reload
	jb	.LBB32_229
# %bb.227:                              #   in Loop: Header=BB32_226 Depth=1
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB32_228:                             #   Parent Loop BB32_226 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmulss	-28(%rcx,%rdi), %xmm0, %xmm1
	vaddss	(%rbp,%rdi), %xmm1, %xmm1
	vmovss	%xmm1, (%rbp,%rdi)
	vmulss	-24(%rcx,%rdi), %xmm0, %xmm1
	vaddss	4(%rbp,%rdi), %xmm1, %xmm1
	vmovss	%xmm1, 4(%rbp,%rdi)
	vmulss	-20(%rcx,%rdi), %xmm0, %xmm1
	vaddss	8(%rbp,%rdi), %xmm1, %xmm1
	vmulss	-16(%rcx,%rdi), %xmm0, %xmm2
	vmovss	%xmm1, 8(%rbp,%rdi)
	vaddss	12(%rbp,%rdi), %xmm2, %xmm1
	vmovss	%xmm1, 12(%rbp,%rdi)
	vmulss	-12(%rcx,%rdi), %xmm0, %xmm1
	vaddss	16(%rbp,%rdi), %xmm1, %xmm1
	vmovss	%xmm1, 16(%rbp,%rdi)
	vmulss	-8(%rcx,%rdi), %xmm0, %xmm1
	vaddss	20(%rbp,%rdi), %xmm1, %xmm1
	vmulss	-4(%rcx,%rdi), %xmm0, %xmm2
	vmovss	%xmm1, 20(%rbp,%rdi)
	vaddss	24(%rbp,%rdi), %xmm2, %xmm1
	vmovss	%xmm1, 24(%rbp,%rdi)
	vmulss	(%rcx,%rdi), %xmm0, %xmm1
	vaddss	28(%rbp,%rdi), %xmm1, %xmm1
	vmovss	%xmm1, 28(%rbp,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r9
	jne	.LBB32_228
.LBB32_229:                             #   in Loop: Header=BB32_226 Depth=1
	cmpq	%r15, %r10
	jae	.LBB32_225
# %bb.230:                              #   in Loop: Header=BB32_226 Depth=1
	movq	%r10, %rdi
	.p2align	4, 0x90
.LBB32_231:                             #   Parent Loop BB32_226 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmulss	(%rdx,%rdi,4), %xmm0, %xmm1
	vaddss	(%rbp,%rdi,4), %xmm1, %xmm1
	vmovss	%xmm1, (%rbp,%rdi,4)
	incq	%rdi
	cmpq	%rdi, %r15
	jne	.LBB32_231
	jmp	.LBB32_225
.LBB32_232:
	callq	mpfr_get_default_prec
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %r12d
	leaq	96(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
	leaq	96(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
	cmpl	$0, 36(%rsp)                    # 4-byte Folded Reload
	jle	.LBB32_276
# %bb.233:
	xorl	%ecx, %ecx
	jmp	.LBB32_235
	.p2align	4, 0x90
.LBB32_234:                             #   in Loop: Header=BB32_235 Depth=1
	movq	88(%rsp), %rcx                  # 8-byte Reload
	incq	%rcx
	cmpq	%rcx, %r15
	je	.LBB32_276
.LBB32_235:                             # =>This Inner Loop Header: Depth=1
	movq	216(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	movq	%rax, 208(%rsp)                 # 8-byte Spill
	movq	%rcx, 88(%rsp)                  # 8-byte Spill
	vmovss	(%rbp,%rcx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmovsd	%xmm0, 72(%rsp)                 # 8-byte Spill
.Ltmp1766:
	callq	mpfr_get_default_prec
.Ltmp1767:
# %bb.236:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1768:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp1769:
# %bb.237:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1770:
	movl	%eax, %r13d
	leaq	128(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp1771:
# %bb.238:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1772:
	leaq	128(%rsp), %rdi
	vmovsd	72(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%r13d, %esi
	callq	mpfr_set_d
.Ltmp1773:
# %bb.239:                              #   in Loop: Header=BB32_235 Depth=1
	movq	88(%rsp), %rax                  # 8-byte Reload
	movq	208(%rsp), %rcx                 # 8-byte Reload
	addl	%ecx, %eax
	addl	$8, %eax
	movslq	%eax, %r12
	shlq	$5, %r12
	addq	160(%rsp), %r12                 # 8-byte Folded Reload
.Ltmp1775:
	movq	%r12, %rdi
	callq	mpfr_get_prec
	movq	%rax, 72(%rsp)                  # 8-byte Spill
.Ltmp1776:
# %bb.240:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1777:
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1778:
# %bb.241:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1779:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
	movl	%eax, 208(%rsp)                 # 4-byte Spill
.Ltmp1780:
# %bb.242:                              #   in Loop: Header=BB32_235 Depth=1
	movq	72(%rsp), %rax                  # 8-byte Reload
	cmpq	%r13, %rax
	cmovgq	%rax, %r13
.Ltmp1781:
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1782:
# %bb.243:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1783:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	208(%rsp), %edx                 # 4-byte Reload
	callq	mpfr_set_si
.Ltmp1784:
# %bb.244:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1786:
	callq	mpfr_get_default_rounding_mode
.Ltmp1787:
# %bb.245:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1788:
	leaq	40(%rsp), %rdi
	movq	%r12, %rsi
	leaq	128(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp1789:
# %bb.246:                              #   in Loop: Header=BB32_235 Depth=1
	cmpq	$0, 152(%rsp)
	je	.LBB32_248
# %bb.247:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1791:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1792:
.LBB32_248:                             #   in Loop: Header=BB32_235 Depth=1
.Ltmp1794:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1795:
# %bb.249:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1796:
	movq	%rax, %r12
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1797:
# %bb.250:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1798:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
	movl	%eax, 72(%rsp)                  # 4-byte Spill
.Ltmp1799:
# %bb.251:                              #   in Loop: Header=BB32_235 Depth=1
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp1800:
	leaq	176(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1801:
# %bb.252:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1802:
	leaq	176(%rsp), %rdi
	xorl	%esi, %esi
	movl	72(%rsp), %edx                  # 4-byte Reload
	callq	mpfr_set_si
.Ltmp1803:
# %bb.253:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1805:
	callq	mpfr_get_default_rounding_mode
.Ltmp1806:
# %bb.254:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1807:
	leaq	176(%rsp), %rdi
	leaq	40(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp1808:
# %bb.255:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1810:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1811:
# %bb.256:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1812:
	movq	%rax, %r12
	leaq	176(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1813:
# %bb.257:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1814:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
	movl	%eax, 72(%rsp)                  # 4-byte Spill
.Ltmp1815:
# %bb.258:                              #   in Loop: Header=BB32_235 Depth=1
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp1816:
	leaq	128(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1817:
# %bb.259:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1818:
	leaq	128(%rsp), %rdi
	xorl	%esi, %esi
	movl	72(%rsp), %edx                  # 4-byte Reload
	callq	mpfr_set_si
.Ltmp1819:
# %bb.260:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1821:
	callq	mpfr_get_default_rounding_mode
.Ltmp1822:
	leaq	96(%rsp), %r12
# %bb.261:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1823:
	leaq	128(%rsp), %rdi
	movq	%r12, %rsi
	leaq	176(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp1824:
# %bb.262:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1826:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp1827:
# %bb.263:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1828:
	movq	%rax, %r13
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1829:
# %bb.264:                              #   in Loop: Header=BB32_235 Depth=1
	movq	%rax, %r12
	cmpq	%rax, %r13
	je	.LBB32_268
# %bb.265:                              #   in Loop: Header=BB32_235 Depth=1
	cmpq	$0, 120(%rsp)
	je	.LBB32_267
# %bb.266:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1830:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1831:
.LBB32_267:                             #   in Loop: Header=BB32_235 Depth=1
.Ltmp1832:
	leaq	96(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp1833:
.LBB32_268:                             #   in Loop: Header=BB32_235 Depth=1
.Ltmp1834:
	callq	mpfr_get_default_rounding_mode
.Ltmp1835:
# %bb.269:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1836:
	leaq	96(%rsp), %rdi
	leaq	128(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp1837:
# %bb.270:                              #   in Loop: Header=BB32_235 Depth=1
	cmpq	$0, 152(%rsp)
	je	.LBB32_272
# %bb.271:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1839:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1840:
.LBB32_272:                             #   in Loop: Header=BB32_235 Depth=1
	cmpq	$0, 200(%rsp)
	je	.LBB32_274
# %bb.273:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1842:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1843:
.LBB32_274:                             #   in Loop: Header=BB32_235 Depth=1
	cmpq	$0, 64(%rsp)
	je	.LBB32_234
# %bb.275:                              #   in Loop: Header=BB32_235 Depth=1
.Ltmp1845:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1846:
	jmp	.LBB32_234
.LBB32_276:
.Ltmp1848:
	callq	mpfr_get_default_rounding_mode
	movl	%eax, 88(%rsp)                  # 4-byte Spill
.Ltmp1849:
# %bb.277:
.Ltmp1850:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1851:
# %bb.278:
.Ltmp1852:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp1853:
# %bb.279:
.Ltmp1854:
	movl	%eax, %r12d
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1855:
# %bb.280:
.Ltmp1856:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp1857:
# %bb.281:
.Ltmp1859:
	leaq	40(%rsp), %rdi
	leaq	96(%rsp), %rsi
	movl	88(%rsp), %edx                  # 4-byte Reload
	callq	mpfr_sqrt
.Ltmp1860:
# %bb.282:
.Ltmp1862:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1863:
# %bb.283:
.Ltmp1864:
	movq	%rax, %r12
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1865:
# %bb.284:
	movq	%rax, %r13
	cmpq	%rax, %r12
	je	.LBB32_288
# %bb.285:
	cmpq	$0, 120(%rsp)
	je	.LBB32_287
# %bb.286:
.Ltmp1866:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1867:
.LBB32_287:
.Ltmp1868:
	leaq	96(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1869:
.LBB32_288:
.Ltmp1870:
	callq	mpfr_get_default_rounding_mode
.Ltmp1871:
# %bb.289:
.Ltmp1872:
	leaq	96(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp1873:
# %bb.290:
	cmpq	$0, 64(%rsp)
	je	.LBB32_292
# %bb.291:
.Ltmp1875:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1876:
.LBB32_292:
	callq	omp_get_wtime
	vmovq	%xmm0, 88(%rsp)                 # 8-byte Folded Spill
	cmpl	$0, 36(%rsp)                    # 4-byte Folded Reload
	vmovss	172(%rsp), %xmm3                # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	jle	.LBB32_303
# %bb.293:
	leaq	-1(%r15), %rax
	movl	%r15d, %ecx
	andl	$-8, %ecx
	movq	%rbx, %rdx
	addq	$28, %rdx
	movq	24(%rsp), %rsi                  # 8-byte Reload
	andq	$-32, %rsi
	xorl	%edi, %edi
	movq	24(%rsp), %r12                  # 8-byte Reload
	jmp	.LBB32_295
	.p2align	4, 0x90
.LBB32_294:                             #   in Loop: Header=BB32_295 Depth=1
	leal	1(%rdi), %r8d
	cmpl	$9, %edi
	movl	%r8d, %edi
	je	.LBB32_303
.LBB32_295:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB32_297 Depth 2
                                        #       Child Loop BB32_299 Depth 3
                                        #       Child Loop BB32_302 Depth 3
	movq	%rbx, %r8
	movq	%rdx, %r9
	xorl	%r10d, %r10d
	jmp	.LBB32_297
	.p2align	4, 0x90
.LBB32_296:                             #   in Loop: Header=BB32_297 Depth=2
	leaq	1(%r10), %r11
	addq	%r12, %r9
	addq	%r12, %r8
	cmpq	%rax, %r10
	movq	%r11, %r10
	je	.LBB32_294
.LBB32_297:                             #   Parent Loop BB32_295 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB32_299 Depth 3
                                        #       Child Loop BB32_302 Depth 3
	vmulss	(%r14,%r10,4), %xmm3, %xmm0
	cmpl	$8, 36(%rsp)                    # 4-byte Folded Reload
	jb	.LBB32_300
# %bb.298:                              #   in Loop: Header=BB32_297 Depth=2
	xorl	%r11d, %r11d
	.p2align	4, 0x90
.LBB32_299:                             #   Parent Loop BB32_295 Depth=1
                                        #     Parent Loop BB32_297 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmulss	-28(%r9,%r11), %xmm0, %xmm1
	vaddss	(%rbp,%r11), %xmm1, %xmm1
	vmovss	%xmm1, (%rbp,%r11)
	vmulss	-24(%r9,%r11), %xmm0, %xmm1
	vaddss	4(%rbp,%r11), %xmm1, %xmm1
	vmovss	%xmm1, 4(%rbp,%r11)
	vmulss	-20(%r9,%r11), %xmm0, %xmm1
	vaddss	8(%rbp,%r11), %xmm1, %xmm1
	vmulss	-16(%r9,%r11), %xmm0, %xmm2
	vmovss	%xmm1, 8(%rbp,%r11)
	vaddss	12(%rbp,%r11), %xmm2, %xmm1
	vmovss	%xmm1, 12(%rbp,%r11)
	vmulss	-12(%r9,%r11), %xmm0, %xmm1
	vaddss	16(%rbp,%r11), %xmm1, %xmm1
	vmovss	%xmm1, 16(%rbp,%r11)
	vmulss	-8(%r9,%r11), %xmm0, %xmm1
	vaddss	20(%rbp,%r11), %xmm1, %xmm1
	vmulss	-4(%r9,%r11), %xmm0, %xmm2
	vmovss	%xmm1, 20(%rbp,%r11)
	vaddss	24(%rbp,%r11), %xmm2, %xmm1
	vmovss	%xmm1, 24(%rbp,%r11)
	vmulss	(%r9,%r11), %xmm0, %xmm1
	vaddss	28(%rbp,%r11), %xmm1, %xmm1
	vmovss	%xmm1, 28(%rbp,%r11)
	addq	$32, %r11
	cmpq	%r11, %rsi
	jne	.LBB32_299
.LBB32_300:                             #   in Loop: Header=BB32_297 Depth=2
	cmpq	%r15, %rcx
	jae	.LBB32_296
# %bb.301:                              #   in Loop: Header=BB32_297 Depth=2
	movq	%rcx, %r11
	.p2align	4, 0x90
.LBB32_302:                             #   Parent Loop BB32_295 Depth=1
                                        #     Parent Loop BB32_297 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmulss	(%r8,%r11,4), %xmm0, %xmm1
	vaddss	(%rbp,%r11,4), %xmm1, %xmm1
	vmovss	%xmm1, (%rbp,%r11,4)
	incq	%r11
	cmpq	%r11, %r15
	jne	.LBB32_302
	jmp	.LBB32_296
.LBB32_303:
	callq	omp_get_wtime
	vsubsd	88(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm4, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp1878:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1879:
# %bb.304:
	movq	%rax, %r12
	movq	160(%rsp), %rdi                 # 8-byte Reload
	subq	$-128, %rdi
.Ltmp1880:
	movq	%rdi, 160(%rsp)                 # 8-byte Spill
	callq	mpfr_get_prec
.Ltmp1881:
# %bb.305:
.Ltmp1882:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp1883:
# %bb.306:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp1884:
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1885:
# %bb.307:
.Ltmp1886:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp1887:
# %bb.308:
.Ltmp1889:
	callq	mpfr_get_default_rounding_mode
.Ltmp1890:
# %bb.309:
.Ltmp1891:
	leaq	40(%rsp), %rdi
	leaq	96(%rsp), %rsi
	movq	160(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp1892:
# %bb.310:
.Ltmp1894:
	leaq	40(%rsp), %rdi
	callq	_ZN7mX_real7convertIfEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovss	%xmm0, 36(%rsp)                 # 4-byte Spill
.Ltmp1895:
# %bb.311:
	cmpq	$0, 64(%rsp)
	je	.LBB32_313
# %bb.312:
.Ltmp1897:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1898:
.LBB32_313:
	leaq	296(%rsp), %r15
	movq	%r15, 280(%rsp)
	movl	$1986880871, 296(%rsp)          # imm = 0x766D6567
	movw	$32, 300(%rsp)
	movq	$5, 288(%rsp)
	vmovss	36(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 228(%rsp)
	flds	228(%rsp)
	fstpt	24(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp1900:
	movl	$_ZSt4cout, %edi
	movl	$5, %edx
	movq	%r15, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1901:
# %bb.314:
	vmovd	36(%rsp), %xmm0                 # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, %esi
	fldt	24(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	movq	280(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB32_316
# %bb.315:
	callq	_ZdlPv
.LBB32_316:
	movq	%rbp, %rdi
	callq	_ZdaPv
	cmpq	$0, 120(%rsp)
	je	.LBB32_318
# %bb.317:
.Ltmp1903:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1904:
.LBB32_318:
	movq	%rbx, %rdi
	callq	_ZdaPv
	movq	%r14, %rdi
	callq	_ZdaPv
	xorl	%eax, %eax
	vcvtsi2ss	%eax, %xmm4, %xmm1
	movl	$2097145, %eax                  # imm = 0x1FFFF9
	movl	$1, %ecx
	.p2align	4, 0x90
.LBB32_319:                             # =>This Inner Loop Header: Depth=1
	leal	7(%rax), %edx
	vcvtsi2ss	%edx, %xmm4, %xmm0
	vmulss	%xmm0, %xmm0, %xmm2
	vcvtsi2ss	%ecx, %xmm4, %xmm0
	vmulss	%xmm2, %xmm2, %xmm2
	vdivss	%xmm2, %xmm0, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	leal	6(%rax), %edx
	vcvtsi2ss	%edx, %xmm4, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vdivss	%xmm2, %xmm0, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	leal	5(%rax), %edx
	vcvtsi2ss	%edx, %xmm4, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vdivss	%xmm2, %xmm0, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	leal	4(%rax), %edx
	vcvtsi2ss	%edx, %xmm4, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vdivss	%xmm2, %xmm0, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	leal	3(%rax), %edx
	vcvtsi2ss	%edx, %xmm4, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vdivss	%xmm2, %xmm0, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	leal	2(%rax), %edx
	vcvtsi2ss	%edx, %xmm4, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vdivss	%xmm2, %xmm0, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	leal	1(%rax), %edx
	vcvtsi2ss	%edx, %xmm4, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vcvtsi2ss	%eax, %xmm4, %xmm3
	vdivss	%xmm2, %xmm0, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vmulss	%xmm3, %xmm3, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vdivss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm1
	addl	$-8, %eax
	cmpl	$-7, %eax
	jne	.LBB32_319
# %bb.320:
	movl	$90, %eax
	vcvtsi2ss	%eax, %xmm4, %xmm0
	vmulss	%xmm0, %xmm1, %xmm0
	callq	sqrtf
	callq	sqrtf
	vmovss	%xmm0, 36(%rsp)                 # 4-byte Spill
	leaq	264(%rsp), %r14
	movq	%r14, 248(%rsp)
	movq	$32, 96(%rsp)
	leaq	248(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
	movq	%rax, 248(%rsp)
	movq	96(%rsp), %rcx
	movq	%rcx, 264(%rsp)
	vmovups	.L.str.63(%rip), %ymm0
	vmovups	%ymm0, (%rax)
	movq	%rcx, 256(%rsp)
	movq	248(%rsp), %rax
	movb	$0, (%rax,%rcx)
	vmovss	36(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 224(%rsp)
	flds	224(%rsp)
	fstpt	24(%rsp)                        # 10-byte Folded Spill
	wait
	movq	248(%rsp), %rsi
	movq	256(%rsp), %rdx
.Ltmp1906:
	movl	$_ZSt4cout, %edi
	vzeroupper
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp1907:
# %bb.321:
	vmovd	36(%rsp), %xmm0                 # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, %esi
	fldt	24(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.5, %edi
	xorl	%eax, %eax
	callq	printf
	movq	248(%rsp), %rdi
	cmpq	%r14, %rdi
	je	.LBB32_323
# %bb.322:
	callq	_ZdlPv
.LBB32_323:
	addq	$440, %rsp                      # imm = 0x1B8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB32_324:
	.cfi_def_cfa_offset 496
.Ltmp1905:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_325:
.Ltmp1899:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_326:
.Ltmp1877:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_327:
.Ltmp1765:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_328:
.Ltmp1759:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_329:
.Ltmp1737:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_330:
.Ltmp1622:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_331:
.Ltmp1619:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_332:
.Ltmp1616:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_333:
.Ltmp1613:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_334:
.Ltmp1558:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_335:
.Ltmp1555:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_336:
.Ltmp1552:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_337:
.Ltmp1549:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_338:
.Ltmp1494:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_339:
.Ltmp1491:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_340:
.Ltmp1488:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_341:
.Ltmp1485:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_342:
.Ltmp1908:
	movq	%rax, %rbx
	movq	248(%rsp), %rdi
	cmpq	%r14, %rdi
	jne	.LBB32_359
	jmp	.LBB32_360
.LBB32_343:
.Ltmp1902:
	movq	%rax, %rbx
	movq	280(%rsp), %rdi
	jmp	.LBB32_347
.LBB32_344:
.Ltmp1896:
	movq	%rax, %rbx
	jmp	.LBB32_417
.LBB32_345:
.Ltmp1861:
	movq	%rax, %rbx
	jmp	.LBB32_417
.LBB32_346:
.Ltmp1762:
	movq	%rax, %rbx
	movq	312(%rsp), %rdi
.LBB32_347:
	cmpq	%r15, %rdi
	je	.LBB32_423
# %bb.348:
	callq	_ZdlPv
	jmp	.LBB32_423
.LBB32_349:
.Ltmp1756:
	movq	%rax, %rbx
	jmp	.LBB32_417
.LBB32_350:
.Ltmp1721:
	movq	%rax, %rbx
	jmp	.LBB32_417
.LBB32_351:
.Ltmp1625:
	movq	%rax, %rbx
	movq	344(%rsp), %rdi
	jmp	.LBB32_358
.LBB32_352:
.Ltmp1610:
	jmp	.LBB32_363
.LBB32_353:
.Ltmp1607:
	jmp	.LBB32_363
.LBB32_354:
.Ltmp1561:
	movq	%rax, %rbx
	movq	376(%rsp), %rdi
	jmp	.LBB32_358
.LBB32_355:
.Ltmp1546:
	jmp	.LBB32_363
.LBB32_356:
.Ltmp1543:
	jmp	.LBB32_363
.LBB32_357:
.Ltmp1497:
	movq	%rax, %rbx
	movq	408(%rsp), %rdi
.LBB32_358:
	cmpq	%r15, %rdi
	je	.LBB32_360
.LBB32_359:
	callq	_ZdlPv
.LBB32_360:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB32_361:
.Ltmp1482:
	jmp	.LBB32_363
.LBB32_362:
.Ltmp1479:
.LBB32_363:
	movq	%rax, %rbx
	leaq	96(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB32_386
.LBB32_364:
.Ltmp1893:
	movq	%rax, %rbx
	jmp	.LBB32_417
.LBB32_365:
.Ltmp1753:
	movq	%rax, %rbx
	jmp	.LBB32_417
.LBB32_366:
.Ltmp1593:
	jmp	.LBB32_385
.LBB32_367:
.Ltmp1577:
	movq	%rax, %rbx
	jmp	.LBB32_387
.LBB32_368:
.Ltmp1529:
	jmp	.LBB32_385
.LBB32_369:
.Ltmp1513:
	movq	%rax, %rbx
	jmp	.LBB32_387
.LBB32_370:
.Ltmp1465:
	jmp	.LBB32_385
.LBB32_371:
.Ltmp1449:
	movq	%rax, %rbx
	jmp	.LBB32_387
.LBB32_372:
.Ltmp1874:
	movq	%rax, %rbx
	jmp	.LBB32_417
.LBB32_373:
.Ltmp1734:
	movq	%rax, %rbx
	jmp	.LBB32_417
.LBB32_374:
.Ltmp1888:
	movq	%rax, %rbx
	jmp	.LBB32_423
.LBB32_375:
.Ltmp1858:
	movq	%rax, %rbx
	jmp	.LBB32_423
.LBB32_376:
.Ltmp1748:
	movq	%rax, %rbx
	jmp	.LBB32_423
.LBB32_377:
.Ltmp1718:
	movq	%rax, %rbx
	jmp	.LBB32_423
.LBB32_378:
.Ltmp1604:
	jmp	.LBB32_385
.LBB32_379:
.Ltmp1588:
	movq	%rax, %rbx
	jmp	.LBB32_387
.LBB32_380:
.Ltmp1572:
	jmp	.LBB32_390
.LBB32_381:
.Ltmp1540:
	jmp	.LBB32_385
.LBB32_382:
.Ltmp1524:
	movq	%rax, %rbx
	jmp	.LBB32_387
.LBB32_383:
.Ltmp1508:
	jmp	.LBB32_390
.LBB32_384:
.Ltmp1476:
.LBB32_385:
	movq	%rax, %rbx
.LBB32_386:
	leaq	40(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB32_387:
	leaq	128(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	leaq	176(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB32_388:
.Ltmp1460:
	movq	%rax, %rbx
	jmp	.LBB32_387
.LBB32_389:
.Ltmp1444:
.LBB32_390:
	movq	%rax, %rbx
	leaq	176(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB32_391:
.Ltmp1847:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_392:
.Ltmp1844:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_393:
.Ltmp1841:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_394:
.Ltmp1793:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_395:
.Ltmp1707:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_396:
.Ltmp1704:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_397:
.Ltmp1701:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_398:
.Ltmp1653:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB32_399:
.Ltmp1825:
	jmp	.LBB32_410
.LBB32_400:
.Ltmp1809:
	jmp	.LBB32_415
.LBB32_401:
.Ltmp1790:
	jmp	.LBB32_405
.LBB32_402:
.Ltmp1685:
	jmp	.LBB32_410
.LBB32_403:
.Ltmp1669:
	jmp	.LBB32_415
.LBB32_404:
.Ltmp1650:
.LBB32_405:
	movq	%rax, %rbx
	leaq	40(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB32_421
.LBB32_406:
.Ltmp1774:
	movq	%rax, %rbx
	jmp	.LBB32_423
.LBB32_407:
.Ltmp1634:
	movq	%rax, %rbx
	jmp	.LBB32_423
.LBB32_408:
.Ltmp1838:
	jmp	.LBB32_410
.LBB32_409:
.Ltmp1698:
.LBB32_410:
	movq	%rax, %rbx
	leaq	128(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB32_416
.LBB32_411:
.Ltmp1820:
	jmp	.LBB32_415
.LBB32_412:
.Ltmp1804:
	movq	%rax, %rbx
	jmp	.LBB32_417
.LBB32_413:
.Ltmp1785:
	jmp	.LBB32_420
.LBB32_414:
.Ltmp1680:
.LBB32_415:
	movq	%rax, %rbx
.LBB32_416:
	leaq	176(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB32_417:
	leaq	40(%rsp), %rdi
	jmp	.LBB32_422
.LBB32_418:
.Ltmp1664:
	movq	%rax, %rbx
	jmp	.LBB32_417
.LBB32_419:
.Ltmp1645:
.LBB32_420:
	movq	%rax, %rbx
.LBB32_421:
	leaq	128(%rsp), %rdi
.LBB32_422:
	callq	_ZN4mpfr6mprealD2Ev
.LBB32_423:
	leaq	96(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end32:
	.size	_Z6verifyIfEvRKiRKN4mpfr6mprealEPS3_S6_S6_, .Lfunc_end32-_Z6verifyIfEvRKiRKN4mpfr6mprealEPS3_S6_S6_
	.cfi_endproc
	.section	.gcc_except_table._Z6verifyIfEvRKiRKN4mpfr6mprealEPS3_S6_S6_,"aG",@progbits,_Z6verifyIfEvRKiRKN4mpfr6mprealEPS3_S6_S6_,comdat
	.p2align	2, 0x0
GCC_except_table32:
.Lexception25:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase14-.Lttbaseref14
.Lttbaseref14:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end25-.Lcst_begin25
.Lcst_begin25:
	.uleb128 .Lfunc_begin25-.Lfunc_begin25  # >> Call Site 1 <<
	.uleb128 .Ltmp1434-.Lfunc_begin25       #   Call between .Lfunc_begin25 and .Ltmp1434
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1434-.Lfunc_begin25       # >> Call Site 2 <<
	.uleb128 .Ltmp1443-.Ltmp1434            #   Call between .Ltmp1434 and .Ltmp1443
	.uleb128 .Ltmp1444-.Lfunc_begin25       #     jumps to .Ltmp1444
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1445-.Lfunc_begin25       # >> Call Site 3 <<
	.uleb128 .Ltmp1448-.Ltmp1445            #   Call between .Ltmp1445 and .Ltmp1448
	.uleb128 .Ltmp1449-.Lfunc_begin25       #     jumps to .Ltmp1449
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1450-.Lfunc_begin25       # >> Call Site 4 <<
	.uleb128 .Ltmp1459-.Ltmp1450            #   Call between .Ltmp1450 and .Ltmp1459
	.uleb128 .Ltmp1460-.Lfunc_begin25       #     jumps to .Ltmp1460
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1461-.Lfunc_begin25       # >> Call Site 5 <<
	.uleb128 .Ltmp1464-.Ltmp1461            #   Call between .Ltmp1461 and .Ltmp1464
	.uleb128 .Ltmp1465-.Lfunc_begin25       #     jumps to .Ltmp1465
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1466-.Lfunc_begin25       # >> Call Site 6 <<
	.uleb128 .Ltmp1475-.Ltmp1466            #   Call between .Ltmp1466 and .Ltmp1475
	.uleb128 .Ltmp1476-.Lfunc_begin25       #     jumps to .Ltmp1476
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1477-.Lfunc_begin25       # >> Call Site 7 <<
	.uleb128 .Ltmp1478-.Ltmp1477            #   Call between .Ltmp1477 and .Ltmp1478
	.uleb128 .Ltmp1479-.Lfunc_begin25       #     jumps to .Ltmp1479
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1480-.Lfunc_begin25       # >> Call Site 8 <<
	.uleb128 .Ltmp1481-.Ltmp1480            #   Call between .Ltmp1480 and .Ltmp1481
	.uleb128 .Ltmp1482-.Lfunc_begin25       #     jumps to .Ltmp1482
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1483-.Lfunc_begin25       # >> Call Site 9 <<
	.uleb128 .Ltmp1484-.Ltmp1483            #   Call between .Ltmp1483 and .Ltmp1484
	.uleb128 .Ltmp1485-.Lfunc_begin25       #     jumps to .Ltmp1485
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1486-.Lfunc_begin25       # >> Call Site 10 <<
	.uleb128 .Ltmp1487-.Ltmp1486            #   Call between .Ltmp1486 and .Ltmp1487
	.uleb128 .Ltmp1488-.Lfunc_begin25       #     jumps to .Ltmp1488
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1489-.Lfunc_begin25       # >> Call Site 11 <<
	.uleb128 .Ltmp1490-.Ltmp1489            #   Call between .Ltmp1489 and .Ltmp1490
	.uleb128 .Ltmp1491-.Lfunc_begin25       #     jumps to .Ltmp1491
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1492-.Lfunc_begin25       # >> Call Site 12 <<
	.uleb128 .Ltmp1493-.Ltmp1492            #   Call between .Ltmp1492 and .Ltmp1493
	.uleb128 .Ltmp1494-.Lfunc_begin25       #     jumps to .Ltmp1494
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1495-.Lfunc_begin25       # >> Call Site 13 <<
	.uleb128 .Ltmp1496-.Ltmp1495            #   Call between .Ltmp1495 and .Ltmp1496
	.uleb128 .Ltmp1497-.Lfunc_begin25       #     jumps to .Ltmp1497
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1496-.Lfunc_begin25       # >> Call Site 14 <<
	.uleb128 .Ltmp1498-.Ltmp1496            #   Call between .Ltmp1496 and .Ltmp1498
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1498-.Lfunc_begin25       # >> Call Site 15 <<
	.uleb128 .Ltmp1507-.Ltmp1498            #   Call between .Ltmp1498 and .Ltmp1507
	.uleb128 .Ltmp1508-.Lfunc_begin25       #     jumps to .Ltmp1508
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1509-.Lfunc_begin25       # >> Call Site 16 <<
	.uleb128 .Ltmp1512-.Ltmp1509            #   Call between .Ltmp1509 and .Ltmp1512
	.uleb128 .Ltmp1513-.Lfunc_begin25       #     jumps to .Ltmp1513
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1514-.Lfunc_begin25       # >> Call Site 17 <<
	.uleb128 .Ltmp1523-.Ltmp1514            #   Call between .Ltmp1514 and .Ltmp1523
	.uleb128 .Ltmp1524-.Lfunc_begin25       #     jumps to .Ltmp1524
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1525-.Lfunc_begin25       # >> Call Site 18 <<
	.uleb128 .Ltmp1528-.Ltmp1525            #   Call between .Ltmp1525 and .Ltmp1528
	.uleb128 .Ltmp1529-.Lfunc_begin25       #     jumps to .Ltmp1529
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1530-.Lfunc_begin25       # >> Call Site 19 <<
	.uleb128 .Ltmp1539-.Ltmp1530            #   Call between .Ltmp1530 and .Ltmp1539
	.uleb128 .Ltmp1540-.Lfunc_begin25       #     jumps to .Ltmp1540
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1541-.Lfunc_begin25       # >> Call Site 20 <<
	.uleb128 .Ltmp1542-.Ltmp1541            #   Call between .Ltmp1541 and .Ltmp1542
	.uleb128 .Ltmp1543-.Lfunc_begin25       #     jumps to .Ltmp1543
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1544-.Lfunc_begin25       # >> Call Site 21 <<
	.uleb128 .Ltmp1545-.Ltmp1544            #   Call between .Ltmp1544 and .Ltmp1545
	.uleb128 .Ltmp1546-.Lfunc_begin25       #     jumps to .Ltmp1546
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1547-.Lfunc_begin25       # >> Call Site 22 <<
	.uleb128 .Ltmp1548-.Ltmp1547            #   Call between .Ltmp1547 and .Ltmp1548
	.uleb128 .Ltmp1549-.Lfunc_begin25       #     jumps to .Ltmp1549
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1550-.Lfunc_begin25       # >> Call Site 23 <<
	.uleb128 .Ltmp1551-.Ltmp1550            #   Call between .Ltmp1550 and .Ltmp1551
	.uleb128 .Ltmp1552-.Lfunc_begin25       #     jumps to .Ltmp1552
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1553-.Lfunc_begin25       # >> Call Site 24 <<
	.uleb128 .Ltmp1554-.Ltmp1553            #   Call between .Ltmp1553 and .Ltmp1554
	.uleb128 .Ltmp1555-.Lfunc_begin25       #     jumps to .Ltmp1555
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1556-.Lfunc_begin25       # >> Call Site 25 <<
	.uleb128 .Ltmp1557-.Ltmp1556            #   Call between .Ltmp1556 and .Ltmp1557
	.uleb128 .Ltmp1558-.Lfunc_begin25       #     jumps to .Ltmp1558
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1559-.Lfunc_begin25       # >> Call Site 26 <<
	.uleb128 .Ltmp1560-.Ltmp1559            #   Call between .Ltmp1559 and .Ltmp1560
	.uleb128 .Ltmp1561-.Lfunc_begin25       #     jumps to .Ltmp1561
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1560-.Lfunc_begin25       # >> Call Site 27 <<
	.uleb128 .Ltmp1562-.Ltmp1560            #   Call between .Ltmp1560 and .Ltmp1562
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1562-.Lfunc_begin25       # >> Call Site 28 <<
	.uleb128 .Ltmp1571-.Ltmp1562            #   Call between .Ltmp1562 and .Ltmp1571
	.uleb128 .Ltmp1572-.Lfunc_begin25       #     jumps to .Ltmp1572
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1573-.Lfunc_begin25       # >> Call Site 29 <<
	.uleb128 .Ltmp1576-.Ltmp1573            #   Call between .Ltmp1573 and .Ltmp1576
	.uleb128 .Ltmp1577-.Lfunc_begin25       #     jumps to .Ltmp1577
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1578-.Lfunc_begin25       # >> Call Site 30 <<
	.uleb128 .Ltmp1587-.Ltmp1578            #   Call between .Ltmp1578 and .Ltmp1587
	.uleb128 .Ltmp1588-.Lfunc_begin25       #     jumps to .Ltmp1588
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1589-.Lfunc_begin25       # >> Call Site 31 <<
	.uleb128 .Ltmp1592-.Ltmp1589            #   Call between .Ltmp1589 and .Ltmp1592
	.uleb128 .Ltmp1593-.Lfunc_begin25       #     jumps to .Ltmp1593
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1594-.Lfunc_begin25       # >> Call Site 32 <<
	.uleb128 .Ltmp1603-.Ltmp1594            #   Call between .Ltmp1594 and .Ltmp1603
	.uleb128 .Ltmp1604-.Lfunc_begin25       #     jumps to .Ltmp1604
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1605-.Lfunc_begin25       # >> Call Site 33 <<
	.uleb128 .Ltmp1606-.Ltmp1605            #   Call between .Ltmp1605 and .Ltmp1606
	.uleb128 .Ltmp1607-.Lfunc_begin25       #     jumps to .Ltmp1607
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1608-.Lfunc_begin25       # >> Call Site 34 <<
	.uleb128 .Ltmp1609-.Ltmp1608            #   Call between .Ltmp1608 and .Ltmp1609
	.uleb128 .Ltmp1610-.Lfunc_begin25       #     jumps to .Ltmp1610
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1611-.Lfunc_begin25       # >> Call Site 35 <<
	.uleb128 .Ltmp1612-.Ltmp1611            #   Call between .Ltmp1611 and .Ltmp1612
	.uleb128 .Ltmp1613-.Lfunc_begin25       #     jumps to .Ltmp1613
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1614-.Lfunc_begin25       # >> Call Site 36 <<
	.uleb128 .Ltmp1615-.Ltmp1614            #   Call between .Ltmp1614 and .Ltmp1615
	.uleb128 .Ltmp1616-.Lfunc_begin25       #     jumps to .Ltmp1616
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1617-.Lfunc_begin25       # >> Call Site 37 <<
	.uleb128 .Ltmp1618-.Ltmp1617            #   Call between .Ltmp1617 and .Ltmp1618
	.uleb128 .Ltmp1619-.Lfunc_begin25       #     jumps to .Ltmp1619
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1620-.Lfunc_begin25       # >> Call Site 38 <<
	.uleb128 .Ltmp1621-.Ltmp1620            #   Call between .Ltmp1620 and .Ltmp1621
	.uleb128 .Ltmp1622-.Lfunc_begin25       #     jumps to .Ltmp1622
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1623-.Lfunc_begin25       # >> Call Site 39 <<
	.uleb128 .Ltmp1624-.Ltmp1623            #   Call between .Ltmp1623 and .Ltmp1624
	.uleb128 .Ltmp1625-.Lfunc_begin25       #     jumps to .Ltmp1625
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1624-.Lfunc_begin25       # >> Call Site 40 <<
	.uleb128 .Ltmp1626-.Ltmp1624            #   Call between .Ltmp1624 and .Ltmp1626
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1626-.Lfunc_begin25       # >> Call Site 41 <<
	.uleb128 .Ltmp1633-.Ltmp1626            #   Call between .Ltmp1626 and .Ltmp1633
	.uleb128 .Ltmp1634-.Lfunc_begin25       #     jumps to .Ltmp1634
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1635-.Lfunc_begin25       # >> Call Site 42 <<
	.uleb128 .Ltmp1644-.Ltmp1635            #   Call between .Ltmp1635 and .Ltmp1644
	.uleb128 .Ltmp1645-.Lfunc_begin25       #     jumps to .Ltmp1645
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1646-.Lfunc_begin25       # >> Call Site 43 <<
	.uleb128 .Ltmp1649-.Ltmp1646            #   Call between .Ltmp1646 and .Ltmp1649
	.uleb128 .Ltmp1650-.Lfunc_begin25       #     jumps to .Ltmp1650
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1651-.Lfunc_begin25       # >> Call Site 44 <<
	.uleb128 .Ltmp1652-.Ltmp1651            #   Call between .Ltmp1651 and .Ltmp1652
	.uleb128 .Ltmp1653-.Lfunc_begin25       #     jumps to .Ltmp1653
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1654-.Lfunc_begin25       # >> Call Site 45 <<
	.uleb128 .Ltmp1663-.Ltmp1654            #   Call between .Ltmp1654 and .Ltmp1663
	.uleb128 .Ltmp1664-.Lfunc_begin25       #     jumps to .Ltmp1664
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1665-.Lfunc_begin25       # >> Call Site 46 <<
	.uleb128 .Ltmp1668-.Ltmp1665            #   Call between .Ltmp1665 and .Ltmp1668
	.uleb128 .Ltmp1669-.Lfunc_begin25       #     jumps to .Ltmp1669
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1670-.Lfunc_begin25       # >> Call Site 47 <<
	.uleb128 .Ltmp1679-.Ltmp1670            #   Call between .Ltmp1670 and .Ltmp1679
	.uleb128 .Ltmp1680-.Lfunc_begin25       #     jumps to .Ltmp1680
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1681-.Lfunc_begin25       # >> Call Site 48 <<
	.uleb128 .Ltmp1684-.Ltmp1681            #   Call between .Ltmp1681 and .Ltmp1684
	.uleb128 .Ltmp1685-.Lfunc_begin25       #     jumps to .Ltmp1685
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1686-.Lfunc_begin25       # >> Call Site 49 <<
	.uleb128 .Ltmp1697-.Ltmp1686            #   Call between .Ltmp1686 and .Ltmp1697
	.uleb128 .Ltmp1698-.Lfunc_begin25       #     jumps to .Ltmp1698
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1699-.Lfunc_begin25       # >> Call Site 50 <<
	.uleb128 .Ltmp1700-.Ltmp1699            #   Call between .Ltmp1699 and .Ltmp1700
	.uleb128 .Ltmp1701-.Lfunc_begin25       #     jumps to .Ltmp1701
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1702-.Lfunc_begin25       # >> Call Site 51 <<
	.uleb128 .Ltmp1703-.Ltmp1702            #   Call between .Ltmp1702 and .Ltmp1703
	.uleb128 .Ltmp1704-.Lfunc_begin25       #     jumps to .Ltmp1704
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1705-.Lfunc_begin25       # >> Call Site 52 <<
	.uleb128 .Ltmp1706-.Ltmp1705            #   Call between .Ltmp1705 and .Ltmp1706
	.uleb128 .Ltmp1707-.Lfunc_begin25       #     jumps to .Ltmp1707
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1708-.Lfunc_begin25       # >> Call Site 53 <<
	.uleb128 .Ltmp1717-.Ltmp1708            #   Call between .Ltmp1708 and .Ltmp1717
	.uleb128 .Ltmp1718-.Lfunc_begin25       #     jumps to .Ltmp1718
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1719-.Lfunc_begin25       # >> Call Site 54 <<
	.uleb128 .Ltmp1720-.Ltmp1719            #   Call between .Ltmp1719 and .Ltmp1720
	.uleb128 .Ltmp1721-.Lfunc_begin25       #     jumps to .Ltmp1721
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1722-.Lfunc_begin25       # >> Call Site 55 <<
	.uleb128 .Ltmp1733-.Ltmp1722            #   Call between .Ltmp1722 and .Ltmp1733
	.uleb128 .Ltmp1734-.Lfunc_begin25       #     jumps to .Ltmp1734
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1735-.Lfunc_begin25       # >> Call Site 56 <<
	.uleb128 .Ltmp1736-.Ltmp1735            #   Call between .Ltmp1735 and .Ltmp1736
	.uleb128 .Ltmp1737-.Lfunc_begin25       #     jumps to .Ltmp1737
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1738-.Lfunc_begin25       # >> Call Site 57 <<
	.uleb128 .Ltmp1747-.Ltmp1738            #   Call between .Ltmp1738 and .Ltmp1747
	.uleb128 .Ltmp1748-.Lfunc_begin25       #     jumps to .Ltmp1748
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1749-.Lfunc_begin25       # >> Call Site 58 <<
	.uleb128 .Ltmp1752-.Ltmp1749            #   Call between .Ltmp1749 and .Ltmp1752
	.uleb128 .Ltmp1753-.Lfunc_begin25       #     jumps to .Ltmp1753
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1754-.Lfunc_begin25       # >> Call Site 59 <<
	.uleb128 .Ltmp1755-.Ltmp1754            #   Call between .Ltmp1754 and .Ltmp1755
	.uleb128 .Ltmp1756-.Lfunc_begin25       #     jumps to .Ltmp1756
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1757-.Lfunc_begin25       # >> Call Site 60 <<
	.uleb128 .Ltmp1758-.Ltmp1757            #   Call between .Ltmp1757 and .Ltmp1758
	.uleb128 .Ltmp1759-.Lfunc_begin25       #     jumps to .Ltmp1759
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1760-.Lfunc_begin25       # >> Call Site 61 <<
	.uleb128 .Ltmp1761-.Ltmp1760            #   Call between .Ltmp1760 and .Ltmp1761
	.uleb128 .Ltmp1762-.Lfunc_begin25       #     jumps to .Ltmp1762
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1763-.Lfunc_begin25       # >> Call Site 62 <<
	.uleb128 .Ltmp1764-.Ltmp1763            #   Call between .Ltmp1763 and .Ltmp1764
	.uleb128 .Ltmp1765-.Lfunc_begin25       #     jumps to .Ltmp1765
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1764-.Lfunc_begin25       # >> Call Site 63 <<
	.uleb128 .Ltmp1766-.Ltmp1764            #   Call between .Ltmp1764 and .Ltmp1766
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1766-.Lfunc_begin25       # >> Call Site 64 <<
	.uleb128 .Ltmp1773-.Ltmp1766            #   Call between .Ltmp1766 and .Ltmp1773
	.uleb128 .Ltmp1774-.Lfunc_begin25       #     jumps to .Ltmp1774
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1775-.Lfunc_begin25       # >> Call Site 65 <<
	.uleb128 .Ltmp1784-.Ltmp1775            #   Call between .Ltmp1775 and .Ltmp1784
	.uleb128 .Ltmp1785-.Lfunc_begin25       #     jumps to .Ltmp1785
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1786-.Lfunc_begin25       # >> Call Site 66 <<
	.uleb128 .Ltmp1789-.Ltmp1786            #   Call between .Ltmp1786 and .Ltmp1789
	.uleb128 .Ltmp1790-.Lfunc_begin25       #     jumps to .Ltmp1790
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1791-.Lfunc_begin25       # >> Call Site 67 <<
	.uleb128 .Ltmp1792-.Ltmp1791            #   Call between .Ltmp1791 and .Ltmp1792
	.uleb128 .Ltmp1793-.Lfunc_begin25       #     jumps to .Ltmp1793
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1794-.Lfunc_begin25       # >> Call Site 68 <<
	.uleb128 .Ltmp1803-.Ltmp1794            #   Call between .Ltmp1794 and .Ltmp1803
	.uleb128 .Ltmp1804-.Lfunc_begin25       #     jumps to .Ltmp1804
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1805-.Lfunc_begin25       # >> Call Site 69 <<
	.uleb128 .Ltmp1808-.Ltmp1805            #   Call between .Ltmp1805 and .Ltmp1808
	.uleb128 .Ltmp1809-.Lfunc_begin25       #     jumps to .Ltmp1809
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1810-.Lfunc_begin25       # >> Call Site 70 <<
	.uleb128 .Ltmp1819-.Ltmp1810            #   Call between .Ltmp1810 and .Ltmp1819
	.uleb128 .Ltmp1820-.Lfunc_begin25       #     jumps to .Ltmp1820
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1821-.Lfunc_begin25       # >> Call Site 71 <<
	.uleb128 .Ltmp1824-.Ltmp1821            #   Call between .Ltmp1821 and .Ltmp1824
	.uleb128 .Ltmp1825-.Lfunc_begin25       #     jumps to .Ltmp1825
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1826-.Lfunc_begin25       # >> Call Site 72 <<
	.uleb128 .Ltmp1837-.Ltmp1826            #   Call between .Ltmp1826 and .Ltmp1837
	.uleb128 .Ltmp1838-.Lfunc_begin25       #     jumps to .Ltmp1838
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1839-.Lfunc_begin25       # >> Call Site 73 <<
	.uleb128 .Ltmp1840-.Ltmp1839            #   Call between .Ltmp1839 and .Ltmp1840
	.uleb128 .Ltmp1841-.Lfunc_begin25       #     jumps to .Ltmp1841
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1842-.Lfunc_begin25       # >> Call Site 74 <<
	.uleb128 .Ltmp1843-.Ltmp1842            #   Call between .Ltmp1842 and .Ltmp1843
	.uleb128 .Ltmp1844-.Lfunc_begin25       #     jumps to .Ltmp1844
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1845-.Lfunc_begin25       # >> Call Site 75 <<
	.uleb128 .Ltmp1846-.Ltmp1845            #   Call between .Ltmp1845 and .Ltmp1846
	.uleb128 .Ltmp1847-.Lfunc_begin25       #     jumps to .Ltmp1847
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1848-.Lfunc_begin25       # >> Call Site 76 <<
	.uleb128 .Ltmp1857-.Ltmp1848            #   Call between .Ltmp1848 and .Ltmp1857
	.uleb128 .Ltmp1858-.Lfunc_begin25       #     jumps to .Ltmp1858
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1859-.Lfunc_begin25       # >> Call Site 77 <<
	.uleb128 .Ltmp1860-.Ltmp1859            #   Call between .Ltmp1859 and .Ltmp1860
	.uleb128 .Ltmp1861-.Lfunc_begin25       #     jumps to .Ltmp1861
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1862-.Lfunc_begin25       # >> Call Site 78 <<
	.uleb128 .Ltmp1873-.Ltmp1862            #   Call between .Ltmp1862 and .Ltmp1873
	.uleb128 .Ltmp1874-.Lfunc_begin25       #     jumps to .Ltmp1874
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1875-.Lfunc_begin25       # >> Call Site 79 <<
	.uleb128 .Ltmp1876-.Ltmp1875            #   Call between .Ltmp1875 and .Ltmp1876
	.uleb128 .Ltmp1877-.Lfunc_begin25       #     jumps to .Ltmp1877
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1878-.Lfunc_begin25       # >> Call Site 80 <<
	.uleb128 .Ltmp1887-.Ltmp1878            #   Call between .Ltmp1878 and .Ltmp1887
	.uleb128 .Ltmp1888-.Lfunc_begin25       #     jumps to .Ltmp1888
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1889-.Lfunc_begin25       # >> Call Site 81 <<
	.uleb128 .Ltmp1892-.Ltmp1889            #   Call between .Ltmp1889 and .Ltmp1892
	.uleb128 .Ltmp1893-.Lfunc_begin25       #     jumps to .Ltmp1893
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1894-.Lfunc_begin25       # >> Call Site 82 <<
	.uleb128 .Ltmp1895-.Ltmp1894            #   Call between .Ltmp1894 and .Ltmp1895
	.uleb128 .Ltmp1896-.Lfunc_begin25       #     jumps to .Ltmp1896
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1897-.Lfunc_begin25       # >> Call Site 83 <<
	.uleb128 .Ltmp1898-.Ltmp1897            #   Call between .Ltmp1897 and .Ltmp1898
	.uleb128 .Ltmp1899-.Lfunc_begin25       #     jumps to .Ltmp1899
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1900-.Lfunc_begin25       # >> Call Site 84 <<
	.uleb128 .Ltmp1901-.Ltmp1900            #   Call between .Ltmp1900 and .Ltmp1901
	.uleb128 .Ltmp1902-.Lfunc_begin25       #     jumps to .Ltmp1902
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1903-.Lfunc_begin25       # >> Call Site 85 <<
	.uleb128 .Ltmp1904-.Ltmp1903            #   Call between .Ltmp1903 and .Ltmp1904
	.uleb128 .Ltmp1905-.Lfunc_begin25       #     jumps to .Ltmp1905
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1904-.Lfunc_begin25       # >> Call Site 86 <<
	.uleb128 .Ltmp1906-.Ltmp1904            #   Call between .Ltmp1904 and .Ltmp1906
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1906-.Lfunc_begin25       # >> Call Site 87 <<
	.uleb128 .Ltmp1907-.Ltmp1906            #   Call between .Ltmp1906 and .Ltmp1907
	.uleb128 .Ltmp1908-.Lfunc_begin25       #     jumps to .Ltmp1908
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1907-.Lfunc_begin25       # >> Call Site 88 <<
	.uleb128 .Lfunc_end32-.Ltmp1907         #   Call between .Ltmp1907 and .Lfunc_end32
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end25:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase14:
	.p2align	2, 0x0
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2, 0x0                          # -- Begin function _Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.LCPI33_0:
	.long	0x80000000                      #  -0
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4, 0x0
.LCPI33_1:
	.zero	16
	.section	.text._Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"axG",@progbits,_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.weak	_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.p2align	4, 0x90
	.type	_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,@function
_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_: # 
.Lfunc_begin26:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception26
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$440, %rsp                      # imm = 0x1B8
	.cfi_def_cfa_offset 496
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, 144(%rsp)                  # 8-byte Spill
	movq	%rcx, 184(%rsp)                 # 8-byte Spill
	movq	%rdx, %r12
	movq	%rsi, %r15
	movq	%rdi, %rbx
	vcvtsi2sdl	(%rdi), %xmm0, %xmm0
	callq	sqrt
	vcvttsd2si	%xmm0, %eax
	movl	%eax, 44(%rsp)                  # 4-byte Spill
	movq	%rbx, 128(%rsp)                 # 8-byte Spill
	movslq	(%rbx), %rbp
	leaq	(,%rbp,8), %r13
	testq	%rbp, %rbp
	movq	$-1, %r14
	cmovnsq	%r13, %r14
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %rbx
	testq	%rbp, %rbp
	je	.LBB33_5
# %bb.1:
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movq	%r13, %rdx
	callq	_intel_fast_memset@PLT
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %r14
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%r13, %rdx
	callq	_intel_fast_memset@PLT
	testl	%ebp, %ebp
	jle	.LBB33_6
# %bb.2:
	movq	%r15, 48(%rsp)                  # 8-byte Spill
	xorl	%ebp, %ebp
	movq	184(%rsp), %r13                 # 8-byte Reload
	movq	128(%rsp), %r15                 # 8-byte Reload
	.p2align	4, 0x90
.LBB33_3:                               # =>This Inner Loop Header: Depth=1
	movq	%r12, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, (%rbx,%rbp,8)
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, (%r14,%rbp,8)
	incq	%rbp
	movslq	(%r15), %rax
	addq	$32, %r13
	addq	$32, %r12
	cmpq	%rax, %rbp
	jl	.LBB33_3
# %bb.4:
	movq	48(%rsp), %r15                  # 8-byte Reload
	jmp	.LBB33_6
.LBB33_5:
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %r14
.LBB33_6:
	movq	%r15, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovupd	%xmm0, 208(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	movl	$80, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, 64(%rax)
	vzeroupper
	callq	omp_get_wtime
	vxorps	%xmm8, %xmm8, %xmm8
	vmovsd	%xmm0, 48(%rsp)                 # 8-byte Spill
	movq	128(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	movl	%eax, %ecx
	andl	$-4, %ecx
	leaq	(,%rax,8), %rdx
	andq	$-32, %rdx
	xorl	%esi, %esi
	xorl	%edi, %edi
	jmp	.LBB33_8
	.p2align	4, 0x90
.LBB33_7:                               #   in Loop: Header=BB33_8 Depth=1
	vmovlps	%xmm0, (%rbp,%rdi,8)
	leaq	1(%rdi), %r8
	cmpq	$9, %rdi
	movq	%r8, %rdi
	je	.LBB33_15
.LBB33_8:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB33_11 Depth 2
                                        #     Child Loop BB33_14 Depth 2
	vcvtsi2ss	%esi, %xmm9, %xmm0
	vblendps	$1, %xmm0, %xmm8, %xmm0         # xmm0 = xmm0[0],xmm8[1,2,3]
	testl	%eax, %eax
	jle	.LBB33_7
# %bb.9:                                #   in Loop: Header=BB33_8 Depth=1
	cmpl	$4, %eax
	jb	.LBB33_12
# %bb.10:                               #   in Loop: Header=BB33_8 Depth=1
	xorl	%r8d, %r8d
	.p2align	4, 0x90
.LBB33_11:                              #   Parent Loop BB33_8 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%r8), %xmm1               # xmm1 = mem[0],zero,zero,zero
	vmovss	8(%rbx,%r8), %xmm2              # xmm2 = mem[0],zero,zero,zero
	vmovss	(%r14,%r8), %xmm3               # xmm3 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm1, %xmm4
	vmovaps	%xmm3, %xmm5
	vfmsub213ss	%xmm4, %xmm1, %xmm5     # xmm5 = (xmm1 * xmm5) - xmm4
	vfmadd231ss	4(%rbx,%r8), %xmm3, %xmm5 # xmm5 = (xmm3 * mem) + xmm5
	vmovss	8(%r14,%r8), %xmm3              # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	4(%r14,%r8), %xmm1, %xmm5 # xmm5 = (xmm1 * mem) + xmm5
	vaddss	%xmm4, %xmm0, %xmm1
	vsubss	%xmm0, %xmm1, %xmm6
	vsubss	%xmm6, %xmm1, %xmm7
	vsubss	%xmm7, %xmm0, %xmm7
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vmovshdup	%xmm0, %xmm0            # xmm0 = xmm0[1,1,3,3]
	vaddss	%xmm5, %xmm0, %xmm0
	vmulss	%xmm3, %xmm2, %xmm5
	vmovaps	%xmm3, %xmm6
	vfmsub213ss	%xmm5, %xmm2, %xmm6     # xmm6 = (xmm2 * xmm6) - xmm5
	vfmadd231ss	12(%rbx,%r8), %xmm3, %xmm6 # xmm6 = (xmm3 * mem) + xmm6
	vaddss	%xmm4, %xmm0, %xmm0
	vfmadd231ss	12(%r14,%r8), %xmm2, %xmm6 # xmm6 = (xmm2 * mem) + xmm6
	vaddss	%xmm5, %xmm1, %xmm2
	vsubss	%xmm1, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm1, %xmm1
	vsubss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	16(%rbx,%r8), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vmovss	16(%r14,%r8), %xmm3             # xmm3 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm1, %xmm4
	vmovaps	%xmm3, %xmm5
	vfmsub213ss	%xmm4, %xmm1, %xmm5     # xmm5 = (xmm1 * xmm5) - xmm4
	vfmadd231ss	20(%rbx,%r8), %xmm3, %xmm5 # xmm5 = (xmm3 * mem) + xmm5
	vfmadd231ss	20(%r14,%r8), %xmm1, %xmm5 # xmm5 = (xmm1 * mem) + xmm5
	vaddss	%xmm4, %xmm2, %xmm1
	vsubss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm3, %xmm1, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vsubss	%xmm3, %xmm4, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm5, %xmm0, %xmm0
	vmovss	24(%rbx,%r8), %xmm3             # xmm3 = mem[0],zero,zero,zero
	vmovss	24(%r14,%r8), %xmm4             # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm3, %xmm5
	vmovaps	%xmm4, %xmm6
	vfmsub213ss	%xmm5, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm6) - xmm5
	vfmadd231ss	28(%rbx,%r8), %xmm4, %xmm6 # xmm6 = (xmm4 * mem) + xmm6
	vfmadd231ss	28(%r14,%r8), %xmm3, %xmm6 # xmm6 = (xmm3 * mem) + xmm6
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm5, %xmm1, %xmm2
	vsubss	%xmm1, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm1, %xmm1
	vsubss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
	addq	$32, %r8
	cmpq	%r8, %rdx
	jne	.LBB33_11
.LBB33_12:                              #   in Loop: Header=BB33_8 Depth=1
	cmpq	%rax, %rcx
	jae	.LBB33_7
# %bb.13:                               #   in Loop: Header=BB33_8 Depth=1
	movq	%rcx, %r8
	.p2align	4, 0x90
.LBB33_14:                              #   Parent Loop BB33_8 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%r8,8), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vmovss	(%r14,%r8,8), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm1, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm4) - xmm3
	vfmadd231ss	4(%rbx,%r8,8), %xmm2, %xmm4 # xmm4 = (xmm2 * mem) + xmm4
	vfmadd231ss	4(%r14,%r8,8), %xmm1, %xmm4 # xmm4 = (xmm1 * mem) + xmm4
	vaddss	%xmm3, %xmm0, %xmm1
	vsubss	%xmm0, %xmm1, %xmm2
	vsubss	%xmm2, %xmm1, %xmm5
	vsubss	%xmm5, %xmm0, %xmm5
	vsubss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vmovshdup	%xmm0, %xmm0            # xmm0 = xmm0[1,1,3,3]
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	incq	%r8
	cmpq	%r8, %rax
	jne	.LBB33_14
	jmp	.LBB33_7
.LBB33_15:
	callq	omp_get_wtime
	vsubsd	48(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm9, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	152(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp1909:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp1910:
# %bb.16:
.Ltmp1911:
	movq	%rax, %r12
	movq	144(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp1912:
# %bb.17:
.Ltmp1913:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp1914:
# %bb.18:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp1915:
	leaq	96(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1916:
# %bb.19:
.Ltmp1917:
	leaq	96(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp1918:
# %bb.20:
.Ltmp1920:
	callq	mpfr_get_default_rounding_mode
.Ltmp1921:
# %bb.21:
.Ltmp1922:
	leaq	96(%rsp), %rdi
	leaq	152(%rsp), %rsi
	movq	144(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp1923:
# %bb.22:
.Ltmp1925:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1926:
# %bb.23:
.Ltmp1927:
	movq	%rax, %r12
	movq	144(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp1928:
# %bb.24:
.Ltmp1929:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp1930:
# %bb.25:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp1931:
	leaq	8(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1932:
# %bb.26:
.Ltmp1933:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp1934:
# %bb.27:
.Ltmp1936:
	callq	mpfr_get_default_rounding_mode
.Ltmp1937:
# %bb.28:
.Ltmp1938:
	leaq	8(%rsp), %rdi
	leaq	96(%rsp), %rsi
	movq	144(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp1939:
# %bb.29:
.Ltmp1941:
	callq	mpfr_get_default_rounding_mode
.Ltmp1942:
# %bb.30:
.Ltmp1943:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1944:
# %bb.31:
.Ltmp1945:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp1946:
# %bb.32:
.Ltmp1947:
	movl	%eax, %r12d
	leaq	64(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp1948:
# %bb.33:
.Ltmp1949:
	leaq	64(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp1950:
# %bb.34:
.Ltmp1952:
	leaq	64(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp1953:
# %bb.35:
.Ltmp1955:
	leaq	64(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp1956:
# %bb.36:
	vmovlpd	%xmm0, 192(%rsp)
	cmpq	$0, 88(%rsp)
	je	.LBB33_38
# %bb.37:
.Ltmp1958:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1959:
.LBB33_38:
	cmpq	$0, 32(%rsp)
	je	.LBB33_40
# %bb.39:
.Ltmp1961:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1962:
.LBB33_40:
	cmpq	$0, 120(%rsp)
	je	.LBB33_42
# %bb.41:
.Ltmp1964:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1965:
.LBB33_42:
	cmpq	$0, 176(%rsp)
	je	.LBB33_44
# %bb.43:
.Ltmp1967:
	leaq	152(%rsp), %rdi
	callq	mpfr_clear
.Ltmp1968:
.LBB33_44:
	leaq	424(%rsp), %r15
	movq	%r15, 408(%rsp)
	movl	$544501604, 424(%rsp)           # imm = 0x20746F64
	movw	$32, 428(%rsp)
	movq	$5, 416(%rsp)
.Ltmp1970:
	leaq	408(%rsp), %rdi
	leaq	192(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp1971:
# %bb.45:
	movq	408(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB33_47
# %bb.46:
	callq	_ZdlPv
.LBB33_47:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$80, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, 64(%rax)
	xorl	%r15d, %r15d
	vzeroupper
	callq	omp_get_wtime
	vmovsd	%xmm0, 136(%rsp)                # 8-byte Spill
	vxorps	%xmm7, %xmm7, %xmm7
	xorl	%r12d, %r12d
	jmp	.LBB33_50
	.p2align	4, 0x90
.LBB33_48:                              #   in Loop: Header=BB33_50 Depth=1
	vmovups	%xmm0, 48(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovaps	%xmm0, %xmm1
	vmovups	48(%rsp), %xmm2                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vfnmadd213ss	%xmm2, %xmm0, %xmm1     # xmm1 = -(xmm0 * xmm1) + xmm2
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm0, %xmm2
	vdivss	%xmm2, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	vxorps	%xmm7, %xmm7, %xmm7
.LBB33_49:                              #   in Loop: Header=BB33_50 Depth=1
	vmovlps	%xmm0, (%rbp,%r12,8)
	leaq	1(%r12), %rax
	cmpq	$9, %r12
	movq	%rax, %r12
	je	.LBB33_60
.LBB33_50:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB33_53 Depth 2
                                        #     Child Loop BB33_55 Depth 2
	vcvtsi2ss	%r15d, %xmm9, %xmm0
	vblendps	$14, .LCPI33_1(%rip), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[1,2,3]
	movq	128(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	jle	.LBB33_56
# %bb.51:                               #   in Loop: Header=BB33_50 Depth=1
	cmpl	$4, %eax
	jb	.LBB33_54
# %bb.52:                               #   in Loop: Header=BB33_50 Depth=1
	leaq	(,%rax,8), %rcx
	andq	$-32, %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB33_53:                              #   Parent Loop BB33_50 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rdx), %xmm4              # xmm4 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%rdx), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm4, %xmm2
	vmovaps	%xmm4, %xmm3
	vfmsub213ss	%xmm2, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm3) - xmm2
	vfmadd231ss	%xmm1, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm1) + xmm3
	vfmadd231ss	%xmm1, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm1) + xmm3
	vaddss	%xmm2, %xmm0, %xmm8
	vsubss	%xmm0, %xmm8, %xmm1
	vsubss	%xmm1, %xmm8, %xmm4
	vsubss	%xmm4, %xmm0, %xmm4
	vsubss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vmovshdup	%xmm0, %xmm2            # xmm2 = xmm0[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmovss	8(%rbx,%rdx), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm2, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vmovss	12(%rbx,%rdx), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vfmadd231ss	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vaddss	%xmm3, %xmm8, %xmm2
	vsubss	%xmm8, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm6
	vsubss	%xmm6, %xmm8, %xmm0
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vmovss	16(%rbx,%rdx), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm1, %xmm3
	vmovaps	%xmm1, %xmm4
	vfmsub213ss	%xmm3, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm4) - xmm3
	vmovss	20(%rbx,%rdx), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm5, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm5) + xmm4
	vfmadd231ss	%xmm5, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm5) + xmm4
	vaddss	%xmm3, %xmm2, %xmm1
	vsubss	%xmm2, %xmm1, %xmm5
	vsubss	%xmm5, %xmm1, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vmovss	24(%rbx,%rdx), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm2, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vmovss	28(%rbx,%rdx), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vfmadd231ss	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vaddss	%xmm3, %xmm1, %xmm2
	vsubss	%xmm1, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm6
	vsubss	%xmm6, %xmm1, %xmm1
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
	addq	$32, %rdx
	cmpq	%rdx, %rcx
	jne	.LBB33_53
.LBB33_54:                              #   in Loop: Header=BB33_50 Depth=1
	movl	%eax, %ecx
	andl	$-4, %ecx
	cmpq	%rax, %rcx
	jae	.LBB33_56
	.p2align	4, 0x90
.LBB33_55:                              #   Parent Loop BB33_50 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rcx,8), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%rcx,8), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm4, %xmm2
	vmovaps	%xmm4, %xmm3
	vfmsub213ss	%xmm2, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm3) - xmm2
	vfmadd231ss	%xmm1, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm1) + xmm3
	vfmadd231ss	%xmm1, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm1) + xmm3
	vaddss	%xmm2, %xmm0, %xmm5
	vsubss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm4
	vsubss	%xmm4, %xmm0, %xmm4
	vsubss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vmovshdup	%xmm0, %xmm2            # xmm2 = xmm0[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vinsertps	$16, %xmm1, %xmm5, %xmm0 # xmm0 = xmm5[0],xmm1[0],xmm5[2,3]
	incq	%rcx
	cmpq	%rcx, %rax
	jne	.LBB33_55
.LBB33_56:                              #   in Loop: Header=BB33_50 Depth=1
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
	vucomiss	%xmm7, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB33_49
# %bb.57:                               #   in Loop: Header=BB33_50 Depth=1
	vucomiss	%xmm7, %xmm0
	jne	.LBB33_48
# %bb.58:                               #   in Loop: Header=BB33_50 Depth=1
	jp	.LBB33_48
# %bb.59:                               #   in Loop: Header=BB33_50 Depth=1
	vshufps	$225, %xmm0, %xmm0, %xmm0       # xmm0 = xmm0[1,0,2,3]
	jmp	.LBB33_48
.LBB33_60:
	callq	omp_get_wtime
	vsubsd	136(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm9, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	152(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp1973:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp1974:
# %bb.61:
	movq	%rax, %r13
	movq	144(%rsp), %rax                 # 8-byte Reload
	leaq	32(%rax), %rdi
.Ltmp1975:
	movq	%rdi, 48(%rsp)                  # 8-byte Spill
	callq	mpfr_get_prec
.Ltmp1976:
# %bb.62:
.Ltmp1977:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp1978:
# %bb.63:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp1979:
	leaq	96(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp1980:
# %bb.64:
.Ltmp1981:
	leaq	96(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp1982:
# %bb.65:
.Ltmp1984:
	callq	mpfr_get_default_rounding_mode
.Ltmp1985:
# %bb.66:
.Ltmp1986:
	leaq	96(%rsp), %rdi
	leaq	152(%rsp), %rsi
	movq	48(%rsp), %rdx                  # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp1987:
# %bb.67:
.Ltmp1989:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp1990:
# %bb.68:
.Ltmp1991:
	movq	%rax, %r13
	movq	48(%rsp), %rdi                  # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp1992:
# %bb.69:
.Ltmp1993:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp1994:
# %bb.70:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp1995:
	leaq	8(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp1996:
# %bb.71:
.Ltmp1997:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp1998:
# %bb.72:
.Ltmp2000:
	callq	mpfr_get_default_rounding_mode
.Ltmp2001:
# %bb.73:
.Ltmp2002:
	leaq	8(%rsp), %rdi
	leaq	96(%rsp), %rsi
	movq	48(%rsp), %rdx                  # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp2003:
# %bb.74:
.Ltmp2005:
	callq	mpfr_get_default_rounding_mode
.Ltmp2006:
# %bb.75:
.Ltmp2007:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2008:
# %bb.76:
.Ltmp2009:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2010:
# %bb.77:
.Ltmp2011:
	movl	%eax, %r12d
	leaq	64(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2012:
# %bb.78:
.Ltmp2013:
	leaq	64(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2014:
# %bb.79:
.Ltmp2016:
	leaq	64(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp2017:
# %bb.80:
.Ltmp2019:
	leaq	64(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp2020:
# %bb.81:
	vmovlpd	%xmm0, 192(%rsp)
	cmpq	$0, 88(%rsp)
	je	.LBB33_83
# %bb.82:
.Ltmp2022:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2023:
.LBB33_83:
	cmpq	$0, 32(%rsp)
	je	.LBB33_85
# %bb.84:
.Ltmp2025:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2026:
.LBB33_85:
	cmpq	$0, 120(%rsp)
	je	.LBB33_87
# %bb.86:
.Ltmp2028:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2029:
.LBB33_87:
	cmpq	$0, 176(%rsp)
	je	.LBB33_89
# %bb.88:
.Ltmp2031:
	leaq	152(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2032:
.LBB33_89:
	leaq	392(%rsp), %r15
	movq	%r15, 376(%rsp)
	movl	$846033518, 392(%rsp)           # imm = 0x326D726E
	movw	$32, 396(%rsp)
	movq	$5, 384(%rsp)
.Ltmp2034:
	leaq	376(%rsp), %rdi
	leaq	192(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp2035:
# %bb.90:
	movq	376(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB33_92
# %bb.91:
	callq	_ZdlPv
.LBB33_92:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$80, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, 64(%rax)
	vzeroupper
	callq	omp_get_wtime
	vxorps	%xmm9, %xmm9, %xmm9
	vmovsd	%xmm0, 48(%rsp)                 # 8-byte Spill
	movq	128(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	movl	%eax, %ecx
	andl	$-4, %ecx
	leaq	(,%rax,8), %rdx
	andq	$-32, %rdx
	xorl	%esi, %esi
	vxorpd	%xmm0, %xmm0, %xmm0
	vbroadcastss	.LCPI33_0(%rip), %xmm1  # xmm1 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	xorl	%edi, %edi
	jmp	.LBB33_94
	.p2align	4, 0x90
.LBB33_93:                              #   in Loop: Header=BB33_94 Depth=1
	vmovlps	%xmm2, (%rbp,%rdi,8)
	leaq	1(%rdi), %r8
	cmpq	$9, %rdi
	movq	%r8, %rdi
	je	.LBB33_111
.LBB33_94:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB33_103 Depth 2
                                        #     Child Loop BB33_99 Depth 2
	vcvtsi2ss	%esi, %xmm10, %xmm2
	vblendps	$1, %xmm2, %xmm9, %xmm2         # xmm2 = xmm2[0],xmm9[1,2,3]
	testl	%eax, %eax
	jle	.LBB33_93
# %bb.95:                               #   in Loop: Header=BB33_94 Depth=1
	cmpl	$4, %eax
	jae	.LBB33_101
.LBB33_96:                              #   in Loop: Header=BB33_94 Depth=1
	cmpq	%rax, %rcx
	jae	.LBB33_93
# %bb.97:                               #   in Loop: Header=BB33_94 Depth=1
	movq	%rcx, %r8
	jmp	.LBB33_99
	.p2align	4, 0x90
.LBB33_98:                              #   in Loop: Header=BB33_99 Depth=2
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm6
	vsubss	%xmm5, %xmm3, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vmovshdup	%xmm3, %xmm3            # xmm3 = xmm3[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm5, %xmm2, %xmm2
	vinsertps	$16, %xmm2, %xmm4, %xmm2 # xmm2 = xmm4[0],xmm2[0],xmm4[2,3]
	incq	%r8
	cmpq	%r8, %rax
	je	.LBB33_93
.LBB33_99:                              #   Parent Loop BB33_94 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	(%rbx,%r8,8), %xmm3             # xmm3 = mem[0],zero
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm4
	vcomiss	%xmm4, %xmm0
	jbe	.LBB33_98
# %bb.100:                              #   in Loop: Header=BB33_99 Depth=2
	vxorps	%xmm1, %xmm3, %xmm3
	jmp	.LBB33_98
	.p2align	4, 0x90
.LBB33_101:                             #   in Loop: Header=BB33_94 Depth=1
	xorl	%r8d, %r8d
	jmp	.LBB33_103
	.p2align	4, 0x90
.LBB33_102:                             #   in Loop: Header=BB33_103 Depth=2
	vaddss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm4, %xmm4
	vsubss	%xmm6, %xmm3, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vmovshdup	%xmm3, %xmm3            # xmm3 = xmm3[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm2
	vinsertps	$16, %xmm2, %xmm5, %xmm2 # xmm2 = xmm5[0],xmm2[0],xmm5[2,3]
	addq	$32, %r8
	cmpq	%r8, %rdx
	je	.LBB33_96
.LBB33_103:                             #   Parent Loop BB33_94 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	(%rbx,%r8), %xmm5               # xmm5 = mem[0],zero
	vmovshdup	%xmm5, %xmm3            # xmm3 = xmm5[1,1,3,3]
	vaddss	%xmm3, %xmm5, %xmm3
	vcomiss	%xmm3, %xmm0
	jbe	.LBB33_105
# %bb.104:                              #   in Loop: Header=BB33_103 Depth=2
	vxorps	%xmm1, %xmm5, %xmm5
.LBB33_105:                             #   in Loop: Header=BB33_103 Depth=2
	vmovsd	8(%rbx,%r8), %xmm3              # xmm3 = mem[0],zero
	vaddss	%xmm5, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm2, %xmm7
	vsubss	%xmm6, %xmm5, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vmovshdup	%xmm5, %xmm5            # xmm5 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm6, %xmm2, %xmm6
	vmovshdup	%xmm3, %xmm2            # xmm2 = xmm3[1,1,3,3]
	vaddss	%xmm2, %xmm3, %xmm2
	vcomiss	%xmm2, %xmm0
	jbe	.LBB33_107
# %bb.106:                              #   in Loop: Header=BB33_103 Depth=2
	vxorps	%xmm1, %xmm3, %xmm3
.LBB33_107:                             #   in Loop: Header=BB33_103 Depth=2
	vmovsd	16(%rbx,%r8), %xmm2             # xmm2 = mem[0],zero
	vaddss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vsubss	%xmm7, %xmm3, %xmm7
	vaddss	%xmm7, %xmm4, %xmm4
	vmovshdup	%xmm3, %xmm3            # xmm3 = xmm3[1,1,3,3]
	vaddss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm4, %xmm3, %xmm6
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm3
	vcomiss	%xmm3, %xmm0
	jbe	.LBB33_109
# %bb.108:                              #   in Loop: Header=BB33_103 Depth=2
	vxorps	%xmm1, %xmm2, %xmm2
.LBB33_109:                             #   in Loop: Header=BB33_103 Depth=2
	vmovsd	24(%rbx,%r8), %xmm3             # xmm3 = mem[0],zero
	vaddss	%xmm2, %xmm5, %xmm4
	vsubss	%xmm5, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm5, %xmm5
	vsubss	%xmm7, %xmm2, %xmm7
	vaddss	%xmm7, %xmm5, %xmm5
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm6, %xmm2
	vaddss	%xmm5, %xmm2, %xmm2
	vmovshdup	%xmm3, %xmm5            # xmm5 = xmm3[1,1,3,3]
	vaddss	%xmm5, %xmm3, %xmm5
	vcomiss	%xmm5, %xmm0
	jbe	.LBB33_102
# %bb.110:                              #   in Loop: Header=BB33_103 Depth=2
	vxorps	%xmm1, %xmm3, %xmm3
	jmp	.LBB33_102
.LBB33_111:
	callq	omp_get_wtime
	vsubsd	48(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm10, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	152(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp2037:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp2038:
# %bb.112:
	movq	%rax, %r13
	movq	144(%rsp), %rax                 # 8-byte Reload
	leaq	64(%rax), %rdi
.Ltmp2039:
	movq	%rdi, 48(%rsp)                  # 8-byte Spill
	callq	mpfr_get_prec
.Ltmp2040:
# %bb.113:
.Ltmp2041:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp2042:
# %bb.114:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp2043:
	leaq	96(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp2044:
# %bb.115:
.Ltmp2045:
	leaq	96(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2046:
# %bb.116:
.Ltmp2048:
	callq	mpfr_get_default_rounding_mode
.Ltmp2049:
# %bb.117:
.Ltmp2050:
	leaq	96(%rsp), %rdi
	leaq	152(%rsp), %rsi
	movq	48(%rsp), %rdx                  # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp2051:
# %bb.118:
.Ltmp2053:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2054:
# %bb.119:
.Ltmp2055:
	movq	%rax, %r13
	movq	48(%rsp), %rdi                  # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp2056:
# %bb.120:
.Ltmp2057:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp2058:
# %bb.121:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp2059:
	leaq	8(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp2060:
# %bb.122:
.Ltmp2061:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2062:
# %bb.123:
.Ltmp2064:
	callq	mpfr_get_default_rounding_mode
.Ltmp2065:
# %bb.124:
.Ltmp2066:
	leaq	8(%rsp), %rdi
	leaq	96(%rsp), %rsi
	movq	48(%rsp), %rdx                  # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp2067:
# %bb.125:
.Ltmp2069:
	callq	mpfr_get_default_rounding_mode
.Ltmp2070:
# %bb.126:
.Ltmp2071:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2072:
# %bb.127:
.Ltmp2073:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2074:
# %bb.128:
.Ltmp2075:
	movl	%eax, %r12d
	leaq	64(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2076:
# %bb.129:
.Ltmp2077:
	leaq	64(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2078:
# %bb.130:
.Ltmp2080:
	leaq	64(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp2081:
# %bb.131:
.Ltmp2083:
	leaq	64(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp2084:
# %bb.132:
	vmovlpd	%xmm0, 192(%rsp)
	cmpq	$0, 88(%rsp)
	je	.LBB33_134
# %bb.133:
.Ltmp2086:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2087:
.LBB33_134:
	cmpq	$0, 32(%rsp)
	je	.LBB33_136
# %bb.135:
.Ltmp2089:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2090:
.LBB33_136:
	cmpq	$0, 120(%rsp)
	je	.LBB33_138
# %bb.137:
.Ltmp2092:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2093:
.LBB33_138:
	cmpq	$0, 176(%rsp)
	je	.LBB33_140
# %bb.139:
.Ltmp2095:
	leaq	152(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2096:
.LBB33_140:
	leaq	360(%rsp), %r15
	movq	%r15, 344(%rsp)
	movl	$1836413793, 360(%rsp)          # imm = 0x6D757361
	movw	$32, 364(%rsp)
	movq	$5, 352(%rsp)
.Ltmp2098:
	leaq	344(%rsp), %rdi
	leaq	192(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp2099:
# %bb.141:
	movq	344(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB33_143
# %bb.142:
	callq	_ZdlPv
.LBB33_143:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movq	128(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	vmovups	208(%rsp), %xmm7                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	testl	%eax, %eax
	jle	.LBB33_149
# %bb.144:
	cmpl	$4, %eax
	jb	.LBB33_147
# %bb.145:
	leaq	(,%rax,8), %rcx
	andq	$-32, %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB33_146:                             # =>This Inner Loop Header: Depth=1
	vmovss	(%rbx,%rdx), %xmm0              # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm7, %xmm1
	vmovaps	%xmm0, %xmm2
	vfmsub213ss	%xmm1, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm1
	vfmadd231ss	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm0) + xmm2
	vfmadd231ss	4(%rbx,%rdx), %xmm7, %xmm2 # xmm2 = (xmm7 * mem) + xmm2
	vmovss	(%r14,%rdx), %xmm0              # xmm0 = mem[0],zero,zero,zero
	vmovss	8(%r14,%rdx), %xmm3             # xmm3 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm1, %xmm1
	vsubss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	4(%r14,%rdx), %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vinsertps	$16, %xmm0, %xmm4, %xmm0 # xmm0 = xmm4[0],xmm0[0],xmm4[2,3]
	vmovss	8(%rbx,%rdx), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm7, %xmm2
	vmovaps	%xmm1, %xmm4
	vfmsub213ss	%xmm2, %xmm7, %xmm4     # xmm4 = (xmm7 * xmm4) - xmm2
	vfmadd231ss	%xmm1, %xmm8, %xmm4     # xmm4 = (xmm8 * xmm1) + xmm4
	vfmadd231ss	12(%rbx,%rdx), %xmm7, %xmm4 # xmm4 = (xmm7 * mem) + xmm4
	vmovlps	%xmm0, (%r14,%rdx)
	vaddss	%xmm3, %xmm2, %xmm0
	vsubss	%xmm2, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm5
	vsubss	%xmm5, %xmm2, %xmm2
	vsubss	%xmm1, %xmm3, %xmm1
	vaddss	12(%r14,%rdx), %xmm4, %xmm3
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	vmovlps	%xmm0, 8(%r14,%rdx)
	vmovss	16(%rbx,%rdx), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm7, %xmm1
	vmovaps	%xmm0, %xmm2
	vfmsub213ss	%xmm1, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm1
	vfmadd231ss	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm0) + xmm2
	vfmadd231ss	20(%rbx,%rdx), %xmm7, %xmm2 # xmm2 = (xmm7 * mem) + xmm2
	vmovss	16(%r14,%rdx), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vsubss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	20(%r14,%rdx), %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vinsertps	$16, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0],xmm0[0],xmm3[2,3]
	vmovlps	%xmm0, 16(%r14,%rdx)
	vmovss	24(%rbx,%rdx), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm7, %xmm1
	vmovaps	%xmm0, %xmm2
	vfmsub213ss	%xmm1, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm1
	vfmadd231ss	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm0) + xmm2
	vfmadd231ss	28(%rbx,%rdx), %xmm7, %xmm2 # xmm2 = (xmm7 * mem) + xmm2
	vmovss	24(%r14,%rdx), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vsubss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	28(%r14,%rdx), %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vinsertps	$16, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0],xmm0[0],xmm3[2,3]
	vmovlps	%xmm0, 24(%r14,%rdx)
	addq	$32, %rdx
	cmpq	%rdx, %rcx
	jne	.LBB33_146
.LBB33_147:
	movl	%eax, %ecx
	andl	$-4, %ecx
	cmpq	%rax, %rcx
	jae	.LBB33_149
	.p2align	4, 0x90
.LBB33_148:                             # =>This Inner Loop Header: Depth=1
	vmovss	(%rbx,%rcx,8), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm7, %xmm1
	vmovaps	%xmm0, %xmm2
	vfmsub213ss	%xmm1, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm1
	vfmadd231ss	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm0) + xmm2
	vfmadd231ss	4(%rbx,%rcx,8), %xmm7, %xmm2 # xmm2 = (xmm7 * mem) + xmm2
	vmovss	(%r14,%rcx,8), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vsubss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	4(%r14,%rcx,8), %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vinsertps	$16, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0],xmm0[0],xmm3[2,3]
	vmovlps	%xmm0, (%r14,%rcx,8)
	incq	%rcx
	cmpq	%rcx, %rax
	jne	.LBB33_148
.LBB33_149:
	vmovups	%xmm8, 256(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	64(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	movq	128(%rsp), %rax                 # 8-byte Reload
	cmpl	$0, (%rax)
	jle	.LBB33_190
# %bb.150:
	movq	144(%rsp), %rax                 # 8-byte Reload
	leaq	256(%rax), %r15
	xorl	%eax, %eax
	movq	%rax, 48(%rsp)                  # 8-byte Spill
	movq	%r14, %rsi
	jmp	.LBB33_152
	.p2align	4, 0x90
.LBB33_151:                             #   in Loop: Header=BB33_152 Depth=1
	movq	48(%rsp), %rdx                  # 8-byte Reload
	incq	%rdx
	movq	128(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r15
	movq	136(%rsp), %rsi                 # 8-byte Reload
	addq	$8, %rsi
	movq	%rdx, %rcx
	movq	%rdx, 48(%rsp)                  # 8-byte Spill
	cmpq	%rax, %rdx
	jge	.LBB33_190
.LBB33_152:                             # =>This Inner Loop Header: Depth=1
.Ltmp2101:
	leaq	96(%rsp), %rdi
	movq	%rsi, 136(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp2102:
# %bb.153:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2104:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp2105:
# %bb.154:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2106:
	movq	%rax, %rbp
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2107:
# %bb.155:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2108:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2109:
# %bb.156:                              #   in Loop: Header=BB33_152 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp2110:
	leaq	8(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2111:
# %bb.157:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2112:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2113:
# %bb.158:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2115:
	callq	mpfr_get_default_rounding_mode
.Ltmp2116:
# %bb.159:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2117:
	leaq	8(%rsp), %rdi
	movq	%r15, %rsi
	leaq	96(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp2118:
# %bb.160:                              #   in Loop: Header=BB33_152 Depth=1
	cmpq	$0, 120(%rsp)
	je	.LBB33_162
# %bb.161:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2120:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2121:
.LBB33_162:                             #   in Loop: Header=BB33_152 Depth=1
.Ltmp2123:
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2124:
# %bb.163:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2125:
	movq	%rax, %rbp
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2126:
# %bb.164:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2127:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2128:
# %bb.165:                              #   in Loop: Header=BB33_152 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp2129:
	leaq	152(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2130:
# %bb.166:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2131:
	leaq	152(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2132:
# %bb.167:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2134:
	callq	mpfr_get_default_rounding_mode
.Ltmp2135:
# %bb.168:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2136:
	leaq	152(%rsp), %rdi
	leaq	8(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp2137:
# %bb.169:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2139:
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2140:
# %bb.170:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2141:
	movq	%rax, %rbp
	leaq	152(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2142:
# %bb.171:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2143:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2144:
# %bb.172:                              #   in Loop: Header=BB33_152 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp2145:
	leaq	96(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2146:
# %bb.173:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2147:
	leaq	96(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2148:
# %bb.174:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2150:
	callq	mpfr_get_default_rounding_mode
.Ltmp2151:
	leaq	64(%rsp), %r12
# %bb.175:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2152:
	leaq	96(%rsp), %rdi
	movq	%r12, %rsi
	leaq	152(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp2153:
# %bb.176:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2155:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp2156:
# %bb.177:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2157:
	movq	%rax, %r12
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2158:
# %bb.178:                              #   in Loop: Header=BB33_152 Depth=1
	movq	%rax, %rbp
	cmpq	%rax, %r12
	je	.LBB33_182
# %bb.179:                              #   in Loop: Header=BB33_152 Depth=1
	cmpq	$0, 88(%rsp)
	je	.LBB33_181
# %bb.180:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2159:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2160:
.LBB33_181:                             #   in Loop: Header=BB33_152 Depth=1
.Ltmp2161:
	leaq	64(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp2162:
.LBB33_182:                             #   in Loop: Header=BB33_152 Depth=1
.Ltmp2163:
	callq	mpfr_get_default_rounding_mode
.Ltmp2164:
# %bb.183:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2165:
	leaq	64(%rsp), %rdi
	leaq	96(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp2166:
# %bb.184:                              #   in Loop: Header=BB33_152 Depth=1
	cmpq	$0, 120(%rsp)
	je	.LBB33_186
# %bb.185:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2168:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2169:
.LBB33_186:                             #   in Loop: Header=BB33_152 Depth=1
	cmpq	$0, 176(%rsp)
	je	.LBB33_188
# %bb.187:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2171:
	leaq	152(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2172:
.LBB33_188:                             #   in Loop: Header=BB33_152 Depth=1
	cmpq	$0, 32(%rsp)
	je	.LBB33_151
# %bb.189:                              #   in Loop: Header=BB33_152 Depth=1
.Ltmp2174:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2175:
	jmp	.LBB33_151
.LBB33_190:
.Ltmp2177:
	callq	mpfr_get_default_rounding_mode
.Ltmp2178:
# %bb.191:
.Ltmp2179:
	movl	%eax, %ebp
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2180:
# %bb.192:
.Ltmp2181:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp2182:
# %bb.193:
.Ltmp2183:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp2184:
# %bb.194:
.Ltmp2185:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp2186:
# %bb.195:
.Ltmp2188:
	leaq	8(%rsp), %rdi
	leaq	64(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp2189:
# %bb.196:
.Ltmp2191:
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2192:
# %bb.197:
.Ltmp2193:
	movq	%rax, %r12
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2194:
# %bb.198:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB33_202
# %bb.199:
	cmpq	$0, 88(%rsp)
	je	.LBB33_201
# %bb.200:
.Ltmp2195:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2196:
.LBB33_201:
.Ltmp2197:
	leaq	64(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp2198:
.LBB33_202:
.Ltmp2199:
	callq	mpfr_get_default_rounding_mode
.Ltmp2200:
# %bb.203:
.Ltmp2201:
	leaq	64(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp2202:
# %bb.204:
	cmpq	$0, 32(%rsp)
	je	.LBB33_206
# %bb.205:
.Ltmp2204:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2205:
.LBB33_206:
	callq	omp_get_wtime
	vmovsd	%xmm0, 48(%rsp)                 # 8-byte Spill
	movq	128(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	testq	%rax, %rax
	vmovups	208(%rsp), %xmm7                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	256(%rsp), %xmm8                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	jle	.LBB33_215
# %bb.207:
	movq	%rax, %rcx
	shrq	$2, %rcx
	movq	%rax, %rdx
	andq	$-4, %rdx
	xorl	%esi, %esi
	jmp	.LBB33_209
	.p2align	4, 0x90
.LBB33_208:                             #   in Loop: Header=BB33_209 Depth=1
	leal	1(%rsi), %edi
	cmpl	$9, %esi
	movl	%edi, %esi
	je	.LBB33_215
.LBB33_209:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB33_211 Depth 2
                                        #     Child Loop BB33_214 Depth 2
	cmpl	$4, %eax
	jb	.LBB33_212
# %bb.210:                              #   in Loop: Header=BB33_209 Depth=1
	movl	$28, %edi
	movq	%rcx, %r8
	.p2align	4, 0x90
.LBB33_211:                             #   Parent Loop BB33_209 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	-28(%rbx,%rdi), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm7, %xmm1
	vmovaps	%xmm0, %xmm2
	vfmsub213ss	%xmm1, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm1
	vfmadd231ss	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm0) + xmm2
	vfmadd231ss	-24(%rbx,%rdi), %xmm7, %xmm2 # xmm2 = (xmm7 * mem) + xmm2
	vmovss	-28(%r14,%rdi), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmovss	-20(%r14,%rdi), %xmm3           # xmm3 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm1, %xmm1
	vsubss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	-24(%r14,%rdi), %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vinsertps	$16, %xmm0, %xmm4, %xmm0 # xmm0 = xmm4[0],xmm0[0],xmm4[2,3]
	vmovss	-20(%rbx,%rdi), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm7, %xmm2
	vmovaps	%xmm1, %xmm4
	vfmsub213ss	%xmm2, %xmm7, %xmm4     # xmm4 = (xmm7 * xmm4) - xmm2
	vfmadd231ss	%xmm1, %xmm8, %xmm4     # xmm4 = (xmm8 * xmm1) + xmm4
	vfmadd231ss	-16(%rbx,%rdi), %xmm7, %xmm4 # xmm4 = (xmm7 * mem) + xmm4
	vmovlps	%xmm0, -28(%r14,%rdi)
	vaddss	%xmm3, %xmm2, %xmm0
	vsubss	%xmm2, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm5
	vsubss	%xmm5, %xmm2, %xmm2
	vsubss	%xmm1, %xmm3, %xmm1
	vaddss	-16(%r14,%rdi), %xmm4, %xmm3
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	vmovlps	%xmm0, -20(%r14,%rdi)
	vmovss	-12(%rbx,%rdi), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm7, %xmm1
	vmovaps	%xmm0, %xmm2
	vfmsub213ss	%xmm1, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm1
	vfmadd231ss	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm0) + xmm2
	vfmadd231ss	-8(%rbx,%rdi), %xmm7, %xmm2 # xmm2 = (xmm7 * mem) + xmm2
	vmovss	-12(%r14,%rdi), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vsubss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	-8(%r14,%rdi), %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vinsertps	$16, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0],xmm0[0],xmm3[2,3]
	vmovlps	%xmm0, -12(%r14,%rdi)
	vmovss	-4(%rbx,%rdi), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm7, %xmm1
	vmovaps	%xmm0, %xmm2
	vfmsub213ss	%xmm1, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm1
	vfmadd231ss	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm0) + xmm2
	vfmadd231ss	(%rbx,%rdi), %xmm7, %xmm2 # xmm2 = (xmm7 * mem) + xmm2
	vmovss	-4(%r14,%rdi), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vsubss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	(%r14,%rdi), %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vinsertps	$16, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0],xmm0[0],xmm3[2,3]
	vmovlps	%xmm0, -4(%r14,%rdi)
	addq	$32, %rdi
	decq	%r8
	jne	.LBB33_211
.LBB33_212:                             #   in Loop: Header=BB33_209 Depth=1
	cmpq	%rax, %rdx
	jae	.LBB33_208
# %bb.213:                              #   in Loop: Header=BB33_209 Depth=1
	movq	%rdx, %rdi
	.p2align	4, 0x90
.LBB33_214:                             #   Parent Loop BB33_209 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rdi,8), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm7, %xmm1
	vmovaps	%xmm0, %xmm2
	vfmsub213ss	%xmm1, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm1
	vfmadd231ss	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm0) + xmm2
	vfmadd231ss	4(%rbx,%rdi,8), %xmm7, %xmm2 # xmm2 = (xmm7 * mem) + xmm2
	vmovss	(%r14,%rdi,8), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vsubss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	4(%r14,%rdi,8), %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vinsertps	$16, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0],xmm0[0],xmm3[2,3]
	vmovlps	%xmm0, (%r14,%rdi,8)
	incq	%rdi
	cmpq	%rdi, %rax
	jne	.LBB33_214
	jmp	.LBB33_208
.LBB33_215:
	callq	omp_get_wtime
	vsubsd	48(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm9, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp2207:
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2208:
# %bb.216:
	movq	%rax, %r12
	movq	144(%rsp), %rax                 # 8-byte Reload
	leaq	96(%rax), %r15
.Ltmp2209:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp2210:
# %bb.217:
.Ltmp2211:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2212:
# %bb.218:
	movl	%eax, %ebp
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp2213:
	leaq	8(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2214:
# %bb.219:
.Ltmp2215:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp2216:
# %bb.220:
.Ltmp2218:
	callq	mpfr_get_default_rounding_mode
.Ltmp2219:
# %bb.221:
.Ltmp2220:
	leaq	8(%rsp), %rdi
	leaq	64(%rsp), %rsi
	movq	%r15, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp2221:
# %bb.222:
.Ltmp2223:
	leaq	8(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp2224:
# %bb.223:
	vmovlpd	%xmm0, 96(%rsp)
	cmpq	$0, 32(%rsp)
	je	.LBB33_225
# %bb.224:
.Ltmp2226:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2227:
.LBB33_225:
	leaq	328(%rsp), %r15
	movq	%r15, 312(%rsp)
	movl	$2037413985, 328(%rsp)          # imm = 0x79707861
	movw	$32, 332(%rsp)
	movq	$5, 320(%rsp)
.Ltmp2229:
	leaq	312(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp2230:
# %bb.226:
	movq	312(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB33_228
# %bb.227:
	callq	_ZdlPv
.LBB33_228:
	cmpq	$0, 88(%rsp)
	je	.LBB33_230
# %bb.229:
.Ltmp2232:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2233:
.LBB33_230:
	movl	44(%rsp), %ebp                  # 4-byte Reload
	movslq	%ebp, %r12
	leaq	(,%r12,8), %r15
	testl	%r12d, %r12d
	movq	$-1, %rdi
	cmovnsq	%r15, %rdi
	callq	_Znam
	movq	%r12, 48(%rsp)                  # 8-byte Spill
	testl	%r12d, %r12d
	movq	%rax, 136(%rsp)                 # 8-byte Spill
	je	.LBB33_246
# %bb.231:
	xorl	%r13d, %r13d
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_intel_fast_memset@PLT
	testl	%ebp, %ebp
	jle	.LBB33_247
# %bb.232:
	cmpl	$8, %ebp
	movq	136(%rsp), %rbp                 # 8-byte Reload
	jb	.LBB33_235
# %bb.233:
	movq	48(%rsp), %r12                  # 8-byte Reload
	shrq	$3, %r12
	movq	184(%rsp), %rax                 # 8-byte Reload
	leaq	224(%rax), %r15
	movl	$14, %r13d
	.p2align	4, 0x90
.LBB33_234:                             # =>This Inner Loop Header: Depth=1
	leaq	-224(%r15), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, -56(%r14,%r13,4)
	vmovlpd	%xmm0, -56(%rbp,%r13,4)
	leaq	-192(%r15), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, -48(%r14,%r13,4)
	vmovlpd	%xmm0, -48(%rbp,%r13,4)
	leaq	-160(%r15), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, -40(%r14,%r13,4)
	vmovlpd	%xmm0, -40(%rbp,%r13,4)
	leaq	-128(%r15), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, -32(%r14,%r13,4)
	vmovlpd	%xmm0, -32(%rbp,%r13,4)
	leaq	-96(%r15), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, -24(%r14,%r13,4)
	vmovlpd	%xmm0, -24(%rbp,%r13,4)
	leaq	-64(%r15), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, -16(%r14,%r13,4)
	vmovlpd	%xmm0, -16(%rbp,%r13,4)
	leaq	-32(%r15), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, -8(%r14,%r13,4)
	vmovlpd	%xmm0, -8(%rbp,%r13,4)
	movq	%r15, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, (%r14,%r13,4)
	vmovlpd	%xmm0, (%rbp,%r13,4)
	addq	$16, %r13
	addq	$256, %r15                      # imm = 0x100
	decq	%r12
	jne	.LBB33_234
.LBB33_235:
	movq	48(%rsp), %r12                  # 8-byte Reload
	movq	%r12, %r15
	andq	$-8, %r15
	cmpq	%r12, %r15
	movq	184(%rsp), %r13                 # 8-byte Reload
	jae	.LBB33_238
# %bb.236:
	movq	%r12, %rax
	andq	$-8, %rax
	shlq	$5, %rax
	addq	%rax, %r13
	.p2align	4, 0x90
.LBB33_237:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, (%r14,%r15,8)
	vmovlpd	%xmm0, (%rbp,%r15,8)
	incq	%r15
	addq	$32, %r13
	cmpq	%r15, %r12
	jne	.LBB33_237
.LBB33_238:
	movq	%r12, %rax
	shrq	%rax
	movq	%r12, %rcx
	andq	$-2, %rcx
	movl	44(%rsp), %r11d                 # 4-byte Reload
	movl	%r11d, %edx
	andl	$-2, %edx
	movl	$1, %esi
	xorl	%edi, %edi
	vmovups	208(%rsp), %xmm9                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	256(%rsp), %xmm10               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	jmp	.LBB33_240
	.p2align	4, 0x90
.LBB33_239:                             #   in Loop: Header=BB33_240 Depth=1
	incq	%rdi
	movl	44(%rsp), %r11d                 # 4-byte Reload
	addl	%r11d, %esi
	cmpq	48(%rsp), %rdi                  # 8-byte Folded Reload
	je	.LBB33_245
.LBB33_240:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB33_242 Depth 2
	vmovss	(%r14,%rdi,8), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm9, %xmm1
	vmovaps	%xmm2, %xmm0
	vfmsub213ss	%xmm1, %xmm9, %xmm0     # xmm0 = (xmm9 * xmm0) - xmm1
	vfmadd231ss	%xmm2, %xmm10, %xmm0    # xmm0 = (xmm10 * xmm2) + xmm0
	vfmadd231ss	4(%r14,%rdi,8), %xmm9, %xmm0 # xmm0 = (xmm9 * mem) + xmm0
	movq	136(%rsp), %r10                 # 8-byte Reload
	cmpl	$2, %r11d
	jb	.LBB33_243
# %bb.241:                              #   in Loop: Header=BB33_240 Depth=1
	movl	%esi, %r8d
	movq	%rax, %r9
	.p2align	4, 0x90
.LBB33_242:                             #   Parent Loop BB33_240 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	leal	-1(%r8), %r11d
	movslq	%r11d, %r11
	vmovss	(%rbx,%r11,8), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm2, %xmm3
	vmovaps	%xmm1, %xmm4
	vfmsub213ss	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vfmadd231ss	4(%rbx,%r11,8), %xmm1, %xmm4 # xmm4 = (xmm1 * mem) + xmm4
	vfmadd231ss	%xmm2, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm2) + xmm4
	vmovss	(%r10), %xmm2                   # xmm2 = mem[0],zero,zero,zero
	vmovss	8(%r10), %xmm5                  # xmm5 = mem[0],zero,zero,zero
	vaddss	%xmm3, %xmm2, %xmm6
	vsubss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	4(%r10), %xmm4, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vinsertps	$16, %xmm2, %xmm6, %xmm2 # xmm2 = xmm6[0],xmm2[0],xmm6[2,3]
	movslq	%r8d, %r8
	vmovss	(%rbx,%r8,8), %xmm3             # xmm3 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm3, %xmm4
	vmovaps	%xmm1, %xmm6
	vfmsub213ss	%xmm4, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm6) - xmm4
	vfmadd231ss	4(%rbx,%r8,8), %xmm1, %xmm6 # xmm6 = (xmm1 * mem) + xmm6
	vmovlps	%xmm2, (%r10)
	vfmadd231ss	%xmm3, %xmm0, %xmm6     # xmm6 = (xmm0 * xmm3) + xmm6
	vaddss	%xmm4, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm7
	vsubss	%xmm7, %xmm5, %xmm5
	vsubss	%xmm3, %xmm4, %xmm3
	vaddss	%xmm3, %xmm5, %xmm3
	vaddss	12(%r10), %xmm6, %xmm4
	vaddss	%xmm3, %xmm4, %xmm3
	vinsertps	$16, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[0],xmm3[0],xmm2[2,3]
	vmovlps	%xmm2, 8(%r10)
	addq	$16, %r10
	addl	$2, %r8d
	decq	%r9
	jne	.LBB33_242
.LBB33_243:                             #   in Loop: Header=BB33_240 Depth=1
	cmpq	48(%rsp), %rcx                  # 8-byte Folded Reload
	jae	.LBB33_239
# %bb.244:                              #   in Loop: Header=BB33_240 Depth=1
	movl	44(%rsp), %r8d                  # 4-byte Reload
	imull	%edi, %r8d
	addl	%edx, %r8d
	movslq	%r8d, %r8
	vmovss	(%rbx,%r8,8), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm2, %xmm3
	vmovaps	%xmm1, %xmm4
	vfmsub213ss	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vfmadd231ss	4(%rbx,%r8,8), %xmm1, %xmm4 # xmm4 = (xmm1 * mem) + xmm4
	vfmadd213ss	%xmm4, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm0) + xmm4
	movq	136(%rsp), %r8                  # 8-byte Reload
	vmovss	(%r8,%rcx,8), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm3, %xmm1, %xmm2
	vsubss	%xmm1, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	4(%r8,%rcx,8), %xmm0, %xmm0
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
	vmovlps	%xmm0, (%r8,%rcx,8)
	jmp	.LBB33_239
.LBB33_245:
	movb	$1, %r13b
	jmp	.LBB33_247
.LBB33_246:
	xorl	%r13d, %r13d
.LBB33_247:
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	64(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	movl	%r13d, 204(%rsp)                # 4-byte Spill
	testb	%r13b, %r13b
	je	.LBB33_288
# %bb.248:
	xorl	%eax, %eax
	movq	%rax, 184(%rsp)                 # 8-byte Spill
	leaq	96(%rsp), %rbp
	movq	136(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB33_250
	.p2align	4, 0x90
.LBB33_249:                             #   in Loop: Header=BB33_250 Depth=1
	movq	184(%rsp), %rcx                 # 8-byte Reload
	incq	%rcx
	movq	272(%rsp), %rsi                 # 8-byte Reload
	addq	$8, %rsi
	movq	%rcx, %rax
	movq	%rcx, 184(%rsp)                 # 8-byte Spill
	cmpq	%rcx, 48(%rsp)                  # 8-byte Folded Reload
	je	.LBB33_288
.LBB33_250:                             # =>This Inner Loop Header: Depth=1
	movq	128(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %r15d
.Ltmp2235:
	movq	%rbp, %rdi
	movq	%rsi, 272(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp2236:
# %bb.251:                              #   in Loop: Header=BB33_250 Depth=1
	movq	184(%rsp), %rax                 # 8-byte Reload
	addl	%r15d, %eax
	addl	$8, %eax
	movslq	%eax, %r15
	shlq	$5, %r15
	addq	144(%rsp), %r15                 # 8-byte Folded Reload
.Ltmp2238:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp2239:
# %bb.252:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2240:
	movq	%rax, %r13
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2241:
# %bb.253:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2242:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp2243:
# %bb.254:                              #   in Loop: Header=BB33_250 Depth=1
	movl	%eax, %r12d
	cmpq	%rbp, %r13
	cmovgq	%r13, %rbp
.Ltmp2244:
	leaq	8(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp2245:
# %bb.255:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2246:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2247:
# %bb.256:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2249:
	callq	mpfr_get_default_rounding_mode
.Ltmp2250:
	leaq	96(%rsp), %rbp
# %bb.257:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2251:
	leaq	8(%rsp), %rdi
	movq	%r15, %rsi
	movq	%rbp, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp2252:
# %bb.258:                              #   in Loop: Header=BB33_250 Depth=1
	cmpq	$0, 120(%rsp)
	je	.LBB33_260
# %bb.259:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2254:
	movq	%rbp, %rdi
	callq	mpfr_clear
.Ltmp2255:
.LBB33_260:                             #   in Loop: Header=BB33_250 Depth=1
.Ltmp2257:
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2258:
# %bb.261:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2259:
	movq	%rax, %r15
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2260:
# %bb.262:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2261:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2262:
# %bb.263:                              #   in Loop: Header=BB33_250 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %r15
	cmovgq	%r15, %r13
.Ltmp2263:
	leaq	152(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2264:
# %bb.264:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2265:
	leaq	152(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2266:
# %bb.265:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2268:
	callq	mpfr_get_default_rounding_mode
.Ltmp2269:
# %bb.266:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2270:
	leaq	152(%rsp), %rdi
	leaq	8(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp2271:
# %bb.267:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2273:
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2274:
# %bb.268:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2275:
	movq	%rax, %r15
	leaq	152(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2276:
# %bb.269:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2277:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2278:
# %bb.270:                              #   in Loop: Header=BB33_250 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %r15
	cmovgq	%r15, %r13
.Ltmp2279:
	movq	%rbp, %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2280:
# %bb.271:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2281:
	movq	%rbp, %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2282:
# %bb.272:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2284:
	callq	mpfr_get_default_rounding_mode
.Ltmp2285:
	leaq	64(%rsp), %r15
# %bb.273:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2286:
	movq	%rbp, %rdi
	movq	%r15, %rsi
	leaq	152(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp2287:
# %bb.274:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2289:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp2290:
# %bb.275:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2291:
	movq	%rax, %r12
	movq	%rbp, %rdi
	callq	mpfr_get_prec
.Ltmp2292:
# %bb.276:                              #   in Loop: Header=BB33_250 Depth=1
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB33_280
# %bb.277:                              #   in Loop: Header=BB33_250 Depth=1
	cmpq	$0, 88(%rsp)
	je	.LBB33_279
# %bb.278:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2293:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2294:
.LBB33_279:                             #   in Loop: Header=BB33_250 Depth=1
.Ltmp2295:
	leaq	64(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp2296:
.LBB33_280:                             #   in Loop: Header=BB33_250 Depth=1
.Ltmp2297:
	callq	mpfr_get_default_rounding_mode
.Ltmp2298:
# %bb.281:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2299:
	leaq	64(%rsp), %rdi
	movq	%rbp, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp2300:
# %bb.282:                              #   in Loop: Header=BB33_250 Depth=1
	cmpq	$0, 120(%rsp)
	je	.LBB33_284
# %bb.283:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2302:
	movq	%rbp, %rdi
	callq	mpfr_clear
.Ltmp2303:
.LBB33_284:                             #   in Loop: Header=BB33_250 Depth=1
	cmpq	$0, 176(%rsp)
	je	.LBB33_286
# %bb.285:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2305:
	leaq	152(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2306:
.LBB33_286:                             #   in Loop: Header=BB33_250 Depth=1
	cmpq	$0, 32(%rsp)
	je	.LBB33_249
# %bb.287:                              #   in Loop: Header=BB33_250 Depth=1
.Ltmp2308:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2309:
	jmp	.LBB33_249
.LBB33_288:
.Ltmp2311:
	callq	mpfr_get_default_rounding_mode
.Ltmp2312:
# %bb.289:
.Ltmp2313:
	movl	%eax, %r15d
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2314:
# %bb.290:
.Ltmp2315:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2316:
# %bb.291:
.Ltmp2317:
	movl	%eax, %r12d
	leaq	8(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2318:
# %bb.292:
.Ltmp2319:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2320:
# %bb.293:
.Ltmp2322:
	leaq	8(%rsp), %rdi
	leaq	64(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_sqrt
.Ltmp2323:
# %bb.294:
.Ltmp2325:
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2326:
# %bb.295:
.Ltmp2327:
	movq	%rax, %r12
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2328:
# %bb.296:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB33_300
# %bb.297:
	cmpq	$0, 88(%rsp)
	je	.LBB33_299
# %bb.298:
.Ltmp2329:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2330:
.LBB33_299:
.Ltmp2331:
	leaq	64(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp2332:
.LBB33_300:
.Ltmp2333:
	callq	mpfr_get_default_rounding_mode
.Ltmp2334:
# %bb.301:
.Ltmp2335:
	leaq	64(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp2336:
# %bb.302:
	cmpq	$0, 32(%rsp)
	je	.LBB33_304
# %bb.303:
.Ltmp2338:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2339:
.LBB33_304:
	callq	omp_get_wtime
	vmovsd	%xmm0, 128(%rsp)                # 8-byte Spill
	cmpb	$0, 204(%rsp)                   # 1-byte Folded Reload
	vmovups	208(%rsp), %xmm9                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	256(%rsp), %xmm10               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	je	.LBB33_314
# %bb.305:
	movq	48(%rsp), %rcx                  # 8-byte Reload
	movq	%rcx, %rax
	shrq	%rax
	andq	$-2, %rcx
	movl	44(%rsp), %edx                  # 4-byte Reload
	andl	$-2, %edx
	xorl	%esi, %esi
	jmp	.LBB33_307
	.p2align	4, 0x90
.LBB33_306:                             #   in Loop: Header=BB33_307 Depth=1
	incl	%esi
	cmpl	$10, %esi
	je	.LBB33_314
.LBB33_307:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB33_309 Depth 2
                                        #       Child Loop BB33_311 Depth 3
	movl	$1, %edi
	xorl	%r8d, %r8d
	movl	44(%rsp), %ebp                  # 4-byte Reload
	jmp	.LBB33_309
	.p2align	4, 0x90
.LBB33_308:                             #   in Loop: Header=BB33_309 Depth=2
	incq	%r8
	movl	44(%rsp), %ebp                  # 4-byte Reload
	addl	%ebp, %edi
	cmpq	48(%rsp), %r8                   # 8-byte Folded Reload
	je	.LBB33_306
.LBB33_309:                             #   Parent Loop BB33_307 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB33_311 Depth 3
	vmovss	(%r14,%r8,8), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm9, %xmm1
	vmovaps	%xmm2, %xmm0
	vfmsub213ss	%xmm1, %xmm9, %xmm0     # xmm0 = (xmm9 * xmm0) - xmm1
	vfmadd231ss	%xmm2, %xmm10, %xmm0    # xmm0 = (xmm10 * xmm2) + xmm0
	vfmadd231ss	4(%r14,%r8,8), %xmm9, %xmm0 # xmm0 = (xmm9 * mem) + xmm0
	movq	136(%rsp), %r11                 # 8-byte Reload
	cmpl	$2, %ebp
	jb	.LBB33_312
# %bb.310:                              #   in Loop: Header=BB33_309 Depth=2
	movl	%edi, %r9d
	movq	%rax, %r10
	.p2align	4, 0x90
.LBB33_311:                             #   Parent Loop BB33_307 Depth=1
                                        #     Parent Loop BB33_309 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	leal	-1(%r9), %ebp
	movslq	%ebp, %r15
	vmovss	(%rbx,%r15,8), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm2, %xmm3
	vmovaps	%xmm1, %xmm4
	vfmsub213ss	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vfmadd231ss	4(%rbx,%r15,8), %xmm1, %xmm4 # xmm4 = (xmm1 * mem) + xmm4
	vfmadd231ss	%xmm2, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm2) + xmm4
	vmovss	(%r11), %xmm2                   # xmm2 = mem[0],zero,zero,zero
	vmovss	8(%r11), %xmm5                  # xmm5 = mem[0],zero,zero,zero
	vaddss	%xmm3, %xmm2, %xmm6
	vsubss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	4(%r11), %xmm4, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vinsertps	$16, %xmm2, %xmm6, %xmm2 # xmm2 = xmm6[0],xmm2[0],xmm6[2,3]
	movslq	%r9d, %r9
	vmovss	(%rbx,%r9,8), %xmm3             # xmm3 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm3, %xmm4
	vmovaps	%xmm1, %xmm6
	vfmsub213ss	%xmm4, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm6) - xmm4
	vfmadd231ss	4(%rbx,%r9,8), %xmm1, %xmm6 # xmm6 = (xmm1 * mem) + xmm6
	vmovlps	%xmm2, (%r11)
	vfmadd231ss	%xmm3, %xmm0, %xmm6     # xmm6 = (xmm0 * xmm3) + xmm6
	vaddss	%xmm4, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm7
	vsubss	%xmm7, %xmm5, %xmm5
	vsubss	%xmm3, %xmm4, %xmm3
	vaddss	%xmm3, %xmm5, %xmm3
	vaddss	12(%r11), %xmm6, %xmm4
	vaddss	%xmm3, %xmm4, %xmm3
	vinsertps	$16, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[0],xmm3[0],xmm2[2,3]
	vmovlps	%xmm2, 8(%r11)
	addq	$16, %r11
	addl	$2, %r9d
	decq	%r10
	jne	.LBB33_311
.LBB33_312:                             #   in Loop: Header=BB33_309 Depth=2
	cmpq	48(%rsp), %rcx                  # 8-byte Folded Reload
	jae	.LBB33_308
# %bb.313:                              #   in Loop: Header=BB33_309 Depth=2
	movl	44(%rsp), %r9d                  # 4-byte Reload
	imull	%r8d, %r9d
	addl	%edx, %r9d
	movslq	%r9d, %r9
	vmovss	(%rbx,%r9,8), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm2, %xmm3
	vmovaps	%xmm1, %xmm4
	vfmsub213ss	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vfmadd231ss	4(%rbx,%r9,8), %xmm1, %xmm4 # xmm4 = (xmm1 * mem) + xmm4
	vfmadd213ss	%xmm4, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm0) + xmm4
	movq	136(%rsp), %r9                  # 8-byte Reload
	vmovss	(%r9,%rcx,8), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm3, %xmm1, %xmm2
	vsubss	%xmm1, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	4(%r9,%rcx,8), %xmm0, %xmm0
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
	vmovlps	%xmm0, (%r9,%rcx,8)
	jmp	.LBB33_308
.LBB33_314:
	callq	omp_get_wtime
	vsubsd	128(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm11, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp2341:
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2342:
# %bb.315:
	movq	%rax, %r12
	movq	144(%rsp), %rdi                 # 8-byte Reload
	subq	$-128, %rdi
.Ltmp2343:
	movq	%rdi, %rbp
	callq	mpfr_get_prec
.Ltmp2344:
# %bb.316:
.Ltmp2345:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2346:
# %bb.317:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp2347:
	leaq	8(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2348:
# %bb.318:
.Ltmp2349:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp2350:
# %bb.319:
.Ltmp2352:
	callq	mpfr_get_default_rounding_mode
.Ltmp2353:
# %bb.320:
.Ltmp2354:
	leaq	8(%rsp), %rdi
	leaq	64(%rsp), %rsi
	movq	%rbp, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp2355:
# %bb.321:
.Ltmp2357:
	leaq	8(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp2358:
# %bb.322:
	vmovlpd	%xmm0, 96(%rsp)
	cmpq	$0, 32(%rsp)
	je	.LBB33_324
# %bb.323:
.Ltmp2360:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2361:
.LBB33_324:
	leaq	296(%rsp), %r15
	movq	%r15, 280(%rsp)
	movl	$1986880871, 296(%rsp)          # imm = 0x766D6567
	movw	$32, 300(%rsp)
	movq	$5, 288(%rsp)
.Ltmp2363:
	leaq	280(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp2364:
# %bb.325:
	movq	280(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB33_327
# %bb.326:
	callq	_ZdlPv
.LBB33_327:
	movq	136(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	cmpq	$0, 88(%rsp)
	je	.LBB33_329
# %bb.328:
.Ltmp2366:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2367:
.LBB33_329:
	movq	%rbx, %rdi
	callq	_ZdaPv
	movq	%r14, %rdi
	callq	_ZdaPv
	xorl	%eax, %eax
	vcvtsi2ss	%eax, %xmm11, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vblendps	$1, %xmm0, %xmm1, %xmm2         # xmm2 = xmm0[0],xmm1[1,2,3]
	movl	$2097152, %eax                  # imm = 0x200000
	vxorps	%xmm0, %xmm0, %xmm0
	movl	$1, %ecx
	jmp	.LBB33_331
	.p2align	4, 0x90
.LBB33_330:                             #   in Loop: Header=BB33_331 Depth=1
	vdivss	%xmm4, %xmm1, %xmm5
	vfnmadd231ss	%xmm5, %xmm4, %xmm1     # xmm1 = -(xmm4 * xmm5) + xmm1
	vaddss	%xmm0, %xmm1, %xmm1
	vmovshdup	%xmm4, %xmm6            # xmm6 = xmm4[1,1,3,3]
	vfnmadd231ss	%xmm6, %xmm5, %xmm1     # xmm1 = -(xmm5 * xmm6) + xmm1
	vaddss	%xmm6, %xmm4, %xmm4
	vdivss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm5, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm4, %xmm2 # xmm2 = xmm4[0],xmm1[0],xmm4[2,3]
	addl	$-4, %eax
	je	.LBB33_343
.LBB33_331:                             # =>This Inner Loop Header: Depth=1
	vcvtsi2ss	%eax, %xmm11, %xmm1
	vmulss	%xmm1, %xmm1, %xmm3
	vmovaps	%xmm1, %xmm6
	vfmsub213ss	%xmm3, %xmm1, %xmm6     # xmm6 = (xmm1 * xmm6) - xmm3
	vfmadd231ss	%xmm0, %xmm1, %xmm6     # xmm6 = (xmm1 * xmm0) + xmm6
	vfmadd231ss	%xmm1, %xmm0, %xmm6     # xmm6 = (xmm0 * xmm1) + xmm6
	vcvtsi2ss	%ecx, %xmm11, %xmm1
	vmulss	%xmm3, %xmm3, %xmm4
	vmovaps	%xmm3, %xmm5
	vfmsub213ss	%xmm4, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm5) - xmm4
	vfmadd231ss	%xmm6, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm6) + xmm5
	vfmadd231ss	%xmm6, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm6) + xmm5
	vucomiss	%xmm0, %xmm4
	vinsertps	$16, %xmm5, %xmm4, %xmm3 # xmm3 = xmm4[0],xmm5[0],xmm4[2,3]
	jne	.LBB33_334
# %bb.332:                              #   in Loop: Header=BB33_331 Depth=1
	jp	.LBB33_334
# %bb.333:                              #   in Loop: Header=BB33_331 Depth=1
	vinsertps	$16, %xmm4, %xmm5, %xmm3 # xmm3 = xmm5[0],xmm4[0],xmm5[2,3]
.LBB33_334:                             #   in Loop: Header=BB33_331 Depth=1
	vdivss	%xmm3, %xmm1, %xmm4
	vmovaps	%xmm4, %xmm5
	vfnmadd213ss	%xmm1, %xmm3, %xmm5     # xmm5 = -(xmm3 * xmm5) + xmm1
	vaddss	%xmm0, %xmm5, %xmm5
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vfnmadd231ss	%xmm6, %xmm4, %xmm5     # xmm5 = -(xmm4 * xmm6) + xmm5
	vaddss	%xmm6, %xmm3, %xmm3
	vdivss	%xmm3, %xmm5, %xmm5
	vaddss	%xmm4, %xmm2, %xmm3
	vsubss	%xmm2, %xmm3, %xmm6
	vsubss	%xmm6, %xmm3, %xmm7
	vsubss	%xmm7, %xmm2, %xmm7
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm4
	leal	-1(%rax), %edx
	vcvtsi2ss	%edx, %xmm11, %xmm2
	vmulss	%xmm2, %xmm2, %xmm7
	vmovaps	%xmm2, %xmm8
	vfmsub213ss	%xmm7, %xmm2, %xmm8     # xmm8 = (xmm2 * xmm8) - xmm7
	vfmadd231ss	%xmm0, %xmm2, %xmm8     # xmm8 = (xmm2 * xmm0) + xmm8
	vfmadd231ss	%xmm2, %xmm0, %xmm8     # xmm8 = (xmm0 * xmm2) + xmm8
	vmulss	%xmm7, %xmm7, %xmm5
	vmovaps	%xmm7, %xmm6
	vfmsub213ss	%xmm5, %xmm7, %xmm6     # xmm6 = (xmm7 * xmm6) - xmm5
	vfmadd231ss	%xmm8, %xmm7, %xmm6     # xmm6 = (xmm7 * xmm8) + xmm6
	vfmadd231ss	%xmm8, %xmm7, %xmm6     # xmm6 = (xmm7 * xmm8) + xmm6
	vucomiss	%xmm0, %xmm5
	vinsertps	$16, %xmm6, %xmm5, %xmm2 # xmm2 = xmm5[0],xmm6[0],xmm5[2,3]
	jne	.LBB33_337
# %bb.335:                              #   in Loop: Header=BB33_331 Depth=1
	jp	.LBB33_337
# %bb.336:                              #   in Loop: Header=BB33_331 Depth=1
	vinsertps	$16, %xmm5, %xmm6, %xmm2 # xmm2 = xmm6[0],xmm5[0],xmm6[2,3]
.LBB33_337:                             #   in Loop: Header=BB33_331 Depth=1
	vdivss	%xmm2, %xmm1, %xmm5
	vmovaps	%xmm5, %xmm6
	vfnmadd213ss	%xmm1, %xmm2, %xmm6     # xmm6 = -(xmm2 * xmm6) + xmm1
	vaddss	%xmm0, %xmm6, %xmm6
	vmovshdup	%xmm2, %xmm7            # xmm7 = xmm2[1,1,3,3]
	vfnmadd231ss	%xmm7, %xmm5, %xmm6     # xmm6 = -(xmm5 * xmm7) + xmm6
	vaddss	%xmm7, %xmm2, %xmm2
	vdivss	%xmm2, %xmm6, %xmm6
	vaddss	%xmm5, %xmm3, %xmm2
	vsubss	%xmm3, %xmm2, %xmm7
	vsubss	%xmm7, %xmm2, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm3, %xmm4, %xmm4
	leal	-2(%rax), %edx
	vcvtsi2ss	%edx, %xmm9, %xmm3
	vmulss	%xmm3, %xmm3, %xmm7
	vmovaps	%xmm3, %xmm8
	vfmsub213ss	%xmm7, %xmm3, %xmm8     # xmm8 = (xmm3 * xmm8) - xmm7
	vfmadd231ss	%xmm0, %xmm3, %xmm8     # xmm8 = (xmm3 * xmm0) + xmm8
	vfmadd231ss	%xmm3, %xmm0, %xmm8     # xmm8 = (xmm0 * xmm3) + xmm8
	vmulss	%xmm7, %xmm7, %xmm5
	vmovaps	%xmm7, %xmm6
	vfmsub213ss	%xmm5, %xmm7, %xmm6     # xmm6 = (xmm7 * xmm6) - xmm5
	vfmadd231ss	%xmm8, %xmm7, %xmm6     # xmm6 = (xmm7 * xmm8) + xmm6
	vfmadd231ss	%xmm8, %xmm7, %xmm6     # xmm6 = (xmm7 * xmm8) + xmm6
	vucomiss	%xmm0, %xmm5
	vinsertps	$16, %xmm6, %xmm5, %xmm3 # xmm3 = xmm5[0],xmm6[0],xmm5[2,3]
	jne	.LBB33_340
# %bb.338:                              #   in Loop: Header=BB33_331 Depth=1
	jp	.LBB33_340
# %bb.339:                              #   in Loop: Header=BB33_331 Depth=1
	vinsertps	$16, %xmm5, %xmm6, %xmm3 # xmm3 = xmm6[0],xmm5[0],xmm6[2,3]
.LBB33_340:                             #   in Loop: Header=BB33_331 Depth=1
	vdivss	%xmm3, %xmm1, %xmm5
	vmovaps	%xmm5, %xmm6
	vfnmadd213ss	%xmm1, %xmm3, %xmm6     # xmm6 = -(xmm3 * xmm6) + xmm1
	vaddss	%xmm0, %xmm6, %xmm6
	vmovshdup	%xmm3, %xmm7            # xmm7 = xmm3[1,1,3,3]
	vfnmadd231ss	%xmm7, %xmm5, %xmm6     # xmm6 = -(xmm5 * xmm7) + xmm6
	vaddss	%xmm7, %xmm3, %xmm3
	vdivss	%xmm3, %xmm6, %xmm6
	vaddss	%xmm5, %xmm2, %xmm3
	vsubss	%xmm2, %xmm3, %xmm7
	vsubss	%xmm7, %xmm3, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	leal	-3(%rax), %edx
	vcvtsi2ss	%edx, %xmm9, %xmm4
	vmulss	%xmm4, %xmm4, %xmm7
	vmovaps	%xmm4, %xmm8
	vfmsub213ss	%xmm7, %xmm4, %xmm8     # xmm8 = (xmm4 * xmm8) - xmm7
	vfmadd231ss	%xmm0, %xmm4, %xmm8     # xmm8 = (xmm4 * xmm0) + xmm8
	vfmadd231ss	%xmm4, %xmm0, %xmm8     # xmm8 = (xmm0 * xmm4) + xmm8
	vmulss	%xmm7, %xmm7, %xmm5
	vmovaps	%xmm7, %xmm6
	vfmsub213ss	%xmm5, %xmm7, %xmm6     # xmm6 = (xmm7 * xmm6) - xmm5
	vfmadd231ss	%xmm8, %xmm7, %xmm6     # xmm6 = (xmm7 * xmm8) + xmm6
	vfmadd231ss	%xmm8, %xmm7, %xmm6     # xmm6 = (xmm7 * xmm8) + xmm6
	vucomiss	%xmm0, %xmm5
	vinsertps	$16, %xmm6, %xmm5, %xmm4 # xmm4 = xmm5[0],xmm6[0],xmm5[2,3]
	jne	.LBB33_330
# %bb.341:                              #   in Loop: Header=BB33_331 Depth=1
	jp	.LBB33_330
# %bb.342:                              #   in Loop: Header=BB33_331 Depth=1
	vinsertps	$16, %xmm5, %xmm6, %xmm4 # xmm4 = xmm6[0],xmm5[0],xmm6[2,3]
	jmp	.LBB33_330
.LBB33_343:
	movl	$90, %eax
	vcvtsi2ss	%eax, %xmm0, %xmm2
	vmulss	%xmm4, %xmm2, %xmm5
	vfmsub213ss	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm5
	vfmadd213ss	%xmm4, %xmm2, %xmm1     # xmm1 = (xmm2 * xmm1) + xmm4
	vaddss	%xmm1, %xmm5, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	vucomiss	%xmm3, %xmm2
	setnp	%al
	sete	%cl
	testb	%al, %cl
	je	.LBB33_345
# %bb.344:
	vinsertps	$16, %xmm1, %xmm5, %xmm0 # xmm0 = xmm5[0],xmm1[0],xmm5[2,3]
	jmp	.LBB33_349
.LBB33_345:
	vucomiss	%xmm3, %xmm5
	vinsertps	$16, %xmm1, %xmm5, %xmm0 # xmm0 = xmm5[0],xmm1[0],xmm5[2,3]
	jne	.LBB33_348
# %bb.346:
	jp	.LBB33_348
# %bb.347:
	vinsertps	$16, %xmm5, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm5[0],xmm1[2,3]
.LBB33_348:
	vmovups	%xmm0, 48(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovaps	%xmm0, %xmm1
	vmovups	48(%rsp), %xmm2                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vfnmadd213ss	%xmm2, %xmm0, %xmm1     # xmm1 = -(xmm0 * xmm1) + xmm2
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm0, %xmm2
	vdivss	%xmm2, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	vxorps	%xmm3, %xmm3, %xmm3
.LBB33_349:
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
	vucomiss	%xmm3, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB33_354
# %bb.350:
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm0
	jne	.LBB33_353
# %bb.351:
	jp	.LBB33_353
# %bb.352:
	vshufps	$225, %xmm0, %xmm0, %xmm0       # xmm0 = xmm0[1,0,2,3]
.LBB33_353:
	vmovups	%xmm0, 48(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovaps	%xmm0, %xmm1
	vmovups	48(%rsp), %xmm2                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vfnmadd213ss	%xmm2, %xmm0, %xmm1     # xmm1 = -(xmm0 * xmm1) + xmm2
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm0, %xmm2
	vdivss	%xmm2, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
.LBB33_354:
	vmovlps	%xmm0, 8(%rsp)
	leaq	240(%rsp), %r14
	movq	%r14, 224(%rsp)
	movq	$32, 64(%rsp)
.Ltmp2369:
	leaq	224(%rsp), %rdi
	leaq	64(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp2370:
# %bb.355:
	movq	%rax, 224(%rsp)
	movq	64(%rsp), %rcx
	movq	%rcx, 240(%rsp)
	vmovupd	.L.str.63(%rip), %ymm0
	vmovupd	%ymm0, (%rax)
	movq	%rcx, 232(%rsp)
	movq	224(%rsp), %rax
	movb	$0, (%rax,%rcx)
.Ltmp2372:
	leaq	224(%rsp), %rdi
	leaq	8(%rsp), %rsi
	xorl	%edx, %edx
	vzeroupper
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp2373:
# %bb.356:
	movq	224(%rsp), %rdi
	cmpq	%r14, %rdi
	je	.LBB33_358
# %bb.357:
	callq	_ZdlPv
.LBB33_358:
	addq	$440, %rsp                      # imm = 0x1B8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB33_359:
	.cfi_def_cfa_offset 496
.Ltmp2368:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_360:
.Ltmp2362:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_361:
.Ltmp2340:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_362:
.Ltmp2234:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_363:
.Ltmp2228:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_364:
.Ltmp2206:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_365:
.Ltmp2097:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_366:
.Ltmp2094:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_367:
.Ltmp2091:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_368:
.Ltmp2088:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_369:
.Ltmp2033:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_370:
.Ltmp2030:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_371:
.Ltmp2027:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_372:
.Ltmp2024:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_373:
.Ltmp1969:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_374:
.Ltmp1966:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_375:
.Ltmp1963:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_376:
.Ltmp1960:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_377:
.Ltmp2374:
	movq	%rax, %rbx
	movq	224(%rsp), %rdi
	cmpq	%r14, %rdi
	jne	.LBB33_395
	jmp	.LBB33_396
.LBB33_378:
.Ltmp2371:
	movq	%rax, %rdi
	callq	_Unwind_Resume@PLT
.LBB33_379:
.Ltmp2365:
	movq	%rax, %rbx
	movq	280(%rsp), %rdi
	jmp	.LBB33_383
.LBB33_380:
.Ltmp2359:
	movq	%rax, %rbx
	jmp	.LBB33_453
.LBB33_381:
.Ltmp2324:
	movq	%rax, %rbx
	jmp	.LBB33_453
.LBB33_382:
.Ltmp2231:
	movq	%rax, %rbx
	movq	312(%rsp), %rdi
.LBB33_383:
	cmpq	%r15, %rdi
	je	.LBB33_459
# %bb.384:
	callq	_ZdlPv
	jmp	.LBB33_459
.LBB33_385:
.Ltmp2225:
	movq	%rax, %rbx
	jmp	.LBB33_453
.LBB33_386:
.Ltmp2190:
	movq	%rax, %rbx
	jmp	.LBB33_453
.LBB33_387:
.Ltmp2100:
	movq	%rax, %rbx
	movq	344(%rsp), %rdi
	jmp	.LBB33_394
.LBB33_388:
.Ltmp2085:
	jmp	.LBB33_399
.LBB33_389:
.Ltmp2082:
	jmp	.LBB33_399
.LBB33_390:
.Ltmp2036:
	movq	%rax, %rbx
	movq	376(%rsp), %rdi
	jmp	.LBB33_394
.LBB33_391:
.Ltmp2021:
	jmp	.LBB33_399
.LBB33_392:
.Ltmp2018:
	jmp	.LBB33_399
.LBB33_393:
.Ltmp1972:
	movq	%rax, %rbx
	movq	408(%rsp), %rdi
.LBB33_394:
	cmpq	%r15, %rdi
	je	.LBB33_396
.LBB33_395:
	callq	_ZdlPv
.LBB33_396:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB33_397:
.Ltmp1957:
	jmp	.LBB33_399
.LBB33_398:
.Ltmp1954:
.LBB33_399:
	movq	%rax, %rbx
	leaq	64(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB33_422
.LBB33_400:
.Ltmp2356:
	movq	%rax, %rbx
	jmp	.LBB33_453
.LBB33_401:
.Ltmp2222:
	movq	%rax, %rbx
	jmp	.LBB33_453
.LBB33_402:
.Ltmp2068:
	jmp	.LBB33_421
.LBB33_403:
.Ltmp2052:
	movq	%rax, %rbx
	jmp	.LBB33_423
.LBB33_404:
.Ltmp2004:
	jmp	.LBB33_421
.LBB33_405:
.Ltmp1988:
	movq	%rax, %rbx
	jmp	.LBB33_423
.LBB33_406:
.Ltmp1940:
	jmp	.LBB33_421
.LBB33_407:
.Ltmp1924:
	movq	%rax, %rbx
	jmp	.LBB33_423
.LBB33_408:
.Ltmp2337:
	movq	%rax, %rbx
	jmp	.LBB33_453
.LBB33_409:
.Ltmp2203:
	movq	%rax, %rbx
	jmp	.LBB33_453
.LBB33_410:
.Ltmp2351:
	movq	%rax, %rbx
	jmp	.LBB33_459
.LBB33_411:
.Ltmp2321:
	movq	%rax, %rbx
	jmp	.LBB33_459
.LBB33_412:
.Ltmp2217:
	movq	%rax, %rbx
	jmp	.LBB33_459
.LBB33_413:
.Ltmp2187:
	movq	%rax, %rbx
	jmp	.LBB33_459
.LBB33_414:
.Ltmp2079:
	jmp	.LBB33_421
.LBB33_415:
.Ltmp2063:
	movq	%rax, %rbx
	jmp	.LBB33_423
.LBB33_416:
.Ltmp2047:
	jmp	.LBB33_426
.LBB33_417:
.Ltmp2015:
	jmp	.LBB33_421
.LBB33_418:
.Ltmp1999:
	movq	%rax, %rbx
	jmp	.LBB33_423
.LBB33_419:
.Ltmp1983:
	jmp	.LBB33_426
.LBB33_420:
.Ltmp1951:
.LBB33_421:
	movq	%rax, %rbx
.LBB33_422:
	leaq	8(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB33_423:
	leaq	96(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	leaq	152(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB33_424:
.Ltmp1935:
	movq	%rax, %rbx
	jmp	.LBB33_423
.LBB33_425:
.Ltmp1919:
.LBB33_426:
	movq	%rax, %rbx
	leaq	152(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB33_427:
.Ltmp2310:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_428:
.Ltmp2307:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_429:
.Ltmp2304:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_430:
.Ltmp2256:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_431:
.Ltmp2176:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_432:
.Ltmp2173:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_433:
.Ltmp2170:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_434:
.Ltmp2122:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB33_435:
.Ltmp2237:
	movq	%rax, %rbx
	jmp	.LBB33_459
.LBB33_436:
.Ltmp2103:
	movq	%rax, %rbx
	jmp	.LBB33_459
.LBB33_437:
.Ltmp2288:
	jmp	.LBB33_449
.LBB33_438:
.Ltmp2272:
	jmp	.LBB33_451
.LBB33_439:
.Ltmp2253:
	jmp	.LBB33_443
.LBB33_440:
.Ltmp2154:
	jmp	.LBB33_449
.LBB33_441:
.Ltmp2138:
	jmp	.LBB33_451
.LBB33_442:
.Ltmp2119:
.LBB33_443:
	movq	%rax, %rbx
	leaq	8(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB33_457
.LBB33_444:
.Ltmp2301:
	jmp	.LBB33_449
.LBB33_445:
.Ltmp2283:
	jmp	.LBB33_451
.LBB33_446:
.Ltmp2267:
	movq	%rax, %rbx
	jmp	.LBB33_453
.LBB33_447:
.Ltmp2248:
	jmp	.LBB33_456
.LBB33_448:
.Ltmp2167:
.LBB33_449:
	movq	%rax, %rbx
	leaq	96(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB33_452
.LBB33_450:
.Ltmp2149:
.LBB33_451:
	movq	%rax, %rbx
.LBB33_452:
	leaq	152(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB33_453:
	leaq	8(%rsp), %rdi
	jmp	.LBB33_458
.LBB33_454:
.Ltmp2133:
	movq	%rax, %rbx
	jmp	.LBB33_453
.LBB33_455:
.Ltmp2114:
.LBB33_456:
	movq	%rax, %rbx
.LBB33_457:
	leaq	96(%rsp), %rdi
.LBB33_458:
	callq	_ZN4mpfr6mprealD2Ev
.LBB33_459:
	leaq	64(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end33:
	.size	_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_, .Lfunc_end33-_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.cfi_endproc
	.section	.gcc_except_table._Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"aG",@progbits,_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.p2align	2, 0x0
GCC_except_table33:
.Lexception26:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase15-.Lttbaseref15
.Lttbaseref15:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end26-.Lcst_begin26
.Lcst_begin26:
	.uleb128 .Lfunc_begin26-.Lfunc_begin26  # >> Call Site 1 <<
	.uleb128 .Ltmp1909-.Lfunc_begin26       #   Call between .Lfunc_begin26 and .Ltmp1909
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1909-.Lfunc_begin26       # >> Call Site 2 <<
	.uleb128 .Ltmp1918-.Ltmp1909            #   Call between .Ltmp1909 and .Ltmp1918
	.uleb128 .Ltmp1919-.Lfunc_begin26       #     jumps to .Ltmp1919
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1920-.Lfunc_begin26       # >> Call Site 3 <<
	.uleb128 .Ltmp1923-.Ltmp1920            #   Call between .Ltmp1920 and .Ltmp1923
	.uleb128 .Ltmp1924-.Lfunc_begin26       #     jumps to .Ltmp1924
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1925-.Lfunc_begin26       # >> Call Site 4 <<
	.uleb128 .Ltmp1934-.Ltmp1925            #   Call between .Ltmp1925 and .Ltmp1934
	.uleb128 .Ltmp1935-.Lfunc_begin26       #     jumps to .Ltmp1935
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1936-.Lfunc_begin26       # >> Call Site 5 <<
	.uleb128 .Ltmp1939-.Ltmp1936            #   Call between .Ltmp1936 and .Ltmp1939
	.uleb128 .Ltmp1940-.Lfunc_begin26       #     jumps to .Ltmp1940
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1941-.Lfunc_begin26       # >> Call Site 6 <<
	.uleb128 .Ltmp1950-.Ltmp1941            #   Call between .Ltmp1941 and .Ltmp1950
	.uleb128 .Ltmp1951-.Lfunc_begin26       #     jumps to .Ltmp1951
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1952-.Lfunc_begin26       # >> Call Site 7 <<
	.uleb128 .Ltmp1953-.Ltmp1952            #   Call between .Ltmp1952 and .Ltmp1953
	.uleb128 .Ltmp1954-.Lfunc_begin26       #     jumps to .Ltmp1954
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1955-.Lfunc_begin26       # >> Call Site 8 <<
	.uleb128 .Ltmp1956-.Ltmp1955            #   Call between .Ltmp1955 and .Ltmp1956
	.uleb128 .Ltmp1957-.Lfunc_begin26       #     jumps to .Ltmp1957
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1958-.Lfunc_begin26       # >> Call Site 9 <<
	.uleb128 .Ltmp1959-.Ltmp1958            #   Call between .Ltmp1958 and .Ltmp1959
	.uleb128 .Ltmp1960-.Lfunc_begin26       #     jumps to .Ltmp1960
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1961-.Lfunc_begin26       # >> Call Site 10 <<
	.uleb128 .Ltmp1962-.Ltmp1961            #   Call between .Ltmp1961 and .Ltmp1962
	.uleb128 .Ltmp1963-.Lfunc_begin26       #     jumps to .Ltmp1963
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1964-.Lfunc_begin26       # >> Call Site 11 <<
	.uleb128 .Ltmp1965-.Ltmp1964            #   Call between .Ltmp1964 and .Ltmp1965
	.uleb128 .Ltmp1966-.Lfunc_begin26       #     jumps to .Ltmp1966
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1967-.Lfunc_begin26       # >> Call Site 12 <<
	.uleb128 .Ltmp1968-.Ltmp1967            #   Call between .Ltmp1967 and .Ltmp1968
	.uleb128 .Ltmp1969-.Lfunc_begin26       #     jumps to .Ltmp1969
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp1970-.Lfunc_begin26       # >> Call Site 13 <<
	.uleb128 .Ltmp1971-.Ltmp1970            #   Call between .Ltmp1970 and .Ltmp1971
	.uleb128 .Ltmp1972-.Lfunc_begin26       #     jumps to .Ltmp1972
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1971-.Lfunc_begin26       # >> Call Site 14 <<
	.uleb128 .Ltmp1973-.Ltmp1971            #   Call between .Ltmp1971 and .Ltmp1973
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1973-.Lfunc_begin26       # >> Call Site 15 <<
	.uleb128 .Ltmp1982-.Ltmp1973            #   Call between .Ltmp1973 and .Ltmp1982
	.uleb128 .Ltmp1983-.Lfunc_begin26       #     jumps to .Ltmp1983
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1984-.Lfunc_begin26       # >> Call Site 16 <<
	.uleb128 .Ltmp1987-.Ltmp1984            #   Call between .Ltmp1984 and .Ltmp1987
	.uleb128 .Ltmp1988-.Lfunc_begin26       #     jumps to .Ltmp1988
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp1989-.Lfunc_begin26       # >> Call Site 17 <<
	.uleb128 .Ltmp1998-.Ltmp1989            #   Call between .Ltmp1989 and .Ltmp1998
	.uleb128 .Ltmp1999-.Lfunc_begin26       #     jumps to .Ltmp1999
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2000-.Lfunc_begin26       # >> Call Site 18 <<
	.uleb128 .Ltmp2003-.Ltmp2000            #   Call between .Ltmp2000 and .Ltmp2003
	.uleb128 .Ltmp2004-.Lfunc_begin26       #     jumps to .Ltmp2004
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2005-.Lfunc_begin26       # >> Call Site 19 <<
	.uleb128 .Ltmp2014-.Ltmp2005            #   Call between .Ltmp2005 and .Ltmp2014
	.uleb128 .Ltmp2015-.Lfunc_begin26       #     jumps to .Ltmp2015
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2016-.Lfunc_begin26       # >> Call Site 20 <<
	.uleb128 .Ltmp2017-.Ltmp2016            #   Call between .Ltmp2016 and .Ltmp2017
	.uleb128 .Ltmp2018-.Lfunc_begin26       #     jumps to .Ltmp2018
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2019-.Lfunc_begin26       # >> Call Site 21 <<
	.uleb128 .Ltmp2020-.Ltmp2019            #   Call between .Ltmp2019 and .Ltmp2020
	.uleb128 .Ltmp2021-.Lfunc_begin26       #     jumps to .Ltmp2021
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2022-.Lfunc_begin26       # >> Call Site 22 <<
	.uleb128 .Ltmp2023-.Ltmp2022            #   Call between .Ltmp2022 and .Ltmp2023
	.uleb128 .Ltmp2024-.Lfunc_begin26       #     jumps to .Ltmp2024
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2025-.Lfunc_begin26       # >> Call Site 23 <<
	.uleb128 .Ltmp2026-.Ltmp2025            #   Call between .Ltmp2025 and .Ltmp2026
	.uleb128 .Ltmp2027-.Lfunc_begin26       #     jumps to .Ltmp2027
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2028-.Lfunc_begin26       # >> Call Site 24 <<
	.uleb128 .Ltmp2029-.Ltmp2028            #   Call between .Ltmp2028 and .Ltmp2029
	.uleb128 .Ltmp2030-.Lfunc_begin26       #     jumps to .Ltmp2030
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2031-.Lfunc_begin26       # >> Call Site 25 <<
	.uleb128 .Ltmp2032-.Ltmp2031            #   Call between .Ltmp2031 and .Ltmp2032
	.uleb128 .Ltmp2033-.Lfunc_begin26       #     jumps to .Ltmp2033
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2034-.Lfunc_begin26       # >> Call Site 26 <<
	.uleb128 .Ltmp2035-.Ltmp2034            #   Call between .Ltmp2034 and .Ltmp2035
	.uleb128 .Ltmp2036-.Lfunc_begin26       #     jumps to .Ltmp2036
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2035-.Lfunc_begin26       # >> Call Site 27 <<
	.uleb128 .Ltmp2037-.Ltmp2035            #   Call between .Ltmp2035 and .Ltmp2037
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2037-.Lfunc_begin26       # >> Call Site 28 <<
	.uleb128 .Ltmp2046-.Ltmp2037            #   Call between .Ltmp2037 and .Ltmp2046
	.uleb128 .Ltmp2047-.Lfunc_begin26       #     jumps to .Ltmp2047
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2048-.Lfunc_begin26       # >> Call Site 29 <<
	.uleb128 .Ltmp2051-.Ltmp2048            #   Call between .Ltmp2048 and .Ltmp2051
	.uleb128 .Ltmp2052-.Lfunc_begin26       #     jumps to .Ltmp2052
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2053-.Lfunc_begin26       # >> Call Site 30 <<
	.uleb128 .Ltmp2062-.Ltmp2053            #   Call between .Ltmp2053 and .Ltmp2062
	.uleb128 .Ltmp2063-.Lfunc_begin26       #     jumps to .Ltmp2063
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2064-.Lfunc_begin26       # >> Call Site 31 <<
	.uleb128 .Ltmp2067-.Ltmp2064            #   Call between .Ltmp2064 and .Ltmp2067
	.uleb128 .Ltmp2068-.Lfunc_begin26       #     jumps to .Ltmp2068
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2069-.Lfunc_begin26       # >> Call Site 32 <<
	.uleb128 .Ltmp2078-.Ltmp2069            #   Call between .Ltmp2069 and .Ltmp2078
	.uleb128 .Ltmp2079-.Lfunc_begin26       #     jumps to .Ltmp2079
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2080-.Lfunc_begin26       # >> Call Site 33 <<
	.uleb128 .Ltmp2081-.Ltmp2080            #   Call between .Ltmp2080 and .Ltmp2081
	.uleb128 .Ltmp2082-.Lfunc_begin26       #     jumps to .Ltmp2082
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2083-.Lfunc_begin26       # >> Call Site 34 <<
	.uleb128 .Ltmp2084-.Ltmp2083            #   Call between .Ltmp2083 and .Ltmp2084
	.uleb128 .Ltmp2085-.Lfunc_begin26       #     jumps to .Ltmp2085
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2086-.Lfunc_begin26       # >> Call Site 35 <<
	.uleb128 .Ltmp2087-.Ltmp2086            #   Call between .Ltmp2086 and .Ltmp2087
	.uleb128 .Ltmp2088-.Lfunc_begin26       #     jumps to .Ltmp2088
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2089-.Lfunc_begin26       # >> Call Site 36 <<
	.uleb128 .Ltmp2090-.Ltmp2089            #   Call between .Ltmp2089 and .Ltmp2090
	.uleb128 .Ltmp2091-.Lfunc_begin26       #     jumps to .Ltmp2091
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2092-.Lfunc_begin26       # >> Call Site 37 <<
	.uleb128 .Ltmp2093-.Ltmp2092            #   Call between .Ltmp2092 and .Ltmp2093
	.uleb128 .Ltmp2094-.Lfunc_begin26       #     jumps to .Ltmp2094
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2095-.Lfunc_begin26       # >> Call Site 38 <<
	.uleb128 .Ltmp2096-.Ltmp2095            #   Call between .Ltmp2095 and .Ltmp2096
	.uleb128 .Ltmp2097-.Lfunc_begin26       #     jumps to .Ltmp2097
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2098-.Lfunc_begin26       # >> Call Site 39 <<
	.uleb128 .Ltmp2099-.Ltmp2098            #   Call between .Ltmp2098 and .Ltmp2099
	.uleb128 .Ltmp2100-.Lfunc_begin26       #     jumps to .Ltmp2100
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2099-.Lfunc_begin26       # >> Call Site 40 <<
	.uleb128 .Ltmp2101-.Ltmp2099            #   Call between .Ltmp2099 and .Ltmp2101
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2101-.Lfunc_begin26       # >> Call Site 41 <<
	.uleb128 .Ltmp2102-.Ltmp2101            #   Call between .Ltmp2101 and .Ltmp2102
	.uleb128 .Ltmp2103-.Lfunc_begin26       #     jumps to .Ltmp2103
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2104-.Lfunc_begin26       # >> Call Site 42 <<
	.uleb128 .Ltmp2113-.Ltmp2104            #   Call between .Ltmp2104 and .Ltmp2113
	.uleb128 .Ltmp2114-.Lfunc_begin26       #     jumps to .Ltmp2114
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2115-.Lfunc_begin26       # >> Call Site 43 <<
	.uleb128 .Ltmp2118-.Ltmp2115            #   Call between .Ltmp2115 and .Ltmp2118
	.uleb128 .Ltmp2119-.Lfunc_begin26       #     jumps to .Ltmp2119
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2120-.Lfunc_begin26       # >> Call Site 44 <<
	.uleb128 .Ltmp2121-.Ltmp2120            #   Call between .Ltmp2120 and .Ltmp2121
	.uleb128 .Ltmp2122-.Lfunc_begin26       #     jumps to .Ltmp2122
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2123-.Lfunc_begin26       # >> Call Site 45 <<
	.uleb128 .Ltmp2132-.Ltmp2123            #   Call between .Ltmp2123 and .Ltmp2132
	.uleb128 .Ltmp2133-.Lfunc_begin26       #     jumps to .Ltmp2133
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2134-.Lfunc_begin26       # >> Call Site 46 <<
	.uleb128 .Ltmp2137-.Ltmp2134            #   Call between .Ltmp2134 and .Ltmp2137
	.uleb128 .Ltmp2138-.Lfunc_begin26       #     jumps to .Ltmp2138
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2139-.Lfunc_begin26       # >> Call Site 47 <<
	.uleb128 .Ltmp2148-.Ltmp2139            #   Call between .Ltmp2139 and .Ltmp2148
	.uleb128 .Ltmp2149-.Lfunc_begin26       #     jumps to .Ltmp2149
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2150-.Lfunc_begin26       # >> Call Site 48 <<
	.uleb128 .Ltmp2153-.Ltmp2150            #   Call between .Ltmp2150 and .Ltmp2153
	.uleb128 .Ltmp2154-.Lfunc_begin26       #     jumps to .Ltmp2154
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2155-.Lfunc_begin26       # >> Call Site 49 <<
	.uleb128 .Ltmp2166-.Ltmp2155            #   Call between .Ltmp2155 and .Ltmp2166
	.uleb128 .Ltmp2167-.Lfunc_begin26       #     jumps to .Ltmp2167
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2168-.Lfunc_begin26       # >> Call Site 50 <<
	.uleb128 .Ltmp2169-.Ltmp2168            #   Call between .Ltmp2168 and .Ltmp2169
	.uleb128 .Ltmp2170-.Lfunc_begin26       #     jumps to .Ltmp2170
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2171-.Lfunc_begin26       # >> Call Site 51 <<
	.uleb128 .Ltmp2172-.Ltmp2171            #   Call between .Ltmp2171 and .Ltmp2172
	.uleb128 .Ltmp2173-.Lfunc_begin26       #     jumps to .Ltmp2173
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2174-.Lfunc_begin26       # >> Call Site 52 <<
	.uleb128 .Ltmp2175-.Ltmp2174            #   Call between .Ltmp2174 and .Ltmp2175
	.uleb128 .Ltmp2176-.Lfunc_begin26       #     jumps to .Ltmp2176
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2177-.Lfunc_begin26       # >> Call Site 53 <<
	.uleb128 .Ltmp2186-.Ltmp2177            #   Call between .Ltmp2177 and .Ltmp2186
	.uleb128 .Ltmp2187-.Lfunc_begin26       #     jumps to .Ltmp2187
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2188-.Lfunc_begin26       # >> Call Site 54 <<
	.uleb128 .Ltmp2189-.Ltmp2188            #   Call between .Ltmp2188 and .Ltmp2189
	.uleb128 .Ltmp2190-.Lfunc_begin26       #     jumps to .Ltmp2190
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2191-.Lfunc_begin26       # >> Call Site 55 <<
	.uleb128 .Ltmp2202-.Ltmp2191            #   Call between .Ltmp2191 and .Ltmp2202
	.uleb128 .Ltmp2203-.Lfunc_begin26       #     jumps to .Ltmp2203
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2204-.Lfunc_begin26       # >> Call Site 56 <<
	.uleb128 .Ltmp2205-.Ltmp2204            #   Call between .Ltmp2204 and .Ltmp2205
	.uleb128 .Ltmp2206-.Lfunc_begin26       #     jumps to .Ltmp2206
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2207-.Lfunc_begin26       # >> Call Site 57 <<
	.uleb128 .Ltmp2216-.Ltmp2207            #   Call between .Ltmp2207 and .Ltmp2216
	.uleb128 .Ltmp2217-.Lfunc_begin26       #     jumps to .Ltmp2217
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2218-.Lfunc_begin26       # >> Call Site 58 <<
	.uleb128 .Ltmp2221-.Ltmp2218            #   Call between .Ltmp2218 and .Ltmp2221
	.uleb128 .Ltmp2222-.Lfunc_begin26       #     jumps to .Ltmp2222
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2223-.Lfunc_begin26       # >> Call Site 59 <<
	.uleb128 .Ltmp2224-.Ltmp2223            #   Call between .Ltmp2223 and .Ltmp2224
	.uleb128 .Ltmp2225-.Lfunc_begin26       #     jumps to .Ltmp2225
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2226-.Lfunc_begin26       # >> Call Site 60 <<
	.uleb128 .Ltmp2227-.Ltmp2226            #   Call between .Ltmp2226 and .Ltmp2227
	.uleb128 .Ltmp2228-.Lfunc_begin26       #     jumps to .Ltmp2228
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2229-.Lfunc_begin26       # >> Call Site 61 <<
	.uleb128 .Ltmp2230-.Ltmp2229            #   Call between .Ltmp2229 and .Ltmp2230
	.uleb128 .Ltmp2231-.Lfunc_begin26       #     jumps to .Ltmp2231
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2232-.Lfunc_begin26       # >> Call Site 62 <<
	.uleb128 .Ltmp2233-.Ltmp2232            #   Call between .Ltmp2232 and .Ltmp2233
	.uleb128 .Ltmp2234-.Lfunc_begin26       #     jumps to .Ltmp2234
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2233-.Lfunc_begin26       # >> Call Site 63 <<
	.uleb128 .Ltmp2235-.Ltmp2233            #   Call between .Ltmp2233 and .Ltmp2235
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2235-.Lfunc_begin26       # >> Call Site 64 <<
	.uleb128 .Ltmp2236-.Ltmp2235            #   Call between .Ltmp2235 and .Ltmp2236
	.uleb128 .Ltmp2237-.Lfunc_begin26       #     jumps to .Ltmp2237
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2238-.Lfunc_begin26       # >> Call Site 65 <<
	.uleb128 .Ltmp2247-.Ltmp2238            #   Call between .Ltmp2238 and .Ltmp2247
	.uleb128 .Ltmp2248-.Lfunc_begin26       #     jumps to .Ltmp2248
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2249-.Lfunc_begin26       # >> Call Site 66 <<
	.uleb128 .Ltmp2252-.Ltmp2249            #   Call between .Ltmp2249 and .Ltmp2252
	.uleb128 .Ltmp2253-.Lfunc_begin26       #     jumps to .Ltmp2253
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2254-.Lfunc_begin26       # >> Call Site 67 <<
	.uleb128 .Ltmp2255-.Ltmp2254            #   Call between .Ltmp2254 and .Ltmp2255
	.uleb128 .Ltmp2256-.Lfunc_begin26       #     jumps to .Ltmp2256
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2257-.Lfunc_begin26       # >> Call Site 68 <<
	.uleb128 .Ltmp2266-.Ltmp2257            #   Call between .Ltmp2257 and .Ltmp2266
	.uleb128 .Ltmp2267-.Lfunc_begin26       #     jumps to .Ltmp2267
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2268-.Lfunc_begin26       # >> Call Site 69 <<
	.uleb128 .Ltmp2271-.Ltmp2268            #   Call between .Ltmp2268 and .Ltmp2271
	.uleb128 .Ltmp2272-.Lfunc_begin26       #     jumps to .Ltmp2272
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2273-.Lfunc_begin26       # >> Call Site 70 <<
	.uleb128 .Ltmp2282-.Ltmp2273            #   Call between .Ltmp2273 and .Ltmp2282
	.uleb128 .Ltmp2283-.Lfunc_begin26       #     jumps to .Ltmp2283
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2284-.Lfunc_begin26       # >> Call Site 71 <<
	.uleb128 .Ltmp2287-.Ltmp2284            #   Call between .Ltmp2284 and .Ltmp2287
	.uleb128 .Ltmp2288-.Lfunc_begin26       #     jumps to .Ltmp2288
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2289-.Lfunc_begin26       # >> Call Site 72 <<
	.uleb128 .Ltmp2300-.Ltmp2289            #   Call between .Ltmp2289 and .Ltmp2300
	.uleb128 .Ltmp2301-.Lfunc_begin26       #     jumps to .Ltmp2301
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2302-.Lfunc_begin26       # >> Call Site 73 <<
	.uleb128 .Ltmp2303-.Ltmp2302            #   Call between .Ltmp2302 and .Ltmp2303
	.uleb128 .Ltmp2304-.Lfunc_begin26       #     jumps to .Ltmp2304
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2305-.Lfunc_begin26       # >> Call Site 74 <<
	.uleb128 .Ltmp2306-.Ltmp2305            #   Call between .Ltmp2305 and .Ltmp2306
	.uleb128 .Ltmp2307-.Lfunc_begin26       #     jumps to .Ltmp2307
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2308-.Lfunc_begin26       # >> Call Site 75 <<
	.uleb128 .Ltmp2309-.Ltmp2308            #   Call between .Ltmp2308 and .Ltmp2309
	.uleb128 .Ltmp2310-.Lfunc_begin26       #     jumps to .Ltmp2310
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2311-.Lfunc_begin26       # >> Call Site 76 <<
	.uleb128 .Ltmp2320-.Ltmp2311            #   Call between .Ltmp2311 and .Ltmp2320
	.uleb128 .Ltmp2321-.Lfunc_begin26       #     jumps to .Ltmp2321
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2322-.Lfunc_begin26       # >> Call Site 77 <<
	.uleb128 .Ltmp2323-.Ltmp2322            #   Call between .Ltmp2322 and .Ltmp2323
	.uleb128 .Ltmp2324-.Lfunc_begin26       #     jumps to .Ltmp2324
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2325-.Lfunc_begin26       # >> Call Site 78 <<
	.uleb128 .Ltmp2336-.Ltmp2325            #   Call between .Ltmp2325 and .Ltmp2336
	.uleb128 .Ltmp2337-.Lfunc_begin26       #     jumps to .Ltmp2337
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2338-.Lfunc_begin26       # >> Call Site 79 <<
	.uleb128 .Ltmp2339-.Ltmp2338            #   Call between .Ltmp2338 and .Ltmp2339
	.uleb128 .Ltmp2340-.Lfunc_begin26       #     jumps to .Ltmp2340
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2341-.Lfunc_begin26       # >> Call Site 80 <<
	.uleb128 .Ltmp2350-.Ltmp2341            #   Call between .Ltmp2341 and .Ltmp2350
	.uleb128 .Ltmp2351-.Lfunc_begin26       #     jumps to .Ltmp2351
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2352-.Lfunc_begin26       # >> Call Site 81 <<
	.uleb128 .Ltmp2355-.Ltmp2352            #   Call between .Ltmp2352 and .Ltmp2355
	.uleb128 .Ltmp2356-.Lfunc_begin26       #     jumps to .Ltmp2356
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2357-.Lfunc_begin26       # >> Call Site 82 <<
	.uleb128 .Ltmp2358-.Ltmp2357            #   Call between .Ltmp2357 and .Ltmp2358
	.uleb128 .Ltmp2359-.Lfunc_begin26       #     jumps to .Ltmp2359
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2360-.Lfunc_begin26       # >> Call Site 83 <<
	.uleb128 .Ltmp2361-.Ltmp2360            #   Call between .Ltmp2360 and .Ltmp2361
	.uleb128 .Ltmp2362-.Lfunc_begin26       #     jumps to .Ltmp2362
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2363-.Lfunc_begin26       # >> Call Site 84 <<
	.uleb128 .Ltmp2364-.Ltmp2363            #   Call between .Ltmp2363 and .Ltmp2364
	.uleb128 .Ltmp2365-.Lfunc_begin26       #     jumps to .Ltmp2365
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2366-.Lfunc_begin26       # >> Call Site 85 <<
	.uleb128 .Ltmp2367-.Ltmp2366            #   Call between .Ltmp2366 and .Ltmp2367
	.uleb128 .Ltmp2368-.Lfunc_begin26       #     jumps to .Ltmp2368
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2369-.Lfunc_begin26       # >> Call Site 86 <<
	.uleb128 .Ltmp2370-.Ltmp2369            #   Call between .Ltmp2369 and .Ltmp2370
	.uleb128 .Ltmp2371-.Lfunc_begin26       #     jumps to .Ltmp2371
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2372-.Lfunc_begin26       # >> Call Site 87 <<
	.uleb128 .Ltmp2373-.Ltmp2372            #   Call between .Ltmp2372 and .Ltmp2373
	.uleb128 .Ltmp2374-.Lfunc_begin26       #     jumps to .Ltmp2374
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2373-.Lfunc_begin26       # >> Call Site 88 <<
	.uleb128 .Lfunc_end33-.Ltmp2373         #   Call between .Ltmp2373 and .Lfunc_end33
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end26:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase15:
	.p2align	2, 0x0
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2, 0x0                          # -- Begin function _Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.LCPI34_0:
	.long	0x80000000                      #  -0
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4, 0x0
.LCPI34_1:
	.zero	16
	.section	.text._Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"axG",@progbits,_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.weak	_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.p2align	4, 0x90
	.type	_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,@function
_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_: # 
.Lfunc_begin27:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception27
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$408, %rsp                      # imm = 0x198
	.cfi_def_cfa_offset 464
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, 112(%rsp)                  # 8-byte Spill
	movq	%rcx, %r15
	movq	%rdx, %r12
	movq	%rsi, 128(%rsp)                 # 8-byte Spill
	movq	%rdi, %rbx
	vcvtsi2sdl	(%rdi), %xmm0, %xmm0
	callq	sqrt
	vcvttsd2si	%xmm0, %eax
	movl	%eax, 4(%rsp)
	movq	%rbx, 176(%rsp)                 # 8-byte Spill
	movslq	(%rbx), %rbp
	leaq	(,%rbp,8), %r13
	testq	%rbp, %rbp
	movq	$-1, %r14
	cmovnsq	%r13, %r14
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %rbx
	testq	%rbp, %rbp
	movq	%r15, 192(%rsp)                 # 8-byte Spill
	je	.LBB34_4
# %bb.1:
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movq	%r13, %rdx
	callq	_intel_fast_memset@PLT
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 120(%rsp)                 # 8-byte Spill
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%r13, %rdx
	callq	_intel_fast_memset@PLT
	testl	%ebp, %ebp
	jle	.LBB34_5
# %bb.2:
	xorl	%ebp, %ebp
	movq	%r15, %r13
	movq	176(%rsp), %r15                 # 8-byte Reload
	movq	120(%rsp), %r14                 # 8-byte Reload
	.p2align	4, 0x90
.LBB34_3:                               # =>This Inner Loop Header: Depth=1
	movq	%r12, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, (%rbx,%rbp,8)
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, (%r14,%rbp,8)
	incq	%rbp
	movslq	(%r15), %rax
	addq	$32, %r13
	addq	$32, %r12
	cmpq	%rax, %rbp
	jl	.LBB34_3
	jmp	.LBB34_5
.LBB34_4:
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 120(%rsp)                 # 8-byte Spill
.LBB34_5:
	movq	128(%rsp), %rdi                 # 8-byte Reload
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, 72(%rsp)
	movl	$80, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, 64(%rax)
	vzeroupper
	callq	omp_get_wtime
	vxorps	%xmm8, %xmm8, %xmm8
	vmovsd	%xmm0, 128(%rsp)                # 8-byte Spill
	movq	176(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	movq	%rax, %rcx
	shrq	%rcx
	movl	%eax, %edx
	andl	$-2, %edx
	xorl	%esi, %esi
	xorl	%edi, %edi
	movq	120(%rsp), %r10                 # 8-byte Reload
	jmp	.LBB34_7
	.p2align	4, 0x90
.LBB34_6:                               #   in Loop: Header=BB34_7 Depth=1
	vmovlps	%xmm0, (%rbp,%rdi,8)
	leaq	1(%rdi), %r8
	cmpq	$9, %rdi
	movq	%r8, %rdi
	je	.LBB34_13
.LBB34_7:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB34_10 Depth 2
	vcvtsi2ss	%esi, %xmm9, %xmm0
	vblendps	$1, %xmm0, %xmm8, %xmm0         # xmm0 = xmm0[0],xmm8[1,2,3]
	testl	%eax, %eax
	jle	.LBB34_6
# %bb.8:                                #   in Loop: Header=BB34_7 Depth=1
	cmpl	$1, %eax
	je	.LBB34_11
# %bb.9:                                #   in Loop: Header=BB34_7 Depth=1
	movl	$12, %r8d
	movq	%rcx, %r9
	.p2align	4, 0x90
.LBB34_10:                              #   Parent Loop BB34_7 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	-12(%rbx,%r8), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmovss	-4(%rbx,%r8), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vmovss	-12(%r10,%r8), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm2, %xmm4
	vmovaps	%xmm3, %xmm5
	vfmsub213ss	%xmm4, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm5) - xmm4
	vfmadd231ss	-8(%rbx,%r8), %xmm3, %xmm5 # xmm5 = (xmm3 * mem) + xmm5
	vfmadd231ss	-8(%r10,%r8), %xmm2, %xmm5 # xmm5 = (xmm2 * mem) + xmm5
	vmovss	-4(%r10,%r8), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vaddss	%xmm5, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm3, %xmm0, %xmm5
	vsubss	%xmm0, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm0, %xmm7
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm7, %xmm3
	vmovshdup	%xmm0, %xmm0            # xmm0 = xmm0[1,1,3,3]
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm3
	vsubss	%xmm3, %xmm5, %xmm4
	vmulss	%xmm2, %xmm1, %xmm5
	vmovaps	%xmm2, %xmm6
	vfmsub213ss	%xmm5, %xmm1, %xmm6     # xmm6 = (xmm1 * xmm6) - xmm5
	vfmadd231ss	(%rbx,%r8), %xmm2, %xmm6 # xmm6 = (xmm2 * mem) + xmm6
	vaddss	%xmm0, %xmm4, %xmm0
	vfmadd231ss	(%r10,%r8), %xmm1, %xmm6 # xmm6 = (xmm1 * mem) + xmm6
	vaddss	%xmm6, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm2
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm3, %xmm3
	vsubss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	addq	$16, %r8
	decq	%r9
	jne	.LBB34_10
.LBB34_11:                              #   in Loop: Header=BB34_7 Depth=1
	cmpq	%rax, %rdx
	jae	.LBB34_6
# %bb.12:                               #   in Loop: Header=BB34_7 Depth=1
	vmovss	(%rbx,%rdx,8), %xmm1            # xmm1 = mem[0],zero,zero,zero
	movq	120(%rsp), %r8                  # 8-byte Reload
	vmovss	(%r8,%rdx,8), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm1, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm4) - xmm3
	vfmadd231ss	4(%rbx,%rdx,8), %xmm2, %xmm4 # xmm4 = (xmm2 * mem) + xmm4
	vfmadd231ss	4(%r8,%rdx,8), %xmm1, %xmm4 # xmm4 = (xmm1 * mem) + xmm4
	vaddss	%xmm4, %xmm3, %xmm1
	vsubss	%xmm1, %xmm3, %xmm2
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm1, %xmm0, %xmm3
	vsubss	%xmm0, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm0, %xmm5
	vsubss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm1
	vmovshdup	%xmm0, %xmm0            # xmm0 = xmm0[1,1,3,3]
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm1
	vsubss	%xmm1, %xmm3, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	jmp	.LBB34_6
.LBB34_13:
	callq	omp_get_wtime
	vsubsd	128(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm9, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	144(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp2375:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp2376:
# %bb.14:
.Ltmp2377:
	movq	%rax, %r12
	movq	112(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp2378:
# %bb.15:
.Ltmp2379:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2380:
# %bb.16:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp2381:
	leaq	80(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2382:
# %bb.17:
.Ltmp2383:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp2384:
# %bb.18:
.Ltmp2386:
	callq	mpfr_get_default_rounding_mode
.Ltmp2387:
# %bb.19:
.Ltmp2388:
	leaq	80(%rsp), %rdi
	leaq	144(%rsp), %rsi
	movq	112(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp2389:
# %bb.20:
.Ltmp2391:
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2392:
# %bb.21:
.Ltmp2393:
	movq	%rax, %r12
	movq	112(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp2394:
# %bb.22:
.Ltmp2395:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2396:
# %bb.23:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp2397:
	leaq	8(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2398:
# %bb.24:
.Ltmp2399:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp2400:
# %bb.25:
.Ltmp2402:
	callq	mpfr_get_default_rounding_mode
.Ltmp2403:
# %bb.26:
.Ltmp2404:
	leaq	8(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movq	112(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp2405:
# %bb.27:
.Ltmp2407:
	callq	mpfr_get_default_rounding_mode
.Ltmp2408:
# %bb.28:
.Ltmp2409:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2410:
# %bb.29:
.Ltmp2411:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2412:
# %bb.30:
.Ltmp2413:
	movl	%eax, %r12d
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2414:
# %bb.31:
.Ltmp2415:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2416:
# %bb.32:
.Ltmp2418:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp2419:
# %bb.33:
.Ltmp2421:
	leaq	40(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp2422:
# %bb.34:
	vmovlpd	%xmm0, 200(%rsp)
	cmpq	$0, 64(%rsp)
	je	.LBB34_36
# %bb.35:
.Ltmp2424:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2425:
.LBB34_36:
	cmpq	$0, 32(%rsp)
	je	.LBB34_38
# %bb.37:
.Ltmp2427:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2428:
.LBB34_38:
	cmpq	$0, 104(%rsp)
	je	.LBB34_40
# %bb.39:
.Ltmp2430:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2431:
.LBB34_40:
	cmpq	$0, 168(%rsp)
	je	.LBB34_42
# %bb.41:
.Ltmp2433:
	leaq	144(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2434:
.LBB34_42:
	leaq	392(%rsp), %r15
	movq	%r15, 376(%rsp)
	movl	$544501604, 392(%rsp)           # imm = 0x20746F64
	movw	$32, 396(%rsp)
	movq	$5, 384(%rsp)
.Ltmp2436:
	leaq	376(%rsp), %rdi
	leaq	200(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp2437:
# %bb.43:
	movq	376(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB34_45
# %bb.44:
	callq	_ZdlPv
.LBB34_45:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$80, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, 64(%rax)
	xorl	%r15d, %r15d
	vzeroupper
	callq	omp_get_wtime
	vmovsd	%xmm0, 208(%rsp)                # 8-byte Spill
	vxorps	%xmm7, %xmm7, %xmm7
	xorl	%r12d, %r12d
	jmp	.LBB34_47
	.p2align	4, 0x90
.LBB34_46:                              #   in Loop: Header=BB34_47 Depth=1
	vmovlps	%xmm0, (%rbp,%r12,8)
	leaq	1(%r12), %rax
	cmpq	$9, %r12
	movq	%rax, %r12
	je	.LBB34_55
.LBB34_47:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB34_50 Depth 2
                                        #     Child Loop BB34_52 Depth 2
	vcvtsi2ss	%r15d, %xmm8, %xmm0
	vblendps	$14, .LCPI34_1(%rip), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[1,2,3]
	movq	176(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	jle	.LBB34_53
# %bb.48:                               #   in Loop: Header=BB34_47 Depth=1
	cmpl	$4, %eax
	jb	.LBB34_51
# %bb.49:                               #   in Loop: Header=BB34_47 Depth=1
	leaq	(,%rax,8), %rcx
	andq	$-32, %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB34_50:                              #   Parent Loop BB34_47 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rdx), %xmm4              # xmm4 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%rdx), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm4, %xmm2
	vmovaps	%xmm4, %xmm3
	vfmsub213ss	%xmm2, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm3) - xmm2
	vfmadd231ss	%xmm1, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm1) + xmm3
	vfmadd231ss	%xmm1, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm1) + xmm3
	vaddss	%xmm3, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm5, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm4
	vsubss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm3, %xmm4, %xmm4
	vmovshdup	%xmm0, %xmm3            # xmm3 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm4, %xmm1, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	8(%rbx,%rdx), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm2, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vmovss	12(%rbx,%rdx), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vfmadd231ss	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vaddss	%xmm4, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm2, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm1, %xmm1
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	16(%rbx,%rdx), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm2, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vmovss	20(%rbx,%rdx), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vfmadd231ss	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vaddss	%xmm4, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm2, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm1, %xmm1
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	24(%rbx,%rdx), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm2, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vmovss	28(%rbx,%rdx), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vfmadd231ss	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vaddss	%xmm4, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm2, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm1, %xmm1
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	addq	$32, %rdx
	cmpq	%rdx, %rcx
	jne	.LBB34_50
.LBB34_51:                              #   in Loop: Header=BB34_47 Depth=1
	movl	%eax, %ecx
	andl	$-4, %ecx
	cmpq	%rax, %rcx
	jae	.LBB34_53
	.p2align	4, 0x90
.LBB34_52:                              #   Parent Loop BB34_47 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rcx,8), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%rcx,8), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm4, %xmm2
	vmovaps	%xmm4, %xmm3
	vfmsub213ss	%xmm2, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm3) - xmm2
	vfmadd231ss	%xmm1, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm1) + xmm3
	vfmadd231ss	%xmm1, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm1) + xmm3
	vaddss	%xmm3, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm5, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm4
	vsubss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm3, %xmm4, %xmm4
	vmovshdup	%xmm0, %xmm3            # xmm3 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm4, %xmm1, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	incq	%rcx
	cmpq	%rcx, %rax
	jne	.LBB34_52
.LBB34_53:                              #   in Loop: Header=BB34_47 Depth=1
	vucomiss	%xmm7, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB34_46
# %bb.54:                               #   in Loop: Header=BB34_47 Depth=1
	vmovups	%xmm0, 128(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vxorps	%xmm7, %xmm7, %xmm7
	vmovaps	%xmm0, %xmm1
	vmovups	128(%rsp), %xmm2                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vfnmadd213ss	%xmm2, %xmm0, %xmm1     # xmm1 = -(xmm0 * xmm1) + xmm2
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm0, %xmm2
	vdivss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
	jmp	.LBB34_46
.LBB34_55:
	callq	omp_get_wtime
	vsubsd	208(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm8, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	144(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp2439:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp2440:
# %bb.56:
	movq	%rax, %r13
	movq	112(%rsp), %rax                 # 8-byte Reload
	leaq	32(%rax), %r14
.Ltmp2441:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp2442:
# %bb.57:
.Ltmp2443:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp2444:
# %bb.58:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp2445:
	leaq	80(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp2446:
# %bb.59:
.Ltmp2447:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2448:
# %bb.60:
.Ltmp2450:
	callq	mpfr_get_default_rounding_mode
.Ltmp2451:
# %bb.61:
.Ltmp2452:
	leaq	80(%rsp), %rdi
	leaq	144(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp2453:
# %bb.62:
.Ltmp2455:
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2456:
# %bb.63:
.Ltmp2457:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp2458:
# %bb.64:
.Ltmp2459:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp2460:
# %bb.65:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp2461:
	leaq	8(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp2462:
# %bb.66:
.Ltmp2463:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2464:
# %bb.67:
.Ltmp2466:
	callq	mpfr_get_default_rounding_mode
.Ltmp2467:
# %bb.68:
.Ltmp2468:
	leaq	8(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp2469:
# %bb.69:
.Ltmp2471:
	callq	mpfr_get_default_rounding_mode
.Ltmp2472:
# %bb.70:
.Ltmp2473:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2474:
# %bb.71:
.Ltmp2475:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2476:
# %bb.72:
.Ltmp2477:
	movl	%eax, %r12d
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2478:
# %bb.73:
.Ltmp2479:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2480:
# %bb.74:
.Ltmp2482:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp2483:
# %bb.75:
.Ltmp2485:
	leaq	40(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp2486:
# %bb.76:
	vmovlpd	%xmm0, 200(%rsp)
	cmpq	$0, 64(%rsp)
	je	.LBB34_78
# %bb.77:
.Ltmp2488:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2489:
.LBB34_78:
	cmpq	$0, 32(%rsp)
	je	.LBB34_80
# %bb.79:
.Ltmp2491:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2492:
.LBB34_80:
	cmpq	$0, 104(%rsp)
	je	.LBB34_82
# %bb.81:
.Ltmp2494:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2495:
.LBB34_82:
	cmpq	$0, 168(%rsp)
	je	.LBB34_84
# %bb.83:
.Ltmp2497:
	leaq	144(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2498:
.LBB34_84:
	leaq	360(%rsp), %r15
	movq	%r15, 344(%rsp)
	movl	$846033518, 360(%rsp)           # imm = 0x326D726E
	movw	$32, 364(%rsp)
	movq	$5, 352(%rsp)
.Ltmp2500:
	leaq	344(%rsp), %rdi
	leaq	200(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp2501:
# %bb.85:
	movq	344(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB34_87
# %bb.86:
	callq	_ZdlPv
.LBB34_87:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$80, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, 64(%rax)
	vzeroupper
	callq	omp_get_wtime
	vxorps	%xmm8, %xmm8, %xmm8
	vmovsd	%xmm0, 128(%rsp)                # 8-byte Spill
	movq	176(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	movl	%eax, %ecx
	andl	$-4, %ecx
	leaq	(,%rax,8), %rdx
	andq	$-32, %rdx
	xorl	%esi, %esi
	vxorpd	%xmm0, %xmm0, %xmm0
	vbroadcastss	.LCPI34_0(%rip), %xmm1  # xmm1 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	xorl	%edi, %edi
	jmp	.LBB34_89
	.p2align	4, 0x90
.LBB34_88:                              #   in Loop: Header=BB34_89 Depth=1
	vmovlps	%xmm2, (%rbp,%rdi,8)
	leaq	1(%rdi), %r8
	cmpq	$9, %rdi
	movq	%r8, %rdi
	je	.LBB34_111
.LBB34_89:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB34_100 Depth 2
                                        #     Child Loop BB34_95 Depth 2
	vcvtsi2ss	%esi, %xmm9, %xmm2
	vblendps	$1, %xmm2, %xmm8, %xmm2         # xmm2 = xmm2[0],xmm8[1,2,3]
	testl	%eax, %eax
	jle	.LBB34_88
# %bb.90:                               #   in Loop: Header=BB34_89 Depth=1
	cmpl	$4, %eax
	jae	.LBB34_97
.LBB34_91:                              #   in Loop: Header=BB34_89 Depth=1
	cmpq	%rax, %rcx
	jae	.LBB34_88
# %bb.92:                               #   in Loop: Header=BB34_89 Depth=1
	movq	%rcx, %r8
	jmp	.LBB34_95
	.p2align	4, 0x90
.LBB34_93:                              #   in Loop: Header=BB34_95 Depth=2
	vmovsd	(%rbx,%r8,8), %xmm3             # xmm3 = mem[0],zero
.LBB34_94:                              #   in Loop: Header=BB34_95 Depth=2
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm6
	vsubss	%xmm5, %xmm3, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vmovshdup	%xmm3, %xmm3            # xmm3 = xmm3[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vinsertps	$16, %xmm2, %xmm3, %xmm2 # xmm2 = xmm3[0],xmm2[0],xmm3[2,3]
	incq	%r8
	cmpq	%r8, %rax
	je	.LBB34_88
.LBB34_95:                              #   Parent Loop BB34_89 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%r8,8), %xmm3             # xmm3 = mem[0],zero,zero,zero
	vcomiss	%xmm3, %xmm0
	jbe	.LBB34_93
# %bb.96:                               #   in Loop: Header=BB34_95 Depth=2
	vinsertps	$16, 4(%rbx,%r8,8), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vxorps	%xmm1, %xmm3, %xmm3
	jmp	.LBB34_94
	.p2align	4, 0x90
.LBB34_97:                              #   in Loop: Header=BB34_89 Depth=1
	xorl	%r8d, %r8d
	jmp	.LBB34_100
	.p2align	4, 0x90
.LBB34_98:                              #   in Loop: Header=BB34_100 Depth=2
	vmovsd	24(%rbx,%r8), %xmm4             # xmm4 = mem[0],zero
.LBB34_99:                              #   in Loop: Header=BB34_100 Depth=2
	vaddss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm6, %xmm4, %xmm6
	vaddss	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm4, %xmm4            # xmm4 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm3
	vsubss	%xmm3, %xmm5, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vinsertps	$16, %xmm2, %xmm3, %xmm2 # xmm2 = xmm3[0],xmm2[0],xmm3[2,3]
	addq	$32, %r8
	cmpq	%r8, %rdx
	je	.LBB34_91
.LBB34_100:                             #   Parent Loop BB34_89 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%r8), %xmm3               # xmm3 = mem[0],zero,zero,zero
	vcomiss	%xmm3, %xmm0
	jbe	.LBB34_102
# %bb.101:                              #   in Loop: Header=BB34_100 Depth=2
	vinsertps	$16, 4(%rbx,%r8), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vxorps	%xmm1, %xmm3, %xmm3
	jmp	.LBB34_103
	.p2align	4, 0x90
.LBB34_102:                             #   in Loop: Header=BB34_100 Depth=2
	vmovsd	(%rbx,%r8), %xmm3               # xmm3 = mem[0],zero
.LBB34_103:                             #   in Loop: Header=BB34_100 Depth=2
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm6
	vsubss	%xmm5, %xmm3, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vmovshdup	%xmm3, %xmm3            # xmm3 = xmm3[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vmovss	8(%rbx,%r8), %xmm4              # xmm4 = mem[0],zero,zero,zero
	vcomiss	%xmm4, %xmm0
	jbe	.LBB34_105
# %bb.104:                              #   in Loop: Header=BB34_100 Depth=2
	vinsertps	$16, 12(%rbx,%r8), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vxorps	%xmm1, %xmm4, %xmm4
	jmp	.LBB34_106
	.p2align	4, 0x90
.LBB34_105:                             #   in Loop: Header=BB34_100 Depth=2
	vmovsd	8(%rbx,%r8), %xmm4              # xmm4 = mem[0],zero
.LBB34_106:                             #   in Loop: Header=BB34_100 Depth=2
	vaddss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm6, %xmm4, %xmm6
	vaddss	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm4, %xmm4            # xmm4 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm3
	vsubss	%xmm3, %xmm5, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vmovss	16(%rbx,%r8), %xmm4             # xmm4 = mem[0],zero,zero,zero
	vcomiss	%xmm4, %xmm0
	jbe	.LBB34_108
# %bb.107:                              #   in Loop: Header=BB34_100 Depth=2
	vinsertps	$16, 20(%rbx,%r8), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vxorps	%xmm1, %xmm4, %xmm4
	jmp	.LBB34_109
	.p2align	4, 0x90
.LBB34_108:                             #   in Loop: Header=BB34_100 Depth=2
	vmovsd	16(%rbx,%r8), %xmm4             # xmm4 = mem[0],zero
.LBB34_109:                             #   in Loop: Header=BB34_100 Depth=2
	vaddss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm6, %xmm4, %xmm6
	vaddss	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm4, %xmm4            # xmm4 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm3
	vsubss	%xmm3, %xmm5, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vmovss	24(%rbx,%r8), %xmm4             # xmm4 = mem[0],zero,zero,zero
	vcomiss	%xmm4, %xmm0
	jbe	.LBB34_98
# %bb.110:                              #   in Loop: Header=BB34_100 Depth=2
	vinsertps	$16, 28(%rbx,%r8), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vxorps	%xmm1, %xmm4, %xmm4
	jmp	.LBB34_99
.LBB34_111:
	callq	omp_get_wtime
	vsubsd	128(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm9, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	144(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp2503:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp2504:
# %bb.112:
	movq	%rax, %r13
	movq	112(%rsp), %rax                 # 8-byte Reload
	leaq	64(%rax), %r14
.Ltmp2505:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp2506:
# %bb.113:
.Ltmp2507:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp2508:
# %bb.114:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp2509:
	leaq	80(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp2510:
# %bb.115:
.Ltmp2511:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2512:
# %bb.116:
.Ltmp2514:
	callq	mpfr_get_default_rounding_mode
.Ltmp2515:
# %bb.117:
.Ltmp2516:
	leaq	80(%rsp), %rdi
	leaq	144(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp2517:
# %bb.118:
.Ltmp2519:
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2520:
# %bb.119:
.Ltmp2521:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp2522:
# %bb.120:
.Ltmp2523:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp2524:
# %bb.121:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp2525:
	leaq	8(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp2526:
# %bb.122:
.Ltmp2527:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2528:
# %bb.123:
.Ltmp2530:
	callq	mpfr_get_default_rounding_mode
.Ltmp2531:
# %bb.124:
.Ltmp2532:
	leaq	8(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp2533:
# %bb.125:
.Ltmp2535:
	callq	mpfr_get_default_rounding_mode
.Ltmp2536:
# %bb.126:
.Ltmp2537:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2538:
# %bb.127:
.Ltmp2539:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2540:
# %bb.128:
.Ltmp2541:
	movl	%eax, %r12d
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2542:
# %bb.129:
.Ltmp2543:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2544:
# %bb.130:
.Ltmp2546:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp2547:
# %bb.131:
.Ltmp2549:
	leaq	40(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp2550:
# %bb.132:
	vmovlpd	%xmm0, 200(%rsp)
	cmpq	$0, 64(%rsp)
	je	.LBB34_134
# %bb.133:
.Ltmp2552:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2553:
.LBB34_134:
	cmpq	$0, 32(%rsp)
	je	.LBB34_136
# %bb.135:
.Ltmp2555:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2556:
.LBB34_136:
	cmpq	$0, 104(%rsp)
	je	.LBB34_138
# %bb.137:
.Ltmp2558:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2559:
.LBB34_138:
	cmpq	$0, 168(%rsp)
	je	.LBB34_140
# %bb.139:
.Ltmp2561:
	leaq	144(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2562:
.LBB34_140:
	leaq	328(%rsp), %r15
	movq	%r15, 312(%rsp)
	movl	$1836413793, 328(%rsp)          # imm = 0x6D757361
	movw	$32, 332(%rsp)
	movq	$5, 320(%rsp)
.Ltmp2564:
	leaq	312(%rsp), %rdi
	leaq	200(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp2565:
# %bb.141:
	movq	312(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB34_143
# %bb.142:
	callq	_ZdlPv
.LBB34_143:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movq	176(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	movq	120(%rsp), %rdx                 # 8-byte Reload
	jle	.LBB34_146
# %bb.144:
	vmovss	72(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vmovss	76(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB34_145:                             # =>This Inner Loop Header: Depth=1
	vmovss	(%rbx,%rcx,8), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm0, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm4) - xmm3
	vfmadd231ss	%xmm2, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm2) + xmm4
	vfmadd231ss	4(%rbx,%rcx,8), %xmm0, %xmm4 # xmm4 = (xmm0 * mem) + xmm4
	vaddss	%xmm4, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vmovss	(%rdx,%rcx,8), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	4(%rdx,%rcx,8), %xmm3, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm2, %xmm5, %xmm3
	vsubss	%xmm3, %xmm5, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vinsertps	$16, %xmm2, %xmm3, %xmm2 # xmm2 = xmm3[0],xmm2[0],xmm3[2,3]
	vmovlps	%xmm2, (%rdx,%rcx,8)
	incq	%rcx
	cmpq	%rcx, %rax
	jne	.LBB34_145
.LBB34_146:
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	40(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	movq	176(%rsp), %rax                 # 8-byte Reload
	cmpl	$0, (%rax)
	jle	.LBB34_187
# %bb.147:
	movq	112(%rsp), %rax                 # 8-byte Reload
	leaq	256(%rax), %r15
	xorl	%r14d, %r14d
	movq	120(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB34_149
	.p2align	4, 0x90
.LBB34_148:                             #   in Loop: Header=BB34_149 Depth=1
	incq	%r14
	movq	176(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r15
	movq	128(%rsp), %rsi                 # 8-byte Reload
	addq	$8, %rsi
	cmpq	%rax, %r14
	jge	.LBB34_187
.LBB34_149:                             # =>This Inner Loop Header: Depth=1
.Ltmp2567:
	leaq	80(%rsp), %rdi
	movq	%rsi, 128(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp2568:
# %bb.150:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2570:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp2571:
# %bb.151:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2572:
	movq	%rax, %rbp
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2573:
# %bb.152:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2574:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2575:
# %bb.153:                              #   in Loop: Header=BB34_149 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp2576:
	leaq	8(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2577:
# %bb.154:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2578:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2579:
# %bb.155:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2581:
	callq	mpfr_get_default_rounding_mode
.Ltmp2582:
# %bb.156:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2583:
	leaq	8(%rsp), %rdi
	movq	%r15, %rsi
	leaq	80(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp2584:
# %bb.157:                              #   in Loop: Header=BB34_149 Depth=1
	cmpq	$0, 104(%rsp)
	je	.LBB34_159
# %bb.158:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2586:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2587:
.LBB34_159:                             #   in Loop: Header=BB34_149 Depth=1
.Ltmp2589:
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2590:
# %bb.160:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2591:
	movq	%rax, %rbp
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2592:
# %bb.161:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2593:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2594:
# %bb.162:                              #   in Loop: Header=BB34_149 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp2595:
	leaq	144(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2596:
# %bb.163:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2597:
	leaq	144(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2598:
# %bb.164:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2600:
	callq	mpfr_get_default_rounding_mode
.Ltmp2601:
# %bb.165:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2602:
	leaq	144(%rsp), %rdi
	leaq	8(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp2603:
# %bb.166:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2605:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2606:
# %bb.167:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2607:
	movq	%rax, %rbp
	leaq	144(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2608:
# %bb.168:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2609:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2610:
# %bb.169:                              #   in Loop: Header=BB34_149 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp2611:
	leaq	80(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2612:
# %bb.170:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2613:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2614:
# %bb.171:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2616:
	callq	mpfr_get_default_rounding_mode
.Ltmp2617:
	leaq	40(%rsp), %r12
# %bb.172:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2618:
	leaq	80(%rsp), %rdi
	movq	%r12, %rsi
	leaq	144(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp2619:
# %bb.173:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2621:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp2622:
# %bb.174:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2623:
	movq	%rax, %r12
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2624:
# %bb.175:                              #   in Loop: Header=BB34_149 Depth=1
	movq	%rax, %rbp
	cmpq	%rax, %r12
	je	.LBB34_179
# %bb.176:                              #   in Loop: Header=BB34_149 Depth=1
	cmpq	$0, 64(%rsp)
	je	.LBB34_178
# %bb.177:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2625:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2626:
.LBB34_178:                             #   in Loop: Header=BB34_149 Depth=1
.Ltmp2627:
	leaq	40(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp2628:
.LBB34_179:                             #   in Loop: Header=BB34_149 Depth=1
.Ltmp2629:
	callq	mpfr_get_default_rounding_mode
.Ltmp2630:
# %bb.180:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2631:
	leaq	40(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp2632:
# %bb.181:                              #   in Loop: Header=BB34_149 Depth=1
	cmpq	$0, 104(%rsp)
	je	.LBB34_183
# %bb.182:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2634:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2635:
.LBB34_183:                             #   in Loop: Header=BB34_149 Depth=1
	cmpq	$0, 168(%rsp)
	je	.LBB34_185
# %bb.184:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2637:
	leaq	144(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2638:
.LBB34_185:                             #   in Loop: Header=BB34_149 Depth=1
	cmpq	$0, 32(%rsp)
	je	.LBB34_148
# %bb.186:                              #   in Loop: Header=BB34_149 Depth=1
.Ltmp2640:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2641:
	jmp	.LBB34_148
.LBB34_187:
.Ltmp2643:
	callq	mpfr_get_default_rounding_mode
.Ltmp2644:
# %bb.188:
.Ltmp2645:
	movl	%eax, %ebp
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2646:
# %bb.189:
.Ltmp2647:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp2648:
# %bb.190:
.Ltmp2649:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp2650:
# %bb.191:
.Ltmp2651:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp2652:
# %bb.192:
.Ltmp2654:
	leaq	8(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp2655:
# %bb.193:
.Ltmp2657:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2658:
# %bb.194:
.Ltmp2659:
	movq	%rax, %r12
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2660:
# %bb.195:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB34_199
# %bb.196:
	cmpq	$0, 64(%rsp)
	je	.LBB34_198
# %bb.197:
.Ltmp2661:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2662:
.LBB34_198:
.Ltmp2663:
	leaq	40(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp2664:
.LBB34_199:
.Ltmp2665:
	callq	mpfr_get_default_rounding_mode
.Ltmp2666:
# %bb.200:
.Ltmp2667:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp2668:
# %bb.201:
	cmpq	$0, 32(%rsp)
	je	.LBB34_203
# %bb.202:
.Ltmp2670:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2671:
.LBB34_203:
	callq	omp_get_wtime
	vmovsd	%xmm0, 128(%rsp)                # 8-byte Spill
	movq	176(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	testq	%rax, %rax
	jle	.LBB34_208
# %bb.204:
	vmovss	72(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vmovss	76(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	xorl	%ecx, %ecx
	movq	120(%rsp), %rsi                 # 8-byte Reload
	.p2align	4, 0x90
.LBB34_205:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB34_206 Depth 2
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB34_206:                             #   Parent Loop BB34_205 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rdx,8), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm0, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm4) - xmm3
	vfmadd231ss	%xmm2, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm2) + xmm4
	vfmadd231ss	4(%rbx,%rdx,8), %xmm0, %xmm4 # xmm4 = (xmm0 * mem) + xmm4
	vaddss	%xmm4, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vmovss	(%rsi,%rdx,8), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	4(%rsi,%rdx,8), %xmm3, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm2, %xmm5, %xmm3
	vsubss	%xmm3, %xmm5, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vinsertps	$16, %xmm2, %xmm3, %xmm2 # xmm2 = xmm3[0],xmm2[0],xmm3[2,3]
	vmovlps	%xmm2, (%rsi,%rdx,8)
	incq	%rdx
	cmpq	%rdx, %rax
	jne	.LBB34_206
# %bb.207:                              #   in Loop: Header=BB34_205 Depth=1
	incl	%ecx
	cmpl	$10, %ecx
	jne	.LBB34_205
.LBB34_208:
	callq	omp_get_wtime
	vsubsd	128(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm8, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp2673:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2674:
# %bb.209:
	movq	%rax, %r12
	movq	112(%rsp), %rax                 # 8-byte Reload
	leaq	96(%rax), %r15
.Ltmp2675:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp2676:
# %bb.210:
.Ltmp2677:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2678:
# %bb.211:
	movl	%eax, %ebp
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp2679:
	leaq	8(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2680:
# %bb.212:
.Ltmp2681:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp2682:
# %bb.213:
.Ltmp2684:
	callq	mpfr_get_default_rounding_mode
.Ltmp2685:
# %bb.214:
.Ltmp2686:
	leaq	8(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movq	%r15, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp2687:
# %bb.215:
.Ltmp2689:
	leaq	8(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp2690:
# %bb.216:
	vmovlpd	%xmm0, 80(%rsp)
	cmpq	$0, 32(%rsp)
	je	.LBB34_218
# %bb.217:
.Ltmp2692:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2693:
.LBB34_218:
	leaq	296(%rsp), %r15
	movq	%r15, 280(%rsp)
	movl	$2037413985, 296(%rsp)          # imm = 0x79707861
	movw	$32, 300(%rsp)
	movq	$5, 288(%rsp)
.Ltmp2695:
	leaq	280(%rsp), %rdi
	leaq	80(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp2696:
# %bb.219:
	movq	280(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB34_221
# %bb.220:
	callq	_ZdlPv
.LBB34_221:
	cmpq	$0, 64(%rsp)
	je	.LBB34_223
# %bb.222:
.Ltmp2698:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2699:
.LBB34_223:
	movslq	4(%rsp), %r12
	leaq	(,%r12,8), %r15
	testq	%r12, %r12
	movq	$-1, %rdi
	cmovnsq	%r15, %rdi
	callq	_Znam
	movq	%rax, %rbp
	testq	%r12, %r12
	movq	192(%rsp), %r13                 # 8-byte Reload
	movq	120(%rsp), %r14                 # 8-byte Reload
	je	.LBB34_227
# %bb.224:
	movq	%rbp, %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_intel_fast_memset@PLT
	testl	%r12d, %r12d
	jle	.LBB34_227
# %bb.225:
	xorl	%r15d, %r15d
	.p2align	4, 0x90
.LBB34_226:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, (%r14,%r15,8)
	vmovlpd	%xmm0, (%rbp,%r15,8)
	incq	%r15
	movslq	4(%rsp), %rax
	addq	$32, %r13
	cmpq	%rax, %r15
	jl	.LBB34_226
.LBB34_227:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%rbp, 192(%rsp)                 # 8-byte Spill
	movq	%rbp, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	40(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	cmpl	$0, 4(%rsp)
	jle	.LBB34_268
# %bb.228:
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	leaq	8(%rsp), %r13
	movq	192(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB34_230
	.p2align	4, 0x90
.LBB34_229:                             #   in Loop: Header=BB34_230 Depth=1
	movq	128(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	movslq	4(%rsp), %rax
	movq	208(%rsp), %rsi                 # 8-byte Reload
	addq	$8, %rsi
	movq	%rdx, %rcx
	movq	%rdx, 128(%rsp)                 # 8-byte Spill
	cmpq	%rax, %rdx
	jge	.LBB34_268
.LBB34_230:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %r14
	movq	176(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %r15d
.Ltmp2701:
	leaq	80(%rsp), %rdi
	movq	%rsi, 208(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp2702:
# %bb.231:                              #   in Loop: Header=BB34_230 Depth=1
	movq	128(%rsp), %rax                 # 8-byte Reload
	addl	%r15d, %eax
	addl	$8, %eax
	movslq	%eax, %rbp
	shlq	$5, %rbp
	addq	112(%rsp), %rbp                 # 8-byte Folded Reload
.Ltmp2704:
	movq	%rbp, %rdi
	callq	mpfr_get_prec
.Ltmp2705:
# %bb.232:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2706:
	movq	%rax, %r15
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2707:
# %bb.233:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2708:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp2709:
# %bb.234:                              #   in Loop: Header=BB34_230 Depth=1
	movl	%eax, %r13d
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp2710:
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp2711:
# %bb.235:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2712:
	movq	%r14, %rdi
	xorl	%esi, %esi
	movl	%r13d, %edx
	callq	mpfr_set_si
.Ltmp2713:
# %bb.236:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2715:
	callq	mpfr_get_default_rounding_mode
.Ltmp2716:
# %bb.237:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2717:
	movq	%r14, %r13
	movq	%r14, %rdi
	movq	%rbp, %rsi
	leaq	80(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp2718:
# %bb.238:                              #   in Loop: Header=BB34_230 Depth=1
	cmpq	$0, 104(%rsp)
	je	.LBB34_240
# %bb.239:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2720:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2721:
.LBB34_240:                             #   in Loop: Header=BB34_230 Depth=1
.Ltmp2723:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp2724:
# %bb.241:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2725:
	movq	%rax, %r15
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp2726:
# %bb.242:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2727:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp2728:
# %bb.243:                              #   in Loop: Header=BB34_230 Depth=1
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp2729:
	leaq	144(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp2730:
# %bb.244:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2731:
	leaq	144(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp2732:
# %bb.245:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2734:
	callq	mpfr_get_default_rounding_mode
.Ltmp2735:
# %bb.246:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2736:
	leaq	144(%rsp), %rdi
	movq	%r13, %rsi
	movq	%r13, %rdx
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp2737:
# %bb.247:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2739:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2740:
# %bb.248:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2741:
	movq	%rax, %r15
	leaq	144(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2742:
# %bb.249:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2743:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp2744:
# %bb.250:                              #   in Loop: Header=BB34_230 Depth=1
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp2745:
	leaq	80(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp2746:
# %bb.251:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2747:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp2748:
# %bb.252:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2750:
	callq	mpfr_get_default_rounding_mode
.Ltmp2751:
	leaq	40(%rsp), %r14
# %bb.253:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2752:
	leaq	80(%rsp), %rdi
	movq	%r14, %rsi
	leaq	144(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp2753:
# %bb.254:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2755:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp2756:
# %bb.255:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2757:
	movq	%rax, %r15
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2758:
# %bb.256:                              #   in Loop: Header=BB34_230 Depth=1
	movq	%rax, %rbp
	cmpq	%rax, %r15
	je	.LBB34_260
# %bb.257:                              #   in Loop: Header=BB34_230 Depth=1
	cmpq	$0, 64(%rsp)
	je	.LBB34_259
# %bb.258:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2759:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2760:
.LBB34_259:                             #   in Loop: Header=BB34_230 Depth=1
.Ltmp2761:
	leaq	40(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp2762:
.LBB34_260:                             #   in Loop: Header=BB34_230 Depth=1
.Ltmp2763:
	callq	mpfr_get_default_rounding_mode
.Ltmp2764:
# %bb.261:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2765:
	leaq	40(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp2766:
# %bb.262:                              #   in Loop: Header=BB34_230 Depth=1
	cmpq	$0, 104(%rsp)
	je	.LBB34_264
# %bb.263:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2768:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2769:
.LBB34_264:                             #   in Loop: Header=BB34_230 Depth=1
	cmpq	$0, 168(%rsp)
	je	.LBB34_266
# %bb.265:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2771:
	leaq	144(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2772:
.LBB34_266:                             #   in Loop: Header=BB34_230 Depth=1
	cmpq	$0, 32(%rsp)
	je	.LBB34_229
# %bb.267:                              #   in Loop: Header=BB34_230 Depth=1
.Ltmp2774:
	movq	%r13, %rdi
	callq	mpfr_clear
.Ltmp2775:
	jmp	.LBB34_229
.LBB34_268:
.Ltmp2777:
	callq	mpfr_get_default_rounding_mode
.Ltmp2778:
# %bb.269:
.Ltmp2779:
	movl	%eax, %ebp
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2780:
# %bb.270:
.Ltmp2781:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp2782:
# %bb.271:
.Ltmp2783:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp2784:
# %bb.272:
.Ltmp2785:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp2786:
# %bb.273:
.Ltmp2788:
	leaq	8(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp2789:
# %bb.274:
.Ltmp2791:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2792:
# %bb.275:
.Ltmp2793:
	movq	%rax, %r12
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2794:
# %bb.276:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB34_280
# %bb.277:
	cmpq	$0, 64(%rsp)
	je	.LBB34_279
# %bb.278:
.Ltmp2795:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2796:
.LBB34_279:
.Ltmp2797:
	leaq	40(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp2798:
.LBB34_280:
.Ltmp2799:
	callq	mpfr_get_default_rounding_mode
.Ltmp2800:
# %bb.281:
.Ltmp2801:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp2802:
# %bb.282:
	cmpq	$0, 32(%rsp)
	je	.LBB34_284
# %bb.283:
.Ltmp2804:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2805:
.LBB34_284:
	callq	omp_get_wtime
	vmovsd	%xmm0, 176(%rsp)                # 8-byte Spill
.Ltmp2807:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	120(%rsp), %r14                 # 8-byte Reload
	movq	%r14, %r8
	movq	192(%rsp), %r15                 # 8-byte Reload
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp2808:
# %bb.285:
.Ltmp2809:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp2810:
# %bb.286:
.Ltmp2811:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp2812:
# %bb.287:
.Ltmp2813:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp2814:
# %bb.288:
.Ltmp2815:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp2816:
# %bb.289:
.Ltmp2817:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp2818:
# %bb.290:
.Ltmp2819:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp2820:
# %bb.291:
.Ltmp2821:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp2822:
# %bb.292:
.Ltmp2823:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp2824:
# %bb.293:
.Ltmp2825:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp2826:
# %bb.294:
	callq	omp_get_wtime
	vsubsd	176(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp2828:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2829:
# %bb.295:
	movq	%rax, %r15
	movq	112(%rsp), %rdi                 # 8-byte Reload
	subq	$-128, %rdi
.Ltmp2830:
	movq	%rdi, %r13
	callq	mpfr_get_prec
.Ltmp2831:
# %bb.296:
.Ltmp2832:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp2833:
# %bb.297:
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp2834:
	leaq	8(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp2835:
# %bb.298:
.Ltmp2836:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp2837:
# %bb.299:
.Ltmp2839:
	callq	mpfr_get_default_rounding_mode
.Ltmp2840:
# %bb.300:
.Ltmp2841:
	leaq	8(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movq	%r13, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp2842:
# %bb.301:
.Ltmp2844:
	leaq	8(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp2845:
# %bb.302:
	vmovlpd	%xmm0, 80(%rsp)
	cmpq	$0, 32(%rsp)
	je	.LBB34_304
# %bb.303:
.Ltmp2847:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2848:
.LBB34_304:
	leaq	264(%rsp), %r15
	movq	%r15, 248(%rsp)
	movl	$1986880871, 264(%rsp)          # imm = 0x766D6567
	movw	$32, 268(%rsp)
	movq	$5, 256(%rsp)
.Ltmp2850:
	leaq	248(%rsp), %rdi
	leaq	80(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp2851:
# %bb.305:
	movq	248(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB34_307
# %bb.306:
	callq	_ZdlPv
.LBB34_307:
	movq	192(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	cmpq	$0, 64(%rsp)
	je	.LBB34_309
# %bb.308:
.Ltmp2853:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2854:
.LBB34_309:
	movq	%rbx, %rdi
	callq	_ZdaPv
	movq	120(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	xorl	%eax, %eax
	vcvtsi2ss	%eax, %xmm2, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vblendps	$1, %xmm0, %xmm1, %xmm2         # xmm2 = xmm0[0],xmm1[1,2,3]
	movl	$2097152, %eax                  # imm = 0x200000
	vxorps	%xmm0, %xmm0, %xmm0
	movl	$1, %ecx
	.p2align	4, 0x90
.LBB34_310:                             # =>This Inner Loop Header: Depth=1
	vcvtsi2ss	%eax, %xmm8, %xmm1
	vmulss	%xmm1, %xmm1, %xmm3
	vmovaps	%xmm1, %xmm4
	vfmsub213ss	%xmm3, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm4) - xmm3
	vfmadd231ss	%xmm0, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm0) + xmm4
	vfmadd231ss	%xmm1, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm1) + xmm4
	vaddss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm1
	vaddss	%xmm4, %xmm1, %xmm3
	vcvtsi2ss	%ecx, %xmm8, %xmm1
	vmulss	%xmm5, %xmm5, %xmm4
	vmovaps	%xmm5, %xmm6
	vfmsub213ss	%xmm4, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm6) - xmm4
	vfmadd231ss	%xmm3, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm3) + xmm6
	vfmadd231ss	%xmm3, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm3) + xmm6
	vaddss	%xmm6, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm6, %xmm4, %xmm4
	vdivss	%xmm3, %xmm1, %xmm5
	vmovaps	%xmm5, %xmm6
	vfnmadd213ss	%xmm1, %xmm3, %xmm6     # xmm6 = -(xmm3 * xmm6) + xmm1
	vaddss	%xmm0, %xmm6, %xmm6
	vfnmadd231ss	%xmm4, %xmm5, %xmm6     # xmm6 = -(xmm5 * xmm4) + xmm6
	vaddss	%xmm4, %xmm3, %xmm3
	vdivss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm3, %xmm5, %xmm4
	vsubss	%xmm4, %xmm5, %xmm5
	vaddss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm2, %xmm7
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm3
	vaddss	%xmm3, %xmm5, %xmm2
	leal	-1(%rax), %edx
	vcvtsi2ss	%edx, %xmm8, %xmm4
	vsubss	%xmm2, %xmm5, %xmm5
	vaddss	%xmm3, %xmm5, %xmm3
	vmulss	%xmm4, %xmm4, %xmm5
	vmovaps	%xmm4, %xmm6
	vfmsub213ss	%xmm5, %xmm4, %xmm6     # xmm6 = (xmm4 * xmm6) - xmm5
	vfmadd231ss	%xmm0, %xmm4, %xmm6     # xmm6 = (xmm4 * xmm0) + xmm6
	vfmadd231ss	%xmm4, %xmm0, %xmm6     # xmm6 = (xmm0 * xmm4) + xmm6
	vaddss	%xmm6, %xmm5, %xmm4
	vsubss	%xmm4, %xmm5, %xmm5
	vaddss	%xmm6, %xmm5, %xmm5
	vmulss	%xmm4, %xmm4, %xmm6
	vmovaps	%xmm4, %xmm7
	vfmsub213ss	%xmm6, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm7) - xmm6
	vfmadd231ss	%xmm5, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm5) + xmm7
	vfmadd231ss	%xmm5, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm5) + xmm7
	vaddss	%xmm7, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm5
	vaddss	%xmm7, %xmm5, %xmm5
	vdivss	%xmm4, %xmm1, %xmm6
	vfnmadd231ss	%xmm6, %xmm4, %xmm1     # xmm1 = -(xmm4 * xmm6) + xmm1
	vaddss	%xmm0, %xmm1, %xmm1
	vfnmadd231ss	%xmm5, %xmm6, %xmm1     # xmm1 = -(xmm6 * xmm5) + xmm1
	vaddss	%xmm5, %xmm4, %xmm4
	vdivss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm1, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm5
	vaddss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm3
	vsubss	%xmm3, %xmm5, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vinsertps	$16, %xmm1, %xmm3, %xmm2 # xmm2 = xmm3[0],xmm1[0],xmm3[2,3]
	addl	$-2, %eax
	jne	.LBB34_310
# %bb.311:
	movl	$90, %eax
	vcvtsi2ss	%eax, %xmm8, %xmm0
	vmulss	%xmm3, %xmm0, %xmm2
	vfmsub213ss	%xmm2, %xmm0, %xmm3     # xmm3 = (xmm0 * xmm3) - xmm2
	vfmadd213ss	%xmm3, %xmm0, %xmm1     # xmm1 = (xmm0 * xmm1) + xmm3
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm3
	vxorps	%xmm2, %xmm2, %xmm2
	vucomiss	%xmm2, %xmm0
	vaddss	%xmm1, %xmm3, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	je	.LBB34_313
# %bb.312:
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	jmp	.LBB34_314
.LBB34_313:
	vmovups	%xmm0, 128(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm1, 176(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovups	128(%rsp), %xmm1                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vfnmadd231ss	%xmm0, %xmm0, %xmm1     # xmm1 = -(xmm0 * xmm0) + xmm1
	vaddss	176(%rsp), %xmm1, %xmm1         # 16-byte Folded Reload
	vaddss	%xmm0, %xmm0, %xmm2
	vdivss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
	vxorps	%xmm2, %xmm2, %xmm2
.LBB34_314:
	vucomiss	%xmm2, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB34_316
# %bb.315:
	vmovups	%xmm0, 176(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovaps	%xmm0, %xmm1
	vmovups	176(%rsp), %xmm2                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vfnmadd213ss	%xmm2, %xmm0, %xmm1     # xmm1 = -(xmm0 * xmm1) + xmm2
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm0, %xmm2
	vdivss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
.LBB34_316:
	vmovlps	%xmm0, 8(%rsp)
	leaq	232(%rsp), %r14
	movq	%r14, 216(%rsp)
	movq	$32, 40(%rsp)
.Ltmp2856:
	leaq	216(%rsp), %rdi
	leaq	40(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp2857:
# %bb.317:
	movq	%rax, 216(%rsp)
	movq	40(%rsp), %rcx
	movq	%rcx, 232(%rsp)
	vmovupd	.L.str.63(%rip), %ymm0
	vmovupd	%ymm0, (%rax)
	movq	%rcx, 224(%rsp)
	movq	216(%rsp), %rax
	movb	$0, (%rax,%rcx)
.Ltmp2859:
	leaq	216(%rsp), %rdi
	leaq	8(%rsp), %rsi
	xorl	%edx, %edx
	vzeroupper
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp2860:
# %bb.318:
	movq	216(%rsp), %rdi
	cmpq	%r14, %rdi
	je	.LBB34_320
# %bb.319:
	callq	_ZdlPv
.LBB34_320:
	addq	$408, %rsp                      # imm = 0x198
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB34_321:
	.cfi_def_cfa_offset 464
.Ltmp2855:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_322:
.Ltmp2849:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_323:
.Ltmp2806:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_324:
.Ltmp2700:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_325:
.Ltmp2694:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_326:
.Ltmp2672:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_327:
.Ltmp2563:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_328:
.Ltmp2560:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_329:
.Ltmp2557:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_330:
.Ltmp2554:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_331:
.Ltmp2499:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_332:
.Ltmp2496:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_333:
.Ltmp2493:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_334:
.Ltmp2490:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_335:
.Ltmp2435:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_336:
.Ltmp2432:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_337:
.Ltmp2429:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_338:
.Ltmp2426:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_339:
.Ltmp2861:
	movq	%rax, %rbx
	movq	216(%rsp), %rdi
	cmpq	%r14, %rdi
	jne	.LBB34_357
	jmp	.LBB34_358
.LBB34_340:
.Ltmp2858:
	movq	%rax, %rdi
	callq	_Unwind_Resume@PLT
.LBB34_341:
.Ltmp2852:
	movq	%rax, %rbx
	movq	248(%rsp), %rdi
	jmp	.LBB34_345
.LBB34_342:
.Ltmp2846:
	movq	%rax, %rbx
	jmp	.LBB34_416
.LBB34_343:
.Ltmp2790:
	movq	%rax, %rbx
	jmp	.LBB34_416
.LBB34_344:
.Ltmp2697:
	movq	%rax, %rbx
	movq	280(%rsp), %rdi
.LBB34_345:
	cmpq	%r15, %rdi
	je	.LBB34_422
# %bb.346:
	callq	_ZdlPv
	jmp	.LBB34_422
.LBB34_347:
.Ltmp2691:
	movq	%rax, %rbx
	jmp	.LBB34_416
.LBB34_348:
.Ltmp2656:
	movq	%rax, %rbx
	jmp	.LBB34_416
.LBB34_349:
.Ltmp2566:
	movq	%rax, %rbx
	movq	312(%rsp), %rdi
	jmp	.LBB34_356
.LBB34_350:
.Ltmp2551:
	jmp	.LBB34_361
.LBB34_351:
.Ltmp2548:
	jmp	.LBB34_361
.LBB34_352:
.Ltmp2502:
	movq	%rax, %rbx
	movq	344(%rsp), %rdi
	jmp	.LBB34_356
.LBB34_353:
.Ltmp2487:
	jmp	.LBB34_361
.LBB34_354:
.Ltmp2484:
	jmp	.LBB34_361
.LBB34_355:
.Ltmp2438:
	movq	%rax, %rbx
	movq	376(%rsp), %rdi
.LBB34_356:
	cmpq	%r15, %rdi
	je	.LBB34_358
.LBB34_357:
	callq	_ZdlPv
.LBB34_358:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB34_359:
.Ltmp2423:
	jmp	.LBB34_361
.LBB34_360:
.Ltmp2420:
.LBB34_361:
	movq	%rax, %rbx
	leaq	40(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB34_384
.LBB34_362:
.Ltmp2843:
	movq	%rax, %rbx
	jmp	.LBB34_416
.LBB34_363:
.Ltmp2688:
	movq	%rax, %rbx
	jmp	.LBB34_416
.LBB34_364:
.Ltmp2534:
	jmp	.LBB34_383
.LBB34_365:
.Ltmp2518:
	movq	%rax, %rbx
	jmp	.LBB34_385
.LBB34_366:
.Ltmp2470:
	jmp	.LBB34_383
.LBB34_367:
.Ltmp2454:
	movq	%rax, %rbx
	jmp	.LBB34_385
.LBB34_368:
.Ltmp2406:
	jmp	.LBB34_383
.LBB34_369:
.Ltmp2390:
	movq	%rax, %rbx
	jmp	.LBB34_385
.LBB34_370:
.Ltmp2803:
	movq	%rax, %rbx
	jmp	.LBB34_416
.LBB34_371:
.Ltmp2669:
	movq	%rax, %rbx
	jmp	.LBB34_416
.LBB34_372:
.Ltmp2838:
	movq	%rax, %rbx
	jmp	.LBB34_422
.LBB34_373:
.Ltmp2787:
	movq	%rax, %rbx
	jmp	.LBB34_422
.LBB34_374:
.Ltmp2683:
	movq	%rax, %rbx
	jmp	.LBB34_422
.LBB34_375:
.Ltmp2653:
	movq	%rax, %rbx
	jmp	.LBB34_422
.LBB34_376:
.Ltmp2545:
	jmp	.LBB34_383
.LBB34_377:
.Ltmp2529:
	movq	%rax, %rbx
	jmp	.LBB34_385
.LBB34_378:
.Ltmp2513:
	jmp	.LBB34_388
.LBB34_379:
.Ltmp2481:
	jmp	.LBB34_383
.LBB34_380:
.Ltmp2465:
	movq	%rax, %rbx
	jmp	.LBB34_385
.LBB34_381:
.Ltmp2449:
	jmp	.LBB34_388
.LBB34_382:
.Ltmp2417:
.LBB34_383:
	movq	%rax, %rbx
.LBB34_384:
	leaq	8(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB34_385:
	leaq	80(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	leaq	144(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB34_386:
.Ltmp2401:
	movq	%rax, %rbx
	jmp	.LBB34_385
.LBB34_387:
.Ltmp2385:
.LBB34_388:
	movq	%rax, %rbx
	leaq	144(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB34_389:
.Ltmp2827:
	movq	%rax, %rbx
	jmp	.LBB34_422
.LBB34_390:
.Ltmp2776:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_391:
.Ltmp2773:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_392:
.Ltmp2770:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_393:
.Ltmp2722:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_394:
.Ltmp2642:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_395:
.Ltmp2639:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_396:
.Ltmp2636:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_397:
.Ltmp2588:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB34_398:
.Ltmp2703:
	movq	%rax, %rbx
	jmp	.LBB34_422
.LBB34_399:
.Ltmp2569:
	movq	%rax, %rbx
	jmp	.LBB34_422
.LBB34_400:
.Ltmp2754:
	jmp	.LBB34_409
.LBB34_401:
.Ltmp2738:
	jmp	.LBB34_414
.LBB34_402:
.Ltmp2719:
	jmp	.LBB34_406
.LBB34_403:
.Ltmp2620:
	jmp	.LBB34_409
.LBB34_404:
.Ltmp2604:
	jmp	.LBB34_414
.LBB34_405:
.Ltmp2585:
.LBB34_406:
	movq	%rax, %rbx
	leaq	8(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB34_420
.LBB34_407:
.Ltmp2767:
	jmp	.LBB34_409
.LBB34_408:
.Ltmp2633:
.LBB34_409:
	movq	%rax, %rbx
	leaq	80(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB34_415
.LBB34_410:
.Ltmp2749:
	jmp	.LBB34_414
.LBB34_411:
.Ltmp2733:
	movq	%rax, %rbx
	jmp	.LBB34_416
.LBB34_412:
.Ltmp2714:
	jmp	.LBB34_419
.LBB34_413:
.Ltmp2615:
.LBB34_414:
	movq	%rax, %rbx
.LBB34_415:
	leaq	144(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB34_416:
	leaq	8(%rsp), %rdi
	jmp	.LBB34_421
.LBB34_417:
.Ltmp2599:
	movq	%rax, %rbx
	jmp	.LBB34_416
.LBB34_418:
.Ltmp2580:
.LBB34_419:
	movq	%rax, %rbx
.LBB34_420:
	leaq	80(%rsp), %rdi
.LBB34_421:
	callq	_ZN4mpfr6mprealD2Ev
.LBB34_422:
	leaq	40(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end34:
	.size	_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_, .Lfunc_end34-_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.cfi_endproc
	.section	.gcc_except_table._Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"aG",@progbits,_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.p2align	2, 0x0
GCC_except_table34:
.Lexception27:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase16-.Lttbaseref16
.Lttbaseref16:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end27-.Lcst_begin27
.Lcst_begin27:
	.uleb128 .Lfunc_begin27-.Lfunc_begin27  # >> Call Site 1 <<
	.uleb128 .Ltmp2375-.Lfunc_begin27       #   Call between .Lfunc_begin27 and .Ltmp2375
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2375-.Lfunc_begin27       # >> Call Site 2 <<
	.uleb128 .Ltmp2384-.Ltmp2375            #   Call between .Ltmp2375 and .Ltmp2384
	.uleb128 .Ltmp2385-.Lfunc_begin27       #     jumps to .Ltmp2385
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2386-.Lfunc_begin27       # >> Call Site 3 <<
	.uleb128 .Ltmp2389-.Ltmp2386            #   Call between .Ltmp2386 and .Ltmp2389
	.uleb128 .Ltmp2390-.Lfunc_begin27       #     jumps to .Ltmp2390
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2391-.Lfunc_begin27       # >> Call Site 4 <<
	.uleb128 .Ltmp2400-.Ltmp2391            #   Call between .Ltmp2391 and .Ltmp2400
	.uleb128 .Ltmp2401-.Lfunc_begin27       #     jumps to .Ltmp2401
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2402-.Lfunc_begin27       # >> Call Site 5 <<
	.uleb128 .Ltmp2405-.Ltmp2402            #   Call between .Ltmp2402 and .Ltmp2405
	.uleb128 .Ltmp2406-.Lfunc_begin27       #     jumps to .Ltmp2406
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2407-.Lfunc_begin27       # >> Call Site 6 <<
	.uleb128 .Ltmp2416-.Ltmp2407            #   Call between .Ltmp2407 and .Ltmp2416
	.uleb128 .Ltmp2417-.Lfunc_begin27       #     jumps to .Ltmp2417
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2418-.Lfunc_begin27       # >> Call Site 7 <<
	.uleb128 .Ltmp2419-.Ltmp2418            #   Call between .Ltmp2418 and .Ltmp2419
	.uleb128 .Ltmp2420-.Lfunc_begin27       #     jumps to .Ltmp2420
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2421-.Lfunc_begin27       # >> Call Site 8 <<
	.uleb128 .Ltmp2422-.Ltmp2421            #   Call between .Ltmp2421 and .Ltmp2422
	.uleb128 .Ltmp2423-.Lfunc_begin27       #     jumps to .Ltmp2423
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2424-.Lfunc_begin27       # >> Call Site 9 <<
	.uleb128 .Ltmp2425-.Ltmp2424            #   Call between .Ltmp2424 and .Ltmp2425
	.uleb128 .Ltmp2426-.Lfunc_begin27       #     jumps to .Ltmp2426
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2427-.Lfunc_begin27       # >> Call Site 10 <<
	.uleb128 .Ltmp2428-.Ltmp2427            #   Call between .Ltmp2427 and .Ltmp2428
	.uleb128 .Ltmp2429-.Lfunc_begin27       #     jumps to .Ltmp2429
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2430-.Lfunc_begin27       # >> Call Site 11 <<
	.uleb128 .Ltmp2431-.Ltmp2430            #   Call between .Ltmp2430 and .Ltmp2431
	.uleb128 .Ltmp2432-.Lfunc_begin27       #     jumps to .Ltmp2432
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2433-.Lfunc_begin27       # >> Call Site 12 <<
	.uleb128 .Ltmp2434-.Ltmp2433            #   Call between .Ltmp2433 and .Ltmp2434
	.uleb128 .Ltmp2435-.Lfunc_begin27       #     jumps to .Ltmp2435
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2436-.Lfunc_begin27       # >> Call Site 13 <<
	.uleb128 .Ltmp2437-.Ltmp2436            #   Call between .Ltmp2436 and .Ltmp2437
	.uleb128 .Ltmp2438-.Lfunc_begin27       #     jumps to .Ltmp2438
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2437-.Lfunc_begin27       # >> Call Site 14 <<
	.uleb128 .Ltmp2439-.Ltmp2437            #   Call between .Ltmp2437 and .Ltmp2439
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2439-.Lfunc_begin27       # >> Call Site 15 <<
	.uleb128 .Ltmp2448-.Ltmp2439            #   Call between .Ltmp2439 and .Ltmp2448
	.uleb128 .Ltmp2449-.Lfunc_begin27       #     jumps to .Ltmp2449
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2450-.Lfunc_begin27       # >> Call Site 16 <<
	.uleb128 .Ltmp2453-.Ltmp2450            #   Call between .Ltmp2450 and .Ltmp2453
	.uleb128 .Ltmp2454-.Lfunc_begin27       #     jumps to .Ltmp2454
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2455-.Lfunc_begin27       # >> Call Site 17 <<
	.uleb128 .Ltmp2464-.Ltmp2455            #   Call between .Ltmp2455 and .Ltmp2464
	.uleb128 .Ltmp2465-.Lfunc_begin27       #     jumps to .Ltmp2465
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2466-.Lfunc_begin27       # >> Call Site 18 <<
	.uleb128 .Ltmp2469-.Ltmp2466            #   Call between .Ltmp2466 and .Ltmp2469
	.uleb128 .Ltmp2470-.Lfunc_begin27       #     jumps to .Ltmp2470
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2471-.Lfunc_begin27       # >> Call Site 19 <<
	.uleb128 .Ltmp2480-.Ltmp2471            #   Call between .Ltmp2471 and .Ltmp2480
	.uleb128 .Ltmp2481-.Lfunc_begin27       #     jumps to .Ltmp2481
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2482-.Lfunc_begin27       # >> Call Site 20 <<
	.uleb128 .Ltmp2483-.Ltmp2482            #   Call between .Ltmp2482 and .Ltmp2483
	.uleb128 .Ltmp2484-.Lfunc_begin27       #     jumps to .Ltmp2484
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2485-.Lfunc_begin27       # >> Call Site 21 <<
	.uleb128 .Ltmp2486-.Ltmp2485            #   Call between .Ltmp2485 and .Ltmp2486
	.uleb128 .Ltmp2487-.Lfunc_begin27       #     jumps to .Ltmp2487
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2488-.Lfunc_begin27       # >> Call Site 22 <<
	.uleb128 .Ltmp2489-.Ltmp2488            #   Call between .Ltmp2488 and .Ltmp2489
	.uleb128 .Ltmp2490-.Lfunc_begin27       #     jumps to .Ltmp2490
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2491-.Lfunc_begin27       # >> Call Site 23 <<
	.uleb128 .Ltmp2492-.Ltmp2491            #   Call between .Ltmp2491 and .Ltmp2492
	.uleb128 .Ltmp2493-.Lfunc_begin27       #     jumps to .Ltmp2493
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2494-.Lfunc_begin27       # >> Call Site 24 <<
	.uleb128 .Ltmp2495-.Ltmp2494            #   Call between .Ltmp2494 and .Ltmp2495
	.uleb128 .Ltmp2496-.Lfunc_begin27       #     jumps to .Ltmp2496
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2497-.Lfunc_begin27       # >> Call Site 25 <<
	.uleb128 .Ltmp2498-.Ltmp2497            #   Call between .Ltmp2497 and .Ltmp2498
	.uleb128 .Ltmp2499-.Lfunc_begin27       #     jumps to .Ltmp2499
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2500-.Lfunc_begin27       # >> Call Site 26 <<
	.uleb128 .Ltmp2501-.Ltmp2500            #   Call between .Ltmp2500 and .Ltmp2501
	.uleb128 .Ltmp2502-.Lfunc_begin27       #     jumps to .Ltmp2502
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2501-.Lfunc_begin27       # >> Call Site 27 <<
	.uleb128 .Ltmp2503-.Ltmp2501            #   Call between .Ltmp2501 and .Ltmp2503
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2503-.Lfunc_begin27       # >> Call Site 28 <<
	.uleb128 .Ltmp2512-.Ltmp2503            #   Call between .Ltmp2503 and .Ltmp2512
	.uleb128 .Ltmp2513-.Lfunc_begin27       #     jumps to .Ltmp2513
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2514-.Lfunc_begin27       # >> Call Site 29 <<
	.uleb128 .Ltmp2517-.Ltmp2514            #   Call between .Ltmp2514 and .Ltmp2517
	.uleb128 .Ltmp2518-.Lfunc_begin27       #     jumps to .Ltmp2518
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2519-.Lfunc_begin27       # >> Call Site 30 <<
	.uleb128 .Ltmp2528-.Ltmp2519            #   Call between .Ltmp2519 and .Ltmp2528
	.uleb128 .Ltmp2529-.Lfunc_begin27       #     jumps to .Ltmp2529
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2530-.Lfunc_begin27       # >> Call Site 31 <<
	.uleb128 .Ltmp2533-.Ltmp2530            #   Call between .Ltmp2530 and .Ltmp2533
	.uleb128 .Ltmp2534-.Lfunc_begin27       #     jumps to .Ltmp2534
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2535-.Lfunc_begin27       # >> Call Site 32 <<
	.uleb128 .Ltmp2544-.Ltmp2535            #   Call between .Ltmp2535 and .Ltmp2544
	.uleb128 .Ltmp2545-.Lfunc_begin27       #     jumps to .Ltmp2545
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2546-.Lfunc_begin27       # >> Call Site 33 <<
	.uleb128 .Ltmp2547-.Ltmp2546            #   Call between .Ltmp2546 and .Ltmp2547
	.uleb128 .Ltmp2548-.Lfunc_begin27       #     jumps to .Ltmp2548
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2549-.Lfunc_begin27       # >> Call Site 34 <<
	.uleb128 .Ltmp2550-.Ltmp2549            #   Call between .Ltmp2549 and .Ltmp2550
	.uleb128 .Ltmp2551-.Lfunc_begin27       #     jumps to .Ltmp2551
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2552-.Lfunc_begin27       # >> Call Site 35 <<
	.uleb128 .Ltmp2553-.Ltmp2552            #   Call between .Ltmp2552 and .Ltmp2553
	.uleb128 .Ltmp2554-.Lfunc_begin27       #     jumps to .Ltmp2554
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2555-.Lfunc_begin27       # >> Call Site 36 <<
	.uleb128 .Ltmp2556-.Ltmp2555            #   Call between .Ltmp2555 and .Ltmp2556
	.uleb128 .Ltmp2557-.Lfunc_begin27       #     jumps to .Ltmp2557
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2558-.Lfunc_begin27       # >> Call Site 37 <<
	.uleb128 .Ltmp2559-.Ltmp2558            #   Call between .Ltmp2558 and .Ltmp2559
	.uleb128 .Ltmp2560-.Lfunc_begin27       #     jumps to .Ltmp2560
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2561-.Lfunc_begin27       # >> Call Site 38 <<
	.uleb128 .Ltmp2562-.Ltmp2561            #   Call between .Ltmp2561 and .Ltmp2562
	.uleb128 .Ltmp2563-.Lfunc_begin27       #     jumps to .Ltmp2563
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2564-.Lfunc_begin27       # >> Call Site 39 <<
	.uleb128 .Ltmp2565-.Ltmp2564            #   Call between .Ltmp2564 and .Ltmp2565
	.uleb128 .Ltmp2566-.Lfunc_begin27       #     jumps to .Ltmp2566
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2565-.Lfunc_begin27       # >> Call Site 40 <<
	.uleb128 .Ltmp2567-.Ltmp2565            #   Call between .Ltmp2565 and .Ltmp2567
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2567-.Lfunc_begin27       # >> Call Site 41 <<
	.uleb128 .Ltmp2568-.Ltmp2567            #   Call between .Ltmp2567 and .Ltmp2568
	.uleb128 .Ltmp2569-.Lfunc_begin27       #     jumps to .Ltmp2569
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2570-.Lfunc_begin27       # >> Call Site 42 <<
	.uleb128 .Ltmp2579-.Ltmp2570            #   Call between .Ltmp2570 and .Ltmp2579
	.uleb128 .Ltmp2580-.Lfunc_begin27       #     jumps to .Ltmp2580
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2581-.Lfunc_begin27       # >> Call Site 43 <<
	.uleb128 .Ltmp2584-.Ltmp2581            #   Call between .Ltmp2581 and .Ltmp2584
	.uleb128 .Ltmp2585-.Lfunc_begin27       #     jumps to .Ltmp2585
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2586-.Lfunc_begin27       # >> Call Site 44 <<
	.uleb128 .Ltmp2587-.Ltmp2586            #   Call between .Ltmp2586 and .Ltmp2587
	.uleb128 .Ltmp2588-.Lfunc_begin27       #     jumps to .Ltmp2588
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2589-.Lfunc_begin27       # >> Call Site 45 <<
	.uleb128 .Ltmp2598-.Ltmp2589            #   Call between .Ltmp2589 and .Ltmp2598
	.uleb128 .Ltmp2599-.Lfunc_begin27       #     jumps to .Ltmp2599
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2600-.Lfunc_begin27       # >> Call Site 46 <<
	.uleb128 .Ltmp2603-.Ltmp2600            #   Call between .Ltmp2600 and .Ltmp2603
	.uleb128 .Ltmp2604-.Lfunc_begin27       #     jumps to .Ltmp2604
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2605-.Lfunc_begin27       # >> Call Site 47 <<
	.uleb128 .Ltmp2614-.Ltmp2605            #   Call between .Ltmp2605 and .Ltmp2614
	.uleb128 .Ltmp2615-.Lfunc_begin27       #     jumps to .Ltmp2615
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2616-.Lfunc_begin27       # >> Call Site 48 <<
	.uleb128 .Ltmp2619-.Ltmp2616            #   Call between .Ltmp2616 and .Ltmp2619
	.uleb128 .Ltmp2620-.Lfunc_begin27       #     jumps to .Ltmp2620
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2621-.Lfunc_begin27       # >> Call Site 49 <<
	.uleb128 .Ltmp2632-.Ltmp2621            #   Call between .Ltmp2621 and .Ltmp2632
	.uleb128 .Ltmp2633-.Lfunc_begin27       #     jumps to .Ltmp2633
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2634-.Lfunc_begin27       # >> Call Site 50 <<
	.uleb128 .Ltmp2635-.Ltmp2634            #   Call between .Ltmp2634 and .Ltmp2635
	.uleb128 .Ltmp2636-.Lfunc_begin27       #     jumps to .Ltmp2636
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2637-.Lfunc_begin27       # >> Call Site 51 <<
	.uleb128 .Ltmp2638-.Ltmp2637            #   Call between .Ltmp2637 and .Ltmp2638
	.uleb128 .Ltmp2639-.Lfunc_begin27       #     jumps to .Ltmp2639
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2640-.Lfunc_begin27       # >> Call Site 52 <<
	.uleb128 .Ltmp2641-.Ltmp2640            #   Call between .Ltmp2640 and .Ltmp2641
	.uleb128 .Ltmp2642-.Lfunc_begin27       #     jumps to .Ltmp2642
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2643-.Lfunc_begin27       # >> Call Site 53 <<
	.uleb128 .Ltmp2652-.Ltmp2643            #   Call between .Ltmp2643 and .Ltmp2652
	.uleb128 .Ltmp2653-.Lfunc_begin27       #     jumps to .Ltmp2653
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2654-.Lfunc_begin27       # >> Call Site 54 <<
	.uleb128 .Ltmp2655-.Ltmp2654            #   Call between .Ltmp2654 and .Ltmp2655
	.uleb128 .Ltmp2656-.Lfunc_begin27       #     jumps to .Ltmp2656
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2657-.Lfunc_begin27       # >> Call Site 55 <<
	.uleb128 .Ltmp2668-.Ltmp2657            #   Call between .Ltmp2657 and .Ltmp2668
	.uleb128 .Ltmp2669-.Lfunc_begin27       #     jumps to .Ltmp2669
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2670-.Lfunc_begin27       # >> Call Site 56 <<
	.uleb128 .Ltmp2671-.Ltmp2670            #   Call between .Ltmp2670 and .Ltmp2671
	.uleb128 .Ltmp2672-.Lfunc_begin27       #     jumps to .Ltmp2672
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2673-.Lfunc_begin27       # >> Call Site 57 <<
	.uleb128 .Ltmp2682-.Ltmp2673            #   Call between .Ltmp2673 and .Ltmp2682
	.uleb128 .Ltmp2683-.Lfunc_begin27       #     jumps to .Ltmp2683
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2684-.Lfunc_begin27       # >> Call Site 58 <<
	.uleb128 .Ltmp2687-.Ltmp2684            #   Call between .Ltmp2684 and .Ltmp2687
	.uleb128 .Ltmp2688-.Lfunc_begin27       #     jumps to .Ltmp2688
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2689-.Lfunc_begin27       # >> Call Site 59 <<
	.uleb128 .Ltmp2690-.Ltmp2689            #   Call between .Ltmp2689 and .Ltmp2690
	.uleb128 .Ltmp2691-.Lfunc_begin27       #     jumps to .Ltmp2691
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2692-.Lfunc_begin27       # >> Call Site 60 <<
	.uleb128 .Ltmp2693-.Ltmp2692            #   Call between .Ltmp2692 and .Ltmp2693
	.uleb128 .Ltmp2694-.Lfunc_begin27       #     jumps to .Ltmp2694
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2695-.Lfunc_begin27       # >> Call Site 61 <<
	.uleb128 .Ltmp2696-.Ltmp2695            #   Call between .Ltmp2695 and .Ltmp2696
	.uleb128 .Ltmp2697-.Lfunc_begin27       #     jumps to .Ltmp2697
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2698-.Lfunc_begin27       # >> Call Site 62 <<
	.uleb128 .Ltmp2699-.Ltmp2698            #   Call between .Ltmp2698 and .Ltmp2699
	.uleb128 .Ltmp2700-.Lfunc_begin27       #     jumps to .Ltmp2700
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2699-.Lfunc_begin27       # >> Call Site 63 <<
	.uleb128 .Ltmp2701-.Ltmp2699            #   Call between .Ltmp2699 and .Ltmp2701
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2701-.Lfunc_begin27       # >> Call Site 64 <<
	.uleb128 .Ltmp2702-.Ltmp2701            #   Call between .Ltmp2701 and .Ltmp2702
	.uleb128 .Ltmp2703-.Lfunc_begin27       #     jumps to .Ltmp2703
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2704-.Lfunc_begin27       # >> Call Site 65 <<
	.uleb128 .Ltmp2713-.Ltmp2704            #   Call between .Ltmp2704 and .Ltmp2713
	.uleb128 .Ltmp2714-.Lfunc_begin27       #     jumps to .Ltmp2714
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2715-.Lfunc_begin27       # >> Call Site 66 <<
	.uleb128 .Ltmp2718-.Ltmp2715            #   Call between .Ltmp2715 and .Ltmp2718
	.uleb128 .Ltmp2719-.Lfunc_begin27       #     jumps to .Ltmp2719
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2720-.Lfunc_begin27       # >> Call Site 67 <<
	.uleb128 .Ltmp2721-.Ltmp2720            #   Call between .Ltmp2720 and .Ltmp2721
	.uleb128 .Ltmp2722-.Lfunc_begin27       #     jumps to .Ltmp2722
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2723-.Lfunc_begin27       # >> Call Site 68 <<
	.uleb128 .Ltmp2732-.Ltmp2723            #   Call between .Ltmp2723 and .Ltmp2732
	.uleb128 .Ltmp2733-.Lfunc_begin27       #     jumps to .Ltmp2733
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2734-.Lfunc_begin27       # >> Call Site 69 <<
	.uleb128 .Ltmp2737-.Ltmp2734            #   Call between .Ltmp2734 and .Ltmp2737
	.uleb128 .Ltmp2738-.Lfunc_begin27       #     jumps to .Ltmp2738
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2739-.Lfunc_begin27       # >> Call Site 70 <<
	.uleb128 .Ltmp2748-.Ltmp2739            #   Call between .Ltmp2739 and .Ltmp2748
	.uleb128 .Ltmp2749-.Lfunc_begin27       #     jumps to .Ltmp2749
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2750-.Lfunc_begin27       # >> Call Site 71 <<
	.uleb128 .Ltmp2753-.Ltmp2750            #   Call between .Ltmp2750 and .Ltmp2753
	.uleb128 .Ltmp2754-.Lfunc_begin27       #     jumps to .Ltmp2754
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2755-.Lfunc_begin27       # >> Call Site 72 <<
	.uleb128 .Ltmp2766-.Ltmp2755            #   Call between .Ltmp2755 and .Ltmp2766
	.uleb128 .Ltmp2767-.Lfunc_begin27       #     jumps to .Ltmp2767
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2768-.Lfunc_begin27       # >> Call Site 73 <<
	.uleb128 .Ltmp2769-.Ltmp2768            #   Call between .Ltmp2768 and .Ltmp2769
	.uleb128 .Ltmp2770-.Lfunc_begin27       #     jumps to .Ltmp2770
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2771-.Lfunc_begin27       # >> Call Site 74 <<
	.uleb128 .Ltmp2772-.Ltmp2771            #   Call between .Ltmp2771 and .Ltmp2772
	.uleb128 .Ltmp2773-.Lfunc_begin27       #     jumps to .Ltmp2773
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2774-.Lfunc_begin27       # >> Call Site 75 <<
	.uleb128 .Ltmp2775-.Ltmp2774            #   Call between .Ltmp2774 and .Ltmp2775
	.uleb128 .Ltmp2776-.Lfunc_begin27       #     jumps to .Ltmp2776
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2777-.Lfunc_begin27       # >> Call Site 76 <<
	.uleb128 .Ltmp2786-.Ltmp2777            #   Call between .Ltmp2777 and .Ltmp2786
	.uleb128 .Ltmp2787-.Lfunc_begin27       #     jumps to .Ltmp2787
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2788-.Lfunc_begin27       # >> Call Site 77 <<
	.uleb128 .Ltmp2789-.Ltmp2788            #   Call between .Ltmp2788 and .Ltmp2789
	.uleb128 .Ltmp2790-.Lfunc_begin27       #     jumps to .Ltmp2790
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2791-.Lfunc_begin27       # >> Call Site 78 <<
	.uleb128 .Ltmp2802-.Ltmp2791            #   Call between .Ltmp2791 and .Ltmp2802
	.uleb128 .Ltmp2803-.Lfunc_begin27       #     jumps to .Ltmp2803
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2804-.Lfunc_begin27       # >> Call Site 79 <<
	.uleb128 .Ltmp2805-.Ltmp2804            #   Call between .Ltmp2804 and .Ltmp2805
	.uleb128 .Ltmp2806-.Lfunc_begin27       #     jumps to .Ltmp2806
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2807-.Lfunc_begin27       # >> Call Site 80 <<
	.uleb128 .Ltmp2826-.Ltmp2807            #   Call between .Ltmp2807 and .Ltmp2826
	.uleb128 .Ltmp2827-.Lfunc_begin27       #     jumps to .Ltmp2827
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2828-.Lfunc_begin27       # >> Call Site 81 <<
	.uleb128 .Ltmp2837-.Ltmp2828            #   Call between .Ltmp2828 and .Ltmp2837
	.uleb128 .Ltmp2838-.Lfunc_begin27       #     jumps to .Ltmp2838
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2839-.Lfunc_begin27       # >> Call Site 82 <<
	.uleb128 .Ltmp2842-.Ltmp2839            #   Call between .Ltmp2839 and .Ltmp2842
	.uleb128 .Ltmp2843-.Lfunc_begin27       #     jumps to .Ltmp2843
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2844-.Lfunc_begin27       # >> Call Site 83 <<
	.uleb128 .Ltmp2845-.Ltmp2844            #   Call between .Ltmp2844 and .Ltmp2845
	.uleb128 .Ltmp2846-.Lfunc_begin27       #     jumps to .Ltmp2846
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2847-.Lfunc_begin27       # >> Call Site 84 <<
	.uleb128 .Ltmp2848-.Ltmp2847            #   Call between .Ltmp2847 and .Ltmp2848
	.uleb128 .Ltmp2849-.Lfunc_begin27       #     jumps to .Ltmp2849
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2850-.Lfunc_begin27       # >> Call Site 85 <<
	.uleb128 .Ltmp2851-.Ltmp2850            #   Call between .Ltmp2850 and .Ltmp2851
	.uleb128 .Ltmp2852-.Lfunc_begin27       #     jumps to .Ltmp2852
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2853-.Lfunc_begin27       # >> Call Site 86 <<
	.uleb128 .Ltmp2854-.Ltmp2853            #   Call between .Ltmp2853 and .Ltmp2854
	.uleb128 .Ltmp2855-.Lfunc_begin27       #     jumps to .Ltmp2855
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2856-.Lfunc_begin27       # >> Call Site 87 <<
	.uleb128 .Ltmp2857-.Ltmp2856            #   Call between .Ltmp2856 and .Ltmp2857
	.uleb128 .Ltmp2858-.Lfunc_begin27       #     jumps to .Ltmp2858
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2859-.Lfunc_begin27       # >> Call Site 88 <<
	.uleb128 .Ltmp2860-.Ltmp2859            #   Call between .Ltmp2859 and .Ltmp2860
	.uleb128 .Ltmp2861-.Lfunc_begin27       #     jumps to .Ltmp2861
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2860-.Lfunc_begin27       # >> Call Site 89 <<
	.uleb128 .Lfunc_end34-.Ltmp2860         #   Call between .Ltmp2860 and .Lfunc_end34
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end27:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase16:
	.p2align	2, 0x0
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2, 0x0                          # -- Begin function _Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.LCPI35_0:
	.long	0x3f800000                      #  1
.LCPI35_1:
	.long	0x80000000                      #  -0
.LCPI35_2:
	.long	0x3f000000                      #  0.5
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4, 0x0
.LCPI35_3:
	.zero	16
	.section	.text._Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"axG",@progbits,_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.weak	_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.p2align	4, 0x90
	.type	_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,@function
_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_: # 
.Lfunc_begin28:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception28
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$440, %rsp                      # imm = 0x1B8
	.cfi_def_cfa_offset 496
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, 112(%rsp)                  # 8-byte Spill
	movq	%rcx, %r15
	movq	%rdx, %r12
	movq	%rsi, 128(%rsp)                 # 8-byte Spill
	movq	%rdi, %rbx
	vcvtsi2sdl	(%rdi), %xmm0, %xmm0
	callq	sqrt
	vcvttsd2si	%xmm0, %eax
	movl	%eax, 4(%rsp)
	movq	%rbx, 176(%rsp)                 # 8-byte Spill
	movslq	(%rbx), %rbp
	leaq	(,%rbp,8), %r13
	testq	%rbp, %rbp
	movq	$-1, %r14
	cmovnsq	%r13, %r14
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %rbx
	testq	%rbp, %rbp
	movq	%r15, 200(%rsp)                 # 8-byte Spill
	je	.LBB35_4
# %bb.1:
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movq	%r13, %rdx
	callq	_intel_fast_memset@PLT
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 120(%rsp)                 # 8-byte Spill
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%r13, %rdx
	callq	_intel_fast_memset@PLT
	testl	%ebp, %ebp
	jle	.LBB35_5
# %bb.2:
	xorl	%ebp, %ebp
	movq	%r15, %r13
	movq	176(%rsp), %r15                 # 8-byte Reload
	movq	120(%rsp), %r14                 # 8-byte Reload
	.p2align	4, 0x90
.LBB35_3:                               # =>This Inner Loop Header: Depth=1
	movq	%r12, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, (%rbx,%rbp,8)
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, (%r14,%rbp,8)
	incq	%rbp
	movslq	(%r15), %rax
	addq	$32, %r13
	addq	$32, %r12
	cmpq	%rax, %rbp
	jl	.LBB35_3
	jmp	.LBB35_5
.LBB35_4:
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 120(%rsp)                 # 8-byte Spill
.LBB35_5:
	movq	128(%rsp), %rdi                 # 8-byte Reload
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, 72(%rsp)
	movl	$80, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, 64(%rax)
	vzeroupper
	callq	omp_get_wtime
	vxorps	%xmm7, %xmm7, %xmm7
	vmovsd	%xmm0, 128(%rsp)                # 8-byte Spill
	movq	176(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	movq	120(%rsp), %rdi                 # 8-byte Reload
	jmp	.LBB35_7
	.p2align	4, 0x90
.LBB35_6:                               #   in Loop: Header=BB35_7 Depth=1
	vmovlps	%xmm0, (%rbp,%rdx,8)
	incq	%rdx
	cmpq	$10, %rdx
	je	.LBB35_10
.LBB35_7:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB35_9 Depth 2
	vcvtsi2ss	%ecx, %xmm8, %xmm0
	vblendps	$1, %xmm0, %xmm7, %xmm0         # xmm0 = xmm0[0],xmm7[1,2,3]
	testl	%eax, %eax
	jle	.LBB35_6
# %bb.8:                                #   in Loop: Header=BB35_7 Depth=1
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB35_9:                               #   Parent Loop BB35_7 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rsi,8), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmovss	(%rdi,%rsi,8), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm1, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm4) - xmm3
	vfmadd231ss	4(%rdi,%rsi,8), %xmm1, %xmm4 # xmm4 = (xmm1 * mem) + xmm4
	vfmadd231ss	4(%rbx,%rsi,8), %xmm2, %xmm4 # xmm4 = (xmm2 * mem) + xmm4
	vaddss	%xmm4, %xmm3, %xmm1
	vsubss	%xmm1, %xmm3, %xmm2
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm1, %xmm0, %xmm3
	vsubss	%xmm0, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm0, %xmm5
	vsubss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm1
	vmovshdup	%xmm0, %xmm0            # xmm0 = xmm0[1,1,3,3]
	vaddss	%xmm2, %xmm0, %xmm4
	vsubss	%xmm0, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm0, %xmm0
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	incq	%rsi
	cmpq	%rsi, %rax
	jne	.LBB35_9
	jmp	.LBB35_6
.LBB35_10:
	callq	omp_get_wtime
	vsubsd	128(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm8, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	144(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp2862:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp2863:
# %bb.11:
.Ltmp2864:
	movq	%rax, %r12
	movq	112(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp2865:
# %bb.12:
.Ltmp2866:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2867:
# %bb.13:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp2868:
	leaq	80(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2869:
# %bb.14:
.Ltmp2870:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp2871:
# %bb.15:
.Ltmp2873:
	callq	mpfr_get_default_rounding_mode
.Ltmp2874:
# %bb.16:
.Ltmp2875:
	leaq	80(%rsp), %rdi
	leaq	144(%rsp), %rsi
	movq	112(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp2876:
# %bb.17:
.Ltmp2878:
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2879:
# %bb.18:
.Ltmp2880:
	movq	%rax, %r12
	movq	112(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp2881:
# %bb.19:
.Ltmp2882:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2883:
# %bb.20:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp2884:
	leaq	8(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2885:
# %bb.21:
.Ltmp2886:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp2887:
# %bb.22:
.Ltmp2889:
	callq	mpfr_get_default_rounding_mode
.Ltmp2890:
# %bb.23:
.Ltmp2891:
	leaq	8(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movq	112(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp2892:
# %bb.24:
.Ltmp2894:
	callq	mpfr_get_default_rounding_mode
.Ltmp2895:
# %bb.25:
.Ltmp2896:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2897:
# %bb.26:
.Ltmp2898:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2899:
# %bb.27:
.Ltmp2900:
	movl	%eax, %r12d
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2901:
# %bb.28:
.Ltmp2902:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2903:
# %bb.29:
.Ltmp2905:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp2906:
# %bb.30:
.Ltmp2908:
	leaq	40(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp2909:
# %bb.31:
	vmovlpd	%xmm0, 208(%rsp)
	cmpq	$0, 64(%rsp)
	je	.LBB35_33
# %bb.32:
.Ltmp2911:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2912:
.LBB35_33:
	cmpq	$0, 32(%rsp)
	je	.LBB35_35
# %bb.34:
.Ltmp2914:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2915:
.LBB35_35:
	cmpq	$0, 104(%rsp)
	je	.LBB35_37
# %bb.36:
.Ltmp2917:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2918:
.LBB35_37:
	cmpq	$0, 168(%rsp)
	je	.LBB35_39
# %bb.38:
.Ltmp2920:
	leaq	144(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2921:
.LBB35_39:
	leaq	400(%rsp), %r15
	movq	%r15, 384(%rsp)
	movl	$544501604, 400(%rsp)           # imm = 0x20746F64
	movw	$32, 404(%rsp)
	movq	$5, 392(%rsp)
.Ltmp2923:
	leaq	384(%rsp), %rdi
	leaq	208(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp2924:
# %bb.40:
	movq	384(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB35_42
# %bb.41:
	callq	_ZdlPv
.LBB35_42:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$80, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, 64(%rax)
	xorl	%r15d, %r15d
	vzeroupper
	callq	omp_get_wtime
	vmovsd	%xmm0, 216(%rsp)                # 8-byte Spill
	vxorps	%xmm7, %xmm7, %xmm7
	xorl	%r12d, %r12d
	jmp	.LBB35_44
	.p2align	4, 0x90
.LBB35_43:                              #   in Loop: Header=BB35_44 Depth=1
	vmovlps	%xmm0, (%rbp,%r12,8)
	leaq	1(%r12), %rax
	cmpq	$9, %r12
	movq	%rax, %r12
	je	.LBB35_52
.LBB35_44:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB35_47 Depth 2
                                        #     Child Loop BB35_49 Depth 2
	vcvtsi2ss	%r15d, %xmm9, %xmm0
	vblendps	$14, .LCPI35_3(%rip), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[1,2,3]
	movq	176(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	jle	.LBB35_50
# %bb.45:                               #   in Loop: Header=BB35_44 Depth=1
	cmpl	$4, %eax
	jb	.LBB35_48
# %bb.46:                               #   in Loop: Header=BB35_44 Depth=1
	leaq	(,%rax,8), %rcx
	andq	$-32, %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB35_47:                              #   Parent Loop BB35_44 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rdx), %xmm4              # xmm4 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%rdx), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm4, %xmm2
	vmovaps	%xmm4, %xmm3
	vfmsub213ss	%xmm2, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm3) - xmm2
	vfmadd231ss	%xmm1, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm1) + xmm3
	vfmadd231ss	%xmm4, %xmm1, %xmm3     # xmm3 = (xmm1 * xmm4) + xmm3
	vaddss	%xmm3, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm5, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm4
	vsubss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm3, %xmm4, %xmm8
	vmovshdup	%xmm0, %xmm3            # xmm3 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm3, %xmm3
	vsubss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm4, %xmm8, %xmm0
	vaddss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm1
	vsubss	%xmm1, %xmm3, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	8(%rbx,%rdx), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm2, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vmovss	12(%rbx,%rdx), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vfmadd231ss	%xmm2, %xmm5, %xmm4     # xmm4 = (xmm5 * xmm2) + xmm4
	vaddss	%xmm4, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm2, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm1, %xmm1
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm6
	vsubss	%xmm6, %xmm0, %xmm0
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	16(%rbx,%rdx), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm2, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vmovss	20(%rbx,%rdx), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vfmadd231ss	%xmm2, %xmm5, %xmm4     # xmm4 = (xmm5 * xmm2) + xmm4
	vaddss	%xmm4, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm2, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm1, %xmm1
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm6
	vsubss	%xmm6, %xmm0, %xmm0
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	24(%rbx,%rdx), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm2, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vmovss	28(%rbx,%rdx), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vfmadd231ss	%xmm2, %xmm5, %xmm4     # xmm4 = (xmm5 * xmm2) + xmm4
	vaddss	%xmm4, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm2, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm1, %xmm1
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm6
	vsubss	%xmm6, %xmm0, %xmm0
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	addq	$32, %rdx
	cmpq	%rdx, %rcx
	jne	.LBB35_47
.LBB35_48:                              #   in Loop: Header=BB35_44 Depth=1
	movl	%eax, %ecx
	andl	$-4, %ecx
	cmpq	%rax, %rcx
	jae	.LBB35_50
	.p2align	4, 0x90
.LBB35_49:                              #   Parent Loop BB35_44 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rcx,8), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%rcx,8), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm4, %xmm2
	vmovaps	%xmm4, %xmm3
	vfmsub213ss	%xmm2, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm3) - xmm2
	vfmadd231ss	%xmm1, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm1) + xmm3
	vfmadd231ss	%xmm4, %xmm1, %xmm3     # xmm3 = (xmm1 * xmm4) + xmm3
	vaddss	%xmm3, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm5, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm4
	vsubss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm3, %xmm4, %xmm8
	vmovshdup	%xmm0, %xmm3            # xmm3 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm3, %xmm3
	vsubss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm4, %xmm8, %xmm0
	vaddss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm1
	vsubss	%xmm1, %xmm3, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	incq	%rcx
	cmpq	%rcx, %rax
	jne	.LBB35_49
.LBB35_50:                              #   in Loop: Header=BB35_44 Depth=1
	vucomiss	%xmm7, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB35_43
# %bb.51:                               #   in Loop: Header=BB35_44 Depth=1
	vmovups	%xmm0, 128(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovss	.LCPI35_0(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm1, %xmm1
	vmovups	128(%rsp), %xmm8                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmulss	%xmm1, %xmm8, %xmm0
	vmulss	%xmm0, %xmm0, %xmm2
	vbroadcastss	.LCPI35_1(%rip), %xmm5  # xmm5 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm5, %xmm2, %xmm3
	vmovaps	%xmm0, %xmm4
	vfmsub213ss	%xmm2, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm4) - xmm2
	vxorps	%xmm5, %xmm4, %xmm5
	vsubss	%xmm2, %xmm8, %xmm2
	vsubss	%xmm8, %xmm2, %xmm6
	vsubss	%xmm6, %xmm2, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm7, %xmm3
	vmovshdup	%xmm8, %xmm6            # xmm6 = xmm8[1,1,3,3]
	vsubss	%xmm4, %xmm6, %xmm4
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm6, %xmm6
	vsubss	%xmm7, %xmm5, %xmm5
	vxorps	%xmm7, %xmm7, %xmm7
	vaddss	%xmm5, %xmm6, %xmm5
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm2
	vmulss	.LCPI35_2(%rip), %xmm1, %xmm1
	vmulss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
	jmp	.LBB35_43
.LBB35_52:
	callq	omp_get_wtime
	vsubsd	216(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm9, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	144(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp2926:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp2927:
# %bb.53:
	movq	%rax, %r13
	movq	112(%rsp), %rax                 # 8-byte Reload
	leaq	32(%rax), %r14
.Ltmp2928:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp2929:
# %bb.54:
.Ltmp2930:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp2931:
# %bb.55:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp2932:
	leaq	80(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp2933:
# %bb.56:
.Ltmp2934:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2935:
# %bb.57:
.Ltmp2937:
	callq	mpfr_get_default_rounding_mode
.Ltmp2938:
# %bb.58:
.Ltmp2939:
	leaq	80(%rsp), %rdi
	leaq	144(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp2940:
# %bb.59:
.Ltmp2942:
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2943:
# %bb.60:
.Ltmp2944:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp2945:
# %bb.61:
.Ltmp2946:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp2947:
# %bb.62:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp2948:
	leaq	8(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp2949:
# %bb.63:
.Ltmp2950:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2951:
# %bb.64:
.Ltmp2953:
	callq	mpfr_get_default_rounding_mode
.Ltmp2954:
# %bb.65:
.Ltmp2955:
	leaq	8(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp2956:
# %bb.66:
.Ltmp2958:
	callq	mpfr_get_default_rounding_mode
.Ltmp2959:
# %bb.67:
.Ltmp2960:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp2961:
# %bb.68:
.Ltmp2962:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp2963:
# %bb.69:
.Ltmp2964:
	movl	%eax, %r12d
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp2965:
# %bb.70:
.Ltmp2966:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2967:
# %bb.71:
.Ltmp2969:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp2970:
# %bb.72:
.Ltmp2972:
	leaq	40(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp2973:
# %bb.73:
	vmovlpd	%xmm0, 208(%rsp)
	cmpq	$0, 64(%rsp)
	je	.LBB35_75
# %bb.74:
.Ltmp2975:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2976:
.LBB35_75:
	cmpq	$0, 32(%rsp)
	je	.LBB35_77
# %bb.76:
.Ltmp2978:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2979:
.LBB35_77:
	cmpq	$0, 104(%rsp)
	je	.LBB35_79
# %bb.78:
.Ltmp2981:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2982:
.LBB35_79:
	cmpq	$0, 168(%rsp)
	je	.LBB35_81
# %bb.80:
.Ltmp2984:
	leaq	144(%rsp), %rdi
	callq	mpfr_clear
.Ltmp2985:
.LBB35_81:
	leaq	368(%rsp), %r15
	movq	%r15, 352(%rsp)
	movl	$846033518, 368(%rsp)           # imm = 0x326D726E
	movw	$32, 372(%rsp)
	movq	$5, 360(%rsp)
.Ltmp2987:
	leaq	352(%rsp), %rdi
	leaq	208(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp2988:
# %bb.82:
	movq	352(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB35_84
# %bb.83:
	callq	_ZdlPv
.LBB35_84:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$80, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, 64(%rax)
	vzeroupper
	callq	omp_get_wtime
	vxorps	%xmm9, %xmm9, %xmm9
	vmovsd	%xmm0, 128(%rsp)                # 8-byte Spill
	movq	176(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	xorl	%ecx, %ecx
	vxorpd	%xmm0, %xmm0, %xmm0
	vbroadcastss	.LCPI35_1(%rip), %xmm8  # xmm8 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	xorl	%edx, %edx
	jmp	.LBB35_86
	.p2align	4, 0x90
.LBB35_85:                              #   in Loop: Header=BB35_86 Depth=1
	vmovlps	%xmm1, (%rbp,%rdx,8)
	incq	%rdx
	cmpq	$10, %rdx
	je	.LBB35_92
.LBB35_86:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB35_90 Depth 2
	vcvtsi2ss	%ecx, %xmm10, %xmm1
	vblendps	$1, %xmm1, %xmm9, %xmm1         # xmm1 = xmm1[0],xmm9[1,2,3]
	testl	%eax, %eax
	jle	.LBB35_85
# %bb.87:                               #   in Loop: Header=BB35_86 Depth=1
	xorl	%esi, %esi
	jmp	.LBB35_90
	.p2align	4, 0x90
.LBB35_88:                              #   in Loop: Header=BB35_90 Depth=2
	vmovsd	(%rbx,%rsi,8), %xmm2            # xmm2 = mem[0],zero
.LBB35_89:                              #   in Loop: Header=BB35_90 Depth=2
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm1, %xmm5
	vsubss	%xmm4, %xmm2, %xmm4
	vaddss	%xmm4, %xmm5, %xmm4
	vmovshdup	%xmm1, %xmm1            # xmm1 = xmm1[1,1,3,3]
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm5
	vsubss	%xmm1, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm5, %xmm4, %xmm2
	vaddss	%xmm2, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vinsertps	$16, %xmm1, %xmm2, %xmm1 # xmm1 = xmm2[0],xmm1[0],xmm2[2,3]
	incq	%rsi
	cmpq	%rsi, %rax
	je	.LBB35_85
.LBB35_90:                              #   Parent Loop BB35_86 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rsi,8), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vcomiss	%xmm2, %xmm0
	jbe	.LBB35_88
# %bb.91:                               #   in Loop: Header=BB35_90 Depth=2
	vinsertps	$16, 4(%rbx,%rsi,8), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vxorps	%xmm2, %xmm8, %xmm2
	jmp	.LBB35_89
.LBB35_92:
	vmovups	%xmm8, 416(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	omp_get_wtime
	vsubsd	128(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm10, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	144(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp2990:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp2991:
# %bb.93:
	movq	%rax, %r13
	movq	112(%rsp), %rax                 # 8-byte Reload
	leaq	64(%rax), %r14
.Ltmp2992:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp2993:
# %bb.94:
.Ltmp2994:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp2995:
# %bb.95:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp2996:
	leaq	80(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp2997:
# %bb.96:
.Ltmp2998:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp2999:
# %bb.97:
.Ltmp3001:
	callq	mpfr_get_default_rounding_mode
.Ltmp3002:
# %bb.98:
.Ltmp3003:
	leaq	80(%rsp), %rdi
	leaq	144(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp3004:
# %bb.99:
.Ltmp3006:
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3007:
# %bb.100:
.Ltmp3008:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp3009:
# %bb.101:
.Ltmp3010:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp3011:
# %bb.102:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp3012:
	leaq	8(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp3013:
# %bb.103:
.Ltmp3014:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3015:
# %bb.104:
.Ltmp3017:
	callq	mpfr_get_default_rounding_mode
.Ltmp3018:
# %bb.105:
.Ltmp3019:
	leaq	8(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp3020:
# %bb.106:
.Ltmp3022:
	callq	mpfr_get_default_rounding_mode
.Ltmp3023:
# %bb.107:
.Ltmp3024:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3025:
# %bb.108:
.Ltmp3026:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3027:
# %bb.109:
.Ltmp3028:
	movl	%eax, %r12d
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3029:
# %bb.110:
.Ltmp3030:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3031:
# %bb.111:
.Ltmp3033:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp3034:
# %bb.112:
.Ltmp3036:
	leaq	40(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp3037:
# %bb.113:
	vmovlpd	%xmm0, 208(%rsp)
	cmpq	$0, 64(%rsp)
	je	.LBB35_115
# %bb.114:
.Ltmp3039:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3040:
.LBB35_115:
	cmpq	$0, 32(%rsp)
	je	.LBB35_117
# %bb.116:
.Ltmp3042:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3043:
.LBB35_117:
	cmpq	$0, 104(%rsp)
	je	.LBB35_119
# %bb.118:
.Ltmp3045:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3046:
.LBB35_119:
	cmpq	$0, 168(%rsp)
	je	.LBB35_121
# %bb.120:
.Ltmp3048:
	leaq	144(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3049:
.LBB35_121:
	leaq	336(%rsp), %r15
	movq	%r15, 320(%rsp)
	movl	$1836413793, 336(%rsp)          # imm = 0x6D757361
	movw	$32, 340(%rsp)
	movq	$5, 328(%rsp)
.Ltmp3051:
	leaq	320(%rsp), %rdi
	leaq	208(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp3052:
# %bb.122:
	movq	320(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB35_124
# %bb.123:
	callq	_ZdlPv
.LBB35_124:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movq	176(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	movq	120(%rsp), %rdx                 # 8-byte Reload
	jle	.LBB35_127
# %bb.125:
	vmovss	72(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vmovss	76(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB35_126:                             # =>This Inner Loop Header: Depth=1
	vmovss	(%rbx,%rcx,8), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm0, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm4) - xmm3
	vfmadd231ss	4(%rbx,%rcx,8), %xmm0, %xmm4 # xmm4 = (xmm0 * mem) + xmm4
	vfmadd231ss	%xmm2, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm2) + xmm4
	vaddss	%xmm4, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vmovss	(%rdx,%rcx,8), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmovss	4(%rdx,%rcx,8), %xmm5           # xmm5 = mem[0],zero,zero,zero
	vaddss	%xmm4, %xmm2, %xmm6
	vsubss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm5, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm5
	vaddss	%xmm2, %xmm5, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vinsertps	$16, %xmm2, %xmm3, %xmm2 # xmm2 = xmm3[0],xmm2[0],xmm3[2,3]
	vmovlps	%xmm2, (%rdx,%rcx,8)
	incq	%rcx
	cmpq	%rcx, %rax
	jne	.LBB35_126
.LBB35_127:
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	40(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	movq	176(%rsp), %rax                 # 8-byte Reload
	cmpl	$0, (%rax)
	jle	.LBB35_168
# %bb.128:
	movq	112(%rsp), %rax                 # 8-byte Reload
	leaq	256(%rax), %r15
	xorl	%r14d, %r14d
	movq	120(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB35_130
	.p2align	4, 0x90
.LBB35_129:                             #   in Loop: Header=BB35_130 Depth=1
	incq	%r14
	movq	176(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r15
	movq	128(%rsp), %rsi                 # 8-byte Reload
	addq	$8, %rsi
	cmpq	%rax, %r14
	jge	.LBB35_168
.LBB35_130:                             # =>This Inner Loop Header: Depth=1
.Ltmp3054:
	leaq	80(%rsp), %rdi
	movq	%rsi, 128(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp3055:
# %bb.131:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3057:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp3058:
# %bb.132:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3059:
	movq	%rax, %rbp
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3060:
# %bb.133:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3061:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3062:
# %bb.134:                              #   in Loop: Header=BB35_130 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp3063:
	leaq	8(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3064:
# %bb.135:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3065:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3066:
# %bb.136:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3068:
	callq	mpfr_get_default_rounding_mode
.Ltmp3069:
# %bb.137:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3070:
	leaq	8(%rsp), %rdi
	movq	%r15, %rsi
	leaq	80(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp3071:
# %bb.138:                              #   in Loop: Header=BB35_130 Depth=1
	cmpq	$0, 104(%rsp)
	je	.LBB35_140
# %bb.139:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3073:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3074:
.LBB35_140:                             #   in Loop: Header=BB35_130 Depth=1
.Ltmp3076:
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3077:
# %bb.141:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3078:
	movq	%rax, %rbp
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3079:
# %bb.142:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3080:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3081:
# %bb.143:                              #   in Loop: Header=BB35_130 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp3082:
	leaq	144(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3083:
# %bb.144:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3084:
	leaq	144(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3085:
# %bb.145:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3087:
	callq	mpfr_get_default_rounding_mode
.Ltmp3088:
# %bb.146:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3089:
	leaq	144(%rsp), %rdi
	leaq	8(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp3090:
# %bb.147:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3092:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3093:
# %bb.148:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3094:
	movq	%rax, %rbp
	leaq	144(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3095:
# %bb.149:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3096:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3097:
# %bb.150:                              #   in Loop: Header=BB35_130 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp3098:
	leaq	80(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3099:
# %bb.151:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3100:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3101:
# %bb.152:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3103:
	callq	mpfr_get_default_rounding_mode
.Ltmp3104:
	leaq	40(%rsp), %r12
# %bb.153:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3105:
	leaq	80(%rsp), %rdi
	movq	%r12, %rsi
	leaq	144(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp3106:
# %bb.154:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3108:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp3109:
# %bb.155:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3110:
	movq	%rax, %r12
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3111:
# %bb.156:                              #   in Loop: Header=BB35_130 Depth=1
	movq	%rax, %rbp
	cmpq	%rax, %r12
	je	.LBB35_160
# %bb.157:                              #   in Loop: Header=BB35_130 Depth=1
	cmpq	$0, 64(%rsp)
	je	.LBB35_159
# %bb.158:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3112:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3113:
.LBB35_159:                             #   in Loop: Header=BB35_130 Depth=1
.Ltmp3114:
	leaq	40(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp3115:
.LBB35_160:                             #   in Loop: Header=BB35_130 Depth=1
.Ltmp3116:
	callq	mpfr_get_default_rounding_mode
.Ltmp3117:
# %bb.161:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3118:
	leaq	40(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp3119:
# %bb.162:                              #   in Loop: Header=BB35_130 Depth=1
	cmpq	$0, 104(%rsp)
	je	.LBB35_164
# %bb.163:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3121:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3122:
.LBB35_164:                             #   in Loop: Header=BB35_130 Depth=1
	cmpq	$0, 168(%rsp)
	je	.LBB35_166
# %bb.165:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3124:
	leaq	144(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3125:
.LBB35_166:                             #   in Loop: Header=BB35_130 Depth=1
	cmpq	$0, 32(%rsp)
	je	.LBB35_129
# %bb.167:                              #   in Loop: Header=BB35_130 Depth=1
.Ltmp3127:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3128:
	jmp	.LBB35_129
.LBB35_168:
.Ltmp3130:
	callq	mpfr_get_default_rounding_mode
.Ltmp3131:
# %bb.169:
.Ltmp3132:
	movl	%eax, %ebp
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3133:
# %bb.170:
.Ltmp3134:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp3135:
# %bb.171:
.Ltmp3136:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp3137:
# %bb.172:
.Ltmp3138:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp3139:
# %bb.173:
.Ltmp3141:
	leaq	8(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp3142:
# %bb.174:
.Ltmp3144:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3145:
# %bb.175:
.Ltmp3146:
	movq	%rax, %r12
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3147:
# %bb.176:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB35_180
# %bb.177:
	cmpq	$0, 64(%rsp)
	je	.LBB35_179
# %bb.178:
.Ltmp3148:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3149:
.LBB35_179:
.Ltmp3150:
	leaq	40(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp3151:
.LBB35_180:
.Ltmp3152:
	callq	mpfr_get_default_rounding_mode
.Ltmp3153:
# %bb.181:
.Ltmp3154:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp3155:
# %bb.182:
	cmpq	$0, 32(%rsp)
	je	.LBB35_184
# %bb.183:
.Ltmp3157:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3158:
.LBB35_184:
	callq	omp_get_wtime
	vmovsd	%xmm0, 128(%rsp)                # 8-byte Spill
	movq	176(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	testq	%rax, %rax
	jle	.LBB35_189
# %bb.185:
	vmovss	72(%rsp), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vmovss	76(%rsp), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	xorl	%ecx, %ecx
	movq	120(%rsp), %rsi                 # 8-byte Reload
	.p2align	4, 0x90
.LBB35_186:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB35_187 Depth 2
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB35_187:                             #   Parent Loop BB35_186 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rdx,8), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm0, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm4) - xmm3
	vfmadd231ss	4(%rbx,%rdx,8), %xmm0, %xmm4 # xmm4 = (xmm0 * mem) + xmm4
	vfmadd231ss	%xmm2, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm2) + xmm4
	vaddss	%xmm4, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vmovss	(%rsi,%rdx,8), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmovss	4(%rsi,%rdx,8), %xmm5           # xmm5 = mem[0],zero,zero,zero
	vaddss	%xmm4, %xmm2, %xmm6
	vsubss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm5, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm5
	vaddss	%xmm2, %xmm5, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vinsertps	$16, %xmm2, %xmm3, %xmm2 # xmm2 = xmm3[0],xmm2[0],xmm3[2,3]
	vmovlps	%xmm2, (%rsi,%rdx,8)
	incq	%rdx
	cmpq	%rdx, %rax
	jne	.LBB35_187
# %bb.188:                              #   in Loop: Header=BB35_186 Depth=1
	incl	%ecx
	cmpl	$10, %ecx
	jne	.LBB35_186
.LBB35_189:
	callq	omp_get_wtime
	vsubsd	128(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm9, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp3160:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3161:
# %bb.190:
	movq	%rax, %r12
	movq	112(%rsp), %rax                 # 8-byte Reload
	leaq	96(%rax), %r15
.Ltmp3162:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp3163:
# %bb.191:
.Ltmp3164:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3165:
# %bb.192:
	movl	%eax, %ebp
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp3166:
	leaq	8(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3167:
# %bb.193:
.Ltmp3168:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp3169:
# %bb.194:
.Ltmp3171:
	callq	mpfr_get_default_rounding_mode
.Ltmp3172:
# %bb.195:
.Ltmp3173:
	leaq	8(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movq	%r15, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp3174:
# %bb.196:
.Ltmp3176:
	leaq	8(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp3177:
# %bb.197:
	vmovlpd	%xmm0, 80(%rsp)
	cmpq	$0, 32(%rsp)
	je	.LBB35_199
# %bb.198:
.Ltmp3179:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3180:
.LBB35_199:
	leaq	304(%rsp), %r15
	movq	%r15, 288(%rsp)
	movl	$2037413985, 304(%rsp)          # imm = 0x79707861
	movw	$32, 308(%rsp)
	movq	$5, 296(%rsp)
.Ltmp3182:
	leaq	288(%rsp), %rdi
	leaq	80(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp3183:
# %bb.200:
	movq	288(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB35_202
# %bb.201:
	callq	_ZdlPv
.LBB35_202:
	cmpq	$0, 64(%rsp)
	je	.LBB35_204
# %bb.203:
.Ltmp3185:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3186:
.LBB35_204:
	movslq	4(%rsp), %r12
	leaq	(,%r12,8), %r15
	testq	%r12, %r12
	movq	$-1, %rdi
	cmovnsq	%r15, %rdi
	callq	_Znam
	movq	%rax, %rbp
	testq	%r12, %r12
	movq	200(%rsp), %r13                 # 8-byte Reload
	movq	120(%rsp), %r14                 # 8-byte Reload
	je	.LBB35_208
# %bb.205:
	movq	%rbp, %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_intel_fast_memset@PLT
	testl	%r12d, %r12d
	jle	.LBB35_208
# %bb.206:
	xorl	%r15d, %r15d
	.p2align	4, 0x90
.LBB35_207:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, (%r14,%r15,8)
	vmovlpd	%xmm0, (%rbp,%r15,8)
	incq	%r15
	movslq	4(%rsp), %rax
	addq	$32, %r13
	cmpq	%rax, %r15
	jl	.LBB35_207
.LBB35_208:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%rbp, 200(%rsp)                 # 8-byte Spill
	movq	%rbp, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	40(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	cmpl	$0, 4(%rsp)
	jle	.LBB35_249
# %bb.209:
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	leaq	8(%rsp), %r13
	movq	200(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB35_211
	.p2align	4, 0x90
.LBB35_210:                             #   in Loop: Header=BB35_211 Depth=1
	movq	128(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	movslq	4(%rsp), %rax
	movq	216(%rsp), %rsi                 # 8-byte Reload
	addq	$8, %rsi
	movq	%rdx, %rcx
	movq	%rdx, 128(%rsp)                 # 8-byte Spill
	cmpq	%rax, %rdx
	jge	.LBB35_249
.LBB35_211:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %r14
	movq	176(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %r15d
.Ltmp3188:
	leaq	80(%rsp), %rdi
	movq	%rsi, 216(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp3189:
# %bb.212:                              #   in Loop: Header=BB35_211 Depth=1
	movq	128(%rsp), %rax                 # 8-byte Reload
	addl	%r15d, %eax
	addl	$8, %eax
	movslq	%eax, %rbp
	shlq	$5, %rbp
	addq	112(%rsp), %rbp                 # 8-byte Folded Reload
.Ltmp3191:
	movq	%rbp, %rdi
	callq	mpfr_get_prec
.Ltmp3192:
# %bb.213:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3193:
	movq	%rax, %r15
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3194:
# %bb.214:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3195:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp3196:
# %bb.215:                              #   in Loop: Header=BB35_211 Depth=1
	movl	%eax, %r13d
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp3197:
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp3198:
# %bb.216:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3199:
	movq	%r14, %rdi
	xorl	%esi, %esi
	movl	%r13d, %edx
	callq	mpfr_set_si
.Ltmp3200:
# %bb.217:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3202:
	callq	mpfr_get_default_rounding_mode
.Ltmp3203:
# %bb.218:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3204:
	movq	%r14, %r13
	movq	%r14, %rdi
	movq	%rbp, %rsi
	leaq	80(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp3205:
# %bb.219:                              #   in Loop: Header=BB35_211 Depth=1
	cmpq	$0, 104(%rsp)
	je	.LBB35_221
# %bb.220:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3207:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3208:
.LBB35_221:                             #   in Loop: Header=BB35_211 Depth=1
.Ltmp3210:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp3211:
# %bb.222:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3212:
	movq	%rax, %r15
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp3213:
# %bb.223:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3214:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp3215:
# %bb.224:                              #   in Loop: Header=BB35_211 Depth=1
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp3216:
	leaq	144(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp3217:
# %bb.225:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3218:
	leaq	144(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp3219:
# %bb.226:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3221:
	callq	mpfr_get_default_rounding_mode
.Ltmp3222:
# %bb.227:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3223:
	leaq	144(%rsp), %rdi
	movq	%r13, %rsi
	movq	%r13, %rdx
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp3224:
# %bb.228:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3226:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3227:
# %bb.229:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3228:
	movq	%rax, %r15
	leaq	144(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3229:
# %bb.230:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3230:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp3231:
# %bb.231:                              #   in Loop: Header=BB35_211 Depth=1
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp3232:
	leaq	80(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp3233:
# %bb.232:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3234:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp3235:
# %bb.233:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3237:
	callq	mpfr_get_default_rounding_mode
.Ltmp3238:
	leaq	40(%rsp), %r14
# %bb.234:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3239:
	leaq	80(%rsp), %rdi
	movq	%r14, %rsi
	leaq	144(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp3240:
# %bb.235:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3242:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp3243:
# %bb.236:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3244:
	movq	%rax, %r15
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3245:
# %bb.237:                              #   in Loop: Header=BB35_211 Depth=1
	movq	%rax, %rbp
	cmpq	%rax, %r15
	je	.LBB35_241
# %bb.238:                              #   in Loop: Header=BB35_211 Depth=1
	cmpq	$0, 64(%rsp)
	je	.LBB35_240
# %bb.239:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3246:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3247:
.LBB35_240:                             #   in Loop: Header=BB35_211 Depth=1
.Ltmp3248:
	leaq	40(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp3249:
.LBB35_241:                             #   in Loop: Header=BB35_211 Depth=1
.Ltmp3250:
	callq	mpfr_get_default_rounding_mode
.Ltmp3251:
# %bb.242:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3252:
	leaq	40(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp3253:
# %bb.243:                              #   in Loop: Header=BB35_211 Depth=1
	cmpq	$0, 104(%rsp)
	je	.LBB35_245
# %bb.244:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3255:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3256:
.LBB35_245:                             #   in Loop: Header=BB35_211 Depth=1
	cmpq	$0, 168(%rsp)
	je	.LBB35_247
# %bb.246:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3258:
	leaq	144(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3259:
.LBB35_247:                             #   in Loop: Header=BB35_211 Depth=1
	cmpq	$0, 32(%rsp)
	je	.LBB35_210
# %bb.248:                              #   in Loop: Header=BB35_211 Depth=1
.Ltmp3261:
	movq	%r13, %rdi
	callq	mpfr_clear
.Ltmp3262:
	jmp	.LBB35_210
.LBB35_249:
.Ltmp3264:
	callq	mpfr_get_default_rounding_mode
.Ltmp3265:
# %bb.250:
.Ltmp3266:
	movl	%eax, %ebp
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3267:
# %bb.251:
.Ltmp3268:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp3269:
# %bb.252:
.Ltmp3270:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp3271:
# %bb.253:
.Ltmp3272:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp3273:
# %bb.254:
.Ltmp3275:
	leaq	8(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp3276:
# %bb.255:
.Ltmp3278:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3279:
# %bb.256:
.Ltmp3280:
	movq	%rax, %r12
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3281:
# %bb.257:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB35_261
# %bb.258:
	cmpq	$0, 64(%rsp)
	je	.LBB35_260
# %bb.259:
.Ltmp3282:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3283:
.LBB35_260:
.Ltmp3284:
	leaq	40(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp3285:
.LBB35_261:
.Ltmp3286:
	callq	mpfr_get_default_rounding_mode
.Ltmp3287:
# %bb.262:
.Ltmp3288:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp3289:
# %bb.263:
	cmpq	$0, 32(%rsp)
	je	.LBB35_265
# %bb.264:
.Ltmp3291:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3292:
.LBB35_265:
	callq	omp_get_wtime
	vmovsd	%xmm0, 176(%rsp)                # 8-byte Spill
.Ltmp3294:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	120(%rsp), %r14                 # 8-byte Reload
	movq	%r14, %r8
	movq	200(%rsp), %r15                 # 8-byte Reload
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp3295:
# %bb.266:
.Ltmp3296:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp3297:
# %bb.267:
.Ltmp3298:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp3299:
# %bb.268:
.Ltmp3300:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp3301:
# %bb.269:
.Ltmp3302:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp3303:
# %bb.270:
.Ltmp3304:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp3305:
# %bb.271:
.Ltmp3306:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp3307:
# %bb.272:
.Ltmp3308:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp3309:
# %bb.273:
.Ltmp3310:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp3311:
# %bb.274:
.Ltmp3312:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp3313:
# %bb.275:
	callq	omp_get_wtime
	vsubsd	176(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp3315:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3316:
# %bb.276:
	movq	%rax, %r15
	movq	112(%rsp), %rdi                 # 8-byte Reload
	subq	$-128, %rdi
.Ltmp3317:
	movq	%rdi, %r13
	callq	mpfr_get_prec
.Ltmp3318:
# %bb.277:
.Ltmp3319:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp3320:
# %bb.278:
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp3321:
	leaq	8(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp3322:
# %bb.279:
.Ltmp3323:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp3324:
# %bb.280:
.Ltmp3326:
	callq	mpfr_get_default_rounding_mode
.Ltmp3327:
# %bb.281:
.Ltmp3328:
	leaq	8(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movq	%r13, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp3329:
# %bb.282:
.Ltmp3331:
	leaq	8(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp3332:
# %bb.283:
	vmovlpd	%xmm0, 80(%rsp)
	cmpq	$0, 32(%rsp)
	je	.LBB35_285
# %bb.284:
.Ltmp3334:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3335:
.LBB35_285:
	leaq	272(%rsp), %r15
	movq	%r15, 256(%rsp)
	movl	$1986880871, 272(%rsp)          # imm = 0x766D6567
	movw	$32, 276(%rsp)
	movq	$5, 264(%rsp)
.Ltmp3337:
	leaq	256(%rsp), %rdi
	leaq	80(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp3338:
# %bb.286:
	movq	256(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB35_288
# %bb.287:
	callq	_ZdlPv
.LBB35_288:
	movq	200(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	cmpq	$0, 64(%rsp)
	je	.LBB35_290
# %bb.289:
.Ltmp3340:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3341:
.LBB35_290:
	movq	%rbx, %rdi
	callq	_ZdaPv
	movq	120(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	xorl	%eax, %eax
	vcvtsi2ss	%eax, %xmm2, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vblendps	$1, %xmm0, %xmm1, %xmm1         # xmm1 = xmm0[0],xmm1[1,2,3]
	movl	$2097153, %eax                  # imm = 0x200001
	vxorps	%xmm0, %xmm0, %xmm0
	movl	$1, %ecx
	vmovups	416(%rsp), %xmm11               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	.p2align	4, 0x90
.LBB35_291:                             # =>This Inner Loop Header: Depth=1
	decl	%eax
	vcvtsi2ss	%eax, %xmm12, %xmm2
	vmulss	%xmm2, %xmm2, %xmm3
	vmovaps	%xmm2, %xmm4
	vfmsub213ss	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vfmadd231ss	%xmm0, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm0) + xmm4
	vfmadd231ss	%xmm2, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm2) + xmm4
	vaddss	%xmm4, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vmulss	%xmm2, %xmm2, %xmm4
	vmovaps	%xmm2, %xmm5
	vfmsub213ss	%xmm4, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm5) - xmm4
	vfmadd231ss	%xmm3, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm3) + xmm5
	vfmadd231ss	%xmm2, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm2) + xmm5
	vcvtsi2ss	%ecx, %xmm12, %xmm2
	vaddss	%xmm5, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vdivss	%xmm3, %xmm2, %xmm6
	vaddss	%xmm5, %xmm4, %xmm4
	vmulss	%xmm3, %xmm6, %xmm5
	vmovaps	%xmm3, %xmm7
	vfmsub213ss	%xmm5, %xmm6, %xmm7     # xmm7 = (xmm6 * xmm7) - xmm5
	vfmadd231ss	%xmm4, %xmm6, %xmm7     # xmm7 = (xmm6 * xmm4) + xmm7
	vaddss	%xmm7, %xmm5, %xmm4
	vsubss	%xmm4, %xmm2, %xmm8
	vsubss	%xmm2, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm2, %xmm2
	vsubss	%xmm4, %xmm5, %xmm5
	vxorps	%xmm4, %xmm11, %xmm4
	vaddss	%xmm7, %xmm5, %xmm5
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vdivss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm3
	vsubss	%xmm3, %xmm6, %xmm4
	vaddss	%xmm3, %xmm1, %xmm5
	vsubss	%xmm1, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vaddss	%xmm2, %xmm4, %xmm2
	vsubss	%xmm7, %xmm1, %xmm4
	vsubss	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm1, %xmm1            # xmm1 = xmm1[1,1,3,3]
	vaddss	%xmm3, %xmm4, %xmm3
	vaddss	%xmm2, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm5, %xmm4
	vaddss	%xmm2, %xmm1, %xmm1
	vsubss	%xmm4, %xmm5, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm2
	vaddss	%xmm1, %xmm2, %xmm2
	vinsertps	$16, %xmm2, %xmm3, %xmm1 # xmm1 = xmm3[0],xmm2[0],xmm3[2,3]
	cmpl	$1, %eax
	ja	.LBB35_291
# %bb.292:
	movl	$90, %eax
	vcvtsi2ss	%eax, %xmm12, %xmm0
	vmulss	%xmm3, %xmm0, %xmm1
	vfmsub213ss	%xmm1, %xmm0, %xmm3     # xmm3 = (xmm0 * xmm3) - xmm1
	vfmadd213ss	%xmm3, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm2) + xmm3
	vaddss	%xmm2, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm3
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm0
	vaddss	%xmm2, %xmm3, %xmm2
	setnp	%al
	sete	%cl
	testb	%al, %cl
	je	.LBB35_294
# %bb.293:
	vinsertps	$16, %xmm2, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm2[0],xmm0[2,3]
	jmp	.LBB35_295
.LBB35_294:
	vmovups	%xmm2, 176(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm0, 128(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovss	.LCPI35_0(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm1, %xmm0
	vmovups	128(%rsp), %xmm7                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmulss	%xmm0, %xmm7, %xmm1
	vmulss	%xmm1, %xmm1, %xmm2
	vbroadcastss	.LCPI35_1(%rip), %xmm5  # xmm5 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm5, %xmm2, %xmm3
	vmovaps	%xmm1, %xmm4
	vfmsub213ss	%xmm2, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm4) - xmm2
	vxorps	%xmm5, %xmm4, %xmm5
	vsubss	%xmm2, %xmm7, %xmm2
	vsubss	%xmm7, %xmm2, %xmm6
	vmovaps	%xmm7, %xmm8
	vsubss	%xmm6, %xmm2, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm7, %xmm3
	vmovups	176(%rsp), %xmm7                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm4, %xmm7, %xmm4
	vsubss	%xmm7, %xmm4, %xmm6
	vmovaps	%xmm7, %xmm8
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm7, %xmm5
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm2
	vmulss	.LCPI35_2(%rip), %xmm0, %xmm0
	vmulss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm0, %xmm1, %xmm2
	vsubss	%xmm1, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm1, %xmm1
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
.LBB35_295:
	vucomiss	%xmm1, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB35_297
# %bb.296:
	vmovups	%xmm0, 176(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovss	.LCPI35_0(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm1, %xmm1
	vmovups	176(%rsp), %xmm8                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmulss	%xmm1, %xmm8, %xmm0
	vmulss	%xmm0, %xmm0, %xmm2
	vbroadcastss	.LCPI35_1(%rip), %xmm5  # xmm5 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm5, %xmm2, %xmm3
	vmovaps	%xmm0, %xmm4
	vfmsub213ss	%xmm2, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm4) - xmm2
	vxorps	%xmm5, %xmm4, %xmm5
	vsubss	%xmm2, %xmm8, %xmm2
	vsubss	%xmm8, %xmm2, %xmm6
	vsubss	%xmm6, %xmm2, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm7, %xmm3
	vmovshdup	%xmm8, %xmm6            # xmm6 = xmm8[1,1,3,3]
	vsubss	%xmm4, %xmm6, %xmm4
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm6, %xmm6
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm2
	vmulss	.LCPI35_2(%rip), %xmm1, %xmm1
	vmulss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
.LBB35_297:
	vmovlps	%xmm0, 8(%rsp)
	leaq	240(%rsp), %r14
	movq	%r14, 224(%rsp)
	movq	$32, 40(%rsp)
.Ltmp3343:
	leaq	224(%rsp), %rdi
	leaq	40(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp3344:
# %bb.298:
	movq	%rax, 224(%rsp)
	movq	40(%rsp), %rcx
	movq	%rcx, 240(%rsp)
	vmovupd	.L.str.63(%rip), %ymm0
	vmovupd	%ymm0, (%rax)
	movq	%rcx, 232(%rsp)
	movq	224(%rsp), %rax
	movb	$0, (%rax,%rcx)
.Ltmp3346:
	leaq	224(%rsp), %rdi
	leaq	8(%rsp), %rsi
	xorl	%edx, %edx
	vzeroupper
	callq	_Z5printIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp3347:
# %bb.299:
	movq	224(%rsp), %rdi
	cmpq	%r14, %rdi
	je	.LBB35_301
# %bb.300:
	callq	_ZdlPv
.LBB35_301:
	addq	$440, %rsp                      # imm = 0x1B8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB35_302:
	.cfi_def_cfa_offset 496
.Ltmp3342:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_303:
.Ltmp3336:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_304:
.Ltmp3293:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_305:
.Ltmp3187:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_306:
.Ltmp3181:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_307:
.Ltmp3159:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_308:
.Ltmp3050:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_309:
.Ltmp3047:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_310:
.Ltmp3044:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_311:
.Ltmp3041:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_312:
.Ltmp2986:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_313:
.Ltmp2983:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_314:
.Ltmp2980:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_315:
.Ltmp2977:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_316:
.Ltmp2922:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_317:
.Ltmp2919:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_318:
.Ltmp2916:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_319:
.Ltmp2913:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_320:
.Ltmp3348:
	movq	%rax, %rbx
	movq	224(%rsp), %rdi
	cmpq	%r14, %rdi
	jne	.LBB35_338
	jmp	.LBB35_339
.LBB35_321:
.Ltmp3345:
	movq	%rax, %rdi
	callq	_Unwind_Resume@PLT
.LBB35_322:
.Ltmp3339:
	movq	%rax, %rbx
	movq	256(%rsp), %rdi
	jmp	.LBB35_326
.LBB35_323:
.Ltmp3333:
	movq	%rax, %rbx
	jmp	.LBB35_397
.LBB35_324:
.Ltmp3277:
	movq	%rax, %rbx
	jmp	.LBB35_397
.LBB35_325:
.Ltmp3184:
	movq	%rax, %rbx
	movq	288(%rsp), %rdi
.LBB35_326:
	cmpq	%r15, %rdi
	je	.LBB35_403
# %bb.327:
	callq	_ZdlPv
	jmp	.LBB35_403
.LBB35_328:
.Ltmp3178:
	movq	%rax, %rbx
	jmp	.LBB35_397
.LBB35_329:
.Ltmp3143:
	movq	%rax, %rbx
	jmp	.LBB35_397
.LBB35_330:
.Ltmp3053:
	movq	%rax, %rbx
	movq	320(%rsp), %rdi
	jmp	.LBB35_337
.LBB35_331:
.Ltmp3038:
	jmp	.LBB35_342
.LBB35_332:
.Ltmp3035:
	jmp	.LBB35_342
.LBB35_333:
.Ltmp2989:
	movq	%rax, %rbx
	movq	352(%rsp), %rdi
	jmp	.LBB35_337
.LBB35_334:
.Ltmp2974:
	jmp	.LBB35_342
.LBB35_335:
.Ltmp2971:
	jmp	.LBB35_342
.LBB35_336:
.Ltmp2925:
	movq	%rax, %rbx
	movq	384(%rsp), %rdi
.LBB35_337:
	cmpq	%r15, %rdi
	je	.LBB35_339
.LBB35_338:
	callq	_ZdlPv
.LBB35_339:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB35_340:
.Ltmp2910:
	jmp	.LBB35_342
.LBB35_341:
.Ltmp2907:
.LBB35_342:
	movq	%rax, %rbx
	leaq	40(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB35_365
.LBB35_343:
.Ltmp3330:
	movq	%rax, %rbx
	jmp	.LBB35_397
.LBB35_344:
.Ltmp3175:
	movq	%rax, %rbx
	jmp	.LBB35_397
.LBB35_345:
.Ltmp3021:
	jmp	.LBB35_364
.LBB35_346:
.Ltmp3005:
	movq	%rax, %rbx
	jmp	.LBB35_366
.LBB35_347:
.Ltmp2957:
	jmp	.LBB35_364
.LBB35_348:
.Ltmp2941:
	movq	%rax, %rbx
	jmp	.LBB35_366
.LBB35_349:
.Ltmp2893:
	jmp	.LBB35_364
.LBB35_350:
.Ltmp2877:
	movq	%rax, %rbx
	jmp	.LBB35_366
.LBB35_351:
.Ltmp3290:
	movq	%rax, %rbx
	jmp	.LBB35_397
.LBB35_352:
.Ltmp3156:
	movq	%rax, %rbx
	jmp	.LBB35_397
.LBB35_353:
.Ltmp3325:
	movq	%rax, %rbx
	jmp	.LBB35_403
.LBB35_354:
.Ltmp3274:
	movq	%rax, %rbx
	jmp	.LBB35_403
.LBB35_355:
.Ltmp3170:
	movq	%rax, %rbx
	jmp	.LBB35_403
.LBB35_356:
.Ltmp3140:
	movq	%rax, %rbx
	jmp	.LBB35_403
.LBB35_357:
.Ltmp3032:
	jmp	.LBB35_364
.LBB35_358:
.Ltmp3016:
	movq	%rax, %rbx
	jmp	.LBB35_366
.LBB35_359:
.Ltmp3000:
	jmp	.LBB35_369
.LBB35_360:
.Ltmp2968:
	jmp	.LBB35_364
.LBB35_361:
.Ltmp2952:
	movq	%rax, %rbx
	jmp	.LBB35_366
.LBB35_362:
.Ltmp2936:
	jmp	.LBB35_369
.LBB35_363:
.Ltmp2904:
.LBB35_364:
	movq	%rax, %rbx
.LBB35_365:
	leaq	8(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB35_366:
	leaq	80(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	leaq	144(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB35_367:
.Ltmp2888:
	movq	%rax, %rbx
	jmp	.LBB35_366
.LBB35_368:
.Ltmp2872:
.LBB35_369:
	movq	%rax, %rbx
	leaq	144(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB35_370:
.Ltmp3314:
	movq	%rax, %rbx
	jmp	.LBB35_403
.LBB35_371:
.Ltmp3263:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_372:
.Ltmp3260:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_373:
.Ltmp3257:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_374:
.Ltmp3209:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_375:
.Ltmp3129:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_376:
.Ltmp3126:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_377:
.Ltmp3123:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_378:
.Ltmp3075:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB35_379:
.Ltmp3190:
	movq	%rax, %rbx
	jmp	.LBB35_403
.LBB35_380:
.Ltmp3056:
	movq	%rax, %rbx
	jmp	.LBB35_403
.LBB35_381:
.Ltmp3241:
	jmp	.LBB35_390
.LBB35_382:
.Ltmp3225:
	jmp	.LBB35_395
.LBB35_383:
.Ltmp3206:
	jmp	.LBB35_387
.LBB35_384:
.Ltmp3107:
	jmp	.LBB35_390
.LBB35_385:
.Ltmp3091:
	jmp	.LBB35_395
.LBB35_386:
.Ltmp3072:
.LBB35_387:
	movq	%rax, %rbx
	leaq	8(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB35_401
.LBB35_388:
.Ltmp3254:
	jmp	.LBB35_390
.LBB35_389:
.Ltmp3120:
.LBB35_390:
	movq	%rax, %rbx
	leaq	80(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB35_396
.LBB35_391:
.Ltmp3236:
	jmp	.LBB35_395
.LBB35_392:
.Ltmp3220:
	movq	%rax, %rbx
	jmp	.LBB35_397
.LBB35_393:
.Ltmp3201:
	jmp	.LBB35_400
.LBB35_394:
.Ltmp3102:
.LBB35_395:
	movq	%rax, %rbx
.LBB35_396:
	leaq	144(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB35_397:
	leaq	8(%rsp), %rdi
	jmp	.LBB35_402
.LBB35_398:
.Ltmp3086:
	movq	%rax, %rbx
	jmp	.LBB35_397
.LBB35_399:
.Ltmp3067:
.LBB35_400:
	movq	%rax, %rbx
.LBB35_401:
	leaq	80(%rsp), %rdi
.LBB35_402:
	callq	_ZN4mpfr6mprealD2Ev
.LBB35_403:
	leaq	40(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end35:
	.size	_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_, .Lfunc_end35-_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.cfi_endproc
	.section	.gcc_except_table._Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"aG",@progbits,_Z6verifyIN7mX_real7dX_real7dx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.p2align	2, 0x0
GCC_except_table35:
.Lexception28:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase17-.Lttbaseref17
.Lttbaseref17:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end28-.Lcst_begin28
.Lcst_begin28:
	.uleb128 .Lfunc_begin28-.Lfunc_begin28  # >> Call Site 1 <<
	.uleb128 .Ltmp2862-.Lfunc_begin28       #   Call between .Lfunc_begin28 and .Ltmp2862
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2862-.Lfunc_begin28       # >> Call Site 2 <<
	.uleb128 .Ltmp2871-.Ltmp2862            #   Call between .Ltmp2862 and .Ltmp2871
	.uleb128 .Ltmp2872-.Lfunc_begin28       #     jumps to .Ltmp2872
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2873-.Lfunc_begin28       # >> Call Site 3 <<
	.uleb128 .Ltmp2876-.Ltmp2873            #   Call between .Ltmp2873 and .Ltmp2876
	.uleb128 .Ltmp2877-.Lfunc_begin28       #     jumps to .Ltmp2877
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2878-.Lfunc_begin28       # >> Call Site 4 <<
	.uleb128 .Ltmp2887-.Ltmp2878            #   Call between .Ltmp2878 and .Ltmp2887
	.uleb128 .Ltmp2888-.Lfunc_begin28       #     jumps to .Ltmp2888
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2889-.Lfunc_begin28       # >> Call Site 5 <<
	.uleb128 .Ltmp2892-.Ltmp2889            #   Call between .Ltmp2889 and .Ltmp2892
	.uleb128 .Ltmp2893-.Lfunc_begin28       #     jumps to .Ltmp2893
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2894-.Lfunc_begin28       # >> Call Site 6 <<
	.uleb128 .Ltmp2903-.Ltmp2894            #   Call between .Ltmp2894 and .Ltmp2903
	.uleb128 .Ltmp2904-.Lfunc_begin28       #     jumps to .Ltmp2904
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2905-.Lfunc_begin28       # >> Call Site 7 <<
	.uleb128 .Ltmp2906-.Ltmp2905            #   Call between .Ltmp2905 and .Ltmp2906
	.uleb128 .Ltmp2907-.Lfunc_begin28       #     jumps to .Ltmp2907
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2908-.Lfunc_begin28       # >> Call Site 8 <<
	.uleb128 .Ltmp2909-.Ltmp2908            #   Call between .Ltmp2908 and .Ltmp2909
	.uleb128 .Ltmp2910-.Lfunc_begin28       #     jumps to .Ltmp2910
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2911-.Lfunc_begin28       # >> Call Site 9 <<
	.uleb128 .Ltmp2912-.Ltmp2911            #   Call between .Ltmp2911 and .Ltmp2912
	.uleb128 .Ltmp2913-.Lfunc_begin28       #     jumps to .Ltmp2913
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2914-.Lfunc_begin28       # >> Call Site 10 <<
	.uleb128 .Ltmp2915-.Ltmp2914            #   Call between .Ltmp2914 and .Ltmp2915
	.uleb128 .Ltmp2916-.Lfunc_begin28       #     jumps to .Ltmp2916
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2917-.Lfunc_begin28       # >> Call Site 11 <<
	.uleb128 .Ltmp2918-.Ltmp2917            #   Call between .Ltmp2917 and .Ltmp2918
	.uleb128 .Ltmp2919-.Lfunc_begin28       #     jumps to .Ltmp2919
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2920-.Lfunc_begin28       # >> Call Site 12 <<
	.uleb128 .Ltmp2921-.Ltmp2920            #   Call between .Ltmp2920 and .Ltmp2921
	.uleb128 .Ltmp2922-.Lfunc_begin28       #     jumps to .Ltmp2922
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2923-.Lfunc_begin28       # >> Call Site 13 <<
	.uleb128 .Ltmp2924-.Ltmp2923            #   Call between .Ltmp2923 and .Ltmp2924
	.uleb128 .Ltmp2925-.Lfunc_begin28       #     jumps to .Ltmp2925
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2924-.Lfunc_begin28       # >> Call Site 14 <<
	.uleb128 .Ltmp2926-.Ltmp2924            #   Call between .Ltmp2924 and .Ltmp2926
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2926-.Lfunc_begin28       # >> Call Site 15 <<
	.uleb128 .Ltmp2935-.Ltmp2926            #   Call between .Ltmp2926 and .Ltmp2935
	.uleb128 .Ltmp2936-.Lfunc_begin28       #     jumps to .Ltmp2936
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2937-.Lfunc_begin28       # >> Call Site 16 <<
	.uleb128 .Ltmp2940-.Ltmp2937            #   Call between .Ltmp2937 and .Ltmp2940
	.uleb128 .Ltmp2941-.Lfunc_begin28       #     jumps to .Ltmp2941
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2942-.Lfunc_begin28       # >> Call Site 17 <<
	.uleb128 .Ltmp2951-.Ltmp2942            #   Call between .Ltmp2942 and .Ltmp2951
	.uleb128 .Ltmp2952-.Lfunc_begin28       #     jumps to .Ltmp2952
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2953-.Lfunc_begin28       # >> Call Site 18 <<
	.uleb128 .Ltmp2956-.Ltmp2953            #   Call between .Ltmp2953 and .Ltmp2956
	.uleb128 .Ltmp2957-.Lfunc_begin28       #     jumps to .Ltmp2957
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2958-.Lfunc_begin28       # >> Call Site 19 <<
	.uleb128 .Ltmp2967-.Ltmp2958            #   Call between .Ltmp2958 and .Ltmp2967
	.uleb128 .Ltmp2968-.Lfunc_begin28       #     jumps to .Ltmp2968
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2969-.Lfunc_begin28       # >> Call Site 20 <<
	.uleb128 .Ltmp2970-.Ltmp2969            #   Call between .Ltmp2969 and .Ltmp2970
	.uleb128 .Ltmp2971-.Lfunc_begin28       #     jumps to .Ltmp2971
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2972-.Lfunc_begin28       # >> Call Site 21 <<
	.uleb128 .Ltmp2973-.Ltmp2972            #   Call between .Ltmp2972 and .Ltmp2973
	.uleb128 .Ltmp2974-.Lfunc_begin28       #     jumps to .Ltmp2974
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2975-.Lfunc_begin28       # >> Call Site 22 <<
	.uleb128 .Ltmp2976-.Ltmp2975            #   Call between .Ltmp2975 and .Ltmp2976
	.uleb128 .Ltmp2977-.Lfunc_begin28       #     jumps to .Ltmp2977
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2978-.Lfunc_begin28       # >> Call Site 23 <<
	.uleb128 .Ltmp2979-.Ltmp2978            #   Call between .Ltmp2978 and .Ltmp2979
	.uleb128 .Ltmp2980-.Lfunc_begin28       #     jumps to .Ltmp2980
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2981-.Lfunc_begin28       # >> Call Site 24 <<
	.uleb128 .Ltmp2982-.Ltmp2981            #   Call between .Ltmp2981 and .Ltmp2982
	.uleb128 .Ltmp2983-.Lfunc_begin28       #     jumps to .Ltmp2983
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2984-.Lfunc_begin28       # >> Call Site 25 <<
	.uleb128 .Ltmp2985-.Ltmp2984            #   Call between .Ltmp2984 and .Ltmp2985
	.uleb128 .Ltmp2986-.Lfunc_begin28       #     jumps to .Ltmp2986
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp2987-.Lfunc_begin28       # >> Call Site 26 <<
	.uleb128 .Ltmp2988-.Ltmp2987            #   Call between .Ltmp2987 and .Ltmp2988
	.uleb128 .Ltmp2989-.Lfunc_begin28       #     jumps to .Ltmp2989
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2988-.Lfunc_begin28       # >> Call Site 27 <<
	.uleb128 .Ltmp2990-.Ltmp2988            #   Call between .Ltmp2988 and .Ltmp2990
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp2990-.Lfunc_begin28       # >> Call Site 28 <<
	.uleb128 .Ltmp2999-.Ltmp2990            #   Call between .Ltmp2990 and .Ltmp2999
	.uleb128 .Ltmp3000-.Lfunc_begin28       #     jumps to .Ltmp3000
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3001-.Lfunc_begin28       # >> Call Site 29 <<
	.uleb128 .Ltmp3004-.Ltmp3001            #   Call between .Ltmp3001 and .Ltmp3004
	.uleb128 .Ltmp3005-.Lfunc_begin28       #     jumps to .Ltmp3005
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3006-.Lfunc_begin28       # >> Call Site 30 <<
	.uleb128 .Ltmp3015-.Ltmp3006            #   Call between .Ltmp3006 and .Ltmp3015
	.uleb128 .Ltmp3016-.Lfunc_begin28       #     jumps to .Ltmp3016
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3017-.Lfunc_begin28       # >> Call Site 31 <<
	.uleb128 .Ltmp3020-.Ltmp3017            #   Call between .Ltmp3017 and .Ltmp3020
	.uleb128 .Ltmp3021-.Lfunc_begin28       #     jumps to .Ltmp3021
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3022-.Lfunc_begin28       # >> Call Site 32 <<
	.uleb128 .Ltmp3031-.Ltmp3022            #   Call between .Ltmp3022 and .Ltmp3031
	.uleb128 .Ltmp3032-.Lfunc_begin28       #     jumps to .Ltmp3032
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3033-.Lfunc_begin28       # >> Call Site 33 <<
	.uleb128 .Ltmp3034-.Ltmp3033            #   Call between .Ltmp3033 and .Ltmp3034
	.uleb128 .Ltmp3035-.Lfunc_begin28       #     jumps to .Ltmp3035
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3036-.Lfunc_begin28       # >> Call Site 34 <<
	.uleb128 .Ltmp3037-.Ltmp3036            #   Call between .Ltmp3036 and .Ltmp3037
	.uleb128 .Ltmp3038-.Lfunc_begin28       #     jumps to .Ltmp3038
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3039-.Lfunc_begin28       # >> Call Site 35 <<
	.uleb128 .Ltmp3040-.Ltmp3039            #   Call between .Ltmp3039 and .Ltmp3040
	.uleb128 .Ltmp3041-.Lfunc_begin28       #     jumps to .Ltmp3041
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3042-.Lfunc_begin28       # >> Call Site 36 <<
	.uleb128 .Ltmp3043-.Ltmp3042            #   Call between .Ltmp3042 and .Ltmp3043
	.uleb128 .Ltmp3044-.Lfunc_begin28       #     jumps to .Ltmp3044
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3045-.Lfunc_begin28       # >> Call Site 37 <<
	.uleb128 .Ltmp3046-.Ltmp3045            #   Call between .Ltmp3045 and .Ltmp3046
	.uleb128 .Ltmp3047-.Lfunc_begin28       #     jumps to .Ltmp3047
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3048-.Lfunc_begin28       # >> Call Site 38 <<
	.uleb128 .Ltmp3049-.Ltmp3048            #   Call between .Ltmp3048 and .Ltmp3049
	.uleb128 .Ltmp3050-.Lfunc_begin28       #     jumps to .Ltmp3050
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3051-.Lfunc_begin28       # >> Call Site 39 <<
	.uleb128 .Ltmp3052-.Ltmp3051            #   Call between .Ltmp3051 and .Ltmp3052
	.uleb128 .Ltmp3053-.Lfunc_begin28       #     jumps to .Ltmp3053
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3052-.Lfunc_begin28       # >> Call Site 40 <<
	.uleb128 .Ltmp3054-.Ltmp3052            #   Call between .Ltmp3052 and .Ltmp3054
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3054-.Lfunc_begin28       # >> Call Site 41 <<
	.uleb128 .Ltmp3055-.Ltmp3054            #   Call between .Ltmp3054 and .Ltmp3055
	.uleb128 .Ltmp3056-.Lfunc_begin28       #     jumps to .Ltmp3056
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3057-.Lfunc_begin28       # >> Call Site 42 <<
	.uleb128 .Ltmp3066-.Ltmp3057            #   Call between .Ltmp3057 and .Ltmp3066
	.uleb128 .Ltmp3067-.Lfunc_begin28       #     jumps to .Ltmp3067
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3068-.Lfunc_begin28       # >> Call Site 43 <<
	.uleb128 .Ltmp3071-.Ltmp3068            #   Call between .Ltmp3068 and .Ltmp3071
	.uleb128 .Ltmp3072-.Lfunc_begin28       #     jumps to .Ltmp3072
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3073-.Lfunc_begin28       # >> Call Site 44 <<
	.uleb128 .Ltmp3074-.Ltmp3073            #   Call between .Ltmp3073 and .Ltmp3074
	.uleb128 .Ltmp3075-.Lfunc_begin28       #     jumps to .Ltmp3075
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3076-.Lfunc_begin28       # >> Call Site 45 <<
	.uleb128 .Ltmp3085-.Ltmp3076            #   Call between .Ltmp3076 and .Ltmp3085
	.uleb128 .Ltmp3086-.Lfunc_begin28       #     jumps to .Ltmp3086
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3087-.Lfunc_begin28       # >> Call Site 46 <<
	.uleb128 .Ltmp3090-.Ltmp3087            #   Call between .Ltmp3087 and .Ltmp3090
	.uleb128 .Ltmp3091-.Lfunc_begin28       #     jumps to .Ltmp3091
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3092-.Lfunc_begin28       # >> Call Site 47 <<
	.uleb128 .Ltmp3101-.Ltmp3092            #   Call between .Ltmp3092 and .Ltmp3101
	.uleb128 .Ltmp3102-.Lfunc_begin28       #     jumps to .Ltmp3102
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3103-.Lfunc_begin28       # >> Call Site 48 <<
	.uleb128 .Ltmp3106-.Ltmp3103            #   Call between .Ltmp3103 and .Ltmp3106
	.uleb128 .Ltmp3107-.Lfunc_begin28       #     jumps to .Ltmp3107
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3108-.Lfunc_begin28       # >> Call Site 49 <<
	.uleb128 .Ltmp3119-.Ltmp3108            #   Call between .Ltmp3108 and .Ltmp3119
	.uleb128 .Ltmp3120-.Lfunc_begin28       #     jumps to .Ltmp3120
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3121-.Lfunc_begin28       # >> Call Site 50 <<
	.uleb128 .Ltmp3122-.Ltmp3121            #   Call between .Ltmp3121 and .Ltmp3122
	.uleb128 .Ltmp3123-.Lfunc_begin28       #     jumps to .Ltmp3123
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3124-.Lfunc_begin28       # >> Call Site 51 <<
	.uleb128 .Ltmp3125-.Ltmp3124            #   Call between .Ltmp3124 and .Ltmp3125
	.uleb128 .Ltmp3126-.Lfunc_begin28       #     jumps to .Ltmp3126
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3127-.Lfunc_begin28       # >> Call Site 52 <<
	.uleb128 .Ltmp3128-.Ltmp3127            #   Call between .Ltmp3127 and .Ltmp3128
	.uleb128 .Ltmp3129-.Lfunc_begin28       #     jumps to .Ltmp3129
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3130-.Lfunc_begin28       # >> Call Site 53 <<
	.uleb128 .Ltmp3139-.Ltmp3130            #   Call between .Ltmp3130 and .Ltmp3139
	.uleb128 .Ltmp3140-.Lfunc_begin28       #     jumps to .Ltmp3140
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3141-.Lfunc_begin28       # >> Call Site 54 <<
	.uleb128 .Ltmp3142-.Ltmp3141            #   Call between .Ltmp3141 and .Ltmp3142
	.uleb128 .Ltmp3143-.Lfunc_begin28       #     jumps to .Ltmp3143
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3144-.Lfunc_begin28       # >> Call Site 55 <<
	.uleb128 .Ltmp3155-.Ltmp3144            #   Call between .Ltmp3144 and .Ltmp3155
	.uleb128 .Ltmp3156-.Lfunc_begin28       #     jumps to .Ltmp3156
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3157-.Lfunc_begin28       # >> Call Site 56 <<
	.uleb128 .Ltmp3158-.Ltmp3157            #   Call between .Ltmp3157 and .Ltmp3158
	.uleb128 .Ltmp3159-.Lfunc_begin28       #     jumps to .Ltmp3159
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3160-.Lfunc_begin28       # >> Call Site 57 <<
	.uleb128 .Ltmp3169-.Ltmp3160            #   Call between .Ltmp3160 and .Ltmp3169
	.uleb128 .Ltmp3170-.Lfunc_begin28       #     jumps to .Ltmp3170
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3171-.Lfunc_begin28       # >> Call Site 58 <<
	.uleb128 .Ltmp3174-.Ltmp3171            #   Call between .Ltmp3171 and .Ltmp3174
	.uleb128 .Ltmp3175-.Lfunc_begin28       #     jumps to .Ltmp3175
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3176-.Lfunc_begin28       # >> Call Site 59 <<
	.uleb128 .Ltmp3177-.Ltmp3176            #   Call between .Ltmp3176 and .Ltmp3177
	.uleb128 .Ltmp3178-.Lfunc_begin28       #     jumps to .Ltmp3178
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3179-.Lfunc_begin28       # >> Call Site 60 <<
	.uleb128 .Ltmp3180-.Ltmp3179            #   Call between .Ltmp3179 and .Ltmp3180
	.uleb128 .Ltmp3181-.Lfunc_begin28       #     jumps to .Ltmp3181
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3182-.Lfunc_begin28       # >> Call Site 61 <<
	.uleb128 .Ltmp3183-.Ltmp3182            #   Call between .Ltmp3182 and .Ltmp3183
	.uleb128 .Ltmp3184-.Lfunc_begin28       #     jumps to .Ltmp3184
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3185-.Lfunc_begin28       # >> Call Site 62 <<
	.uleb128 .Ltmp3186-.Ltmp3185            #   Call between .Ltmp3185 and .Ltmp3186
	.uleb128 .Ltmp3187-.Lfunc_begin28       #     jumps to .Ltmp3187
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3186-.Lfunc_begin28       # >> Call Site 63 <<
	.uleb128 .Ltmp3188-.Ltmp3186            #   Call between .Ltmp3186 and .Ltmp3188
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3188-.Lfunc_begin28       # >> Call Site 64 <<
	.uleb128 .Ltmp3189-.Ltmp3188            #   Call between .Ltmp3188 and .Ltmp3189
	.uleb128 .Ltmp3190-.Lfunc_begin28       #     jumps to .Ltmp3190
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3191-.Lfunc_begin28       # >> Call Site 65 <<
	.uleb128 .Ltmp3200-.Ltmp3191            #   Call between .Ltmp3191 and .Ltmp3200
	.uleb128 .Ltmp3201-.Lfunc_begin28       #     jumps to .Ltmp3201
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3202-.Lfunc_begin28       # >> Call Site 66 <<
	.uleb128 .Ltmp3205-.Ltmp3202            #   Call between .Ltmp3202 and .Ltmp3205
	.uleb128 .Ltmp3206-.Lfunc_begin28       #     jumps to .Ltmp3206
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3207-.Lfunc_begin28       # >> Call Site 67 <<
	.uleb128 .Ltmp3208-.Ltmp3207            #   Call between .Ltmp3207 and .Ltmp3208
	.uleb128 .Ltmp3209-.Lfunc_begin28       #     jumps to .Ltmp3209
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3210-.Lfunc_begin28       # >> Call Site 68 <<
	.uleb128 .Ltmp3219-.Ltmp3210            #   Call between .Ltmp3210 and .Ltmp3219
	.uleb128 .Ltmp3220-.Lfunc_begin28       #     jumps to .Ltmp3220
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3221-.Lfunc_begin28       # >> Call Site 69 <<
	.uleb128 .Ltmp3224-.Ltmp3221            #   Call between .Ltmp3221 and .Ltmp3224
	.uleb128 .Ltmp3225-.Lfunc_begin28       #     jumps to .Ltmp3225
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3226-.Lfunc_begin28       # >> Call Site 70 <<
	.uleb128 .Ltmp3235-.Ltmp3226            #   Call between .Ltmp3226 and .Ltmp3235
	.uleb128 .Ltmp3236-.Lfunc_begin28       #     jumps to .Ltmp3236
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3237-.Lfunc_begin28       # >> Call Site 71 <<
	.uleb128 .Ltmp3240-.Ltmp3237            #   Call between .Ltmp3237 and .Ltmp3240
	.uleb128 .Ltmp3241-.Lfunc_begin28       #     jumps to .Ltmp3241
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3242-.Lfunc_begin28       # >> Call Site 72 <<
	.uleb128 .Ltmp3253-.Ltmp3242            #   Call between .Ltmp3242 and .Ltmp3253
	.uleb128 .Ltmp3254-.Lfunc_begin28       #     jumps to .Ltmp3254
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3255-.Lfunc_begin28       # >> Call Site 73 <<
	.uleb128 .Ltmp3256-.Ltmp3255            #   Call between .Ltmp3255 and .Ltmp3256
	.uleb128 .Ltmp3257-.Lfunc_begin28       #     jumps to .Ltmp3257
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3258-.Lfunc_begin28       # >> Call Site 74 <<
	.uleb128 .Ltmp3259-.Ltmp3258            #   Call between .Ltmp3258 and .Ltmp3259
	.uleb128 .Ltmp3260-.Lfunc_begin28       #     jumps to .Ltmp3260
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3261-.Lfunc_begin28       # >> Call Site 75 <<
	.uleb128 .Ltmp3262-.Ltmp3261            #   Call between .Ltmp3261 and .Ltmp3262
	.uleb128 .Ltmp3263-.Lfunc_begin28       #     jumps to .Ltmp3263
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3264-.Lfunc_begin28       # >> Call Site 76 <<
	.uleb128 .Ltmp3273-.Ltmp3264            #   Call between .Ltmp3264 and .Ltmp3273
	.uleb128 .Ltmp3274-.Lfunc_begin28       #     jumps to .Ltmp3274
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3275-.Lfunc_begin28       # >> Call Site 77 <<
	.uleb128 .Ltmp3276-.Ltmp3275            #   Call between .Ltmp3275 and .Ltmp3276
	.uleb128 .Ltmp3277-.Lfunc_begin28       #     jumps to .Ltmp3277
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3278-.Lfunc_begin28       # >> Call Site 78 <<
	.uleb128 .Ltmp3289-.Ltmp3278            #   Call between .Ltmp3278 and .Ltmp3289
	.uleb128 .Ltmp3290-.Lfunc_begin28       #     jumps to .Ltmp3290
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3291-.Lfunc_begin28       # >> Call Site 79 <<
	.uleb128 .Ltmp3292-.Ltmp3291            #   Call between .Ltmp3291 and .Ltmp3292
	.uleb128 .Ltmp3293-.Lfunc_begin28       #     jumps to .Ltmp3293
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3294-.Lfunc_begin28       # >> Call Site 80 <<
	.uleb128 .Ltmp3313-.Ltmp3294            #   Call between .Ltmp3294 and .Ltmp3313
	.uleb128 .Ltmp3314-.Lfunc_begin28       #     jumps to .Ltmp3314
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3315-.Lfunc_begin28       # >> Call Site 81 <<
	.uleb128 .Ltmp3324-.Ltmp3315            #   Call between .Ltmp3315 and .Ltmp3324
	.uleb128 .Ltmp3325-.Lfunc_begin28       #     jumps to .Ltmp3325
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3326-.Lfunc_begin28       # >> Call Site 82 <<
	.uleb128 .Ltmp3329-.Ltmp3326            #   Call between .Ltmp3326 and .Ltmp3329
	.uleb128 .Ltmp3330-.Lfunc_begin28       #     jumps to .Ltmp3330
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3331-.Lfunc_begin28       # >> Call Site 83 <<
	.uleb128 .Ltmp3332-.Ltmp3331            #   Call between .Ltmp3331 and .Ltmp3332
	.uleb128 .Ltmp3333-.Lfunc_begin28       #     jumps to .Ltmp3333
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3334-.Lfunc_begin28       # >> Call Site 84 <<
	.uleb128 .Ltmp3335-.Ltmp3334            #   Call between .Ltmp3334 and .Ltmp3335
	.uleb128 .Ltmp3336-.Lfunc_begin28       #     jumps to .Ltmp3336
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3337-.Lfunc_begin28       # >> Call Site 85 <<
	.uleb128 .Ltmp3338-.Ltmp3337            #   Call between .Ltmp3337 and .Ltmp3338
	.uleb128 .Ltmp3339-.Lfunc_begin28       #     jumps to .Ltmp3339
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3340-.Lfunc_begin28       # >> Call Site 86 <<
	.uleb128 .Ltmp3341-.Ltmp3340            #   Call between .Ltmp3340 and .Ltmp3341
	.uleb128 .Ltmp3342-.Lfunc_begin28       #     jumps to .Ltmp3342
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3343-.Lfunc_begin28       # >> Call Site 87 <<
	.uleb128 .Ltmp3344-.Ltmp3343            #   Call between .Ltmp3343 and .Ltmp3344
	.uleb128 .Ltmp3345-.Lfunc_begin28       #     jumps to .Ltmp3345
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3346-.Lfunc_begin28       # >> Call Site 88 <<
	.uleb128 .Ltmp3347-.Ltmp3346            #   Call between .Ltmp3346 and .Ltmp3347
	.uleb128 .Ltmp3348-.Lfunc_begin28       #     jumps to .Ltmp3348
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3347-.Lfunc_begin28       # >> Call Site 89 <<
	.uleb128 .Lfunc_end35-.Ltmp3347         #   Call between .Ltmp3347 and .Lfunc_end35
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end28:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase17:
	.p2align	2, 0x0
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4, 0x0                          # -- Begin function _Z6verifyIdEvRKiRKN4mpfr6mprealEPS3_S6_S6_
.LCPI36_0:
	.quad	0x7fffffffffffffff              #  NaN
	.quad	0x7fffffffffffffff              #  NaN
	.section	.text._Z6verifyIdEvRKiRKN4mpfr6mprealEPS3_S6_S6_,"axG",@progbits,_Z6verifyIdEvRKiRKN4mpfr6mprealEPS3_S6_S6_,comdat
	.weak	_Z6verifyIdEvRKiRKN4mpfr6mprealEPS3_S6_S6_
	.p2align	4, 0x90
	.type	_Z6verifyIdEvRKiRKN4mpfr6mprealEPS3_S6_S6_,@function
_Z6verifyIdEvRKiRKN4mpfr6mprealEPS3_S6_S6_: # 
.Lfunc_begin29:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception29
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$472, %rsp                      # imm = 0x1D8
	.cfi_def_cfa_offset 528
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, 160(%rsp)                  # 8-byte Spill
	movq	%rcx, 88(%rsp)                  # 8-byte Spill
	movq	%rdx, %r12
	movq	%rsi, 24(%rsp)                  # 8-byte Spill
	movq	%rdi, %r15
	vcvtsi2sdl	(%rdi), %xmm0, %xmm0
	callq	sqrt
	vcvttsd2si	%xmm0, %eax
	movl	%eax, 168(%rsp)                 # 4-byte Spill
	movl	(%r15), %r13d
	movq	%r13, %rax
	shlq	$3, %rax
	testl	%r13d, %r13d
	movq	$-1, %r14
	cmovnsq	%rax, %r14
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %rbx
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %r14
	testl	%r13d, %r13d
	jle	.LBB36_3
# %bb.1:
	xorl	%ebp, %ebp
	movq	88(%rsp), %r13                  # 8-byte Reload
	.p2align	4, 0x90
.LBB36_2:                               # =>This Inner Loop Header: Depth=1
	movq	%r12, %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, (%rbx,%rbp,8)
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, (%r14,%rbp,8)
	incq	%rbp
	movslq	(%r15), %rax
	addq	$32, %r13
	addq	$32, %r12
	cmpq	%rax, %rbp
	jl	.LBB36_2
.LBB36_3:
	movq	24(%rsp), %rdi                  # 8-byte Reload
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, 224(%rsp)                # 8-byte Spill
	movl	$80, %edi
	callq	_Znam
	movq	%rax, %rbp
	callq	omp_get_wtime
	movq	%r15, 216(%rsp)                 # 8-byte Spill
	movl	(%r15), %eax
	testl	%eax, %eax
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
	jle	.LBB36_12
# %bb.4:
	movl	%eax, %ecx
	andl	$-8, %ecx
	leaq	(,%rax,8), %rdx
	andq	$-64, %rdx
	xorl	%esi, %esi
	vcvtsi2sd	%esi, %xmm1, %xmm0
	jmp	.LBB36_6
	.p2align	4, 0x90
.LBB36_5:                               #   in Loop: Header=BB36_6 Depth=1
	vmovsd	%xmm1, (%rbp,%rsi,8)
	leaq	1(%rsi), %rdi
	cmpq	$9, %rsi
	movq	%rdi, %rsi
	je	.LBB36_13
.LBB36_6:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB36_8 Depth 2
                                        #     Child Loop BB36_11 Depth 2
	vmovapd	%xmm0, %xmm1
	cmpl	$8, %eax
	jb	.LBB36_9
# %bb.7:                                #   in Loop: Header=BB36_6 Depth=1
	xorl	%edi, %edi
	vmovapd	%xmm0, %xmm1
	.p2align	4, 0x90
.LBB36_8:                               #   Parent Loop BB36_6 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	(%rbx,%rdi), %xmm2              # xmm2 = mem[0],zero
	vmovsd	8(%rbx,%rdi), %xmm3             # xmm3 = mem[0],zero
	vmulsd	(%r14,%rdi), %xmm2, %xmm2
	vmulsd	8(%r14,%rdi), %xmm3, %xmm3
	vaddsd	%xmm2, %xmm1, %xmm1
	vmovsd	16(%rbx,%rdi), %xmm2            # xmm2 = mem[0],zero
	vmulsd	16(%r14,%rdi), %xmm2, %xmm2
	vaddsd	%xmm3, %xmm1, %xmm1
	vmovsd	24(%rbx,%rdi), %xmm3            # xmm3 = mem[0],zero
	vmulsd	24(%r14,%rdi), %xmm3, %xmm3
	vaddsd	%xmm2, %xmm1, %xmm1
	vmovsd	32(%rbx,%rdi), %xmm2            # xmm2 = mem[0],zero
	vmulsd	32(%r14,%rdi), %xmm2, %xmm2
	vaddsd	%xmm3, %xmm1, %xmm1
	vmovsd	40(%rbx,%rdi), %xmm3            # xmm3 = mem[0],zero
	vmulsd	40(%r14,%rdi), %xmm3, %xmm3
	vaddsd	%xmm2, %xmm1, %xmm1
	vmovsd	48(%rbx,%rdi), %xmm2            # xmm2 = mem[0],zero
	vmulsd	48(%r14,%rdi), %xmm2, %xmm2
	vaddsd	%xmm3, %xmm1, %xmm1
	vmovsd	56(%rbx,%rdi), %xmm3            # xmm3 = mem[0],zero
	vmulsd	56(%r14,%rdi), %xmm3, %xmm3
	vaddsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm3, %xmm1, %xmm1
	addq	$64, %rdi
	cmpq	%rdi, %rdx
	jne	.LBB36_8
.LBB36_9:                               #   in Loop: Header=BB36_6 Depth=1
	cmpq	%rax, %rcx
	jae	.LBB36_5
# %bb.10:                               #   in Loop: Header=BB36_6 Depth=1
	movq	%rcx, %rdi
	.p2align	4, 0x90
.LBB36_11:                              #   Parent Loop BB36_6 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	(%rbx,%rdi,8), %xmm2            # xmm2 = mem[0],zero
	vmulsd	(%r14,%rdi,8), %xmm2, %xmm2
	vaddsd	%xmm2, %xmm1, %xmm1
	incq	%rdi
	cmpq	%rdi, %rax
	jne	.LBB36_11
	jmp	.LBB36_5
.LBB36_12:
	xorl	%eax, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm0
	vmovsd	%xmm0, (%rbp)
	vcvtsi2sd	%eax, %xmm1, %xmm0
	vmovsd	%xmm0, 8(%rbp)
	vcvtsi2sd	%eax, %xmm1, %xmm0
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vmovsd	%xmm0, 16(%rbp)
	vmovsd	%xmm1, 24(%rbp)
	vcvtsi2sd	%eax, %xmm2, %xmm0
	vmovsd	%xmm0, 32(%rbp)
	vcvtsi2sd	%eax, %xmm2, %xmm0
	vmovsd	%xmm0, 40(%rbp)
	vcvtsi2sd	%eax, %xmm2, %xmm0
	vcvtsi2sd	%eax, %xmm2, %xmm1
	vmovsd	%xmm0, 48(%rbp)
	vmovsd	%xmm1, 56(%rbp)
	vcvtsi2sd	%eax, %xmm2, %xmm0
	vmovsd	%xmm0, 64(%rbp)
	vcvtsi2sd	%eax, %xmm2, %xmm0
	vmovsd	%xmm0, 72(%rbp)
.LBB36_13:
	callq	omp_get_wtime
	vsubsd	24(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm4, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	vmovsd	(%rbp), %xmm0                   # xmm0 = mem[0],zero
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %r12d
	leaq	176(%rsp), %r13
	movq	%r13, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r13, %rdi
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%r12d, %esi
	callq	mpfr_set_d
.Ltmp3349:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp3350:
# %bb.14:
.Ltmp3351:
	movq	%rax, %r12
	movq	160(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp3352:
# %bb.15:
.Ltmp3353:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3354:
# %bb.16:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp3355:
	leaq	128(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3356:
# %bb.17:
.Ltmp3357:
	leaq	128(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp3358:
# %bb.18:
.Ltmp3360:
	callq	mpfr_get_default_rounding_mode
.Ltmp3361:
# %bb.19:
.Ltmp3362:
	leaq	128(%rsp), %rdi
	leaq	176(%rsp), %rsi
	movq	160(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp3363:
# %bb.20:
.Ltmp3365:
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3366:
# %bb.21:
.Ltmp3367:
	movq	%rax, %r12
	movq	160(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp3368:
# %bb.22:
.Ltmp3369:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3370:
# %bb.23:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp3371:
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3372:
# %bb.24:
.Ltmp3373:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp3374:
# %bb.25:
.Ltmp3376:
	callq	mpfr_get_default_rounding_mode
.Ltmp3377:
# %bb.26:
.Ltmp3378:
	leaq	40(%rsp), %rdi
	leaq	128(%rsp), %rsi
	movq	160(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp3379:
# %bb.27:
.Ltmp3381:
	callq	mpfr_get_default_rounding_mode
.Ltmp3382:
# %bb.28:
.Ltmp3383:
	movl	%eax, %r15d
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3384:
# %bb.29:
.Ltmp3385:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3386:
# %bb.30:
.Ltmp3387:
	movl	%eax, %r12d
	leaq	96(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3388:
# %bb.31:
.Ltmp3389:
	leaq	96(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3390:
# %bb.32:
.Ltmp3392:
	leaq	96(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp3393:
# %bb.33:
.Ltmp3395:
	leaq	96(%rsp), %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
.Ltmp3396:
# %bb.34:
	cmpq	$0, 120(%rsp)
	je	.LBB36_36
# %bb.35:
.Ltmp3398:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3399:
.LBB36_36:
	cmpq	$0, 64(%rsp)
	je	.LBB36_38
# %bb.37:
.Ltmp3401:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3402:
.LBB36_38:
	cmpq	$0, 152(%rsp)
	je	.LBB36_40
# %bb.39:
.Ltmp3404:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3405:
.LBB36_40:
	cmpq	$0, 200(%rsp)
	je	.LBB36_42
# %bb.41:
.Ltmp3407:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3408:
.LBB36_42:
	leaq	456(%rsp), %r15
	movq	%r15, 440(%rsp)
	movl	$544501604, 456(%rsp)           # imm = 0x20746F64
	movw	$32, 460(%rsp)
	movq	$5, 448(%rsp)
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	%xmm0, 304(%rsp)
	fldl	304(%rsp)
	fstpt	72(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp3410:
	movl	$_ZSt4cout, %edi
	movl	$5, %edx
	movq	%r15, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp3411:
# %bb.43:
	vmovq	24(%rsp), %xmm0                 # 8-byte Folded Reload
                                        # xmm0 = mem[0],zero
	vmovq	%xmm0, %rsi
	fldt	72(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	xorl	%r12d, %r12d
	movl	$.L.str.8, %edi
	xorl	%eax, %eax
	callq	printf
	movq	440(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB36_45
# %bb.44:
	callq	_ZdlPv
.LBB36_45:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$80, %edi
	callq	_Znam
	movq	%rax, %rbp
	callq	omp_get_wtime
	vmovq	%xmm0, 72(%rsp)                 # 8-byte Folded Spill
	vcvtsi2sd	%r12d, %xmm1, %xmm0
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
	jmp	.LBB36_47
	.p2align	4, 0x90
.LBB36_46:                              #   in Loop: Header=BB36_47 Depth=1
	callq	sqrt
	vmovsd	%xmm0, (%rbp,%r12,8)
	leaq	1(%r12), %rax
	cmpq	$9, %r12
	movq	%rax, %r12
	je	.LBB36_53
.LBB36_47:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB36_50 Depth 2
                                        #     Child Loop BB36_52 Depth 2
	movq	216(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	testl	%eax, %eax
	jle	.LBB36_46
# %bb.48:                               #   in Loop: Header=BB36_47 Depth=1
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	cmpl	$8, %eax
	jb	.LBB36_51
# %bb.49:                               #   in Loop: Header=BB36_47 Depth=1
	leaq	(,%rax,8), %rcx
	andq	$-64, %rcx
	xorl	%edx, %edx
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	.p2align	4, 0x90
.LBB36_50:                              #   Parent Loop BB36_47 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	(%rbx,%rdx), %xmm1              # xmm1 = mem[0],zero
	vmovsd	8(%rbx,%rdx), %xmm2             # xmm2 = mem[0],zero
	vmulsd	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	%xmm2, %xmm2, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovsd	16(%rbx,%rdx), %xmm1            # xmm1 = mem[0],zero
	vmulsd	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovsd	24(%rbx,%rdx), %xmm1            # xmm1 = mem[0],zero
	vmulsd	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovsd	32(%rbx,%rdx), %xmm1            # xmm1 = mem[0],zero
	vmulsd	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovsd	40(%rbx,%rdx), %xmm1            # xmm1 = mem[0],zero
	vmulsd	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovsd	48(%rbx,%rdx), %xmm1            # xmm1 = mem[0],zero
	vmulsd	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovsd	56(%rbx,%rdx), %xmm1            # xmm1 = mem[0],zero
	vmulsd	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	addq	$64, %rdx
	cmpq	%rdx, %rcx
	jne	.LBB36_50
.LBB36_51:                              #   in Loop: Header=BB36_47 Depth=1
	movl	%eax, %ecx
	andl	$-8, %ecx
	cmpq	%rax, %rcx
	jae	.LBB36_46
	.p2align	4, 0x90
.LBB36_52:                              #   Parent Loop BB36_47 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	(%rbx,%rcx,8), %xmm1            # xmm1 = mem[0],zero
	vmulsd	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	incq	%rcx
	cmpq	%rcx, %rax
	jne	.LBB36_52
	jmp	.LBB36_46
.LBB36_53:
	callq	omp_get_wtime
	vsubsd	72(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm3, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	vmovsd	(%rbp), %xmm0                   # xmm0 = mem[0],zero
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %r12d
	leaq	176(%rsp), %r13
	movq	%r13, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r13, %rdi
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%r12d, %esi
	callq	mpfr_set_d
.Ltmp3413:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp3414:
# %bb.54:
	movq	%rax, %r13
	movq	160(%rsp), %rax                 # 8-byte Reload
	leaq	32(%rax), %rdi
.Ltmp3415:
	movq	%rdi, 24(%rsp)                  # 8-byte Spill
	callq	mpfr_get_prec
.Ltmp3416:
# %bb.55:
.Ltmp3417:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp3418:
# %bb.56:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp3419:
	leaq	128(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp3420:
# %bb.57:
.Ltmp3421:
	leaq	128(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3422:
# %bb.58:
.Ltmp3424:
	callq	mpfr_get_default_rounding_mode
.Ltmp3425:
# %bb.59:
.Ltmp3426:
	leaq	128(%rsp), %rdi
	leaq	176(%rsp), %rsi
	movq	24(%rsp), %rdx                  # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp3427:
# %bb.60:
.Ltmp3429:
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3430:
# %bb.61:
.Ltmp3431:
	movq	%rax, %r13
	movq	24(%rsp), %rdi                  # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp3432:
# %bb.62:
.Ltmp3433:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp3434:
# %bb.63:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp3435:
	leaq	40(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp3436:
# %bb.64:
.Ltmp3437:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3438:
# %bb.65:
.Ltmp3440:
	callq	mpfr_get_default_rounding_mode
.Ltmp3441:
# %bb.66:
.Ltmp3442:
	leaq	40(%rsp), %rdi
	leaq	128(%rsp), %rsi
	movq	24(%rsp), %rdx                  # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp3443:
# %bb.67:
.Ltmp3445:
	callq	mpfr_get_default_rounding_mode
.Ltmp3446:
# %bb.68:
.Ltmp3447:
	movl	%eax, %r15d
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3448:
# %bb.69:
.Ltmp3449:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3450:
# %bb.70:
.Ltmp3451:
	movl	%eax, %r12d
	leaq	96(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3452:
# %bb.71:
.Ltmp3453:
	leaq	96(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3454:
# %bb.72:
.Ltmp3456:
	leaq	96(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp3457:
# %bb.73:
.Ltmp3459:
	leaq	96(%rsp), %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
.Ltmp3460:
# %bb.74:
	cmpq	$0, 120(%rsp)
	je	.LBB36_76
# %bb.75:
.Ltmp3462:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3463:
.LBB36_76:
	cmpq	$0, 64(%rsp)
	je	.LBB36_78
# %bb.77:
.Ltmp3465:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3466:
.LBB36_78:
	cmpq	$0, 152(%rsp)
	je	.LBB36_80
# %bb.79:
.Ltmp3468:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3469:
.LBB36_80:
	cmpq	$0, 200(%rsp)
	je	.LBB36_82
# %bb.81:
.Ltmp3471:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3472:
.LBB36_82:
	leaq	424(%rsp), %r15
	movq	%r15, 408(%rsp)
	movl	$846033518, 424(%rsp)           # imm = 0x326D726E
	movw	$32, 428(%rsp)
	movq	$5, 416(%rsp)
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	%xmm0, 296(%rsp)
	fldl	296(%rsp)
	fstpt	72(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp3474:
	movl	$_ZSt4cout, %edi
	movl	$5, %edx
	movq	%r15, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp3475:
# %bb.83:
	vmovq	24(%rsp), %xmm0                 # 8-byte Folded Reload
                                        # xmm0 = mem[0],zero
	vmovq	%xmm0, %rsi
	fldt	72(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.8, %edi
	xorl	%eax, %eax
	callq	printf
	movq	408(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB36_85
# %bb.84:
	callq	_ZdlPv
.LBB36_85:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$80, %edi
	callq	_Znam
	movq	%rax, %rbp
	callq	omp_get_wtime
	movq	216(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	vmovq	%xmm0, 24(%rsp)                 # 8-byte Folded Spill
	jle	.LBB36_94
# %bb.86:
	movl	%eax, %ecx
	andl	$-8, %ecx
	leaq	(,%rax,8), %rdx
	andq	$-64, %rdx
	xorl	%esi, %esi
	vcvtsi2sd	%esi, %xmm1, %xmm0
	vmovupd	.LCPI36_0(%rip), %xmm1          # xmm1 = [NaN,NaN]
                                        # AlignMOV convert to UnAlignMOV 
	vmovupd	.LCPI36_0(%rip), %xmm2          # xmm2 = [NaN,NaN]
                                        # AlignMOV convert to UnAlignMOV 
	jmp	.LBB36_88
	.p2align	4, 0x90
.LBB36_87:                              #   in Loop: Header=BB36_88 Depth=1
	vmovsd	%xmm3, (%rbp,%rsi,8)
	leaq	1(%rsi), %rdi
	cmpq	$9, %rsi
	movq	%rdi, %rsi
	je	.LBB36_95
.LBB36_88:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB36_90 Depth 2
                                        #     Child Loop BB36_93 Depth 2
	vmovapd	%xmm0, %xmm3
	cmpl	$8, %eax
	jb	.LBB36_91
# %bb.89:                               #   in Loop: Header=BB36_88 Depth=1
	xorl	%edi, %edi
	vmovapd	%xmm0, %xmm3
	.p2align	4, 0x90
.LBB36_90:                              #   Parent Loop BB36_88 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	(%rbx,%rdi), %xmm4              # xmm4 = mem[0],zero
	vmovsd	8(%rbx,%rdi), %xmm5             # xmm5 = mem[0],zero
	vmovsd	16(%rbx,%rdi), %xmm6            # xmm6 = mem[0],zero
	vmovsd	24(%rbx,%rdi), %xmm7            # xmm7 = mem[0],zero
	vandpd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm4, %xmm3, %xmm3
	vandpd	%xmm2, %xmm5, %xmm4
	vaddsd	%xmm4, %xmm3, %xmm3
	vandpd	%xmm2, %xmm6, %xmm4
	vaddsd	%xmm4, %xmm3, %xmm3
	vandpd	%xmm2, %xmm7, %xmm4
	vaddsd	%xmm4, %xmm3, %xmm3
	vmovsd	32(%rbx,%rdi), %xmm4            # xmm4 = mem[0],zero
	vandpd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm4, %xmm3, %xmm3
	vmovsd	40(%rbx,%rdi), %xmm4            # xmm4 = mem[0],zero
	vandpd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm4, %xmm3, %xmm3
	vmovsd	48(%rbx,%rdi), %xmm4            # xmm4 = mem[0],zero
	vandpd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm4, %xmm3, %xmm3
	vmovsd	56(%rbx,%rdi), %xmm4            # xmm4 = mem[0],zero
	vandpd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm4, %xmm3, %xmm3
	addq	$64, %rdi
	cmpq	%rdi, %rdx
	jne	.LBB36_90
.LBB36_91:                              #   in Loop: Header=BB36_88 Depth=1
	cmpq	%rax, %rcx
	jae	.LBB36_87
# %bb.92:                               #   in Loop: Header=BB36_88 Depth=1
	movq	%rcx, %rdi
	.p2align	4, 0x90
.LBB36_93:                              #   Parent Loop BB36_88 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	(%rbx,%rdi,8), %xmm4            # xmm4 = mem[0],zero
	vandpd	%xmm1, %xmm4, %xmm4
	vaddsd	%xmm4, %xmm3, %xmm3
	incq	%rdi
	cmpq	%rdi, %rax
	jne	.LBB36_93
	jmp	.LBB36_87
.LBB36_94:
	xorl	%eax, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm0
	vmovsd	%xmm0, (%rbp)
	vcvtsi2sd	%eax, %xmm1, %xmm0
	vmovsd	%xmm0, 8(%rbp)
	vcvtsi2sd	%eax, %xmm1, %xmm0
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vmovsd	%xmm0, 16(%rbp)
	vmovsd	%xmm1, 24(%rbp)
	vcvtsi2sd	%eax, %xmm2, %xmm0
	vmovsd	%xmm0, 32(%rbp)
	vcvtsi2sd	%eax, %xmm2, %xmm0
	vmovsd	%xmm0, 40(%rbp)
	vcvtsi2sd	%eax, %xmm2, %xmm0
	vcvtsi2sd	%eax, %xmm2, %xmm1
	vmovsd	%xmm0, 48(%rbp)
	vmovsd	%xmm1, 56(%rbp)
	vcvtsi2sd	%eax, %xmm2, %xmm0
	vmovsd	%xmm0, 64(%rbp)
	vcvtsi2sd	%eax, %xmm2, %xmm0
	vmovsd	%xmm0, 72(%rbp)
.LBB36_95:
	callq	omp_get_wtime
	vsubsd	24(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm8, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	vmovsd	(%rbp), %xmm0                   # xmm0 = mem[0],zero
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %r12d
	leaq	176(%rsp), %r13
	movq	%r13, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r13, %rdi
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%r12d, %esi
	callq	mpfr_set_d
.Ltmp3477:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp3478:
# %bb.96:
	movq	%rax, %r13
	movq	160(%rsp), %rax                 # 8-byte Reload
	leaq	64(%rax), %rdi
.Ltmp3479:
	movq	%rdi, 24(%rsp)                  # 8-byte Spill
	callq	mpfr_get_prec
.Ltmp3480:
# %bb.97:
.Ltmp3481:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp3482:
# %bb.98:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp3483:
	leaq	128(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp3484:
# %bb.99:
.Ltmp3485:
	leaq	128(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3486:
# %bb.100:
.Ltmp3488:
	callq	mpfr_get_default_rounding_mode
.Ltmp3489:
# %bb.101:
.Ltmp3490:
	leaq	128(%rsp), %rdi
	leaq	176(%rsp), %rsi
	movq	24(%rsp), %rdx                  # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp3491:
# %bb.102:
.Ltmp3493:
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3494:
# %bb.103:
.Ltmp3495:
	movq	%rax, %r13
	movq	24(%rsp), %rdi                  # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp3496:
# %bb.104:
.Ltmp3497:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp3498:
# %bb.105:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp3499:
	leaq	40(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp3500:
# %bb.106:
.Ltmp3501:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3502:
# %bb.107:
.Ltmp3504:
	callq	mpfr_get_default_rounding_mode
.Ltmp3505:
# %bb.108:
.Ltmp3506:
	leaq	40(%rsp), %rdi
	leaq	128(%rsp), %rsi
	movq	24(%rsp), %rdx                  # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp3507:
# %bb.109:
.Ltmp3509:
	callq	mpfr_get_default_rounding_mode
.Ltmp3510:
# %bb.110:
.Ltmp3511:
	movl	%eax, %r15d
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3512:
# %bb.111:
.Ltmp3513:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3514:
# %bb.112:
.Ltmp3515:
	movl	%eax, %r12d
	leaq	96(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3516:
# %bb.113:
.Ltmp3517:
	leaq	96(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3518:
# %bb.114:
.Ltmp3520:
	leaq	96(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp3521:
# %bb.115:
.Ltmp3523:
	leaq	96(%rsp), %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
.Ltmp3524:
# %bb.116:
	cmpq	$0, 120(%rsp)
	je	.LBB36_118
# %bb.117:
.Ltmp3526:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3527:
.LBB36_118:
	cmpq	$0, 64(%rsp)
	je	.LBB36_120
# %bb.119:
.Ltmp3529:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3530:
.LBB36_120:
	cmpq	$0, 152(%rsp)
	je	.LBB36_122
# %bb.121:
.Ltmp3532:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3533:
.LBB36_122:
	cmpq	$0, 200(%rsp)
	je	.LBB36_124
# %bb.123:
.Ltmp3535:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3536:
.LBB36_124:
	leaq	392(%rsp), %r15
	movq	%r15, 376(%rsp)
	movl	$1836413793, 392(%rsp)          # imm = 0x6D757361
	movw	$32, 396(%rsp)
	movq	$5, 384(%rsp)
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	%xmm0, 288(%rsp)
	fldl	288(%rsp)
	fstpt	72(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp3538:
	movl	$_ZSt4cout, %edi
	movl	$5, %edx
	movq	%r15, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp3539:
# %bb.125:
	vmovq	24(%rsp), %xmm0                 # 8-byte Folded Reload
                                        # xmm0 = mem[0],zero
	vmovq	%xmm0, %rsi
	fldt	72(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.8, %edi
	xorl	%eax, %eax
	callq	printf
	movq	376(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB36_127
# %bb.126:
	callq	_ZdlPv
.LBB36_127:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movq	216(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	vmovsd	224(%rsp), %xmm2                # 8-byte Reload
                                        # xmm2 = mem[0],zero
	jle	.LBB36_133
# %bb.128:
	cmpl	$8, %eax
	jb	.LBB36_131
# %bb.129:
	leaq	(,%rax,8), %rcx
	andq	$-64, %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB36_130:                             # =>This Inner Loop Header: Depth=1
	vmulsd	(%rbx,%rdx), %xmm2, %xmm0
	vaddsd	(%r14,%rdx), %xmm0, %xmm0
	vmovsd	%xmm0, (%r14,%rdx)
	vmulsd	8(%rbx,%rdx), %xmm2, %xmm0
	vaddsd	8(%r14,%rdx), %xmm0, %xmm0
	vmovsd	%xmm0, 8(%r14,%rdx)
	vmulsd	16(%rbx,%rdx), %xmm2, %xmm0
	vaddsd	16(%r14,%rdx), %xmm0, %xmm0
	vmulsd	24(%rbx,%rdx), %xmm2, %xmm1
	vmovsd	%xmm0, 16(%r14,%rdx)
	vaddsd	24(%r14,%rdx), %xmm1, %xmm0
	vmovsd	%xmm0, 24(%r14,%rdx)
	vmulsd	32(%rbx,%rdx), %xmm2, %xmm0
	vaddsd	32(%r14,%rdx), %xmm0, %xmm0
	vmovsd	%xmm0, 32(%r14,%rdx)
	vmulsd	40(%rbx,%rdx), %xmm2, %xmm0
	vaddsd	40(%r14,%rdx), %xmm0, %xmm0
	vmulsd	48(%rbx,%rdx), %xmm2, %xmm1
	vmovsd	%xmm0, 40(%r14,%rdx)
	vaddsd	48(%r14,%rdx), %xmm1, %xmm0
	vmovsd	%xmm0, 48(%r14,%rdx)
	vmulsd	56(%rbx,%rdx), %xmm2, %xmm0
	vaddsd	56(%r14,%rdx), %xmm0, %xmm0
	vmovsd	%xmm0, 56(%r14,%rdx)
	addq	$64, %rdx
	cmpq	%rdx, %rcx
	jne	.LBB36_130
.LBB36_131:
	movl	%eax, %ecx
	andl	$-8, %ecx
	cmpq	%rax, %rcx
	jae	.LBB36_133
	.p2align	4, 0x90
.LBB36_132:                             # =>This Inner Loop Header: Depth=1
	vmulsd	(%rbx,%rcx,8), %xmm2, %xmm0
	vaddsd	(%r14,%rcx,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r14,%rcx,8)
	incq	%rcx
	cmpq	%rcx, %rax
	jne	.LBB36_132
.LBB36_133:
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	96(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	movq	216(%rsp), %rax                 # 8-byte Reload
	cmpl	$0, (%rax)
	jle	.LBB36_177
# %bb.134:
	movq	160(%rsp), %rax                 # 8-byte Reload
	leaq	256(%rax), %r15
	xorl	%ecx, %ecx
	jmp	.LBB36_136
	.p2align	4, 0x90
.LBB36_135:                             #   in Loop: Header=BB36_136 Depth=1
	movq	72(%rsp), %rcx                  # 8-byte Reload
	incq	%rcx
	movq	216(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r15
	cmpq	%rax, %rcx
	jge	.LBB36_177
.LBB36_136:                             # =>This Inner Loop Header: Depth=1
	movq	%rcx, 72(%rsp)                  # 8-byte Spill
	vmovq	(%r14,%rcx,8), %xmm0            # xmm0 = mem[0],zero
	vmovq	%xmm0, 24(%rsp)                 # 8-byte Folded Spill
.Ltmp3541:
	callq	mpfr_get_default_prec
.Ltmp3542:
# %bb.137:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3543:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp3544:
# %bb.138:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3545:
	movl	%eax, %ebp
	leaq	128(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp3546:
# %bb.139:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3547:
	leaq	128(%rsp), %rdi
	vmovq	24(%rsp), %xmm0                 # 8-byte Folded Reload
                                        # xmm0 = mem[0],zero
	movl	%ebp, %esi
	callq	mpfr_set_d
.Ltmp3548:
# %bb.140:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3550:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp3551:
# %bb.141:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3552:
	movq	%rax, %r13
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3553:
# %bb.142:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3554:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp3555:
# %bb.143:                              #   in Loop: Header=BB36_136 Depth=1
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp3556:
	leaq	40(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp3557:
# %bb.144:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3558:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp3559:
# %bb.145:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3561:
	callq	mpfr_get_default_rounding_mode
.Ltmp3562:
# %bb.146:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3563:
	leaq	40(%rsp), %rdi
	movq	%r15, %rsi
	leaq	128(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp3564:
# %bb.147:                              #   in Loop: Header=BB36_136 Depth=1
	cmpq	$0, 152(%rsp)
	je	.LBB36_149
# %bb.148:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3566:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3567:
.LBB36_149:                             #   in Loop: Header=BB36_136 Depth=1
.Ltmp3569:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3570:
# %bb.150:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3571:
	movq	%rax, %r13
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3572:
# %bb.151:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3573:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp3574:
# %bb.152:                              #   in Loop: Header=BB36_136 Depth=1
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp3575:
	leaq	176(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp3576:
# %bb.153:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3577:
	leaq	176(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp3578:
# %bb.154:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3580:
	callq	mpfr_get_default_rounding_mode
.Ltmp3581:
# %bb.155:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3582:
	leaq	176(%rsp), %rdi
	leaq	40(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp3583:
# %bb.156:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3585:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3586:
# %bb.157:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3587:
	movq	%rax, %r13
	leaq	176(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3588:
# %bb.158:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3589:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp3590:
# %bb.159:                              #   in Loop: Header=BB36_136 Depth=1
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp3591:
	leaq	128(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp3592:
# %bb.160:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3593:
	leaq	128(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp3594:
# %bb.161:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3596:
	callq	mpfr_get_default_rounding_mode
.Ltmp3597:
	leaq	96(%rsp), %r12
# %bb.162:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3598:
	leaq	128(%rsp), %rdi
	movq	%r12, %rsi
	leaq	176(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp3599:
# %bb.163:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3601:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp3602:
# %bb.164:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3603:
	movq	%rax, %r12
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3604:
# %bb.165:                              #   in Loop: Header=BB36_136 Depth=1
	movq	%rax, %r13
	cmpq	%rax, %r12
	je	.LBB36_169
# %bb.166:                              #   in Loop: Header=BB36_136 Depth=1
	cmpq	$0, 120(%rsp)
	je	.LBB36_168
# %bb.167:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3605:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3606:
.LBB36_168:                             #   in Loop: Header=BB36_136 Depth=1
.Ltmp3607:
	leaq	96(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3608:
.LBB36_169:                             #   in Loop: Header=BB36_136 Depth=1
.Ltmp3609:
	callq	mpfr_get_default_rounding_mode
.Ltmp3610:
# %bb.170:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3611:
	leaq	96(%rsp), %rdi
	leaq	128(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp3612:
# %bb.171:                              #   in Loop: Header=BB36_136 Depth=1
	cmpq	$0, 152(%rsp)
	je	.LBB36_173
# %bb.172:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3614:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3615:
.LBB36_173:                             #   in Loop: Header=BB36_136 Depth=1
	cmpq	$0, 200(%rsp)
	je	.LBB36_175
# %bb.174:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3617:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3618:
.LBB36_175:                             #   in Loop: Header=BB36_136 Depth=1
	cmpq	$0, 64(%rsp)
	je	.LBB36_135
# %bb.176:                              #   in Loop: Header=BB36_136 Depth=1
.Ltmp3620:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3621:
	jmp	.LBB36_135
.LBB36_177:
.Ltmp3623:
	callq	mpfr_get_default_rounding_mode
.Ltmp3624:
# %bb.178:
.Ltmp3625:
	movl	%eax, %ebp
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3626:
# %bb.179:
.Ltmp3627:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp3628:
# %bb.180:
.Ltmp3629:
	movl	%eax, %r15d
	leaq	40(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp3630:
# %bb.181:
.Ltmp3631:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp3632:
# %bb.182:
.Ltmp3634:
	leaq	40(%rsp), %rdi
	leaq	96(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp3635:
# %bb.183:
.Ltmp3637:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3638:
# %bb.184:
.Ltmp3639:
	movq	%rax, %r12
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3640:
# %bb.185:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB36_189
# %bb.186:
	cmpq	$0, 120(%rsp)
	je	.LBB36_188
# %bb.187:
.Ltmp3641:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3642:
.LBB36_188:
.Ltmp3643:
	leaq	96(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp3644:
.LBB36_189:
.Ltmp3645:
	callq	mpfr_get_default_rounding_mode
.Ltmp3646:
# %bb.190:
.Ltmp3647:
	leaq	96(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp3648:
# %bb.191:
	cmpq	$0, 64(%rsp)
	je	.LBB36_193
# %bb.192:
.Ltmp3650:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3651:
.LBB36_193:
	callq	omp_get_wtime
	vmovq	%xmm0, 24(%rsp)                 # 8-byte Folded Spill
	movq	216(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	vmovsd	224(%rsp), %xmm2                # 8-byte Reload
                                        # xmm2 = mem[0],zero
	jle	.LBB36_202
# %bb.194:
	movl	%eax, %ecx
	andl	$-8, %ecx
	leaq	(,%rax,8), %rdx
	andq	$-64, %rdx
	xorl	%esi, %esi
	jmp	.LBB36_196
	.p2align	4, 0x90
.LBB36_195:                             #   in Loop: Header=BB36_196 Depth=1
	leal	1(%rsi), %edi
	cmpl	$9, %esi
	movl	%edi, %esi
	je	.LBB36_202
.LBB36_196:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB36_198 Depth 2
                                        #     Child Loop BB36_201 Depth 2
	cmpl	$8, %eax
	jb	.LBB36_199
# %bb.197:                              #   in Loop: Header=BB36_196 Depth=1
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB36_198:                             #   Parent Loop BB36_196 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmulsd	(%rbx,%rdi), %xmm2, %xmm0
	vaddsd	(%r14,%rdi), %xmm0, %xmm0
	vmovsd	%xmm0, (%r14,%rdi)
	vmulsd	8(%rbx,%rdi), %xmm2, %xmm0
	vaddsd	8(%r14,%rdi), %xmm0, %xmm0
	vmovsd	%xmm0, 8(%r14,%rdi)
	vmulsd	16(%rbx,%rdi), %xmm2, %xmm0
	vaddsd	16(%r14,%rdi), %xmm0, %xmm0
	vmulsd	24(%rbx,%rdi), %xmm2, %xmm1
	vmovsd	%xmm0, 16(%r14,%rdi)
	vaddsd	24(%r14,%rdi), %xmm1, %xmm0
	vmovsd	%xmm0, 24(%r14,%rdi)
	vmulsd	32(%rbx,%rdi), %xmm2, %xmm0
	vaddsd	32(%r14,%rdi), %xmm0, %xmm0
	vmovsd	%xmm0, 32(%r14,%rdi)
	vmulsd	40(%rbx,%rdi), %xmm2, %xmm0
	vaddsd	40(%r14,%rdi), %xmm0, %xmm0
	vmulsd	48(%rbx,%rdi), %xmm2, %xmm1
	vmovsd	%xmm0, 40(%r14,%rdi)
	vaddsd	48(%r14,%rdi), %xmm1, %xmm0
	vmovsd	%xmm0, 48(%r14,%rdi)
	vmulsd	56(%rbx,%rdi), %xmm2, %xmm0
	vaddsd	56(%r14,%rdi), %xmm0, %xmm0
	vmovsd	%xmm0, 56(%r14,%rdi)
	addq	$64, %rdi
	cmpq	%rdi, %rdx
	jne	.LBB36_198
.LBB36_199:                             #   in Loop: Header=BB36_196 Depth=1
	cmpq	%rax, %rcx
	jae	.LBB36_195
# %bb.200:                              #   in Loop: Header=BB36_196 Depth=1
	movq	%rcx, %rdi
	.p2align	4, 0x90
.LBB36_201:                             #   Parent Loop BB36_196 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmulsd	(%rbx,%rdi,8), %xmm2, %xmm0
	vaddsd	(%r14,%rdi,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r14,%rdi,8)
	incq	%rdi
	cmpq	%rdi, %rax
	jne	.LBB36_201
	jmp	.LBB36_195
.LBB36_202:
	callq	omp_get_wtime
	vsubsd	24(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm3, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp3653:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3654:
# %bb.203:
	movq	%rax, %r12
	movq	160(%rsp), %rax                 # 8-byte Reload
	leaq	96(%rax), %r15
.Ltmp3655:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp3656:
# %bb.204:
.Ltmp3657:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3658:
# %bb.205:
	movl	%eax, %ebp
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp3659:
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3660:
# %bb.206:
.Ltmp3661:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp3662:
# %bb.207:
.Ltmp3664:
	callq	mpfr_get_default_rounding_mode
.Ltmp3665:
# %bb.208:
.Ltmp3666:
	leaq	40(%rsp), %rdi
	leaq	96(%rsp), %rsi
	movq	%r15, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp3667:
# %bb.209:
.Ltmp3669:
	leaq	40(%rsp), %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
.Ltmp3670:
# %bb.210:
	cmpq	$0, 64(%rsp)
	je	.LBB36_212
# %bb.211:
.Ltmp3672:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3673:
.LBB36_212:
	leaq	360(%rsp), %r15
	movq	%r15, 344(%rsp)
	movl	$2037413985, 360(%rsp)          # imm = 0x79707861
	movw	$32, 364(%rsp)
	movq	$5, 352(%rsp)
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	%xmm0, 280(%rsp)
	fldl	280(%rsp)
	fstpt	72(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp3675:
	movl	$_ZSt4cout, %edi
	movl	$5, %edx
	movq	%r15, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp3676:
# %bb.213:
	vmovq	24(%rsp), %xmm0                 # 8-byte Folded Reload
                                        # xmm0 = mem[0],zero
	vmovq	%xmm0, %rsi
	fldt	72(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.8, %edi
	xorl	%eax, %eax
	callq	printf
	movq	344(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB36_215
# %bb.214:
	callq	_ZdlPv
.LBB36_215:
	cmpq	$0, 120(%rsp)
	je	.LBB36_217
# %bb.216:
.Ltmp3678:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3679:
.LBB36_217:
	movl	168(%rsp), %r12d                # 4-byte Reload
	movl	%r12d, %r15d
	leaq	(,%r15,8), %rax
	testl	%r12d, %r12d
	movq	$-1, %rdi
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	cmovnsq	%rax, %rdi
	callq	_Znam
	movq	%rax, %rbp
	testl	%r12d, %r12d
	jle	.LBB36_232
# %bb.218:
	movq	%r15, %rax
	shrq	$3, %rax
	movq	%rax, 208(%rsp)                 # 8-byte Spill
	cmpl	$8, 168(%rsp)                   # 4-byte Folded Reload
	movq	88(%rsp), %rcx                  # 8-byte Reload
	jb	.LBB36_221
# %bb.219:
	movq	208(%rsp), %rax                 # 8-byte Reload
	shlq	$8, %rax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	movl	$14, %r13d
	xorl	%r12d, %r12d
	.p2align	4, 0x90
.LBB36_220:                             # =>This Inner Loop Header: Depth=1
	leaq	(%rcx,%r12), %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovq	%xmm0, -56(%r14,%r13,4)
	vmovq	%xmm0, -56(%rbp,%r13,4)
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r12), %rdi
	addq	$32, %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovq	%xmm0, -48(%r14,%r13,4)
	vmovq	%xmm0, -48(%rbp,%r13,4)
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r12), %rdi
	addq	$64, %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovq	%xmm0, -40(%r14,%r13,4)
	vmovq	%xmm0, -40(%rbp,%r13,4)
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r12), %rdi
	addq	$96, %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovq	%xmm0, -32(%r14,%r13,4)
	vmovq	%xmm0, -32(%rbp,%r13,4)
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r12), %rdi
	addq	$128, %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovq	%xmm0, -24(%r14,%r13,4)
	vmovq	%xmm0, -24(%rbp,%r13,4)
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r12), %rdi
	addq	$160, %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovq	%xmm0, -16(%r14,%r13,4)
	vmovq	%xmm0, -16(%rbp,%r13,4)
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r12), %rdi
	addq	$192, %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovq	%xmm0, -8(%r14,%r13,4)
	vmovq	%xmm0, -8(%rbp,%r13,4)
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r12), %rdi
	addq	$224, %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	movq	88(%rsp), %rcx                  # 8-byte Reload
	vmovq	%xmm0, (%r14,%r13,4)
	vmovq	%xmm0, (%rbp,%r13,4)
	addq	$256, %r12                      # imm = 0x100
	addq	$16, %r13
	cmpq	%r12, 72(%rsp)                  # 8-byte Folded Reload
	jne	.LBB36_220
.LBB36_221:
	movl	%r15d, %eax
	andl	$-8, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	cmpq	%r15, %rax
	jae	.LBB36_224
# %bb.222:
	movq	%rcx, %r13
	movq	208(%rsp), %rax                 # 8-byte Reload
	shlq	$8, %rax
	addq	%rax, %r13
	movq	72(%rsp), %r12                  # 8-byte Reload
	.p2align	4, 0x90
.LBB36_223:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovq	%xmm0, (%r14,%r12,8)
	vmovq	%xmm0, (%rbp,%r12,8)
	incq	%r12
	addq	$32, %r13
	cmpq	%r12, %r15
	jne	.LBB36_223
.LBB36_224:
	leaq	-1(%r15), %rax
	leaq	56(%rbx), %rcx
	movq	208(%rsp), %r9                  # 8-byte Reload
	shlq	$6, %r9
	xorl	%esi, %esi
	movq	%rbx, %rdx
	vmovsd	224(%rsp), %xmm3                # 8-byte Reload
                                        # xmm3 = mem[0],zero
	movq	72(%rsp), %r10                  # 8-byte Reload
	movq	24(%rsp), %r8                   # 8-byte Reload
	jmp	.LBB36_226
	.p2align	4, 0x90
.LBB36_225:                             #   in Loop: Header=BB36_226 Depth=1
	leaq	1(%rsi), %rdi
	addq	%r8, %rcx
	addq	%r8, %rdx
	cmpq	%rax, %rsi
	movq	%rdi, %rsi
	je	.LBB36_232
.LBB36_226:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB36_228 Depth 2
                                        #     Child Loop BB36_231 Depth 2
	vmulsd	(%r14,%rsi,8), %xmm3, %xmm0
	cmpl	$8, 168(%rsp)                   # 4-byte Folded Reload
	jb	.LBB36_229
# %bb.227:                              #   in Loop: Header=BB36_226 Depth=1
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB36_228:                             #   Parent Loop BB36_226 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmulsd	-56(%rcx,%rdi), %xmm0, %xmm1
	vaddsd	(%rbp,%rdi), %xmm1, %xmm1
	vmovsd	%xmm1, (%rbp,%rdi)
	vmulsd	-48(%rcx,%rdi), %xmm0, %xmm1
	vaddsd	8(%rbp,%rdi), %xmm1, %xmm1
	vmovsd	%xmm1, 8(%rbp,%rdi)
	vmulsd	-40(%rcx,%rdi), %xmm0, %xmm1
	vaddsd	16(%rbp,%rdi), %xmm1, %xmm1
	vmulsd	-32(%rcx,%rdi), %xmm0, %xmm2
	vmovsd	%xmm1, 16(%rbp,%rdi)
	vaddsd	24(%rbp,%rdi), %xmm2, %xmm1
	vmovsd	%xmm1, 24(%rbp,%rdi)
	vmulsd	-24(%rcx,%rdi), %xmm0, %xmm1
	vaddsd	32(%rbp,%rdi), %xmm1, %xmm1
	vmovsd	%xmm1, 32(%rbp,%rdi)
	vmulsd	-16(%rcx,%rdi), %xmm0, %xmm1
	vaddsd	40(%rbp,%rdi), %xmm1, %xmm1
	vmulsd	-8(%rcx,%rdi), %xmm0, %xmm2
	vmovsd	%xmm1, 40(%rbp,%rdi)
	vaddsd	48(%rbp,%rdi), %xmm2, %xmm1
	vmovsd	%xmm1, 48(%rbp,%rdi)
	vmulsd	(%rcx,%rdi), %xmm0, %xmm1
	vaddsd	56(%rbp,%rdi), %xmm1, %xmm1
	vmovsd	%xmm1, 56(%rbp,%rdi)
	addq	$64, %rdi
	cmpq	%rdi, %r9
	jne	.LBB36_228
.LBB36_229:                             #   in Loop: Header=BB36_226 Depth=1
	cmpq	%r15, %r10
	jae	.LBB36_225
# %bb.230:                              #   in Loop: Header=BB36_226 Depth=1
	movq	%r10, %rdi
	.p2align	4, 0x90
.LBB36_231:                             #   Parent Loop BB36_226 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmulsd	(%rdx,%rdi,8), %xmm0, %xmm1
	vaddsd	(%rbp,%rdi,8), %xmm1, %xmm1
	vmovsd	%xmm1, (%rbp,%rdi,8)
	incq	%rdi
	cmpq	%rdi, %r15
	jne	.LBB36_231
	jmp	.LBB36_225
.LBB36_232:
	callq	mpfr_get_default_prec
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %r12d
	leaq	96(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
	leaq	96(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
	cmpl	$0, 168(%rsp)                   # 4-byte Folded Reload
	jle	.LBB36_276
# %bb.233:
	xorl	%ecx, %ecx
	jmp	.LBB36_235
	.p2align	4, 0x90
.LBB36_234:                             #   in Loop: Header=BB36_235 Depth=1
	movq	88(%rsp), %rcx                  # 8-byte Reload
	incq	%rcx
	cmpq	%rcx, %r15
	je	.LBB36_276
.LBB36_235:                             # =>This Inner Loop Header: Depth=1
	movq	216(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	movq	%rax, 208(%rsp)                 # 8-byte Spill
	movq	%rcx, 88(%rsp)                  # 8-byte Spill
	vmovsd	(%rbp,%rcx,8), %xmm0            # xmm0 = mem[0],zero
	vmovsd	%xmm0, 72(%rsp)                 # 8-byte Spill
.Ltmp3681:
	callq	mpfr_get_default_prec
.Ltmp3682:
# %bb.236:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3683:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp3684:
# %bb.237:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3685:
	movl	%eax, %r13d
	leaq	128(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp3686:
# %bb.238:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3687:
	leaq	128(%rsp), %rdi
	vmovsd	72(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	movl	%r13d, %esi
	callq	mpfr_set_d
.Ltmp3688:
# %bb.239:                              #   in Loop: Header=BB36_235 Depth=1
	movq	88(%rsp), %rax                  # 8-byte Reload
	movq	208(%rsp), %rcx                 # 8-byte Reload
	addl	%ecx, %eax
	addl	$8, %eax
	movslq	%eax, %r12
	shlq	$5, %r12
	addq	160(%rsp), %r12                 # 8-byte Folded Reload
.Ltmp3690:
	movq	%r12, %rdi
	callq	mpfr_get_prec
	movq	%rax, 72(%rsp)                  # 8-byte Spill
.Ltmp3691:
# %bb.240:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3692:
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3693:
# %bb.241:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3694:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
	movl	%eax, 208(%rsp)                 # 4-byte Spill
.Ltmp3695:
# %bb.242:                              #   in Loop: Header=BB36_235 Depth=1
	movq	72(%rsp), %rax                  # 8-byte Reload
	cmpq	%r13, %rax
	cmovgq	%rax, %r13
.Ltmp3696:
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3697:
# %bb.243:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3698:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	208(%rsp), %edx                 # 4-byte Reload
	callq	mpfr_set_si
.Ltmp3699:
# %bb.244:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3701:
	callq	mpfr_get_default_rounding_mode
.Ltmp3702:
# %bb.245:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3703:
	leaq	40(%rsp), %rdi
	movq	%r12, %rsi
	leaq	128(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp3704:
# %bb.246:                              #   in Loop: Header=BB36_235 Depth=1
	cmpq	$0, 152(%rsp)
	je	.LBB36_248
# %bb.247:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3706:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3707:
.LBB36_248:                             #   in Loop: Header=BB36_235 Depth=1
.Ltmp3709:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3710:
# %bb.249:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3711:
	movq	%rax, %r12
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3712:
# %bb.250:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3713:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
	movl	%eax, 72(%rsp)                  # 4-byte Spill
.Ltmp3714:
# %bb.251:                              #   in Loop: Header=BB36_235 Depth=1
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp3715:
	leaq	176(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3716:
# %bb.252:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3717:
	leaq	176(%rsp), %rdi
	xorl	%esi, %esi
	movl	72(%rsp), %edx                  # 4-byte Reload
	callq	mpfr_set_si
.Ltmp3718:
# %bb.253:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3720:
	callq	mpfr_get_default_rounding_mode
.Ltmp3721:
# %bb.254:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3722:
	leaq	176(%rsp), %rdi
	leaq	40(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp3723:
# %bb.255:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3725:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3726:
# %bb.256:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3727:
	movq	%rax, %r12
	leaq	176(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3728:
# %bb.257:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3729:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
	movl	%eax, 72(%rsp)                  # 4-byte Spill
.Ltmp3730:
# %bb.258:                              #   in Loop: Header=BB36_235 Depth=1
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp3731:
	leaq	128(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3732:
# %bb.259:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3733:
	leaq	128(%rsp), %rdi
	xorl	%esi, %esi
	movl	72(%rsp), %edx                  # 4-byte Reload
	callq	mpfr_set_si
.Ltmp3734:
# %bb.260:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3736:
	callq	mpfr_get_default_rounding_mode
.Ltmp3737:
	leaq	96(%rsp), %r12
# %bb.261:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3738:
	leaq	128(%rsp), %rdi
	movq	%r12, %rsi
	leaq	176(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp3739:
# %bb.262:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3741:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp3742:
# %bb.263:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3743:
	movq	%rax, %r13
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3744:
# %bb.264:                              #   in Loop: Header=BB36_235 Depth=1
	movq	%rax, %r12
	cmpq	%rax, %r13
	je	.LBB36_268
# %bb.265:                              #   in Loop: Header=BB36_235 Depth=1
	cmpq	$0, 120(%rsp)
	je	.LBB36_267
# %bb.266:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3745:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3746:
.LBB36_267:                             #   in Loop: Header=BB36_235 Depth=1
.Ltmp3747:
	leaq	96(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp3748:
.LBB36_268:                             #   in Loop: Header=BB36_235 Depth=1
.Ltmp3749:
	callq	mpfr_get_default_rounding_mode
.Ltmp3750:
# %bb.269:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3751:
	leaq	96(%rsp), %rdi
	leaq	128(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp3752:
# %bb.270:                              #   in Loop: Header=BB36_235 Depth=1
	cmpq	$0, 152(%rsp)
	je	.LBB36_272
# %bb.271:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3754:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3755:
.LBB36_272:                             #   in Loop: Header=BB36_235 Depth=1
	cmpq	$0, 200(%rsp)
	je	.LBB36_274
# %bb.273:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3757:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3758:
.LBB36_274:                             #   in Loop: Header=BB36_235 Depth=1
	cmpq	$0, 64(%rsp)
	je	.LBB36_234
# %bb.275:                              #   in Loop: Header=BB36_235 Depth=1
.Ltmp3760:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3761:
	jmp	.LBB36_234
.LBB36_276:
.Ltmp3763:
	callq	mpfr_get_default_rounding_mode
	movl	%eax, 88(%rsp)                  # 4-byte Spill
.Ltmp3764:
# %bb.277:
.Ltmp3765:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3766:
# %bb.278:
.Ltmp3767:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3768:
# %bb.279:
.Ltmp3769:
	movl	%eax, %r12d
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3770:
# %bb.280:
.Ltmp3771:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3772:
# %bb.281:
.Ltmp3774:
	leaq	40(%rsp), %rdi
	leaq	96(%rsp), %rsi
	movl	88(%rsp), %edx                  # 4-byte Reload
	callq	mpfr_sqrt
.Ltmp3775:
# %bb.282:
.Ltmp3777:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3778:
# %bb.283:
.Ltmp3779:
	movq	%rax, %r12
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3780:
# %bb.284:
	movq	%rax, %r13
	cmpq	%rax, %r12
	je	.LBB36_288
# %bb.285:
	cmpq	$0, 120(%rsp)
	je	.LBB36_287
# %bb.286:
.Ltmp3781:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3782:
.LBB36_287:
.Ltmp3783:
	leaq	96(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3784:
.LBB36_288:
.Ltmp3785:
	callq	mpfr_get_default_rounding_mode
.Ltmp3786:
# %bb.289:
.Ltmp3787:
	leaq	96(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp3788:
# %bb.290:
	cmpq	$0, 64(%rsp)
	je	.LBB36_292
# %bb.291:
.Ltmp3790:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3791:
.LBB36_292:
	callq	omp_get_wtime
	vmovq	%xmm0, 88(%rsp)                 # 8-byte Folded Spill
	cmpl	$0, 168(%rsp)                   # 4-byte Folded Reload
	vmovsd	224(%rsp), %xmm3                # 8-byte Reload
                                        # xmm3 = mem[0],zero
	jle	.LBB36_303
# %bb.293:
	leaq	-1(%r15), %rax
	movl	%r15d, %ecx
	andl	$-8, %ecx
	movq	%rbx, %rdx
	addq	$56, %rdx
	movq	24(%rsp), %rsi                  # 8-byte Reload
	andq	$-64, %rsi
	xorl	%edi, %edi
	movq	24(%rsp), %r12                  # 8-byte Reload
	jmp	.LBB36_295
	.p2align	4, 0x90
.LBB36_294:                             #   in Loop: Header=BB36_295 Depth=1
	leal	1(%rdi), %r8d
	cmpl	$9, %edi
	movl	%r8d, %edi
	je	.LBB36_303
.LBB36_295:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB36_297 Depth 2
                                        #       Child Loop BB36_299 Depth 3
                                        #       Child Loop BB36_302 Depth 3
	movq	%rbx, %r8
	movq	%rdx, %r9
	xorl	%r10d, %r10d
	jmp	.LBB36_297
	.p2align	4, 0x90
.LBB36_296:                             #   in Loop: Header=BB36_297 Depth=2
	leaq	1(%r10), %r11
	addq	%r12, %r9
	addq	%r12, %r8
	cmpq	%rax, %r10
	movq	%r11, %r10
	je	.LBB36_294
.LBB36_297:                             #   Parent Loop BB36_295 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB36_299 Depth 3
                                        #       Child Loop BB36_302 Depth 3
	vmulsd	(%r14,%r10,8), %xmm3, %xmm0
	cmpl	$8, 168(%rsp)                   # 4-byte Folded Reload
	jb	.LBB36_300
# %bb.298:                              #   in Loop: Header=BB36_297 Depth=2
	xorl	%r11d, %r11d
	.p2align	4, 0x90
.LBB36_299:                             #   Parent Loop BB36_295 Depth=1
                                        #     Parent Loop BB36_297 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmulsd	-56(%r9,%r11), %xmm0, %xmm1
	vaddsd	(%rbp,%r11), %xmm1, %xmm1
	vmovsd	%xmm1, (%rbp,%r11)
	vmulsd	-48(%r9,%r11), %xmm0, %xmm1
	vaddsd	8(%rbp,%r11), %xmm1, %xmm1
	vmovsd	%xmm1, 8(%rbp,%r11)
	vmulsd	-40(%r9,%r11), %xmm0, %xmm1
	vaddsd	16(%rbp,%r11), %xmm1, %xmm1
	vmulsd	-32(%r9,%r11), %xmm0, %xmm2
	vmovsd	%xmm1, 16(%rbp,%r11)
	vaddsd	24(%rbp,%r11), %xmm2, %xmm1
	vmovsd	%xmm1, 24(%rbp,%r11)
	vmulsd	-24(%r9,%r11), %xmm0, %xmm1
	vaddsd	32(%rbp,%r11), %xmm1, %xmm1
	vmovsd	%xmm1, 32(%rbp,%r11)
	vmulsd	-16(%r9,%r11), %xmm0, %xmm1
	vaddsd	40(%rbp,%r11), %xmm1, %xmm1
	vmulsd	-8(%r9,%r11), %xmm0, %xmm2
	vmovsd	%xmm1, 40(%rbp,%r11)
	vaddsd	48(%rbp,%r11), %xmm2, %xmm1
	vmovsd	%xmm1, 48(%rbp,%r11)
	vmulsd	(%r9,%r11), %xmm0, %xmm1
	vaddsd	56(%rbp,%r11), %xmm1, %xmm1
	vmovsd	%xmm1, 56(%rbp,%r11)
	addq	$64, %r11
	cmpq	%r11, %rsi
	jne	.LBB36_299
.LBB36_300:                             #   in Loop: Header=BB36_297 Depth=2
	cmpq	%r15, %rcx
	jae	.LBB36_296
# %bb.301:                              #   in Loop: Header=BB36_297 Depth=2
	movq	%rcx, %r11
	.p2align	4, 0x90
.LBB36_302:                             #   Parent Loop BB36_295 Depth=1
                                        #     Parent Loop BB36_297 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmulsd	(%r8,%r11,8), %xmm0, %xmm1
	vaddsd	(%rbp,%r11,8), %xmm1, %xmm1
	vmovsd	%xmm1, (%rbp,%r11,8)
	incq	%r11
	cmpq	%r11, %r15
	jne	.LBB36_302
	jmp	.LBB36_296
.LBB36_303:
	callq	omp_get_wtime
	vsubsd	88(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm4, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp3793:
	leaq	96(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3794:
# %bb.304:
	movq	%rax, %r12
	movq	160(%rsp), %rdi                 # 8-byte Reload
	subq	$-128, %rdi
.Ltmp3795:
	movq	%rdi, 160(%rsp)                 # 8-byte Spill
	callq	mpfr_get_prec
.Ltmp3796:
# %bb.305:
.Ltmp3797:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3798:
# %bb.306:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp3799:
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3800:
# %bb.307:
.Ltmp3801:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp3802:
# %bb.308:
.Ltmp3804:
	callq	mpfr_get_default_rounding_mode
.Ltmp3805:
# %bb.309:
.Ltmp3806:
	leaq	40(%rsp), %rdi
	leaq	96(%rsp), %rsi
	movq	160(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp3807:
# %bb.310:
.Ltmp3809:
	leaq	40(%rsp), %rdi
	callq	_ZN7mX_real7convertIdEENSt9enable_ifIXntsr13check_mX_realIT_EE5valueES2_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, 168(%rsp)                # 8-byte Spill
.Ltmp3810:
# %bb.311:
	cmpq	$0, 64(%rsp)
	je	.LBB36_313
# %bb.312:
.Ltmp3812:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3813:
.LBB36_313:
	leaq	328(%rsp), %r15
	movq	%r15, 312(%rsp)
	movl	$1986880871, 328(%rsp)          # imm = 0x766D6567
	movw	$32, 332(%rsp)
	movq	$5, 320(%rsp)
	vmovsd	168(%rsp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	%xmm0, 272(%rsp)
	fldl	272(%rsp)
	fstpt	24(%rsp)                        # 10-byte Folded Spill
	wait
.Ltmp3815:
	movl	$_ZSt4cout, %edi
	movl	$5, %edx
	movq	%r15, %rsi
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp3816:
# %bb.314:
	vmovq	168(%rsp), %xmm0                # 8-byte Folded Reload
                                        # xmm0 = mem[0],zero
	vmovq	%xmm0, %rsi
	fldt	24(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.8, %edi
	xorl	%eax, %eax
	callq	printf
	movq	312(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB36_316
# %bb.315:
	callq	_ZdlPv
.LBB36_316:
	movq	%rbp, %rdi
	callq	_ZdaPv
	cmpq	$0, 120(%rsp)
	je	.LBB36_318
# %bb.317:
.Ltmp3818:
	leaq	96(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3819:
.LBB36_318:
	movq	%rbx, %rdi
	callq	_ZdaPv
	movq	%r14, %rdi
	callq	_ZdaPv
	xorl	%eax, %eax
	vcvtsi2sd	%eax, %xmm4, %xmm1
	movl	$2097145, %eax                  # imm = 0x1FFFF9
	movl	$1, %ecx
	vcvtsi2sd	%ecx, %xmm4, %xmm0
	.p2align	4, 0x90
.LBB36_319:                             # =>This Inner Loop Header: Depth=1
	leal	7(%rax), %ecx
	vcvtsi2sd	%ecx, %xmm4, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm2
	vdivsd	%xmm2, %xmm0, %xmm2
	leal	6(%rax), %ecx
	vcvtsi2sd	%ecx, %xmm4, %xmm3
	vaddsd	%xmm2, %xmm1, %xmm1
	vmulsd	%xmm3, %xmm3, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm2
	vdivsd	%xmm2, %xmm0, %xmm2
	vaddsd	%xmm2, %xmm1, %xmm1
	leal	5(%rax), %ecx
	vcvtsi2sd	%ecx, %xmm4, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm2
	vdivsd	%xmm2, %xmm0, %xmm2
	vaddsd	%xmm2, %xmm1, %xmm1
	leal	4(%rax), %ecx
	vcvtsi2sd	%ecx, %xmm4, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm2
	vdivsd	%xmm2, %xmm0, %xmm2
	vaddsd	%xmm2, %xmm1, %xmm1
	leal	3(%rax), %ecx
	vcvtsi2sd	%ecx, %xmm4, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm2
	vdivsd	%xmm2, %xmm0, %xmm2
	leal	2(%rax), %ecx
	vcvtsi2sd	%ecx, %xmm4, %xmm3
	vaddsd	%xmm2, %xmm1, %xmm1
	vmulsd	%xmm3, %xmm3, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm2
	vdivsd	%xmm2, %xmm0, %xmm2
	vaddsd	%xmm2, %xmm1, %xmm1
	leal	1(%rax), %ecx
	vcvtsi2sd	%ecx, %xmm4, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm2
	vdivsd	%xmm2, %xmm0, %xmm2
	vaddsd	%xmm2, %xmm1, %xmm1
	vcvtsi2sd	%eax, %xmm4, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm2
	vdivsd	%xmm2, %xmm0, %xmm2
	vaddsd	%xmm2, %xmm1, %xmm1
	addl	$-8, %eax
	cmpl	$-7, %eax
	jne	.LBB36_319
# %bb.320:
	movl	$90, %eax
	vcvtsi2sd	%eax, %xmm4, %xmm0
	vmulsd	%xmm0, %xmm1, %xmm0
	callq	sqrt
	callq	sqrt
	vmovsd	%xmm0, 168(%rsp)                # 8-byte Spill
	leaq	248(%rsp), %r14
	movq	%r14, 232(%rsp)
	movq	$32, 96(%rsp)
	leaq	232(%rsp), %rdi
	leaq	96(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
	movq	%rax, 232(%rsp)
	movq	96(%rsp), %rcx
	movq	%rcx, 248(%rsp)
	vmovups	.L.str.63(%rip), %ymm0
	vmovups	%ymm0, (%rax)
	movq	%rcx, 240(%rsp)
	movq	232(%rsp), %rax
	movb	$0, (%rax,%rcx)
	vmovsd	168(%rsp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	vmovsd	%xmm0, 264(%rsp)
	fldl	264(%rsp)
	fstpt	24(%rsp)                        # 10-byte Folded Spill
	wait
	movq	232(%rsp), %rsi
	movq	240(%rsp), %rdx
.Ltmp3821:
	movl	$_ZSt4cout, %edi
	vzeroupper
	callq	_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l
.Ltmp3822:
# %bb.321:
	vmovq	168(%rsp), %xmm0                # 8-byte Folded Reload
                                        # xmm0 = mem[0],zero
	vmovq	%xmm0, %rsi
	fldt	24(%rsp)                        # 10-byte Folded Reload
	fstpt	(%rsp)
	wait
	movl	$.L.str.8, %edi
	xorl	%eax, %eax
	callq	printf
	movq	232(%rsp), %rdi
	cmpq	%r14, %rdi
	je	.LBB36_323
# %bb.322:
	callq	_ZdlPv
.LBB36_323:
	addq	$472, %rsp                      # imm = 0x1D8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB36_324:
	.cfi_def_cfa_offset 528
.Ltmp3820:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_325:
.Ltmp3814:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_326:
.Ltmp3792:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_327:
.Ltmp3680:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_328:
.Ltmp3674:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_329:
.Ltmp3652:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_330:
.Ltmp3537:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_331:
.Ltmp3534:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_332:
.Ltmp3531:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_333:
.Ltmp3528:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_334:
.Ltmp3473:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_335:
.Ltmp3470:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_336:
.Ltmp3467:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_337:
.Ltmp3464:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_338:
.Ltmp3409:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_339:
.Ltmp3406:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_340:
.Ltmp3403:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_341:
.Ltmp3400:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_342:
.Ltmp3823:
	movq	%rax, %rbx
	movq	232(%rsp), %rdi
	cmpq	%r14, %rdi
	jne	.LBB36_359
	jmp	.LBB36_360
.LBB36_343:
.Ltmp3817:
	movq	%rax, %rbx
	movq	312(%rsp), %rdi
	jmp	.LBB36_347
.LBB36_344:
.Ltmp3811:
	movq	%rax, %rbx
	jmp	.LBB36_417
.LBB36_345:
.Ltmp3776:
	movq	%rax, %rbx
	jmp	.LBB36_417
.LBB36_346:
.Ltmp3677:
	movq	%rax, %rbx
	movq	344(%rsp), %rdi
.LBB36_347:
	cmpq	%r15, %rdi
	je	.LBB36_423
# %bb.348:
	callq	_ZdlPv
	jmp	.LBB36_423
.LBB36_349:
.Ltmp3671:
	movq	%rax, %rbx
	jmp	.LBB36_417
.LBB36_350:
.Ltmp3636:
	movq	%rax, %rbx
	jmp	.LBB36_417
.LBB36_351:
.Ltmp3540:
	movq	%rax, %rbx
	movq	376(%rsp), %rdi
	jmp	.LBB36_358
.LBB36_352:
.Ltmp3525:
	jmp	.LBB36_363
.LBB36_353:
.Ltmp3522:
	jmp	.LBB36_363
.LBB36_354:
.Ltmp3476:
	movq	%rax, %rbx
	movq	408(%rsp), %rdi
	jmp	.LBB36_358
.LBB36_355:
.Ltmp3461:
	jmp	.LBB36_363
.LBB36_356:
.Ltmp3458:
	jmp	.LBB36_363
.LBB36_357:
.Ltmp3412:
	movq	%rax, %rbx
	movq	440(%rsp), %rdi
.LBB36_358:
	cmpq	%r15, %rdi
	je	.LBB36_360
.LBB36_359:
	callq	_ZdlPv
.LBB36_360:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB36_361:
.Ltmp3397:
	jmp	.LBB36_363
.LBB36_362:
.Ltmp3394:
.LBB36_363:
	movq	%rax, %rbx
	leaq	96(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB36_386
.LBB36_364:
.Ltmp3808:
	movq	%rax, %rbx
	jmp	.LBB36_417
.LBB36_365:
.Ltmp3668:
	movq	%rax, %rbx
	jmp	.LBB36_417
.LBB36_366:
.Ltmp3508:
	jmp	.LBB36_385
.LBB36_367:
.Ltmp3492:
	movq	%rax, %rbx
	jmp	.LBB36_387
.LBB36_368:
.Ltmp3444:
	jmp	.LBB36_385
.LBB36_369:
.Ltmp3428:
	movq	%rax, %rbx
	jmp	.LBB36_387
.LBB36_370:
.Ltmp3380:
	jmp	.LBB36_385
.LBB36_371:
.Ltmp3364:
	movq	%rax, %rbx
	jmp	.LBB36_387
.LBB36_372:
.Ltmp3789:
	movq	%rax, %rbx
	jmp	.LBB36_417
.LBB36_373:
.Ltmp3649:
	movq	%rax, %rbx
	jmp	.LBB36_417
.LBB36_374:
.Ltmp3803:
	movq	%rax, %rbx
	jmp	.LBB36_423
.LBB36_375:
.Ltmp3773:
	movq	%rax, %rbx
	jmp	.LBB36_423
.LBB36_376:
.Ltmp3663:
	movq	%rax, %rbx
	jmp	.LBB36_423
.LBB36_377:
.Ltmp3633:
	movq	%rax, %rbx
	jmp	.LBB36_423
.LBB36_378:
.Ltmp3519:
	jmp	.LBB36_385
.LBB36_379:
.Ltmp3503:
	movq	%rax, %rbx
	jmp	.LBB36_387
.LBB36_380:
.Ltmp3487:
	jmp	.LBB36_390
.LBB36_381:
.Ltmp3455:
	jmp	.LBB36_385
.LBB36_382:
.Ltmp3439:
	movq	%rax, %rbx
	jmp	.LBB36_387
.LBB36_383:
.Ltmp3423:
	jmp	.LBB36_390
.LBB36_384:
.Ltmp3391:
.LBB36_385:
	movq	%rax, %rbx
.LBB36_386:
	leaq	40(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB36_387:
	leaq	128(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	leaq	176(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB36_388:
.Ltmp3375:
	movq	%rax, %rbx
	jmp	.LBB36_387
.LBB36_389:
.Ltmp3359:
.LBB36_390:
	movq	%rax, %rbx
	leaq	176(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB36_391:
.Ltmp3762:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_392:
.Ltmp3759:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_393:
.Ltmp3756:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_394:
.Ltmp3708:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_395:
.Ltmp3622:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_396:
.Ltmp3619:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_397:
.Ltmp3616:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_398:
.Ltmp3568:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB36_399:
.Ltmp3740:
	jmp	.LBB36_410
.LBB36_400:
.Ltmp3724:
	jmp	.LBB36_415
.LBB36_401:
.Ltmp3705:
	jmp	.LBB36_405
.LBB36_402:
.Ltmp3600:
	jmp	.LBB36_410
.LBB36_403:
.Ltmp3584:
	jmp	.LBB36_415
.LBB36_404:
.Ltmp3565:
.LBB36_405:
	movq	%rax, %rbx
	leaq	40(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB36_421
.LBB36_406:
.Ltmp3689:
	movq	%rax, %rbx
	jmp	.LBB36_423
.LBB36_407:
.Ltmp3549:
	movq	%rax, %rbx
	jmp	.LBB36_423
.LBB36_408:
.Ltmp3753:
	jmp	.LBB36_410
.LBB36_409:
.Ltmp3613:
.LBB36_410:
	movq	%rax, %rbx
	leaq	128(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB36_416
.LBB36_411:
.Ltmp3735:
	jmp	.LBB36_415
.LBB36_412:
.Ltmp3719:
	movq	%rax, %rbx
	jmp	.LBB36_417
.LBB36_413:
.Ltmp3700:
	jmp	.LBB36_420
.LBB36_414:
.Ltmp3595:
.LBB36_415:
	movq	%rax, %rbx
.LBB36_416:
	leaq	176(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB36_417:
	leaq	40(%rsp), %rdi
	jmp	.LBB36_422
.LBB36_418:
.Ltmp3579:
	movq	%rax, %rbx
	jmp	.LBB36_417
.LBB36_419:
.Ltmp3560:
.LBB36_420:
	movq	%rax, %rbx
.LBB36_421:
	leaq	128(%rsp), %rdi
.LBB36_422:
	callq	_ZN4mpfr6mprealD2Ev
.LBB36_423:
	leaq	96(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end36:
	.size	_Z6verifyIdEvRKiRKN4mpfr6mprealEPS3_S6_S6_, .Lfunc_end36-_Z6verifyIdEvRKiRKN4mpfr6mprealEPS3_S6_S6_
	.cfi_endproc
	.section	.gcc_except_table._Z6verifyIdEvRKiRKN4mpfr6mprealEPS3_S6_S6_,"aG",@progbits,_Z6verifyIdEvRKiRKN4mpfr6mprealEPS3_S6_S6_,comdat
	.p2align	2, 0x0
GCC_except_table36:
.Lexception29:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase18-.Lttbaseref18
.Lttbaseref18:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end29-.Lcst_begin29
.Lcst_begin29:
	.uleb128 .Lfunc_begin29-.Lfunc_begin29  # >> Call Site 1 <<
	.uleb128 .Ltmp3349-.Lfunc_begin29       #   Call between .Lfunc_begin29 and .Ltmp3349
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3349-.Lfunc_begin29       # >> Call Site 2 <<
	.uleb128 .Ltmp3358-.Ltmp3349            #   Call between .Ltmp3349 and .Ltmp3358
	.uleb128 .Ltmp3359-.Lfunc_begin29       #     jumps to .Ltmp3359
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3360-.Lfunc_begin29       # >> Call Site 3 <<
	.uleb128 .Ltmp3363-.Ltmp3360            #   Call between .Ltmp3360 and .Ltmp3363
	.uleb128 .Ltmp3364-.Lfunc_begin29       #     jumps to .Ltmp3364
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3365-.Lfunc_begin29       # >> Call Site 4 <<
	.uleb128 .Ltmp3374-.Ltmp3365            #   Call between .Ltmp3365 and .Ltmp3374
	.uleb128 .Ltmp3375-.Lfunc_begin29       #     jumps to .Ltmp3375
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3376-.Lfunc_begin29       # >> Call Site 5 <<
	.uleb128 .Ltmp3379-.Ltmp3376            #   Call between .Ltmp3376 and .Ltmp3379
	.uleb128 .Ltmp3380-.Lfunc_begin29       #     jumps to .Ltmp3380
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3381-.Lfunc_begin29       # >> Call Site 6 <<
	.uleb128 .Ltmp3390-.Ltmp3381            #   Call between .Ltmp3381 and .Ltmp3390
	.uleb128 .Ltmp3391-.Lfunc_begin29       #     jumps to .Ltmp3391
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3392-.Lfunc_begin29       # >> Call Site 7 <<
	.uleb128 .Ltmp3393-.Ltmp3392            #   Call between .Ltmp3392 and .Ltmp3393
	.uleb128 .Ltmp3394-.Lfunc_begin29       #     jumps to .Ltmp3394
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3395-.Lfunc_begin29       # >> Call Site 8 <<
	.uleb128 .Ltmp3396-.Ltmp3395            #   Call between .Ltmp3395 and .Ltmp3396
	.uleb128 .Ltmp3397-.Lfunc_begin29       #     jumps to .Ltmp3397
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3398-.Lfunc_begin29       # >> Call Site 9 <<
	.uleb128 .Ltmp3399-.Ltmp3398            #   Call between .Ltmp3398 and .Ltmp3399
	.uleb128 .Ltmp3400-.Lfunc_begin29       #     jumps to .Ltmp3400
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3401-.Lfunc_begin29       # >> Call Site 10 <<
	.uleb128 .Ltmp3402-.Ltmp3401            #   Call between .Ltmp3401 and .Ltmp3402
	.uleb128 .Ltmp3403-.Lfunc_begin29       #     jumps to .Ltmp3403
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3404-.Lfunc_begin29       # >> Call Site 11 <<
	.uleb128 .Ltmp3405-.Ltmp3404            #   Call between .Ltmp3404 and .Ltmp3405
	.uleb128 .Ltmp3406-.Lfunc_begin29       #     jumps to .Ltmp3406
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3407-.Lfunc_begin29       # >> Call Site 12 <<
	.uleb128 .Ltmp3408-.Ltmp3407            #   Call between .Ltmp3407 and .Ltmp3408
	.uleb128 .Ltmp3409-.Lfunc_begin29       #     jumps to .Ltmp3409
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3410-.Lfunc_begin29       # >> Call Site 13 <<
	.uleb128 .Ltmp3411-.Ltmp3410            #   Call between .Ltmp3410 and .Ltmp3411
	.uleb128 .Ltmp3412-.Lfunc_begin29       #     jumps to .Ltmp3412
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3411-.Lfunc_begin29       # >> Call Site 14 <<
	.uleb128 .Ltmp3413-.Ltmp3411            #   Call between .Ltmp3411 and .Ltmp3413
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3413-.Lfunc_begin29       # >> Call Site 15 <<
	.uleb128 .Ltmp3422-.Ltmp3413            #   Call between .Ltmp3413 and .Ltmp3422
	.uleb128 .Ltmp3423-.Lfunc_begin29       #     jumps to .Ltmp3423
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3424-.Lfunc_begin29       # >> Call Site 16 <<
	.uleb128 .Ltmp3427-.Ltmp3424            #   Call between .Ltmp3424 and .Ltmp3427
	.uleb128 .Ltmp3428-.Lfunc_begin29       #     jumps to .Ltmp3428
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3429-.Lfunc_begin29       # >> Call Site 17 <<
	.uleb128 .Ltmp3438-.Ltmp3429            #   Call between .Ltmp3429 and .Ltmp3438
	.uleb128 .Ltmp3439-.Lfunc_begin29       #     jumps to .Ltmp3439
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3440-.Lfunc_begin29       # >> Call Site 18 <<
	.uleb128 .Ltmp3443-.Ltmp3440            #   Call between .Ltmp3440 and .Ltmp3443
	.uleb128 .Ltmp3444-.Lfunc_begin29       #     jumps to .Ltmp3444
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3445-.Lfunc_begin29       # >> Call Site 19 <<
	.uleb128 .Ltmp3454-.Ltmp3445            #   Call between .Ltmp3445 and .Ltmp3454
	.uleb128 .Ltmp3455-.Lfunc_begin29       #     jumps to .Ltmp3455
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3456-.Lfunc_begin29       # >> Call Site 20 <<
	.uleb128 .Ltmp3457-.Ltmp3456            #   Call between .Ltmp3456 and .Ltmp3457
	.uleb128 .Ltmp3458-.Lfunc_begin29       #     jumps to .Ltmp3458
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3459-.Lfunc_begin29       # >> Call Site 21 <<
	.uleb128 .Ltmp3460-.Ltmp3459            #   Call between .Ltmp3459 and .Ltmp3460
	.uleb128 .Ltmp3461-.Lfunc_begin29       #     jumps to .Ltmp3461
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3462-.Lfunc_begin29       # >> Call Site 22 <<
	.uleb128 .Ltmp3463-.Ltmp3462            #   Call between .Ltmp3462 and .Ltmp3463
	.uleb128 .Ltmp3464-.Lfunc_begin29       #     jumps to .Ltmp3464
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3465-.Lfunc_begin29       # >> Call Site 23 <<
	.uleb128 .Ltmp3466-.Ltmp3465            #   Call between .Ltmp3465 and .Ltmp3466
	.uleb128 .Ltmp3467-.Lfunc_begin29       #     jumps to .Ltmp3467
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3468-.Lfunc_begin29       # >> Call Site 24 <<
	.uleb128 .Ltmp3469-.Ltmp3468            #   Call between .Ltmp3468 and .Ltmp3469
	.uleb128 .Ltmp3470-.Lfunc_begin29       #     jumps to .Ltmp3470
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3471-.Lfunc_begin29       # >> Call Site 25 <<
	.uleb128 .Ltmp3472-.Ltmp3471            #   Call between .Ltmp3471 and .Ltmp3472
	.uleb128 .Ltmp3473-.Lfunc_begin29       #     jumps to .Ltmp3473
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3474-.Lfunc_begin29       # >> Call Site 26 <<
	.uleb128 .Ltmp3475-.Ltmp3474            #   Call between .Ltmp3474 and .Ltmp3475
	.uleb128 .Ltmp3476-.Lfunc_begin29       #     jumps to .Ltmp3476
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3475-.Lfunc_begin29       # >> Call Site 27 <<
	.uleb128 .Ltmp3477-.Ltmp3475            #   Call between .Ltmp3475 and .Ltmp3477
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3477-.Lfunc_begin29       # >> Call Site 28 <<
	.uleb128 .Ltmp3486-.Ltmp3477            #   Call between .Ltmp3477 and .Ltmp3486
	.uleb128 .Ltmp3487-.Lfunc_begin29       #     jumps to .Ltmp3487
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3488-.Lfunc_begin29       # >> Call Site 29 <<
	.uleb128 .Ltmp3491-.Ltmp3488            #   Call between .Ltmp3488 and .Ltmp3491
	.uleb128 .Ltmp3492-.Lfunc_begin29       #     jumps to .Ltmp3492
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3493-.Lfunc_begin29       # >> Call Site 30 <<
	.uleb128 .Ltmp3502-.Ltmp3493            #   Call between .Ltmp3493 and .Ltmp3502
	.uleb128 .Ltmp3503-.Lfunc_begin29       #     jumps to .Ltmp3503
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3504-.Lfunc_begin29       # >> Call Site 31 <<
	.uleb128 .Ltmp3507-.Ltmp3504            #   Call between .Ltmp3504 and .Ltmp3507
	.uleb128 .Ltmp3508-.Lfunc_begin29       #     jumps to .Ltmp3508
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3509-.Lfunc_begin29       # >> Call Site 32 <<
	.uleb128 .Ltmp3518-.Ltmp3509            #   Call between .Ltmp3509 and .Ltmp3518
	.uleb128 .Ltmp3519-.Lfunc_begin29       #     jumps to .Ltmp3519
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3520-.Lfunc_begin29       # >> Call Site 33 <<
	.uleb128 .Ltmp3521-.Ltmp3520            #   Call between .Ltmp3520 and .Ltmp3521
	.uleb128 .Ltmp3522-.Lfunc_begin29       #     jumps to .Ltmp3522
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3523-.Lfunc_begin29       # >> Call Site 34 <<
	.uleb128 .Ltmp3524-.Ltmp3523            #   Call between .Ltmp3523 and .Ltmp3524
	.uleb128 .Ltmp3525-.Lfunc_begin29       #     jumps to .Ltmp3525
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3526-.Lfunc_begin29       # >> Call Site 35 <<
	.uleb128 .Ltmp3527-.Ltmp3526            #   Call between .Ltmp3526 and .Ltmp3527
	.uleb128 .Ltmp3528-.Lfunc_begin29       #     jumps to .Ltmp3528
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3529-.Lfunc_begin29       # >> Call Site 36 <<
	.uleb128 .Ltmp3530-.Ltmp3529            #   Call between .Ltmp3529 and .Ltmp3530
	.uleb128 .Ltmp3531-.Lfunc_begin29       #     jumps to .Ltmp3531
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3532-.Lfunc_begin29       # >> Call Site 37 <<
	.uleb128 .Ltmp3533-.Ltmp3532            #   Call between .Ltmp3532 and .Ltmp3533
	.uleb128 .Ltmp3534-.Lfunc_begin29       #     jumps to .Ltmp3534
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3535-.Lfunc_begin29       # >> Call Site 38 <<
	.uleb128 .Ltmp3536-.Ltmp3535            #   Call between .Ltmp3535 and .Ltmp3536
	.uleb128 .Ltmp3537-.Lfunc_begin29       #     jumps to .Ltmp3537
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3538-.Lfunc_begin29       # >> Call Site 39 <<
	.uleb128 .Ltmp3539-.Ltmp3538            #   Call between .Ltmp3538 and .Ltmp3539
	.uleb128 .Ltmp3540-.Lfunc_begin29       #     jumps to .Ltmp3540
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3539-.Lfunc_begin29       # >> Call Site 40 <<
	.uleb128 .Ltmp3541-.Ltmp3539            #   Call between .Ltmp3539 and .Ltmp3541
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3541-.Lfunc_begin29       # >> Call Site 41 <<
	.uleb128 .Ltmp3548-.Ltmp3541            #   Call between .Ltmp3541 and .Ltmp3548
	.uleb128 .Ltmp3549-.Lfunc_begin29       #     jumps to .Ltmp3549
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3550-.Lfunc_begin29       # >> Call Site 42 <<
	.uleb128 .Ltmp3559-.Ltmp3550            #   Call between .Ltmp3550 and .Ltmp3559
	.uleb128 .Ltmp3560-.Lfunc_begin29       #     jumps to .Ltmp3560
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3561-.Lfunc_begin29       # >> Call Site 43 <<
	.uleb128 .Ltmp3564-.Ltmp3561            #   Call between .Ltmp3561 and .Ltmp3564
	.uleb128 .Ltmp3565-.Lfunc_begin29       #     jumps to .Ltmp3565
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3566-.Lfunc_begin29       # >> Call Site 44 <<
	.uleb128 .Ltmp3567-.Ltmp3566            #   Call between .Ltmp3566 and .Ltmp3567
	.uleb128 .Ltmp3568-.Lfunc_begin29       #     jumps to .Ltmp3568
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3569-.Lfunc_begin29       # >> Call Site 45 <<
	.uleb128 .Ltmp3578-.Ltmp3569            #   Call between .Ltmp3569 and .Ltmp3578
	.uleb128 .Ltmp3579-.Lfunc_begin29       #     jumps to .Ltmp3579
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3580-.Lfunc_begin29       # >> Call Site 46 <<
	.uleb128 .Ltmp3583-.Ltmp3580            #   Call between .Ltmp3580 and .Ltmp3583
	.uleb128 .Ltmp3584-.Lfunc_begin29       #     jumps to .Ltmp3584
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3585-.Lfunc_begin29       # >> Call Site 47 <<
	.uleb128 .Ltmp3594-.Ltmp3585            #   Call between .Ltmp3585 and .Ltmp3594
	.uleb128 .Ltmp3595-.Lfunc_begin29       #     jumps to .Ltmp3595
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3596-.Lfunc_begin29       # >> Call Site 48 <<
	.uleb128 .Ltmp3599-.Ltmp3596            #   Call between .Ltmp3596 and .Ltmp3599
	.uleb128 .Ltmp3600-.Lfunc_begin29       #     jumps to .Ltmp3600
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3601-.Lfunc_begin29       # >> Call Site 49 <<
	.uleb128 .Ltmp3612-.Ltmp3601            #   Call between .Ltmp3601 and .Ltmp3612
	.uleb128 .Ltmp3613-.Lfunc_begin29       #     jumps to .Ltmp3613
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3614-.Lfunc_begin29       # >> Call Site 50 <<
	.uleb128 .Ltmp3615-.Ltmp3614            #   Call between .Ltmp3614 and .Ltmp3615
	.uleb128 .Ltmp3616-.Lfunc_begin29       #     jumps to .Ltmp3616
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3617-.Lfunc_begin29       # >> Call Site 51 <<
	.uleb128 .Ltmp3618-.Ltmp3617            #   Call between .Ltmp3617 and .Ltmp3618
	.uleb128 .Ltmp3619-.Lfunc_begin29       #     jumps to .Ltmp3619
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3620-.Lfunc_begin29       # >> Call Site 52 <<
	.uleb128 .Ltmp3621-.Ltmp3620            #   Call between .Ltmp3620 and .Ltmp3621
	.uleb128 .Ltmp3622-.Lfunc_begin29       #     jumps to .Ltmp3622
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3623-.Lfunc_begin29       # >> Call Site 53 <<
	.uleb128 .Ltmp3632-.Ltmp3623            #   Call between .Ltmp3623 and .Ltmp3632
	.uleb128 .Ltmp3633-.Lfunc_begin29       #     jumps to .Ltmp3633
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3634-.Lfunc_begin29       # >> Call Site 54 <<
	.uleb128 .Ltmp3635-.Ltmp3634            #   Call between .Ltmp3634 and .Ltmp3635
	.uleb128 .Ltmp3636-.Lfunc_begin29       #     jumps to .Ltmp3636
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3637-.Lfunc_begin29       # >> Call Site 55 <<
	.uleb128 .Ltmp3648-.Ltmp3637            #   Call between .Ltmp3637 and .Ltmp3648
	.uleb128 .Ltmp3649-.Lfunc_begin29       #     jumps to .Ltmp3649
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3650-.Lfunc_begin29       # >> Call Site 56 <<
	.uleb128 .Ltmp3651-.Ltmp3650            #   Call between .Ltmp3650 and .Ltmp3651
	.uleb128 .Ltmp3652-.Lfunc_begin29       #     jumps to .Ltmp3652
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3653-.Lfunc_begin29       # >> Call Site 57 <<
	.uleb128 .Ltmp3662-.Ltmp3653            #   Call between .Ltmp3653 and .Ltmp3662
	.uleb128 .Ltmp3663-.Lfunc_begin29       #     jumps to .Ltmp3663
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3664-.Lfunc_begin29       # >> Call Site 58 <<
	.uleb128 .Ltmp3667-.Ltmp3664            #   Call between .Ltmp3664 and .Ltmp3667
	.uleb128 .Ltmp3668-.Lfunc_begin29       #     jumps to .Ltmp3668
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3669-.Lfunc_begin29       # >> Call Site 59 <<
	.uleb128 .Ltmp3670-.Ltmp3669            #   Call between .Ltmp3669 and .Ltmp3670
	.uleb128 .Ltmp3671-.Lfunc_begin29       #     jumps to .Ltmp3671
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3672-.Lfunc_begin29       # >> Call Site 60 <<
	.uleb128 .Ltmp3673-.Ltmp3672            #   Call between .Ltmp3672 and .Ltmp3673
	.uleb128 .Ltmp3674-.Lfunc_begin29       #     jumps to .Ltmp3674
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3675-.Lfunc_begin29       # >> Call Site 61 <<
	.uleb128 .Ltmp3676-.Ltmp3675            #   Call between .Ltmp3675 and .Ltmp3676
	.uleb128 .Ltmp3677-.Lfunc_begin29       #     jumps to .Ltmp3677
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3678-.Lfunc_begin29       # >> Call Site 62 <<
	.uleb128 .Ltmp3679-.Ltmp3678            #   Call between .Ltmp3678 and .Ltmp3679
	.uleb128 .Ltmp3680-.Lfunc_begin29       #     jumps to .Ltmp3680
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3679-.Lfunc_begin29       # >> Call Site 63 <<
	.uleb128 .Ltmp3681-.Ltmp3679            #   Call between .Ltmp3679 and .Ltmp3681
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3681-.Lfunc_begin29       # >> Call Site 64 <<
	.uleb128 .Ltmp3688-.Ltmp3681            #   Call between .Ltmp3681 and .Ltmp3688
	.uleb128 .Ltmp3689-.Lfunc_begin29       #     jumps to .Ltmp3689
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3690-.Lfunc_begin29       # >> Call Site 65 <<
	.uleb128 .Ltmp3699-.Ltmp3690            #   Call between .Ltmp3690 and .Ltmp3699
	.uleb128 .Ltmp3700-.Lfunc_begin29       #     jumps to .Ltmp3700
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3701-.Lfunc_begin29       # >> Call Site 66 <<
	.uleb128 .Ltmp3704-.Ltmp3701            #   Call between .Ltmp3701 and .Ltmp3704
	.uleb128 .Ltmp3705-.Lfunc_begin29       #     jumps to .Ltmp3705
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3706-.Lfunc_begin29       # >> Call Site 67 <<
	.uleb128 .Ltmp3707-.Ltmp3706            #   Call between .Ltmp3706 and .Ltmp3707
	.uleb128 .Ltmp3708-.Lfunc_begin29       #     jumps to .Ltmp3708
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3709-.Lfunc_begin29       # >> Call Site 68 <<
	.uleb128 .Ltmp3718-.Ltmp3709            #   Call between .Ltmp3709 and .Ltmp3718
	.uleb128 .Ltmp3719-.Lfunc_begin29       #     jumps to .Ltmp3719
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3720-.Lfunc_begin29       # >> Call Site 69 <<
	.uleb128 .Ltmp3723-.Ltmp3720            #   Call between .Ltmp3720 and .Ltmp3723
	.uleb128 .Ltmp3724-.Lfunc_begin29       #     jumps to .Ltmp3724
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3725-.Lfunc_begin29       # >> Call Site 70 <<
	.uleb128 .Ltmp3734-.Ltmp3725            #   Call between .Ltmp3725 and .Ltmp3734
	.uleb128 .Ltmp3735-.Lfunc_begin29       #     jumps to .Ltmp3735
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3736-.Lfunc_begin29       # >> Call Site 71 <<
	.uleb128 .Ltmp3739-.Ltmp3736            #   Call between .Ltmp3736 and .Ltmp3739
	.uleb128 .Ltmp3740-.Lfunc_begin29       #     jumps to .Ltmp3740
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3741-.Lfunc_begin29       # >> Call Site 72 <<
	.uleb128 .Ltmp3752-.Ltmp3741            #   Call between .Ltmp3741 and .Ltmp3752
	.uleb128 .Ltmp3753-.Lfunc_begin29       #     jumps to .Ltmp3753
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3754-.Lfunc_begin29       # >> Call Site 73 <<
	.uleb128 .Ltmp3755-.Ltmp3754            #   Call between .Ltmp3754 and .Ltmp3755
	.uleb128 .Ltmp3756-.Lfunc_begin29       #     jumps to .Ltmp3756
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3757-.Lfunc_begin29       # >> Call Site 74 <<
	.uleb128 .Ltmp3758-.Ltmp3757            #   Call between .Ltmp3757 and .Ltmp3758
	.uleb128 .Ltmp3759-.Lfunc_begin29       #     jumps to .Ltmp3759
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3760-.Lfunc_begin29       # >> Call Site 75 <<
	.uleb128 .Ltmp3761-.Ltmp3760            #   Call between .Ltmp3760 and .Ltmp3761
	.uleb128 .Ltmp3762-.Lfunc_begin29       #     jumps to .Ltmp3762
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3763-.Lfunc_begin29       # >> Call Site 76 <<
	.uleb128 .Ltmp3772-.Ltmp3763            #   Call between .Ltmp3763 and .Ltmp3772
	.uleb128 .Ltmp3773-.Lfunc_begin29       #     jumps to .Ltmp3773
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3774-.Lfunc_begin29       # >> Call Site 77 <<
	.uleb128 .Ltmp3775-.Ltmp3774            #   Call between .Ltmp3774 and .Ltmp3775
	.uleb128 .Ltmp3776-.Lfunc_begin29       #     jumps to .Ltmp3776
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3777-.Lfunc_begin29       # >> Call Site 78 <<
	.uleb128 .Ltmp3788-.Ltmp3777            #   Call between .Ltmp3777 and .Ltmp3788
	.uleb128 .Ltmp3789-.Lfunc_begin29       #     jumps to .Ltmp3789
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3790-.Lfunc_begin29       # >> Call Site 79 <<
	.uleb128 .Ltmp3791-.Ltmp3790            #   Call between .Ltmp3790 and .Ltmp3791
	.uleb128 .Ltmp3792-.Lfunc_begin29       #     jumps to .Ltmp3792
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3793-.Lfunc_begin29       # >> Call Site 80 <<
	.uleb128 .Ltmp3802-.Ltmp3793            #   Call between .Ltmp3793 and .Ltmp3802
	.uleb128 .Ltmp3803-.Lfunc_begin29       #     jumps to .Ltmp3803
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3804-.Lfunc_begin29       # >> Call Site 81 <<
	.uleb128 .Ltmp3807-.Ltmp3804            #   Call between .Ltmp3804 and .Ltmp3807
	.uleb128 .Ltmp3808-.Lfunc_begin29       #     jumps to .Ltmp3808
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3809-.Lfunc_begin29       # >> Call Site 82 <<
	.uleb128 .Ltmp3810-.Ltmp3809            #   Call between .Ltmp3809 and .Ltmp3810
	.uleb128 .Ltmp3811-.Lfunc_begin29       #     jumps to .Ltmp3811
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3812-.Lfunc_begin29       # >> Call Site 83 <<
	.uleb128 .Ltmp3813-.Ltmp3812            #   Call between .Ltmp3812 and .Ltmp3813
	.uleb128 .Ltmp3814-.Lfunc_begin29       #     jumps to .Ltmp3814
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3815-.Lfunc_begin29       # >> Call Site 84 <<
	.uleb128 .Ltmp3816-.Ltmp3815            #   Call between .Ltmp3815 and .Ltmp3816
	.uleb128 .Ltmp3817-.Lfunc_begin29       #     jumps to .Ltmp3817
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3818-.Lfunc_begin29       # >> Call Site 85 <<
	.uleb128 .Ltmp3819-.Ltmp3818            #   Call between .Ltmp3818 and .Ltmp3819
	.uleb128 .Ltmp3820-.Lfunc_begin29       #     jumps to .Ltmp3820
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3819-.Lfunc_begin29       # >> Call Site 86 <<
	.uleb128 .Ltmp3821-.Ltmp3819            #   Call between .Ltmp3819 and .Ltmp3821
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3821-.Lfunc_begin29       # >> Call Site 87 <<
	.uleb128 .Ltmp3822-.Ltmp3821            #   Call between .Ltmp3821 and .Ltmp3822
	.uleb128 .Ltmp3823-.Lfunc_begin29       #     jumps to .Ltmp3823
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3822-.Lfunc_begin29       # >> Call Site 88 <<
	.uleb128 .Lfunc_end36-.Ltmp3822         #   Call between .Ltmp3822 and .Lfunc_end36
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end29:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase18:
	.p2align	2, 0x0
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2, 0x0                          # -- Begin function _Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.LCPI37_0:
	.long	0x80000000                      #  -0
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4, 0x0
.LCPI37_1:
	.zero	16
	.section	.text._Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"axG",@progbits,_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.weak	_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.p2align	4, 0x90
	.type	_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,@function
_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_: # 
.Lfunc_begin30:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception30
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$472, %rsp                      # imm = 0x1D8
	.cfi_def_cfa_offset 528
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, 120(%rsp)                  # 8-byte Spill
	movq	%rcx, %r15
	movq	%rdx, %r12
	movq	%rsi, 128(%rsp)                 # 8-byte Spill
	movq	%rdi, %rbx
	vcvtsi2sdl	(%rdi), %xmm0, %xmm0
	callq	sqrt
	vcvttsd2si	%xmm0, %eax
	movl	%eax, 4(%rsp)
	movq	%rbx, 160(%rsp)                 # 8-byte Spill
	movslq	(%rbx), %r13
	movl	$12, %ecx
	movq	%r13, %rax
	mulq	%rcx
	movq	%rax, %rbp
	movq	$-1, %r14
	cmovnoq	%rax, %r14
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %rbx
	movabsq	$-6148914691236517205, %rcx     # imm = 0xAAAAAAAAAAAAAAAB
	testq	%r13, %r13
	movq	%r15, 296(%rsp)                 # 8-byte Spill
	je	.LBB37_4
# %bb.1:
	addq	$-12, %rbp
	movq	%rbp, %rax
	mulq	%rcx
	shrq	$3, %rdx
	leaq	(%rdx,%rdx,2), %rax
	leaq	12(,%rax,4), %rbp
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movq	%rbp, %rdx
	callq	_intel_fast_memset@PLT
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 152(%rsp)                 # 8-byte Spill
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%rbp, %rdx
	callq	_intel_fast_memset@PLT
	testl	%r13d, %r13d
	jle	.LBB37_5
# %bb.2:
	movl	$8, %ebp
	movq	%r15, %r13
	xorl	%r15d, %r15d
	movq	152(%rsp), %r14                 # 8-byte Reload
	.p2align	4, 0x90
.LBB37_3:                               # =>This Inner Loop Header: Depth=1
	movq	%r12, %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, -8(%rbx,%rbp)
	vmovss	%xmm1, (%rbx,%rbp)
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, -8(%r14,%rbp)
	vmovss	%xmm1, (%r14,%rbp)
	incq	%r15
	movq	160(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$12, %rbp
	addq	$32, %r13
	addq	$32, %r12
	cmpq	%rax, %r15
	jl	.LBB37_3
	jmp	.LBB37_5
.LBB37_4:
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 152(%rsp)                 # 8-byte Spill
.LBB37_5:
	movq	128(%rsp), %rdi                 # 8-byte Reload
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, 72(%rsp)
	vmovss	%xmm1, 80(%rsp)
	movl	$120, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vmovupd	%ymm0, 64(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovlpd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 96(%rax)
	vzeroupper
	callq	omp_get_wtime
	vxorps	%xmm12, %xmm12, %xmm12
	vmovsd	%xmm0, 128(%rsp)                # 8-byte Spill
	movq	160(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	movq	%rax, %rcx
	shlq	$2, %rcx
	leaq	(%rcx,%rcx,2), %rcx
	xorl	%edx, %edx
	xorl	%esi, %esi
	movq	152(%rsp), %r8                  # 8-byte Reload
	jmp	.LBB37_7
	.p2align	4, 0x90
.LBB37_6:                               #   in Loop: Header=BB37_7 Depth=1
	leaq	(%rsi,%rsi,2), %rdi
	vmovlps	%xmm1, (%rbp,%rdi,4)
	vmovss	%xmm0, 8(%rbp,%rdi,4)
	incq	%rsi
	cmpq	$10, %rsi
	je	.LBB37_10
.LBB37_7:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB37_9 Depth 2
	vcvtsi2ss	%edx, %xmm13, %xmm0
	vblendps	$1, %xmm0, %xmm12, %xmm1        # xmm1 = xmm0[0],xmm12[1,2,3]
	vxorps	%xmm0, %xmm0, %xmm0
	testl	%eax, %eax
	jle	.LBB37_6
# %bb.8:                                #   in Loop: Header=BB37_7 Depth=1
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB37_9:                               #   Parent Loop BB37_7 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%r8,%rdi), %xmm4               # xmm4 = mem[0],zero,zero,zero
	vmovss	4(%r8,%rdi), %xmm5              # xmm5 = mem[0],zero,zero,zero
	vmovss	(%rbx,%rdi), %xmm3              # xmm3 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%rdi), %xmm6             # xmm6 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm3, %xmm2
	vmulss	%xmm5, %xmm3, %xmm7
	vmulss	%xmm4, %xmm6, %xmm8
	vmovaps	%xmm4, %xmm9
	vfmsub213ss	%xmm8, %xmm6, %xmm9     # xmm9 = (xmm6 * xmm9) - xmm8
	vfmadd231ss	%xmm5, %xmm6, %xmm9     # xmm9 = (xmm6 * xmm5) + xmm9
	vfmsub213ss	%xmm7, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm5) - xmm7
	vfmadd231ss	8(%rbx,%rdi), %xmm4, %xmm5 # xmm5 = (xmm4 * mem) + xmm5
	vfmsub213ss	%xmm2, %xmm3, %xmm4     # xmm4 = (xmm3 * xmm4) - xmm2
	vaddss	%xmm7, %xmm8, %xmm6
	vsubss	%xmm7, %xmm6, %xmm10
	vsubss	%xmm10, %xmm6, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm6, %xmm8
	vsubss	%xmm6, %xmm8, %xmm10
	vsubss	%xmm10, %xmm8, %xmm11
	vsubss	%xmm11, %xmm6, %xmm6
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm6, %xmm4
	vfmadd231ss	8(%r8,%rdi), %xmm3, %xmm7 # xmm7 = (xmm3 * mem) + xmm7
	vaddss	%xmm5, %xmm9, %xmm3
	vaddss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm2, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm1, %xmm6
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vmovshdup	%xmm1, %xmm1            # xmm1 = xmm1[1,1,3,3]
	vaddss	%xmm1, %xmm8, %xmm5
	vsubss	%xmm1, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm6, %xmm1, %xmm6
	vaddss	%xmm2, %xmm5, %xmm7
	vinsertps	$16, %xmm7, %xmm4, %xmm1 # xmm1 = xmm4[0],xmm7[0],xmm4[2,3]
	vsubss	%xmm5, %xmm7, %xmm4
	vsubss	%xmm4, %xmm7, %xmm7
	vsubss	%xmm7, %xmm5, %xmm5
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	addq	$12, %rdi
	cmpq	%rdi, %rcx
	jne	.LBB37_9
	jmp	.LBB37_6
.LBB37_10:
	callq	omp_get_wtime
	vsubsd	128(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm13, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	176(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp3824:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp3825:
# %bb.11:
.Ltmp3826:
	movq	%rax, %r12
	movq	120(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp3827:
# %bb.12:
.Ltmp3828:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3829:
# %bb.13:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp3830:
	leaq	88(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3831:
# %bb.14:
.Ltmp3832:
	leaq	88(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp3833:
# %bb.15:
.Ltmp3835:
	callq	mpfr_get_default_rounding_mode
.Ltmp3836:
# %bb.16:
.Ltmp3837:
	leaq	88(%rsp), %rdi
	leaq	176(%rsp), %rsi
	movq	120(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp3838:
# %bb.17:
.Ltmp3840:
	leaq	88(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3841:
# %bb.18:
.Ltmp3842:
	movq	%rax, %r12
	movq	120(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp3843:
# %bb.19:
.Ltmp3844:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3845:
# %bb.20:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp3846:
	leaq	8(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3847:
# %bb.21:
.Ltmp3848:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp3849:
# %bb.22:
.Ltmp3851:
	callq	mpfr_get_default_rounding_mode
.Ltmp3852:
# %bb.23:
.Ltmp3853:
	leaq	8(%rsp), %rdi
	leaq	88(%rsp), %rsi
	movq	120(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp3854:
# %bb.24:
.Ltmp3856:
	callq	mpfr_get_default_rounding_mode
.Ltmp3857:
# %bb.25:
.Ltmp3858:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3859:
# %bb.26:
.Ltmp3860:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3861:
# %bb.27:
.Ltmp3862:
	movl	%eax, %r12d
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3863:
# %bb.28:
.Ltmp3864:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3865:
# %bb.29:
.Ltmp3867:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp3868:
# %bb.30:
.Ltmp3870:
	leaq	40(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp3871:
# %bb.31:
	vmovlpd	%xmm0, 208(%rsp)
	vmovss	%xmm1, 216(%rsp)
	cmpq	$0, 64(%rsp)
	je	.LBB37_33
# %bb.32:
.Ltmp3873:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3874:
.LBB37_33:
	cmpq	$0, 32(%rsp)
	je	.LBB37_35
# %bb.34:
.Ltmp3876:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3877:
.LBB37_35:
	cmpq	$0, 112(%rsp)
	je	.LBB37_37
# %bb.36:
.Ltmp3879:
	leaq	88(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3880:
.LBB37_37:
	cmpq	$0, 200(%rsp)
	je	.LBB37_39
# %bb.38:
.Ltmp3882:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3883:
.LBB37_39:
	leaq	456(%rsp), %r15
	movq	%r15, 440(%rsp)
	movl	$544501604, 456(%rsp)           # imm = 0x20746F64
	movw	$32, 460(%rsp)
	movq	$5, 448(%rsp)
.Ltmp3885:
	leaq	440(%rsp), %rdi
	leaq	208(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp3886:
# %bb.40:
	movq	440(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB37_42
# %bb.41:
	callq	_ZdlPv
.LBB37_42:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$120, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vmovupd	%ymm0, 64(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovlpd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 96(%rax)
	xorl	%r15d, %r15d
	vzeroupper
	callq	omp_get_wtime
	vmovsd	%xmm0, 304(%rsp)                # 8-byte Spill
	vxorps	%xmm11, %xmm11, %xmm11
	movl	$2, %r12d
	xorl	%r13d, %r13d
	jmp	.LBB37_47
	.p2align	4, 0x90
.LBB37_43:                              #   in Loop: Header=BB37_47 Depth=1
	vmovaps	%xmm0, %xmm2
	vmovaps	%xmm3, %xmm0
.LBB37_44:                              #   in Loop: Header=BB37_47 Depth=1
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	vmovups	%xmm2, 128(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
.LBB37_45:                              #   in Loop: Header=BB37_47 Depth=1
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vmovups	%xmm1, 224(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm0, 240(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovaps	%xmm0, %xmm1
	vmovups	240(%rsp), %xmm14               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vfnmadd213ss	%xmm14, %xmm0, %xmm1    # xmm1 = -(xmm0 * xmm1) + xmm14
	vmovups	224(%rsp), %xmm13               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm1, %xmm13, %xmm1
	vaddss	%xmm0, %xmm0, %xmm2
	vdivss	%xmm2, %xmm1, %xmm4
	vmulss	%xmm0, %xmm0, %xmm3
	vbroadcastss	.LCPI37_0(%rip), %xmm12 # xmm12 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm3, %xmm12, %xmm2
	vmulss	%xmm4, %xmm0, %xmm6
	vmulss	%xmm0, %xmm4, %xmm7
	vmovaps	%xmm0, %xmm5
	vfmsub213ss	%xmm7, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm5) - xmm7
	vfmadd231ss	%xmm4, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm4) + xmm5
	vinsertps	$16, %xmm4, %xmm0, %xmm1 # xmm1 = xmm0[0],xmm4[0],xmm0[2,3]
	vaddss	%xmm4, %xmm0, %xmm8
	vfmsub213ss	%xmm6, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm4) - xmm6
	vfmsub213ss	%xmm3, %xmm0, %xmm0     # xmm0 = (xmm0 * xmm0) - xmm3
	vaddss	%xmm7, %xmm6, %xmm9
	vsubss	%xmm6, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm6, %xmm6
	vsubss	%xmm10, %xmm7, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm0, %xmm9, %xmm7
	vsubss	%xmm9, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vsubss	%xmm10, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vxorps	%xmm7, %xmm12, %xmm4
	vmovaps	%xmm14, %xmm9
	vsubss	%xmm3, %xmm14, %xmm3
	vsubss	%xmm14, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm6
	vsubss	%xmm6, %xmm14, %xmm6
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vmovaps	%xmm13, %xmm9
	vsubss	%xmm7, %xmm13, %xmm5
	vsubss	%xmm13, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm13, %xmm7
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vaddss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vsubss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vmovups	128(%rsp), %xmm5                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vcvtsi2ss	%r12d, %xmm15, %xmm2
	vaddss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vmulss	%xmm2, %xmm8, %xmm2
	vdivss	%xmm2, %xmm0, %xmm0
	vxorps	%xmm11, %xmm11, %xmm11
.LBB37_46:                              #   in Loop: Header=BB37_47 Depth=1
	leaq	(,%r13,2), %rax
	addq	%r13, %rax
	vmovlps	%xmm1, (%rbp,%rax,4)
	vmovss	%xmm0, 8(%rbp,%rax,4)
	leaq	1(%r13), %rax
	cmpq	$9, %r13
	movq	%rax, %r13
	je	.LBB37_56
.LBB37_47:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB37_49 Depth 2
	vcvtsi2ss	%r15d, %xmm15, %xmm0
	vblendps	$14, .LCPI37_1(%rip), %xmm0, %xmm1 # xmm1 = xmm0[0],mem[1,2,3]
	movq	160(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	jle	.LBB37_50
# %bb.48:                               #   in Loop: Header=BB37_47 Depth=1
	shlq	$2, %rax
	leaq	(%rax,%rax,2), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB37_49:                              #   Parent Loop BB37_47 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rcx), %xmm3              # xmm3 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%rcx), %xmm4             # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm3, %xmm2
	vmulss	%xmm4, %xmm3, %xmm5
	vmulss	%xmm3, %xmm4, %xmm6
	vmovaps	%xmm3, %xmm7
	vfmsub213ss	%xmm6, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm7) - xmm6
	vfmadd231ss	%xmm4, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm4) + xmm7
	vfmsub213ss	%xmm5, %xmm3, %xmm4     # xmm4 = (xmm3 * xmm4) - xmm5
	vaddss	%xmm6, %xmm5, %xmm8
	vsubss	%xmm5, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm5, %xmm5
	vsubss	%xmm9, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	8(%rbx,%rcx), %xmm6             # xmm6 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm3, %xmm6, %xmm4     # xmm4 = (xmm6 * xmm3) + xmm4
	vfmadd231ss	%xmm6, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm6) + xmm5
	vfmsub213ss	%xmm2, %xmm3, %xmm3     # xmm3 = (xmm3 * xmm3) - xmm2
	vaddss	%xmm3, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm10
	vsubss	%xmm10, %xmm8, %xmm8
	vsubss	%xmm9, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vaddss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm3, %xmm4, %xmm3
	vaddss	%xmm2, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm7
	vsubss	%xmm7, %xmm1, %xmm7
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm7, %xmm2
	vmovshdup	%xmm1, %xmm1            # xmm1 = xmm1[1,1,3,3]
	vaddss	%xmm6, %xmm1, %xmm5
	vsubss	%xmm1, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vsubss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm6, %xmm1, %xmm6
	vaddss	%xmm2, %xmm5, %xmm7
	vinsertps	$16, %xmm7, %xmm4, %xmm1 # xmm1 = xmm4[0],xmm7[0],xmm4[2,3]
	vsubss	%xmm5, %xmm7, %xmm4
	vsubss	%xmm4, %xmm7, %xmm7
	vsubss	%xmm7, %xmm5, %xmm5
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	addq	$12, %rcx
	cmpq	%rcx, %rax
	jne	.LBB37_49
	jmp	.LBB37_51
	.p2align	4, 0x90
.LBB37_50:                              #   in Loop: Header=BB37_47 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
.LBB37_51:                              #   in Loop: Header=BB37_47 Depth=1
	vmovshdup	%xmm1, %xmm3            # xmm3 = xmm1[1,1,3,3]
	vaddss	%xmm0, %xmm3, %xmm2
	vaddss	%xmm2, %xmm1, %xmm2
	vucomiss	%xmm11, %xmm2
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB37_46
# %bb.52:                               #   in Loop: Header=BB37_47 Depth=1
	vucomiss	%xmm11, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	je	.LBB37_43
# %bb.53:                               #   in Loop: Header=BB37_47 Depth=1
	vucomiss	%xmm11, %xmm3
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB37_55
# %bb.54:                               #   in Loop: Header=BB37_47 Depth=1
	vinsertps	$28, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],zero,zero
	vmovups	%xmm3, 128(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	jmp	.LBB37_45
.LBB37_55:                              #   in Loop: Header=BB37_47 Depth=1
	vmovaps	%xmm1, %xmm2
	vmovaps	%xmm3, %xmm1
	jmp	.LBB37_44
.LBB37_56:
	callq	omp_get_wtime
	vsubsd	304(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm15, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	176(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp3888:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp3889:
# %bb.57:
	movq	%rax, %r13
	movq	120(%rsp), %rax                 # 8-byte Reload
	leaq	32(%rax), %r14
.Ltmp3890:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp3891:
# %bb.58:
.Ltmp3892:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp3893:
# %bb.59:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp3894:
	leaq	88(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp3895:
# %bb.60:
.Ltmp3896:
	leaq	88(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3897:
# %bb.61:
.Ltmp3899:
	callq	mpfr_get_default_rounding_mode
.Ltmp3900:
# %bb.62:
.Ltmp3901:
	leaq	88(%rsp), %rdi
	leaq	176(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp3902:
# %bb.63:
.Ltmp3904:
	leaq	88(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3905:
# %bb.64:
.Ltmp3906:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp3907:
# %bb.65:
.Ltmp3908:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp3909:
# %bb.66:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp3910:
	leaq	8(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp3911:
# %bb.67:
.Ltmp3912:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3913:
# %bb.68:
.Ltmp3915:
	callq	mpfr_get_default_rounding_mode
.Ltmp3916:
# %bb.69:
.Ltmp3917:
	leaq	8(%rsp), %rdi
	leaq	88(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp3918:
# %bb.70:
.Ltmp3920:
	callq	mpfr_get_default_rounding_mode
.Ltmp3921:
# %bb.71:
.Ltmp3922:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3923:
# %bb.72:
.Ltmp3924:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3925:
# %bb.73:
.Ltmp3926:
	movl	%eax, %r12d
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3927:
# %bb.74:
.Ltmp3928:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3929:
# %bb.75:
.Ltmp3931:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp3932:
# %bb.76:
.Ltmp3934:
	leaq	40(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp3935:
# %bb.77:
	vmovlpd	%xmm0, 208(%rsp)
	vmovss	%xmm1, 216(%rsp)
	cmpq	$0, 64(%rsp)
	je	.LBB37_79
# %bb.78:
.Ltmp3937:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3938:
.LBB37_79:
	cmpq	$0, 32(%rsp)
	je	.LBB37_81
# %bb.80:
.Ltmp3940:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3941:
.LBB37_81:
	cmpq	$0, 112(%rsp)
	je	.LBB37_83
# %bb.82:
.Ltmp3943:
	leaq	88(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3944:
.LBB37_83:
	cmpq	$0, 200(%rsp)
	je	.LBB37_85
# %bb.84:
.Ltmp3946:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp3947:
.LBB37_85:
	leaq	424(%rsp), %r15
	movq	%r15, 408(%rsp)
	movl	$846033518, 424(%rsp)           # imm = 0x326D726E
	movw	$32, 428(%rsp)
	movq	$5, 416(%rsp)
.Ltmp3949:
	leaq	408(%rsp), %rdi
	leaq	208(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp3950:
# %bb.86:
	movq	408(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB37_88
# %bb.87:
	callq	_ZdlPv
.LBB37_88:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$120, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vmovupd	%ymm0, 64(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovlpd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 96(%rax)
	vzeroupper
	callq	omp_get_wtime
	vxorps	%xmm11, %xmm11, %xmm11
	vmovsd	%xmm0, 128(%rsp)                # 8-byte Spill
	movq	160(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	movq	%rax, %rcx
	shlq	$2, %rcx
	leaq	(%rcx,%rcx,2), %rcx
	xorl	%edx, %edx
	vxorpd	%xmm0, %xmm0, %xmm0
	vbroadcastss	.LCPI37_0(%rip), %xmm1  # xmm1 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	xorl	%esi, %esi
	jmp	.LBB37_90
	.p2align	4, 0x90
.LBB37_89:                              #   in Loop: Header=BB37_90 Depth=1
	leaq	(%rsi,%rsi,2), %rdi
	vmovlps	%xmm3, (%rbp,%rdi,4)
	vmovss	%xmm2, 8(%rbp,%rdi,4)
	incq	%rsi
	cmpq	$10, %rsi
	je	.LBB37_96
.LBB37_90:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB37_94 Depth 2
	vcvtsi2ss	%edx, %xmm12, %xmm2
	vblendps	$1, %xmm2, %xmm11, %xmm3        # xmm3 = xmm2[0],xmm11[1,2,3]
	vxorps	%xmm2, %xmm2, %xmm2
	testl	%eax, %eax
	jle	.LBB37_89
# %bb.91:                               #   in Loop: Header=BB37_90 Depth=1
	xorl	%edi, %edi
	jmp	.LBB37_94
	.p2align	4, 0x90
.LBB37_92:                              #   in Loop: Header=BB37_94 Depth=2
	vmovsd	(%rbx,%rdi), %xmm5              # xmm5 = mem[0],zero
.LBB37_93:                              #   in Loop: Header=BB37_94 Depth=2
	vaddss	%xmm5, %xmm3, %xmm6
	vsubss	%xmm3, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm3, %xmm8
	vsubss	%xmm7, %xmm5, %xmm7
	vaddss	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm3, %xmm3            # xmm3 = xmm3[1,1,3,3]
	vmovshdup	%xmm5, %xmm5            # xmm5 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm3, %xmm8
	vsubss	%xmm3, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm3, %xmm3
	vsubss	%xmm9, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm5
	vaddss	%xmm7, %xmm8, %xmm9
	vinsertps	$16, %xmm9, %xmm6, %xmm3 # xmm3 = xmm6[0],xmm9[0],xmm6[2,3]
	vsubss	%xmm8, %xmm9, %xmm6
	vsubss	%xmm6, %xmm9, %xmm9
	vsubss	%xmm9, %xmm8, %xmm8
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm6, %xmm2, %xmm2
	addq	$12, %rdi
	cmpq	%rdi, %rcx
	je	.LBB37_89
.LBB37_94:                              #   Parent Loop BB37_90 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	(%rbx,%rdi), %xmm5              # xmm5 = mem[0],zero
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vmovss	8(%rbx,%rdi), %xmm4             # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm4, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm6
	vcomiss	%xmm6, %xmm0
	jbe	.LBB37_92
# %bb.95:                               #   in Loop: Header=BB37_94 Depth=2
	vxorps	%xmm1, %xmm5, %xmm5
	vxorps	%xmm1, %xmm4, %xmm4
	jmp	.LBB37_93
.LBB37_96:
	callq	omp_get_wtime
	vsubsd	128(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm12, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	176(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp3952:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp3953:
# %bb.97:
	movq	%rax, %r13
	movq	120(%rsp), %rax                 # 8-byte Reload
	leaq	64(%rax), %r14
.Ltmp3954:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp3955:
# %bb.98:
.Ltmp3956:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp3957:
# %bb.99:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp3958:
	leaq	88(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp3959:
# %bb.100:
.Ltmp3960:
	leaq	88(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3961:
# %bb.101:
.Ltmp3963:
	callq	mpfr_get_default_rounding_mode
.Ltmp3964:
# %bb.102:
.Ltmp3965:
	leaq	88(%rsp), %rdi
	leaq	176(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp3966:
# %bb.103:
.Ltmp3968:
	leaq	88(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3969:
# %bb.104:
.Ltmp3970:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp3971:
# %bb.105:
.Ltmp3972:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp3973:
# %bb.106:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp3974:
	leaq	8(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp3975:
# %bb.107:
.Ltmp3976:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3977:
# %bb.108:
.Ltmp3979:
	callq	mpfr_get_default_rounding_mode
.Ltmp3980:
# %bb.109:
.Ltmp3981:
	leaq	8(%rsp), %rdi
	leaq	88(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp3982:
# %bb.110:
.Ltmp3984:
	callq	mpfr_get_default_rounding_mode
.Ltmp3985:
# %bb.111:
.Ltmp3986:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp3987:
# %bb.112:
.Ltmp3988:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp3989:
# %bb.113:
.Ltmp3990:
	movl	%eax, %r12d
	leaq	40(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp3991:
# %bb.114:
.Ltmp3992:
	leaq	40(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp3993:
# %bb.115:
.Ltmp3995:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp3996:
# %bb.116:
.Ltmp3998:
	leaq	40(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp3999:
# %bb.117:
	vmovlpd	%xmm0, 208(%rsp)
	vmovss	%xmm1, 216(%rsp)
	cmpq	$0, 64(%rsp)
	je	.LBB37_119
# %bb.118:
.Ltmp4001:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4002:
.LBB37_119:
	cmpq	$0, 32(%rsp)
	je	.LBB37_121
# %bb.120:
.Ltmp4004:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4005:
.LBB37_121:
	cmpq	$0, 112(%rsp)
	je	.LBB37_123
# %bb.122:
.Ltmp4007:
	leaq	88(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4008:
.LBB37_123:
	cmpq	$0, 200(%rsp)
	je	.LBB37_125
# %bb.124:
.Ltmp4010:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4011:
.LBB37_125:
	leaq	392(%rsp), %r15
	movq	%r15, 376(%rsp)
	movl	$1836413793, 392(%rsp)          # imm = 0x6D757361
	movw	$32, 396(%rsp)
	movq	$5, 384(%rsp)
.Ltmp4013:
	leaq	376(%rsp), %rdi
	leaq	208(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp4014:
# %bb.126:
	movq	376(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB37_128
# %bb.127:
	callq	_ZdlPv
.LBB37_128:
	movq	%rbp, %rdi
	callq	_ZdaPv
	leaq	72(%rsp), %rsi
	movq	160(%rsp), %r13                 # 8-byte Reload
	movq	%r13, %rdi
	movq	%rbx, %rdx
	movq	152(%rsp), %rcx                 # 8-byte Reload
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	40(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	cmpl	$0, (%r13)
	jle	.LBB37_169
# %bb.129:
	movq	120(%rsp), %rax                 # 8-byte Reload
	leaq	256(%rax), %r15
	xorl	%r14d, %r14d
	movq	152(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB37_131
	.p2align	4, 0x90
.LBB37_130:                             #   in Loop: Header=BB37_131 Depth=1
	incq	%r14
	movq	160(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r15
	movq	128(%rsp), %rsi                 # 8-byte Reload
	addq	$12, %rsi
	cmpq	%rax, %r14
	jge	.LBB37_169
.LBB37_131:                             # =>This Inner Loop Header: Depth=1
.Ltmp4016:
	leaq	88(%rsp), %rdi
	movq	%rsi, 128(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp4017:
# %bb.132:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4019:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp4020:
# %bb.133:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4021:
	movq	%rax, %rbp
	leaq	88(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4022:
# %bb.134:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4023:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4024:
# %bb.135:                              #   in Loop: Header=BB37_131 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp4025:
	leaq	8(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4026:
# %bb.136:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4027:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4028:
# %bb.137:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4030:
	callq	mpfr_get_default_rounding_mode
.Ltmp4031:
# %bb.138:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4032:
	leaq	8(%rsp), %rdi
	movq	%r15, %rsi
	leaq	88(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp4033:
# %bb.139:                              #   in Loop: Header=BB37_131 Depth=1
	cmpq	$0, 112(%rsp)
	je	.LBB37_141
# %bb.140:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4035:
	leaq	88(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4036:
.LBB37_141:                             #   in Loop: Header=BB37_131 Depth=1
.Ltmp4038:
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4039:
# %bb.142:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4040:
	movq	%rax, %rbp
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4041:
# %bb.143:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4042:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4043:
# %bb.144:                              #   in Loop: Header=BB37_131 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp4044:
	leaq	176(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4045:
# %bb.145:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4046:
	leaq	176(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4047:
# %bb.146:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4049:
	callq	mpfr_get_default_rounding_mode
.Ltmp4050:
# %bb.147:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4051:
	leaq	176(%rsp), %rdi
	leaq	8(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp4052:
# %bb.148:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4054:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4055:
# %bb.149:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4056:
	movq	%rax, %rbp
	leaq	176(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4057:
# %bb.150:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4058:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4059:
# %bb.151:                              #   in Loop: Header=BB37_131 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp4060:
	leaq	88(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4061:
# %bb.152:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4062:
	leaq	88(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4063:
# %bb.153:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4065:
	callq	mpfr_get_default_rounding_mode
.Ltmp4066:
	leaq	40(%rsp), %r12
# %bb.154:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4067:
	leaq	88(%rsp), %rdi
	movq	%r12, %rsi
	leaq	176(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp4068:
# %bb.155:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4070:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp4071:
# %bb.156:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4072:
	movq	%rax, %r12
	leaq	88(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4073:
# %bb.157:                              #   in Loop: Header=BB37_131 Depth=1
	movq	%rax, %rbp
	cmpq	%rax, %r12
	je	.LBB37_161
# %bb.158:                              #   in Loop: Header=BB37_131 Depth=1
	cmpq	$0, 64(%rsp)
	je	.LBB37_160
# %bb.159:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4074:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4075:
.LBB37_160:                             #   in Loop: Header=BB37_131 Depth=1
.Ltmp4076:
	leaq	40(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp4077:
.LBB37_161:                             #   in Loop: Header=BB37_131 Depth=1
.Ltmp4078:
	callq	mpfr_get_default_rounding_mode
.Ltmp4079:
# %bb.162:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4080:
	leaq	40(%rsp), %rdi
	leaq	88(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp4081:
# %bb.163:                              #   in Loop: Header=BB37_131 Depth=1
	cmpq	$0, 112(%rsp)
	je	.LBB37_165
# %bb.164:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4083:
	leaq	88(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4084:
.LBB37_165:                             #   in Loop: Header=BB37_131 Depth=1
	cmpq	$0, 200(%rsp)
	je	.LBB37_167
# %bb.166:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4086:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4087:
.LBB37_167:                             #   in Loop: Header=BB37_131 Depth=1
	cmpq	$0, 32(%rsp)
	je	.LBB37_130
# %bb.168:                              #   in Loop: Header=BB37_131 Depth=1
.Ltmp4089:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4090:
	jmp	.LBB37_130
.LBB37_169:
.Ltmp4092:
	callq	mpfr_get_default_rounding_mode
.Ltmp4093:
# %bb.170:
.Ltmp4094:
	movl	%eax, %ebp
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4095:
# %bb.171:
.Ltmp4096:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp4097:
# %bb.172:
.Ltmp4098:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp4099:
# %bb.173:
.Ltmp4100:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp4101:
# %bb.174:
.Ltmp4103:
	leaq	8(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp4104:
# %bb.175:
.Ltmp4106:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4107:
# %bb.176:
.Ltmp4108:
	movq	%rax, %r12
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4109:
# %bb.177:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB37_181
# %bb.178:
	cmpq	$0, 64(%rsp)
	je	.LBB37_180
# %bb.179:
.Ltmp4110:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4111:
.LBB37_180:
.Ltmp4112:
	leaq	40(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp4113:
.LBB37_181:
.Ltmp4114:
	callq	mpfr_get_default_rounding_mode
.Ltmp4115:
# %bb.182:
.Ltmp4116:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp4117:
# %bb.183:
	cmpq	$0, 32(%rsp)
	je	.LBB37_185
# %bb.184:
.Ltmp4119:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4120:
.LBB37_185:
	callq	omp_get_wtime
	vmovsd	%xmm0, 128(%rsp)                # 8-byte Spill
.Ltmp4122:
	leaq	72(%rsp), %rsi
	movq	160(%rsp), %r15                 # 8-byte Reload
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	152(%rsp), %r14                 # 8-byte Reload
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp4123:
# %bb.186:
.Ltmp4124:
	leaq	72(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp4125:
# %bb.187:
.Ltmp4126:
	leaq	72(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp4127:
# %bb.188:
.Ltmp4128:
	leaq	72(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp4129:
# %bb.189:
.Ltmp4130:
	leaq	72(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp4131:
# %bb.190:
.Ltmp4132:
	leaq	72(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp4133:
# %bb.191:
.Ltmp4134:
	leaq	72(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp4135:
# %bb.192:
.Ltmp4136:
	leaq	72(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp4137:
# %bb.193:
.Ltmp4138:
	leaq	72(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp4139:
# %bb.194:
.Ltmp4140:
	leaq	72(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp4141:
# %bb.195:
	callq	omp_get_wtime
	vsubsd	128(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp4143:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4144:
# %bb.196:
	movq	%rax, %r12
	movq	120(%rsp), %rax                 # 8-byte Reload
	leaq	96(%rax), %r15
.Ltmp4145:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp4146:
# %bb.197:
.Ltmp4147:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4148:
# %bb.198:
	movl	%eax, %ebp
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp4149:
	leaq	8(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4150:
# %bb.199:
.Ltmp4151:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp4152:
# %bb.200:
.Ltmp4154:
	callq	mpfr_get_default_rounding_mode
.Ltmp4155:
# %bb.201:
.Ltmp4156:
	leaq	8(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movq	%r15, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp4157:
# %bb.202:
.Ltmp4159:
	leaq	8(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp4160:
# %bb.203:
	vmovlpd	%xmm0, 88(%rsp)
	vmovss	%xmm1, 96(%rsp)
	cmpq	$0, 32(%rsp)
	je	.LBB37_205
# %bb.204:
.Ltmp4162:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4163:
.LBB37_205:
	leaq	360(%rsp), %r15
	movq	%r15, 344(%rsp)
	movl	$2037413985, 360(%rsp)          # imm = 0x79707861
	movw	$32, 364(%rsp)
	movq	$5, 352(%rsp)
.Ltmp4165:
	leaq	344(%rsp), %rdi
	leaq	88(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp4166:
# %bb.206:
	movq	344(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB37_208
# %bb.207:
	callq	_ZdlPv
.LBB37_208:
	cmpq	$0, 64(%rsp)
	je	.LBB37_210
# %bb.209:
.Ltmp4168:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4169:
.LBB37_210:
	movslq	4(%rsp), %r15
	movl	$12, %ecx
	movq	%r15, %rax
	mulq	%rcx
	movq	$-1, %rdi
	cmovnoq	%rax, %rdi
	callq	_Znam
	movq	%rax, %rbp
	testq	%r15, %r15
	movq	296(%rsp), %r13                 # 8-byte Reload
	movq	152(%rsp), %r14                 # 8-byte Reload
	je	.LBB37_214
# %bb.211:
	leaq	(%r15,%r15,2), %rax
	leaq	-12(,%rax,4), %rax
	movabsq	$-6148914691236517205, %rcx     # imm = 0xAAAAAAAAAAAAAAAB
	mulq	%rcx
	shrq	$3, %rdx
	leaq	(%rdx,%rdx,2), %rax
	leaq	12(,%rax,4), %rdx
	movq	%rbp, %rdi
	xorl	%esi, %esi
	callq	_intel_fast_memset@PLT
	testl	%r15d, %r15d
	jle	.LBB37_214
# %bb.212:
	xorl	%r15d, %r15d
	xorl	%r12d, %r12d
	.p2align	4, 0x90
.LBB37_213:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, (%r14,%r15)
	vmovss	%xmm1, 8(%r14,%r15)
	movl	8(%r14,%r15), %eax
	movl	%eax, 8(%rbp,%r15)
	movq	(%r14,%r15), %rax
	movq	%rax, (%rbp,%r15)
	incq	%r12
	movslq	4(%rsp), %rax
	addq	$12, %r15
	addq	$32, %r13
	cmpq	%rax, %r12
	jl	.LBB37_213
.LBB37_214:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%rbp, 240(%rsp)                 # 8-byte Spill
	movq	%rbp, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	40(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	cmpl	$0, 4(%rsp)
	jle	.LBB37_255
# %bb.215:
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	leaq	88(%rsp), %rbp
	movq	240(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB37_217
	.p2align	4, 0x90
.LBB37_216:                             #   in Loop: Header=BB37_217 Depth=1
	movq	128(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	movslq	4(%rsp), %rax
	movq	224(%rsp), %rsi                 # 8-byte Reload
	addq	$12, %rsi
	movq	%rdx, %rcx
	movq	%rdx, 128(%rsp)                 # 8-byte Spill
	cmpq	%rax, %rdx
	jge	.LBB37_255
.LBB37_217:                             # =>This Inner Loop Header: Depth=1
	movq	160(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %r15d
.Ltmp4171:
	movq	%rbp, %r14
	movq	%rbp, %rdi
	movq	%rsi, 224(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp4172:
# %bb.218:                              #   in Loop: Header=BB37_217 Depth=1
	movq	128(%rsp), %rax                 # 8-byte Reload
	addl	%r15d, %eax
	addl	$8, %eax
	movslq	%eax, %r12
	shlq	$5, %r12
	addq	120(%rsp), %r12                 # 8-byte Folded Reload
.Ltmp4174:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp4175:
# %bb.219:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4176:
	movq	%rax, %r15
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp4177:
# %bb.220:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4178:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp4179:
# %bb.221:                              #   in Loop: Header=BB37_217 Depth=1
	movl	%eax, %r13d
	cmpq	%rbp, %r15
	cmovgq	%r15, %rbp
.Ltmp4180:
	leaq	8(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp4181:
# %bb.222:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4182:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r13d, %edx
	callq	mpfr_set_si
.Ltmp4183:
# %bb.223:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4185:
	callq	mpfr_get_default_rounding_mode
.Ltmp4186:
# %bb.224:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4187:
	movq	%r14, %rbp
	leaq	8(%rsp), %rdi
	movq	%r12, %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp4188:
# %bb.225:                              #   in Loop: Header=BB37_217 Depth=1
	cmpq	$0, 112(%rsp)
	je	.LBB37_227
# %bb.226:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4190:
	movq	%rbp, %rdi
	callq	mpfr_clear
.Ltmp4191:
.LBB37_227:                             #   in Loop: Header=BB37_217 Depth=1
.Ltmp4193:
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4194:
# %bb.228:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4195:
	movq	%rax, %r15
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4196:
# %bb.229:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4197:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4198:
# %bb.230:                              #   in Loop: Header=BB37_217 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %r15
	cmovgq	%r15, %r13
.Ltmp4199:
	leaq	176(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4200:
# %bb.231:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4201:
	leaq	176(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4202:
# %bb.232:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4204:
	callq	mpfr_get_default_rounding_mode
.Ltmp4205:
# %bb.233:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4206:
	leaq	176(%rsp), %rdi
	leaq	8(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp4207:
# %bb.234:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4209:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4210:
# %bb.235:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4211:
	movq	%rax, %r15
	leaq	176(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4212:
# %bb.236:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4213:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4214:
# %bb.237:                              #   in Loop: Header=BB37_217 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %r15
	cmovgq	%r15, %r13
.Ltmp4215:
	movq	%rbp, %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4216:
# %bb.238:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4217:
	movq	%rbp, %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4218:
# %bb.239:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4220:
	callq	mpfr_get_default_rounding_mode
.Ltmp4221:
	leaq	40(%rsp), %r14
# %bb.240:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4222:
	movq	%rbp, %rdi
	movq	%r14, %rsi
	leaq	176(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp4223:
# %bb.241:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4225:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp4226:
# %bb.242:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4227:
	movq	%rax, %r15
	movq	%rbp, %rdi
	callq	mpfr_get_prec
.Ltmp4228:
# %bb.243:                              #   in Loop: Header=BB37_217 Depth=1
	movq	%rax, %r12
	cmpq	%rax, %r15
	je	.LBB37_247
# %bb.244:                              #   in Loop: Header=BB37_217 Depth=1
	cmpq	$0, 64(%rsp)
	je	.LBB37_246
# %bb.245:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4229:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4230:
.LBB37_246:                             #   in Loop: Header=BB37_217 Depth=1
.Ltmp4231:
	leaq	40(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp4232:
.LBB37_247:                             #   in Loop: Header=BB37_217 Depth=1
.Ltmp4233:
	callq	mpfr_get_default_rounding_mode
.Ltmp4234:
# %bb.248:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4235:
	leaq	40(%rsp), %rdi
	movq	%rbp, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp4236:
# %bb.249:                              #   in Loop: Header=BB37_217 Depth=1
	cmpq	$0, 112(%rsp)
	je	.LBB37_251
# %bb.250:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4238:
	movq	%rbp, %rdi
	callq	mpfr_clear
.Ltmp4239:
.LBB37_251:                             #   in Loop: Header=BB37_217 Depth=1
	cmpq	$0, 200(%rsp)
	je	.LBB37_253
# %bb.252:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4241:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4242:
.LBB37_253:                             #   in Loop: Header=BB37_217 Depth=1
	cmpq	$0, 32(%rsp)
	je	.LBB37_216
# %bb.254:                              #   in Loop: Header=BB37_217 Depth=1
.Ltmp4244:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4245:
	jmp	.LBB37_216
.LBB37_255:
.Ltmp4247:
	callq	mpfr_get_default_rounding_mode
.Ltmp4248:
# %bb.256:
.Ltmp4249:
	movl	%eax, %ebp
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4250:
# %bb.257:
.Ltmp4251:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp4252:
# %bb.258:
.Ltmp4253:
	movl	%eax, %r15d
	leaq	8(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp4254:
# %bb.259:
.Ltmp4255:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp4256:
# %bb.260:
.Ltmp4258:
	leaq	8(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp4259:
# %bb.261:
.Ltmp4261:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4262:
# %bb.262:
.Ltmp4263:
	movq	%rax, %r12
	leaq	8(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4264:
# %bb.263:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB37_267
# %bb.264:
	cmpq	$0, 64(%rsp)
	je	.LBB37_266
# %bb.265:
.Ltmp4265:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4266:
.LBB37_266:
.Ltmp4267:
	leaq	40(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp4268:
.LBB37_267:
.Ltmp4269:
	callq	mpfr_get_default_rounding_mode
.Ltmp4270:
# %bb.268:
.Ltmp4271:
	leaq	40(%rsp), %rdi
	leaq	8(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp4272:
# %bb.269:
	cmpq	$0, 32(%rsp)
	je	.LBB37_271
# %bb.270:
.Ltmp4274:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4275:
.LBB37_271:
	callq	omp_get_wtime
	vmovsd	%xmm0, 160(%rsp)                # 8-byte Spill
.Ltmp4277:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	152(%rsp), %r14                 # 8-byte Reload
	movq	%r14, %r8
	movq	240(%rsp), %r15                 # 8-byte Reload
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4278:
# %bb.272:
.Ltmp4279:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4280:
# %bb.273:
.Ltmp4281:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4282:
# %bb.274:
.Ltmp4283:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4284:
# %bb.275:
.Ltmp4285:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4286:
# %bb.276:
.Ltmp4287:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4288:
# %bb.277:
.Ltmp4289:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4290:
# %bb.278:
.Ltmp4291:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4292:
# %bb.279:
.Ltmp4293:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4294:
# %bb.280:
.Ltmp4295:
	leaq	4(%rsp), %rdi
	leaq	72(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4296:
# %bb.281:
	callq	omp_get_wtime
	vsubsd	160(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp4298:
	leaq	40(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4299:
# %bb.282:
	movq	%rax, %r15
	movq	120(%rsp), %rdi                 # 8-byte Reload
	subq	$-128, %rdi
.Ltmp4300:
	movq	%rdi, %r13
	callq	mpfr_get_prec
.Ltmp4301:
# %bb.283:
.Ltmp4302:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp4303:
# %bb.284:
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp4304:
	leaq	8(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp4305:
# %bb.285:
.Ltmp4306:
	leaq	8(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp4307:
# %bb.286:
.Ltmp4309:
	callq	mpfr_get_default_rounding_mode
.Ltmp4310:
# %bb.287:
.Ltmp4311:
	leaq	8(%rsp), %rdi
	leaq	40(%rsp), %rsi
	movq	%r13, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp4312:
# %bb.288:
.Ltmp4314:
	leaq	8(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp4315:
# %bb.289:
	vmovlpd	%xmm0, 88(%rsp)
	vmovss	%xmm1, 96(%rsp)
	cmpq	$0, 32(%rsp)
	je	.LBB37_291
# %bb.290:
.Ltmp4317:
	leaq	8(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4318:
.LBB37_291:
	leaq	328(%rsp), %r15
	movq	%r15, 312(%rsp)
	movl	$1986880871, 328(%rsp)          # imm = 0x766D6567
	movw	$32, 332(%rsp)
	movq	$5, 320(%rsp)
.Ltmp4320:
	leaq	312(%rsp), %rdi
	leaq	88(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp4321:
# %bb.292:
	movq	312(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB37_294
# %bb.293:
	callq	_ZdlPv
.LBB37_294:
	movq	240(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	cmpq	$0, 64(%rsp)
	je	.LBB37_296
# %bb.295:
.Ltmp4323:
	leaq	40(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4324:
.LBB37_296:
	movq	%rbx, %rdi
	callq	_ZdaPv
	movq	152(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	xorl	%eax, %eax
	vcvtsi2ss	%eax, %xmm2, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vblendps	$1, %xmm0, %xmm1, %xmm2         # xmm2 = xmm0[0],xmm1[1,2,3]
	vxorps	%xmm14, %xmm14, %xmm14
	movl	$2097152, %eax                  # imm = 0x200000
	movl	$1, %ecx
	vxorps	%xmm6, %xmm6, %xmm6
	jmp	.LBB37_298
	.p2align	4, 0x90
.LBB37_297:                             #   in Loop: Header=BB37_298 Depth=1
	vdivss	%xmm8, %xmm3, %xmm7
	vmovaps	%xmm7, %xmm9
	vfnmadd213ss	%xmm3, %xmm8, %xmm9     # xmm9 = -(xmm8 * xmm9) + xmm3
	vaddss	%xmm14, %xmm9, %xmm9
	vmovshdup	%xmm8, %xmm10           # xmm10 = xmm8[1,1,3,3]
	vfnmadd231ss	%xmm10, %xmm7, %xmm9    # xmm9 = -(xmm7 * xmm10) + xmm9
	vaddss	%xmm10, %xmm8, %xmm8
	vdivss	%xmm8, %xmm9, %xmm8
	vmulss	%xmm4, %xmm7, %xmm9
	vmovaps	%xmm4, %xmm10
	vfmsub213ss	%xmm9, %xmm7, %xmm10    # xmm10 = (xmm7 * xmm10) - xmm9
	vmulss	%xmm5, %xmm7, %xmm11
	vmovaps	%xmm5, %xmm12
	vfmsub213ss	%xmm11, %xmm7, %xmm12   # xmm12 = (xmm7 * xmm12) - xmm11
	vmulss	%xmm4, %xmm8, %xmm13
	vaddss	%xmm13, %xmm11, %xmm14
	vsubss	%xmm11, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm0
	vsubss	%xmm0, %xmm11, %xmm0
	vmovaps	%xmm4, %xmm11
	vfmsub213ss	%xmm13, %xmm8, %xmm11   # xmm11 = (xmm8 * xmm11) - xmm13
	vsubss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm10, %xmm14, %xmm13
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm13, %xmm1
	vsubss	%xmm1, %xmm14, %xmm1
	vxorps	%xmm14, %xmm14, %xmm14
	vsubss	%xmm15, %xmm10, %xmm10
	vaddss	%xmm1, %xmm10, %xmm1
	vfmadd231ss	%xmm5, %xmm8, %xmm11    # xmm11 = (xmm8 * xmm5) + xmm11
	vaddss	%xmm11, %xmm12, %xmm10
	vbroadcastss	.LCPI37_0(%rip), %xmm12 # xmm12 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm12, %xmm9, %xmm11
	vfmadd213ss	%xmm0, %xmm7, %xmm6     # xmm6 = (xmm7 * xmm6) + xmm0
	vaddss	%xmm6, %xmm10, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vxorps	%xmm12, %xmm13, %xmm1
	vsubss	%xmm9, %xmm3, %xmm6
	vsubss	%xmm3, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm10
	vsubss	%xmm10, %xmm3, %xmm3
	vsubss	%xmm9, %xmm11, %xmm9
	vaddss	%xmm3, %xmm9, %xmm3
	vsubss	%xmm13, %xmm14, %xmm9
	vsubss	%xmm14, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm14, %xmm11
	vsubss	%xmm10, %xmm1, %xmm1
	vaddss	%xmm1, %xmm11, %xmm1
	vaddss	%xmm3, %xmm9, %xmm10
	vsubss	%xmm9, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm9, %xmm9
	vsubss	%xmm11, %xmm3, %xmm3
	vaddss	%xmm3, %xmm9, %xmm3
	vsubss	%xmm0, %xmm14, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm6, %xmm10, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm5, %xmm4, %xmm1
	vdivss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm7, %xmm2, %xmm3
	vsubss	%xmm2, %xmm3, %xmm1
	vsubss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm4, %xmm2, %xmm4
	vsubss	%xmm1, %xmm7, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm8, %xmm5
	vsubss	%xmm2, %xmm5, %xmm4
	vsubss	%xmm4, %xmm5, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vsubss	%xmm4, %xmm8, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm1, %xmm5, %xmm4
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm5, %xmm5
	vsubss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm1
	vaddss	160(%rsp), %xmm0, %xmm0         # 4-byte Folded Reload
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm6
	vinsertps	$16, %xmm4, %xmm3, %xmm2 # xmm2 = xmm3[0],xmm4[0],xmm3[2,3]
	decl	%eax
	je	.LBB37_312
.LBB37_298:                             # =>This Inner Loop Header: Depth=1
	vmovss	%xmm6, 160(%rsp)                # 4-byte Spill
	vxorps	%xmm8, %xmm8, %xmm8
	vcvtsi2ss	%eax, %xmm8, %xmm3
	vmulss	%xmm3, %xmm3, %xmm4
	vmulss	%xmm3, %xmm14, %xmm5
	vxorps	%xmm6, %xmm6, %xmm6
	vfmsub213ss	%xmm5, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm6) - xmm5
	vmulss	%xmm3, %xmm14, %xmm7
	vmovaps	%xmm3, %xmm8
	vaddss	%xmm7, %xmm5, %xmm9
	vsubss	%xmm5, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm5, %xmm5
	vsubss	%xmm10, %xmm7, %xmm10
	vaddss	%xmm5, %xmm10, %xmm5
	vfmadd231ss	%xmm3, %xmm14, %xmm6    # xmm6 = (xmm14 * xmm3) + xmm6
	vfmadd231ss	%xmm3, %xmm14, %xmm5    # xmm5 = (xmm14 * xmm3) + xmm5
	vfmsub213ss	%xmm4, %xmm3, %xmm3     # xmm3 = (xmm3 * xmm3) - xmm4
	vfmsub213ss	%xmm7, %xmm14, %xmm8    # xmm8 = (xmm14 * xmm8) - xmm7
	vaddss	%xmm3, %xmm9, %xmm10
	vsubss	%xmm9, %xmm10, %xmm7
	vsubss	%xmm7, %xmm10, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vsubss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm9, %xmm3
	vfmadd231ss	%xmm14, %xmm14, %xmm8   # xmm8 = (xmm14 * xmm14) + xmm8
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm5, %xmm6, %xmm5
	vaddss	%xmm3, %xmm5, %xmm5
	vxorps	%xmm13, %xmm13, %xmm13
	vcvtsi2ss	%ecx, %xmm13, %xmm3
	vmulss	%xmm4, %xmm4, %xmm7
	vmulss	%xmm4, %xmm10, %xmm6
	vmulss	%xmm4, %xmm10, %xmm8
	vmovaps	%xmm4, %xmm9
	vfmsub213ss	%xmm8, %xmm10, %xmm9    # xmm9 = (xmm10 * xmm9) - xmm8
	vfmadd231ss	%xmm10, %xmm10, %xmm9   # xmm9 = (xmm10 * xmm10) + xmm9
	vfmsub213ss	%xmm6, %xmm4, %xmm10    # xmm10 = (xmm4 * xmm10) - xmm6
	vaddss	%xmm6, %xmm8, %xmm11
	vsubss	%xmm6, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm6, %xmm6
	vsubss	%xmm12, %xmm8, %xmm8
	vaddss	%xmm6, %xmm8, %xmm6
	vfmadd231ss	%xmm4, %xmm5, %xmm10    # xmm10 = (xmm5 * xmm4) + xmm10
	vfmadd231ss	%xmm5, %xmm4, %xmm6     # xmm6 = (xmm4 * xmm5) + xmm6
	vfmsub213ss	%xmm7, %xmm4, %xmm4     # xmm4 = (xmm4 * xmm4) - xmm7
	vaddss	%xmm4, %xmm11, %xmm8
	vsubss	%xmm11, %xmm8, %xmm5
	vsubss	%xmm5, %xmm8, %xmm12
	vsubss	%xmm12, %xmm11, %xmm11
	vsubss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm4, %xmm11, %xmm4
	vaddss	%xmm9, %xmm10, %xmm5
	vaddss	%xmm6, %xmm5, %xmm5
	vucomiss	%xmm14, %xmm7
	vaddss	%xmm4, %xmm5, %xmm6
	setp	%dl
	setne	%sil
	orb	%dl, %sil
	jne	.LBB37_309
# %bb.299:                              #   in Loop: Header=BB37_298 Depth=1
	vucomiss	%xmm14, %xmm8
	vmovaps	%xmm8, %xmm4
	jne	.LBB37_302
# %bb.300:                              #   in Loop: Header=BB37_298 Depth=1
	vmovaps	%xmm8, %xmm4
	jp	.LBB37_302
# %bb.301:                              #   in Loop: Header=BB37_298 Depth=1
	vmovaps	%xmm6, %xmm4
.LBB37_302:                             #   in Loop: Header=BB37_298 Depth=1
	vmovaps	%xmm6, %xmm5
	jne	.LBB37_305
# %bb.303:                              #   in Loop: Header=BB37_298 Depth=1
	jp	.LBB37_305
# %bb.304:                              #   in Loop: Header=BB37_298 Depth=1
	vmovaps	%xmm7, %xmm5
.LBB37_305:                             #   in Loop: Header=BB37_298 Depth=1
	jne	.LBB37_308
# %bb.306:                              #   in Loop: Header=BB37_298 Depth=1
	jp	.LBB37_308
# %bb.307:                              #   in Loop: Header=BB37_298 Depth=1
	vmovaps	%xmm8, %xmm7
.LBB37_308:                             #   in Loop: Header=BB37_298 Depth=1
	vmovaps	%xmm7, %xmm6
	vinsertps	$16, %xmm5, %xmm4, %xmm8 # xmm8 = xmm4[0],xmm5[0],xmm4[2,3]
	vucomiss	%xmm14, %xmm4
	jne	.LBB37_297
	jmp	.LBB37_310
	.p2align	4, 0x90
.LBB37_309:                             #   in Loop: Header=BB37_298 Depth=1
	vmovaps	%xmm7, %xmm4
	vmovaps	%xmm8, %xmm5
	vinsertps	$16, %xmm5, %xmm4, %xmm8 # xmm8 = xmm4[0],xmm5[0],xmm4[2,3]
	vucomiss	%xmm14, %xmm4
	jne	.LBB37_297
.LBB37_310:                             #   in Loop: Header=BB37_298 Depth=1
	jp	.LBB37_297
# %bb.311:                              #   in Loop: Header=BB37_298 Depth=1
	vshufps	$225, %xmm8, %xmm8, %xmm8       # xmm8 = xmm8[1,0,2,3]
	jmp	.LBB37_297
.LBB37_312:
	movl	$90, %eax
	vxorps	%xmm8, %xmm8, %xmm8
	vcvtsi2ss	%eax, %xmm8, %xmm0
	vmulss	%xmm3, %xmm0, %xmm8
	vfmsub213ss	%xmm8, %xmm0, %xmm3     # xmm3 = (xmm0 * xmm3) - xmm8
	vmulss	%xmm4, %xmm0, %xmm1
	vfmsub213ss	%xmm1, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm4) - xmm1
	vaddss	%xmm3, %xmm1, %xmm7
	vsubss	%xmm1, %xmm7, %xmm2
	vsubss	%xmm2, %xmm7, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vsubss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vfmadd213ss	%xmm1, %xmm0, %xmm6     # xmm6 = (xmm0 * xmm6) + xmm1
	vaddss	%xmm6, %xmm4, %xmm0
	vaddss	%xmm0, %xmm7, %xmm1
	vaddss	%xmm1, %xmm8, %xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vucomiss	%xmm3, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	je	.LBB37_314
# %bb.313:
	vinsertps	$16, %xmm7, %xmm8, %xmm1 # xmm1 = xmm8[0],xmm7[0],xmm8[2,3]
	jmp	.LBB37_319
.LBB37_314:
	vucomiss	%xmm3, %xmm8
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB37_316
# %bb.315:
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm7
	setnp	%al
	sete	%cl
	vmovaps	%xmm7, %xmm2
	vmovaps	%xmm0, %xmm1
	testb	%al, %cl
	je	.LBB37_317
	jmp	.LBB37_318
.LBB37_316:
	vmovaps	%xmm8, %xmm2
	vmovaps	%xmm7, %xmm1
	vmovaps	%xmm0, %xmm8
.LBB37_317:
	vmovaps	%xmm8, %xmm7
	vmovaps	%xmm2, %xmm0
	vmovaps	%xmm1, %xmm8
.LBB37_318:
	vmovups	%xmm0, 224(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm8, 128(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm7, 160(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovaps	%xmm0, %xmm1
	vmovups	224(%rsp), %xmm14               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vfnmadd213ss	%xmm14, %xmm0, %xmm1    # xmm1 = -(xmm0 * xmm1) + xmm14
	vmovups	128(%rsp), %xmm13               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm1, %xmm13, %xmm1
	vaddss	%xmm0, %xmm0, %xmm2
	vdivss	%xmm2, %xmm1, %xmm4
	vmulss	%xmm0, %xmm0, %xmm3
	vbroadcastss	.LCPI37_0(%rip), %xmm12 # xmm12 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm3, %xmm12, %xmm2
	vmulss	%xmm4, %xmm0, %xmm6
	vmulss	%xmm0, %xmm4, %xmm7
	vmovaps	%xmm0, %xmm5
	vfmsub213ss	%xmm7, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm5) - xmm7
	vfmadd231ss	%xmm4, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm4) + xmm5
	vinsertps	$16, %xmm4, %xmm0, %xmm1 # xmm1 = xmm0[0],xmm4[0],xmm0[2,3]
	vaddss	%xmm4, %xmm0, %xmm8
	vfmsub213ss	%xmm6, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm4) - xmm6
	vfmsub213ss	%xmm3, %xmm0, %xmm0     # xmm0 = (xmm0 * xmm0) - xmm3
	vaddss	%xmm7, %xmm6, %xmm9
	vsubss	%xmm6, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm6, %xmm6
	vsubss	%xmm10, %xmm7, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm0, %xmm9, %xmm7
	vsubss	%xmm9, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vsubss	%xmm10, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vxorps	%xmm7, %xmm12, %xmm4
	vmovaps	%xmm14, %xmm9
	vsubss	%xmm3, %xmm14, %xmm3
	vsubss	%xmm14, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm6
	vsubss	%xmm6, %xmm14, %xmm6
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vmovaps	%xmm13, %xmm9
	vsubss	%xmm7, %xmm13, %xmm5
	vsubss	%xmm13, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm13, %xmm7
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vaddss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vsubss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vmovups	160(%rsp), %xmm5                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm15, %xmm2
	vaddss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vmulss	%xmm2, %xmm8, %xmm2
	vdivss	%xmm2, %xmm0, %xmm0
	vxorps	%xmm3, %xmm3, %xmm3
.LBB37_319:
	vmovshdup	%xmm1, %xmm4            # xmm4 = xmm1[1,1,3,3]
	vaddss	%xmm0, %xmm4, %xmm2
	vaddss	%xmm2, %xmm1, %xmm2
	vucomiss	%xmm3, %xmm2
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB37_327
# %bb.320:
	vxorps	%xmm2, %xmm2, %xmm2
	vucomiss	%xmm2, %xmm1
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB37_323
# %bb.321:
	vucomiss	%xmm2, %xmm4
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB37_324
# %bb.322:
	vinsertps	$28, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0],xmm1[0],zero,zero
	vmovups	%xmm4, 160(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	jmp	.LBB37_326
.LBB37_323:
	vmovaps	%xmm1, %xmm2
	jmp	.LBB37_325
.LBB37_324:
	vmovaps	%xmm4, %xmm2
	vmovaps	%xmm0, %xmm4
	vmovaps	%xmm1, %xmm0
.LBB37_325:
	vinsertps	$16, %xmm4, %xmm2, %xmm1 # xmm1 = xmm2[0],xmm4[0],xmm2[2,3]
	vmovups	%xmm0, 160(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
.LBB37_326:
	vmovups	%xmm1, 224(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovshdup	%xmm1, %xmm0            # xmm0 = xmm1[1,1,3,3]
	vmovups	%xmm0, 128(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovaps	%xmm1, %xmm0
	callq	sqrtf
	vmovaps	%xmm0, %xmm1
	vmovups	224(%rsp), %xmm14               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vfnmadd213ss	%xmm14, %xmm0, %xmm1    # xmm1 = -(xmm0 * xmm1) + xmm14
	vmovups	128(%rsp), %xmm13               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm1, %xmm13, %xmm1
	vaddss	%xmm0, %xmm0, %xmm2
	vdivss	%xmm2, %xmm1, %xmm4
	vmulss	%xmm0, %xmm0, %xmm3
	vbroadcastss	.LCPI37_0(%rip), %xmm12 # xmm12 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm3, %xmm12, %xmm2
	vmulss	%xmm4, %xmm0, %xmm6
	vmulss	%xmm0, %xmm4, %xmm7
	vmovaps	%xmm0, %xmm5
	vfmsub213ss	%xmm7, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm5) - xmm7
	vfmadd231ss	%xmm4, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm4) + xmm5
	vinsertps	$16, %xmm4, %xmm0, %xmm1 # xmm1 = xmm0[0],xmm4[0],xmm0[2,3]
	vaddss	%xmm4, %xmm0, %xmm8
	vfmsub213ss	%xmm6, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm4) - xmm6
	vfmsub213ss	%xmm3, %xmm0, %xmm0     # xmm0 = (xmm0 * xmm0) - xmm3
	vaddss	%xmm7, %xmm6, %xmm9
	vsubss	%xmm6, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm6, %xmm6
	vsubss	%xmm10, %xmm7, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm0, %xmm9, %xmm7
	vsubss	%xmm9, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vsubss	%xmm10, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vxorps	%xmm7, %xmm12, %xmm4
	vmovaps	%xmm14, %xmm9
	vsubss	%xmm3, %xmm14, %xmm3
	vsubss	%xmm14, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm6
	vsubss	%xmm6, %xmm14, %xmm6
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vmovaps	%xmm13, %xmm9
	vsubss	%xmm7, %xmm13, %xmm5
	vsubss	%xmm13, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm13, %xmm7
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vaddss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vsubss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vmovups	160(%rsp), %xmm5                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm15, %xmm2
	vaddss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vmulss	%xmm2, %xmm8, %xmm2
	vdivss	%xmm2, %xmm0, %xmm0
.LBB37_327:
	vmovlps	%xmm1, 40(%rsp)
	vmovss	%xmm0, 48(%rsp)
	leaq	280(%rsp), %r14
	movq	%r14, 264(%rsp)
	movq	$32, 8(%rsp)
.Ltmp4326:
	leaq	264(%rsp), %rdi
	leaq	8(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp4327:
# %bb.328:
	movq	%rax, 264(%rsp)
	movq	8(%rsp), %rcx
	movq	%rcx, 280(%rsp)
	vmovupd	.L.str.63(%rip), %ymm0
	vmovupd	%ymm0, (%rax)
	movq	%rcx, 272(%rsp)
	movq	264(%rsp), %rax
	movb	$0, (%rax,%rcx)
.Ltmp4329:
	leaq	264(%rsp), %rdi
	leaq	40(%rsp), %rsi
	xorl	%edx, %edx
	vzeroupper
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp4330:
# %bb.329:
	movq	264(%rsp), %rdi
	cmpq	%r14, %rdi
	je	.LBB37_331
# %bb.330:
	callq	_ZdlPv
.LBB37_331:
	addq	$472, %rsp                      # imm = 0x1D8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB37_332:
	.cfi_def_cfa_offset 528
.Ltmp4325:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_333:
.Ltmp4319:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_334:
.Ltmp4276:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_335:
.Ltmp4170:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_336:
.Ltmp4164:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_337:
.Ltmp4121:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_338:
.Ltmp4012:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_339:
.Ltmp4009:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_340:
.Ltmp4006:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_341:
.Ltmp4003:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_342:
.Ltmp3948:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_343:
.Ltmp3945:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_344:
.Ltmp3942:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_345:
.Ltmp3939:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_346:
.Ltmp3884:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_347:
.Ltmp3881:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_348:
.Ltmp3878:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_349:
.Ltmp3875:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_350:
.Ltmp4331:
	movq	%rax, %rbx
	movq	264(%rsp), %rdi
	cmpq	%r14, %rdi
	jne	.LBB37_368
	jmp	.LBB37_369
.LBB37_351:
.Ltmp4328:
	movq	%rax, %rdi
	callq	_Unwind_Resume@PLT
.LBB37_352:
.Ltmp4322:
	movq	%rax, %rbx
	movq	312(%rsp), %rdi
	jmp	.LBB37_356
.LBB37_353:
.Ltmp4316:
	movq	%rax, %rbx
	jmp	.LBB37_428
.LBB37_354:
.Ltmp4260:
	movq	%rax, %rbx
	jmp	.LBB37_428
.LBB37_355:
.Ltmp4167:
	movq	%rax, %rbx
	movq	344(%rsp), %rdi
.LBB37_356:
	cmpq	%r15, %rdi
	je	.LBB37_434
# %bb.357:
	callq	_ZdlPv
	jmp	.LBB37_434
.LBB37_358:
.Ltmp4161:
	movq	%rax, %rbx
	jmp	.LBB37_428
.LBB37_359:
.Ltmp4105:
	movq	%rax, %rbx
	jmp	.LBB37_428
.LBB37_360:
.Ltmp4015:
	movq	%rax, %rbx
	movq	376(%rsp), %rdi
	jmp	.LBB37_367
.LBB37_361:
.Ltmp4000:
	jmp	.LBB37_372
.LBB37_362:
.Ltmp3997:
	jmp	.LBB37_372
.LBB37_363:
.Ltmp3951:
	movq	%rax, %rbx
	movq	408(%rsp), %rdi
	jmp	.LBB37_367
.LBB37_364:
.Ltmp3936:
	jmp	.LBB37_372
.LBB37_365:
.Ltmp3933:
	jmp	.LBB37_372
.LBB37_366:
.Ltmp3887:
	movq	%rax, %rbx
	movq	440(%rsp), %rdi
.LBB37_367:
	cmpq	%r15, %rdi
	je	.LBB37_369
.LBB37_368:
	callq	_ZdlPv
.LBB37_369:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB37_370:
.Ltmp3872:
	jmp	.LBB37_372
.LBB37_371:
.Ltmp3869:
.LBB37_372:
	movq	%rax, %rbx
	leaq	40(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB37_395
.LBB37_373:
.Ltmp4313:
	movq	%rax, %rbx
	jmp	.LBB37_428
.LBB37_374:
.Ltmp4158:
	movq	%rax, %rbx
	jmp	.LBB37_428
.LBB37_375:
.Ltmp3983:
	jmp	.LBB37_394
.LBB37_376:
.Ltmp3967:
	movq	%rax, %rbx
	jmp	.LBB37_396
.LBB37_377:
.Ltmp3919:
	jmp	.LBB37_394
.LBB37_378:
.Ltmp3903:
	movq	%rax, %rbx
	jmp	.LBB37_396
.LBB37_379:
.Ltmp3855:
	jmp	.LBB37_394
.LBB37_380:
.Ltmp3839:
	movq	%rax, %rbx
	jmp	.LBB37_396
.LBB37_381:
.Ltmp4273:
	movq	%rax, %rbx
	jmp	.LBB37_428
.LBB37_382:
.Ltmp4118:
	movq	%rax, %rbx
	jmp	.LBB37_428
.LBB37_383:
.Ltmp4308:
	movq	%rax, %rbx
	jmp	.LBB37_434
.LBB37_384:
.Ltmp4257:
	movq	%rax, %rbx
	jmp	.LBB37_434
.LBB37_385:
.Ltmp4153:
	movq	%rax, %rbx
	jmp	.LBB37_434
.LBB37_386:
.Ltmp4102:
	movq	%rax, %rbx
	jmp	.LBB37_434
.LBB37_387:
.Ltmp3994:
	jmp	.LBB37_394
.LBB37_388:
.Ltmp3978:
	movq	%rax, %rbx
	jmp	.LBB37_396
.LBB37_389:
.Ltmp3962:
	jmp	.LBB37_399
.LBB37_390:
.Ltmp3930:
	jmp	.LBB37_394
.LBB37_391:
.Ltmp3914:
	movq	%rax, %rbx
	jmp	.LBB37_396
.LBB37_392:
.Ltmp3898:
	jmp	.LBB37_399
.LBB37_393:
.Ltmp3866:
.LBB37_394:
	movq	%rax, %rbx
.LBB37_395:
	leaq	8(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB37_396:
	leaq	88(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	leaq	176(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB37_397:
.Ltmp3850:
	movq	%rax, %rbx
	jmp	.LBB37_396
.LBB37_398:
.Ltmp3834:
.LBB37_399:
	movq	%rax, %rbx
	leaq	176(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB37_400:
.Ltmp4297:
	movq	%rax, %rbx
	jmp	.LBB37_434
.LBB37_401:
.Ltmp4142:
	movq	%rax, %rbx
	jmp	.LBB37_434
.LBB37_402:
.Ltmp4246:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_403:
.Ltmp4243:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_404:
.Ltmp4240:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_405:
.Ltmp4192:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_406:
.Ltmp4091:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_407:
.Ltmp4088:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_408:
.Ltmp4085:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_409:
.Ltmp4037:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB37_410:
.Ltmp4173:
	movq	%rax, %rbx
	jmp	.LBB37_434
.LBB37_411:
.Ltmp4018:
	movq	%rax, %rbx
	jmp	.LBB37_434
.LBB37_412:
.Ltmp4224:
	jmp	.LBB37_421
.LBB37_413:
.Ltmp4208:
	jmp	.LBB37_426
.LBB37_414:
.Ltmp4189:
	jmp	.LBB37_418
.LBB37_415:
.Ltmp4069:
	jmp	.LBB37_421
.LBB37_416:
.Ltmp4053:
	jmp	.LBB37_426
.LBB37_417:
.Ltmp4034:
.LBB37_418:
	movq	%rax, %rbx
	leaq	8(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB37_432
.LBB37_419:
.Ltmp4237:
	jmp	.LBB37_421
.LBB37_420:
.Ltmp4082:
.LBB37_421:
	movq	%rax, %rbx
	leaq	88(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB37_427
.LBB37_422:
.Ltmp4219:
	jmp	.LBB37_426
.LBB37_423:
.Ltmp4203:
	movq	%rax, %rbx
	jmp	.LBB37_428
.LBB37_424:
.Ltmp4184:
	jmp	.LBB37_431
.LBB37_425:
.Ltmp4064:
.LBB37_426:
	movq	%rax, %rbx
.LBB37_427:
	leaq	176(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB37_428:
	leaq	8(%rsp), %rdi
	jmp	.LBB37_433
.LBB37_429:
.Ltmp4048:
	movq	%rax, %rbx
	jmp	.LBB37_428
.LBB37_430:
.Ltmp4029:
.LBB37_431:
	movq	%rax, %rbx
.LBB37_432:
	leaq	88(%rsp), %rdi
.LBB37_433:
	callq	_ZN4mpfr6mprealD2Ev
.LBB37_434:
	leaq	40(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end37:
	.size	_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_, .Lfunc_end37-_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.cfi_endproc
	.section	.gcc_except_table._Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"aG",@progbits,_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.p2align	2, 0x0
GCC_except_table37:
.Lexception30:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase19-.Lttbaseref19
.Lttbaseref19:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end30-.Lcst_begin30
.Lcst_begin30:
	.uleb128 .Lfunc_begin30-.Lfunc_begin30  # >> Call Site 1 <<
	.uleb128 .Ltmp3824-.Lfunc_begin30       #   Call between .Lfunc_begin30 and .Ltmp3824
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3824-.Lfunc_begin30       # >> Call Site 2 <<
	.uleb128 .Ltmp3833-.Ltmp3824            #   Call between .Ltmp3824 and .Ltmp3833
	.uleb128 .Ltmp3834-.Lfunc_begin30       #     jumps to .Ltmp3834
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3835-.Lfunc_begin30       # >> Call Site 3 <<
	.uleb128 .Ltmp3838-.Ltmp3835            #   Call between .Ltmp3835 and .Ltmp3838
	.uleb128 .Ltmp3839-.Lfunc_begin30       #     jumps to .Ltmp3839
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3840-.Lfunc_begin30       # >> Call Site 4 <<
	.uleb128 .Ltmp3849-.Ltmp3840            #   Call between .Ltmp3840 and .Ltmp3849
	.uleb128 .Ltmp3850-.Lfunc_begin30       #     jumps to .Ltmp3850
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3851-.Lfunc_begin30       # >> Call Site 5 <<
	.uleb128 .Ltmp3854-.Ltmp3851            #   Call between .Ltmp3851 and .Ltmp3854
	.uleb128 .Ltmp3855-.Lfunc_begin30       #     jumps to .Ltmp3855
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3856-.Lfunc_begin30       # >> Call Site 6 <<
	.uleb128 .Ltmp3865-.Ltmp3856            #   Call between .Ltmp3856 and .Ltmp3865
	.uleb128 .Ltmp3866-.Lfunc_begin30       #     jumps to .Ltmp3866
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3867-.Lfunc_begin30       # >> Call Site 7 <<
	.uleb128 .Ltmp3868-.Ltmp3867            #   Call between .Ltmp3867 and .Ltmp3868
	.uleb128 .Ltmp3869-.Lfunc_begin30       #     jumps to .Ltmp3869
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3870-.Lfunc_begin30       # >> Call Site 8 <<
	.uleb128 .Ltmp3871-.Ltmp3870            #   Call between .Ltmp3870 and .Ltmp3871
	.uleb128 .Ltmp3872-.Lfunc_begin30       #     jumps to .Ltmp3872
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3873-.Lfunc_begin30       # >> Call Site 9 <<
	.uleb128 .Ltmp3874-.Ltmp3873            #   Call between .Ltmp3873 and .Ltmp3874
	.uleb128 .Ltmp3875-.Lfunc_begin30       #     jumps to .Ltmp3875
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3876-.Lfunc_begin30       # >> Call Site 10 <<
	.uleb128 .Ltmp3877-.Ltmp3876            #   Call between .Ltmp3876 and .Ltmp3877
	.uleb128 .Ltmp3878-.Lfunc_begin30       #     jumps to .Ltmp3878
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3879-.Lfunc_begin30       # >> Call Site 11 <<
	.uleb128 .Ltmp3880-.Ltmp3879            #   Call between .Ltmp3879 and .Ltmp3880
	.uleb128 .Ltmp3881-.Lfunc_begin30       #     jumps to .Ltmp3881
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3882-.Lfunc_begin30       # >> Call Site 12 <<
	.uleb128 .Ltmp3883-.Ltmp3882            #   Call between .Ltmp3882 and .Ltmp3883
	.uleb128 .Ltmp3884-.Lfunc_begin30       #     jumps to .Ltmp3884
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3885-.Lfunc_begin30       # >> Call Site 13 <<
	.uleb128 .Ltmp3886-.Ltmp3885            #   Call between .Ltmp3885 and .Ltmp3886
	.uleb128 .Ltmp3887-.Lfunc_begin30       #     jumps to .Ltmp3887
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3886-.Lfunc_begin30       # >> Call Site 14 <<
	.uleb128 .Ltmp3888-.Ltmp3886            #   Call between .Ltmp3886 and .Ltmp3888
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3888-.Lfunc_begin30       # >> Call Site 15 <<
	.uleb128 .Ltmp3897-.Ltmp3888            #   Call between .Ltmp3888 and .Ltmp3897
	.uleb128 .Ltmp3898-.Lfunc_begin30       #     jumps to .Ltmp3898
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3899-.Lfunc_begin30       # >> Call Site 16 <<
	.uleb128 .Ltmp3902-.Ltmp3899            #   Call between .Ltmp3899 and .Ltmp3902
	.uleb128 .Ltmp3903-.Lfunc_begin30       #     jumps to .Ltmp3903
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3904-.Lfunc_begin30       # >> Call Site 17 <<
	.uleb128 .Ltmp3913-.Ltmp3904            #   Call between .Ltmp3904 and .Ltmp3913
	.uleb128 .Ltmp3914-.Lfunc_begin30       #     jumps to .Ltmp3914
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3915-.Lfunc_begin30       # >> Call Site 18 <<
	.uleb128 .Ltmp3918-.Ltmp3915            #   Call between .Ltmp3915 and .Ltmp3918
	.uleb128 .Ltmp3919-.Lfunc_begin30       #     jumps to .Ltmp3919
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3920-.Lfunc_begin30       # >> Call Site 19 <<
	.uleb128 .Ltmp3929-.Ltmp3920            #   Call between .Ltmp3920 and .Ltmp3929
	.uleb128 .Ltmp3930-.Lfunc_begin30       #     jumps to .Ltmp3930
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3931-.Lfunc_begin30       # >> Call Site 20 <<
	.uleb128 .Ltmp3932-.Ltmp3931            #   Call between .Ltmp3931 and .Ltmp3932
	.uleb128 .Ltmp3933-.Lfunc_begin30       #     jumps to .Ltmp3933
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3934-.Lfunc_begin30       # >> Call Site 21 <<
	.uleb128 .Ltmp3935-.Ltmp3934            #   Call between .Ltmp3934 and .Ltmp3935
	.uleb128 .Ltmp3936-.Lfunc_begin30       #     jumps to .Ltmp3936
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3937-.Lfunc_begin30       # >> Call Site 22 <<
	.uleb128 .Ltmp3938-.Ltmp3937            #   Call between .Ltmp3937 and .Ltmp3938
	.uleb128 .Ltmp3939-.Lfunc_begin30       #     jumps to .Ltmp3939
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3940-.Lfunc_begin30       # >> Call Site 23 <<
	.uleb128 .Ltmp3941-.Ltmp3940            #   Call between .Ltmp3940 and .Ltmp3941
	.uleb128 .Ltmp3942-.Lfunc_begin30       #     jumps to .Ltmp3942
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3943-.Lfunc_begin30       # >> Call Site 24 <<
	.uleb128 .Ltmp3944-.Ltmp3943            #   Call between .Ltmp3943 and .Ltmp3944
	.uleb128 .Ltmp3945-.Lfunc_begin30       #     jumps to .Ltmp3945
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3946-.Lfunc_begin30       # >> Call Site 25 <<
	.uleb128 .Ltmp3947-.Ltmp3946            #   Call between .Ltmp3946 and .Ltmp3947
	.uleb128 .Ltmp3948-.Lfunc_begin30       #     jumps to .Ltmp3948
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp3949-.Lfunc_begin30       # >> Call Site 26 <<
	.uleb128 .Ltmp3950-.Ltmp3949            #   Call between .Ltmp3949 and .Ltmp3950
	.uleb128 .Ltmp3951-.Lfunc_begin30       #     jumps to .Ltmp3951
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3950-.Lfunc_begin30       # >> Call Site 27 <<
	.uleb128 .Ltmp3952-.Ltmp3950            #   Call between .Ltmp3950 and .Ltmp3952
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3952-.Lfunc_begin30       # >> Call Site 28 <<
	.uleb128 .Ltmp3961-.Ltmp3952            #   Call between .Ltmp3952 and .Ltmp3961
	.uleb128 .Ltmp3962-.Lfunc_begin30       #     jumps to .Ltmp3962
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3963-.Lfunc_begin30       # >> Call Site 29 <<
	.uleb128 .Ltmp3966-.Ltmp3963            #   Call between .Ltmp3963 and .Ltmp3966
	.uleb128 .Ltmp3967-.Lfunc_begin30       #     jumps to .Ltmp3967
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3968-.Lfunc_begin30       # >> Call Site 30 <<
	.uleb128 .Ltmp3977-.Ltmp3968            #   Call between .Ltmp3968 and .Ltmp3977
	.uleb128 .Ltmp3978-.Lfunc_begin30       #     jumps to .Ltmp3978
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3979-.Lfunc_begin30       # >> Call Site 31 <<
	.uleb128 .Ltmp3982-.Ltmp3979            #   Call between .Ltmp3979 and .Ltmp3982
	.uleb128 .Ltmp3983-.Lfunc_begin30       #     jumps to .Ltmp3983
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3984-.Lfunc_begin30       # >> Call Site 32 <<
	.uleb128 .Ltmp3993-.Ltmp3984            #   Call between .Ltmp3984 and .Ltmp3993
	.uleb128 .Ltmp3994-.Lfunc_begin30       #     jumps to .Ltmp3994
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3995-.Lfunc_begin30       # >> Call Site 33 <<
	.uleb128 .Ltmp3996-.Ltmp3995            #   Call between .Ltmp3995 and .Ltmp3996
	.uleb128 .Ltmp3997-.Lfunc_begin30       #     jumps to .Ltmp3997
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp3998-.Lfunc_begin30       # >> Call Site 34 <<
	.uleb128 .Ltmp3999-.Ltmp3998            #   Call between .Ltmp3998 and .Ltmp3999
	.uleb128 .Ltmp4000-.Lfunc_begin30       #     jumps to .Ltmp4000
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4001-.Lfunc_begin30       # >> Call Site 35 <<
	.uleb128 .Ltmp4002-.Ltmp4001            #   Call between .Ltmp4001 and .Ltmp4002
	.uleb128 .Ltmp4003-.Lfunc_begin30       #     jumps to .Ltmp4003
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4004-.Lfunc_begin30       # >> Call Site 36 <<
	.uleb128 .Ltmp4005-.Ltmp4004            #   Call between .Ltmp4004 and .Ltmp4005
	.uleb128 .Ltmp4006-.Lfunc_begin30       #     jumps to .Ltmp4006
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4007-.Lfunc_begin30       # >> Call Site 37 <<
	.uleb128 .Ltmp4008-.Ltmp4007            #   Call between .Ltmp4007 and .Ltmp4008
	.uleb128 .Ltmp4009-.Lfunc_begin30       #     jumps to .Ltmp4009
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4010-.Lfunc_begin30       # >> Call Site 38 <<
	.uleb128 .Ltmp4011-.Ltmp4010            #   Call between .Ltmp4010 and .Ltmp4011
	.uleb128 .Ltmp4012-.Lfunc_begin30       #     jumps to .Ltmp4012
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4013-.Lfunc_begin30       # >> Call Site 39 <<
	.uleb128 .Ltmp4014-.Ltmp4013            #   Call between .Ltmp4013 and .Ltmp4014
	.uleb128 .Ltmp4015-.Lfunc_begin30       #     jumps to .Ltmp4015
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4014-.Lfunc_begin30       # >> Call Site 40 <<
	.uleb128 .Ltmp4016-.Ltmp4014            #   Call between .Ltmp4014 and .Ltmp4016
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4016-.Lfunc_begin30       # >> Call Site 41 <<
	.uleb128 .Ltmp4017-.Ltmp4016            #   Call between .Ltmp4016 and .Ltmp4017
	.uleb128 .Ltmp4018-.Lfunc_begin30       #     jumps to .Ltmp4018
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4019-.Lfunc_begin30       # >> Call Site 42 <<
	.uleb128 .Ltmp4028-.Ltmp4019            #   Call between .Ltmp4019 and .Ltmp4028
	.uleb128 .Ltmp4029-.Lfunc_begin30       #     jumps to .Ltmp4029
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4030-.Lfunc_begin30       # >> Call Site 43 <<
	.uleb128 .Ltmp4033-.Ltmp4030            #   Call between .Ltmp4030 and .Ltmp4033
	.uleb128 .Ltmp4034-.Lfunc_begin30       #     jumps to .Ltmp4034
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4035-.Lfunc_begin30       # >> Call Site 44 <<
	.uleb128 .Ltmp4036-.Ltmp4035            #   Call between .Ltmp4035 and .Ltmp4036
	.uleb128 .Ltmp4037-.Lfunc_begin30       #     jumps to .Ltmp4037
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4038-.Lfunc_begin30       # >> Call Site 45 <<
	.uleb128 .Ltmp4047-.Ltmp4038            #   Call between .Ltmp4038 and .Ltmp4047
	.uleb128 .Ltmp4048-.Lfunc_begin30       #     jumps to .Ltmp4048
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4049-.Lfunc_begin30       # >> Call Site 46 <<
	.uleb128 .Ltmp4052-.Ltmp4049            #   Call between .Ltmp4049 and .Ltmp4052
	.uleb128 .Ltmp4053-.Lfunc_begin30       #     jumps to .Ltmp4053
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4054-.Lfunc_begin30       # >> Call Site 47 <<
	.uleb128 .Ltmp4063-.Ltmp4054            #   Call between .Ltmp4054 and .Ltmp4063
	.uleb128 .Ltmp4064-.Lfunc_begin30       #     jumps to .Ltmp4064
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4065-.Lfunc_begin30       # >> Call Site 48 <<
	.uleb128 .Ltmp4068-.Ltmp4065            #   Call between .Ltmp4065 and .Ltmp4068
	.uleb128 .Ltmp4069-.Lfunc_begin30       #     jumps to .Ltmp4069
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4070-.Lfunc_begin30       # >> Call Site 49 <<
	.uleb128 .Ltmp4081-.Ltmp4070            #   Call between .Ltmp4070 and .Ltmp4081
	.uleb128 .Ltmp4082-.Lfunc_begin30       #     jumps to .Ltmp4082
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4083-.Lfunc_begin30       # >> Call Site 50 <<
	.uleb128 .Ltmp4084-.Ltmp4083            #   Call between .Ltmp4083 and .Ltmp4084
	.uleb128 .Ltmp4085-.Lfunc_begin30       #     jumps to .Ltmp4085
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4086-.Lfunc_begin30       # >> Call Site 51 <<
	.uleb128 .Ltmp4087-.Ltmp4086            #   Call between .Ltmp4086 and .Ltmp4087
	.uleb128 .Ltmp4088-.Lfunc_begin30       #     jumps to .Ltmp4088
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4089-.Lfunc_begin30       # >> Call Site 52 <<
	.uleb128 .Ltmp4090-.Ltmp4089            #   Call between .Ltmp4089 and .Ltmp4090
	.uleb128 .Ltmp4091-.Lfunc_begin30       #     jumps to .Ltmp4091
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4092-.Lfunc_begin30       # >> Call Site 53 <<
	.uleb128 .Ltmp4101-.Ltmp4092            #   Call between .Ltmp4092 and .Ltmp4101
	.uleb128 .Ltmp4102-.Lfunc_begin30       #     jumps to .Ltmp4102
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4103-.Lfunc_begin30       # >> Call Site 54 <<
	.uleb128 .Ltmp4104-.Ltmp4103            #   Call between .Ltmp4103 and .Ltmp4104
	.uleb128 .Ltmp4105-.Lfunc_begin30       #     jumps to .Ltmp4105
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4106-.Lfunc_begin30       # >> Call Site 55 <<
	.uleb128 .Ltmp4117-.Ltmp4106            #   Call between .Ltmp4106 and .Ltmp4117
	.uleb128 .Ltmp4118-.Lfunc_begin30       #     jumps to .Ltmp4118
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4119-.Lfunc_begin30       # >> Call Site 56 <<
	.uleb128 .Ltmp4120-.Ltmp4119            #   Call between .Ltmp4119 and .Ltmp4120
	.uleb128 .Ltmp4121-.Lfunc_begin30       #     jumps to .Ltmp4121
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4122-.Lfunc_begin30       # >> Call Site 57 <<
	.uleb128 .Ltmp4141-.Ltmp4122            #   Call between .Ltmp4122 and .Ltmp4141
	.uleb128 .Ltmp4142-.Lfunc_begin30       #     jumps to .Ltmp4142
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4143-.Lfunc_begin30       # >> Call Site 58 <<
	.uleb128 .Ltmp4152-.Ltmp4143            #   Call between .Ltmp4143 and .Ltmp4152
	.uleb128 .Ltmp4153-.Lfunc_begin30       #     jumps to .Ltmp4153
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4154-.Lfunc_begin30       # >> Call Site 59 <<
	.uleb128 .Ltmp4157-.Ltmp4154            #   Call between .Ltmp4154 and .Ltmp4157
	.uleb128 .Ltmp4158-.Lfunc_begin30       #     jumps to .Ltmp4158
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4159-.Lfunc_begin30       # >> Call Site 60 <<
	.uleb128 .Ltmp4160-.Ltmp4159            #   Call between .Ltmp4159 and .Ltmp4160
	.uleb128 .Ltmp4161-.Lfunc_begin30       #     jumps to .Ltmp4161
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4162-.Lfunc_begin30       # >> Call Site 61 <<
	.uleb128 .Ltmp4163-.Ltmp4162            #   Call between .Ltmp4162 and .Ltmp4163
	.uleb128 .Ltmp4164-.Lfunc_begin30       #     jumps to .Ltmp4164
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4165-.Lfunc_begin30       # >> Call Site 62 <<
	.uleb128 .Ltmp4166-.Ltmp4165            #   Call between .Ltmp4165 and .Ltmp4166
	.uleb128 .Ltmp4167-.Lfunc_begin30       #     jumps to .Ltmp4167
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4168-.Lfunc_begin30       # >> Call Site 63 <<
	.uleb128 .Ltmp4169-.Ltmp4168            #   Call between .Ltmp4168 and .Ltmp4169
	.uleb128 .Ltmp4170-.Lfunc_begin30       #     jumps to .Ltmp4170
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4169-.Lfunc_begin30       # >> Call Site 64 <<
	.uleb128 .Ltmp4171-.Ltmp4169            #   Call between .Ltmp4169 and .Ltmp4171
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4171-.Lfunc_begin30       # >> Call Site 65 <<
	.uleb128 .Ltmp4172-.Ltmp4171            #   Call between .Ltmp4171 and .Ltmp4172
	.uleb128 .Ltmp4173-.Lfunc_begin30       #     jumps to .Ltmp4173
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4174-.Lfunc_begin30       # >> Call Site 66 <<
	.uleb128 .Ltmp4183-.Ltmp4174            #   Call between .Ltmp4174 and .Ltmp4183
	.uleb128 .Ltmp4184-.Lfunc_begin30       #     jumps to .Ltmp4184
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4185-.Lfunc_begin30       # >> Call Site 67 <<
	.uleb128 .Ltmp4188-.Ltmp4185            #   Call between .Ltmp4185 and .Ltmp4188
	.uleb128 .Ltmp4189-.Lfunc_begin30       #     jumps to .Ltmp4189
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4190-.Lfunc_begin30       # >> Call Site 68 <<
	.uleb128 .Ltmp4191-.Ltmp4190            #   Call between .Ltmp4190 and .Ltmp4191
	.uleb128 .Ltmp4192-.Lfunc_begin30       #     jumps to .Ltmp4192
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4193-.Lfunc_begin30       # >> Call Site 69 <<
	.uleb128 .Ltmp4202-.Ltmp4193            #   Call between .Ltmp4193 and .Ltmp4202
	.uleb128 .Ltmp4203-.Lfunc_begin30       #     jumps to .Ltmp4203
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4204-.Lfunc_begin30       # >> Call Site 70 <<
	.uleb128 .Ltmp4207-.Ltmp4204            #   Call between .Ltmp4204 and .Ltmp4207
	.uleb128 .Ltmp4208-.Lfunc_begin30       #     jumps to .Ltmp4208
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4209-.Lfunc_begin30       # >> Call Site 71 <<
	.uleb128 .Ltmp4218-.Ltmp4209            #   Call between .Ltmp4209 and .Ltmp4218
	.uleb128 .Ltmp4219-.Lfunc_begin30       #     jumps to .Ltmp4219
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4220-.Lfunc_begin30       # >> Call Site 72 <<
	.uleb128 .Ltmp4223-.Ltmp4220            #   Call between .Ltmp4220 and .Ltmp4223
	.uleb128 .Ltmp4224-.Lfunc_begin30       #     jumps to .Ltmp4224
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4225-.Lfunc_begin30       # >> Call Site 73 <<
	.uleb128 .Ltmp4236-.Ltmp4225            #   Call between .Ltmp4225 and .Ltmp4236
	.uleb128 .Ltmp4237-.Lfunc_begin30       #     jumps to .Ltmp4237
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4238-.Lfunc_begin30       # >> Call Site 74 <<
	.uleb128 .Ltmp4239-.Ltmp4238            #   Call between .Ltmp4238 and .Ltmp4239
	.uleb128 .Ltmp4240-.Lfunc_begin30       #     jumps to .Ltmp4240
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4241-.Lfunc_begin30       # >> Call Site 75 <<
	.uleb128 .Ltmp4242-.Ltmp4241            #   Call between .Ltmp4241 and .Ltmp4242
	.uleb128 .Ltmp4243-.Lfunc_begin30       #     jumps to .Ltmp4243
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4244-.Lfunc_begin30       # >> Call Site 76 <<
	.uleb128 .Ltmp4245-.Ltmp4244            #   Call between .Ltmp4244 and .Ltmp4245
	.uleb128 .Ltmp4246-.Lfunc_begin30       #     jumps to .Ltmp4246
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4247-.Lfunc_begin30       # >> Call Site 77 <<
	.uleb128 .Ltmp4256-.Ltmp4247            #   Call between .Ltmp4247 and .Ltmp4256
	.uleb128 .Ltmp4257-.Lfunc_begin30       #     jumps to .Ltmp4257
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4258-.Lfunc_begin30       # >> Call Site 78 <<
	.uleb128 .Ltmp4259-.Ltmp4258            #   Call between .Ltmp4258 and .Ltmp4259
	.uleb128 .Ltmp4260-.Lfunc_begin30       #     jumps to .Ltmp4260
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4261-.Lfunc_begin30       # >> Call Site 79 <<
	.uleb128 .Ltmp4272-.Ltmp4261            #   Call between .Ltmp4261 and .Ltmp4272
	.uleb128 .Ltmp4273-.Lfunc_begin30       #     jumps to .Ltmp4273
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4274-.Lfunc_begin30       # >> Call Site 80 <<
	.uleb128 .Ltmp4275-.Ltmp4274            #   Call between .Ltmp4274 and .Ltmp4275
	.uleb128 .Ltmp4276-.Lfunc_begin30       #     jumps to .Ltmp4276
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4277-.Lfunc_begin30       # >> Call Site 81 <<
	.uleb128 .Ltmp4296-.Ltmp4277            #   Call between .Ltmp4277 and .Ltmp4296
	.uleb128 .Ltmp4297-.Lfunc_begin30       #     jumps to .Ltmp4297
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4298-.Lfunc_begin30       # >> Call Site 82 <<
	.uleb128 .Ltmp4307-.Ltmp4298            #   Call between .Ltmp4298 and .Ltmp4307
	.uleb128 .Ltmp4308-.Lfunc_begin30       #     jumps to .Ltmp4308
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4309-.Lfunc_begin30       # >> Call Site 83 <<
	.uleb128 .Ltmp4312-.Ltmp4309            #   Call between .Ltmp4309 and .Ltmp4312
	.uleb128 .Ltmp4313-.Lfunc_begin30       #     jumps to .Ltmp4313
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4314-.Lfunc_begin30       # >> Call Site 84 <<
	.uleb128 .Ltmp4315-.Ltmp4314            #   Call between .Ltmp4314 and .Ltmp4315
	.uleb128 .Ltmp4316-.Lfunc_begin30       #     jumps to .Ltmp4316
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4317-.Lfunc_begin30       # >> Call Site 85 <<
	.uleb128 .Ltmp4318-.Ltmp4317            #   Call between .Ltmp4317 and .Ltmp4318
	.uleb128 .Ltmp4319-.Lfunc_begin30       #     jumps to .Ltmp4319
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4320-.Lfunc_begin30       # >> Call Site 86 <<
	.uleb128 .Ltmp4321-.Ltmp4320            #   Call between .Ltmp4320 and .Ltmp4321
	.uleb128 .Ltmp4322-.Lfunc_begin30       #     jumps to .Ltmp4322
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4323-.Lfunc_begin30       # >> Call Site 87 <<
	.uleb128 .Ltmp4324-.Ltmp4323            #   Call between .Ltmp4323 and .Ltmp4324
	.uleb128 .Ltmp4325-.Lfunc_begin30       #     jumps to .Ltmp4325
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4326-.Lfunc_begin30       # >> Call Site 88 <<
	.uleb128 .Ltmp4327-.Ltmp4326            #   Call between .Ltmp4326 and .Ltmp4327
	.uleb128 .Ltmp4328-.Lfunc_begin30       #     jumps to .Ltmp4328
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4329-.Lfunc_begin30       # >> Call Site 89 <<
	.uleb128 .Ltmp4330-.Ltmp4329            #   Call between .Ltmp4329 and .Ltmp4330
	.uleb128 .Ltmp4331-.Lfunc_begin30       #     jumps to .Ltmp4331
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4330-.Lfunc_begin30       # >> Call Site 90 <<
	.uleb128 .Lfunc_end37-.Ltmp4330         #   Call between .Ltmp4330 and .Lfunc_end37
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end30:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase19:
	.p2align	2, 0x0
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2, 0x0                          # -- Begin function _Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.LCPI38_0:
	.long	0x3f800000                      #  1
.LCPI38_1:
	.long	0x3f000000                      #  0.5
.LCPI38_2:
	.long	0x80000000                      #  -0
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4, 0x0
.LCPI38_3:
	.zero	16
	.section	.text._Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"axG",@progbits,_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.weak	_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.p2align	4, 0x90
	.type	_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,@function
_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_: # 
.Lfunc_begin31:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception31
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$456, %rsp                      # imm = 0x1C8
	.cfi_def_cfa_offset 512
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, 144(%rsp)                  # 8-byte Spill
	movq	%rcx, %r15
	movq	%rdx, %r12
	movq	%rsi, 96(%rsp)                  # 8-byte Spill
	movq	%rdi, %rbx
	vcvtsi2sdl	(%rdi), %xmm0, %xmm0
	callq	sqrt
	vcvttsd2si	%xmm0, %eax
	movl	%eax, 4(%rsp)
	movq	%rbx, 8(%rsp)                   # 8-byte Spill
	movslq	(%rbx), %r13
	movl	$12, %ecx
	movq	%r13, %rax
	mulq	%rcx
	movq	%rax, %rbp
	movq	$-1, %r14
	cmovnoq	%rax, %r14
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %rbx
	movabsq	$-6148914691236517205, %rcx     # imm = 0xAAAAAAAAAAAAAAAB
	testq	%r13, %r13
	movq	%r15, 280(%rsp)                 # 8-byte Spill
	je	.LBB38_4
# %bb.1:
	addq	$-12, %rbp
	movq	%rbp, %rax
	mulq	%rcx
	shrq	$3, %rdx
	leaq	(%rdx,%rdx,2), %rax
	leaq	12(,%rax,4), %rbp
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movq	%rbp, %rdx
	callq	_intel_fast_memset@PLT
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 152(%rsp)                 # 8-byte Spill
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%rbp, %rdx
	callq	_intel_fast_memset@PLT
	testl	%r13d, %r13d
	jle	.LBB38_5
# %bb.2:
	movl	$8, %ebp
	movq	%r15, %r13
	xorl	%r15d, %r15d
	movq	152(%rsp), %r14                 # 8-byte Reload
	.p2align	4, 0x90
.LBB38_3:                               # =>This Inner Loop Header: Depth=1
	movq	%r12, %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, -8(%rbx,%rbp)
	vmovss	%xmm1, (%rbx,%rbp)
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, -8(%r14,%rbp)
	vmovss	%xmm1, (%r14,%rbp)
	incq	%r15
	movq	8(%rsp), %rax                   # 8-byte Reload
	movslq	(%rax), %rax
	addq	$12, %rbp
	addq	$32, %r13
	addq	$32, %r12
	cmpq	%rax, %r15
	jl	.LBB38_3
	jmp	.LBB38_5
.LBB38_4:
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 152(%rsp)                 # 8-byte Spill
.LBB38_5:
	movq	96(%rsp), %rdi                  # 8-byte Reload
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, 80(%rsp)
	vmovss	%xmm1, 88(%rsp)
	movl	$120, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vmovupd	%ymm0, 64(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovlpd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 96(%rax)
	vzeroupper
	callq	omp_get_wtime
	vxorps	%xmm12, %xmm12, %xmm12
	vmovsd	%xmm0, 96(%rsp)                 # 8-byte Spill
	movq	8(%rsp), %rax                   # 8-byte Reload
	movl	(%rax), %eax
	movq	%rax, %rcx
	shlq	$2, %rcx
	leaq	(%rcx,%rcx,2), %rcx
	xorl	%edx, %edx
	xorl	%esi, %esi
	movq	152(%rsp), %r8                  # 8-byte Reload
	jmp	.LBB38_7
	.p2align	4, 0x90
.LBB38_6:                               #   in Loop: Header=BB38_7 Depth=1
	leaq	(%rsi,%rsi,2), %rdi
	vmovlps	%xmm1, (%rbp,%rdi,4)
	vmovss	%xmm0, 8(%rbp,%rdi,4)
	incq	%rsi
	cmpq	$10, %rsi
	je	.LBB38_10
.LBB38_7:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB38_9 Depth 2
	vcvtsi2ss	%edx, %xmm13, %xmm0
	vblendps	$1, %xmm0, %xmm12, %xmm1        # xmm1 = xmm0[0],xmm12[1,2,3]
	vxorps	%xmm0, %xmm0, %xmm0
	testl	%eax, %eax
	jle	.LBB38_6
# %bb.8:                                #   in Loop: Header=BB38_7 Depth=1
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB38_9:                               #   Parent Loop BB38_7 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%r8,%rdi), %xmm4               # xmm4 = mem[0],zero,zero,zero
	vmovss	4(%r8,%rdi), %xmm5              # xmm5 = mem[0],zero,zero,zero
	vmovss	(%rbx,%rdi), %xmm3              # xmm3 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%rdi), %xmm6             # xmm6 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm3, %xmm2
	vmulss	%xmm5, %xmm3, %xmm7
	vmulss	%xmm4, %xmm6, %xmm8
	vmovaps	%xmm4, %xmm9
	vfmsub213ss	%xmm8, %xmm6, %xmm9     # xmm9 = (xmm6 * xmm9) - xmm8
	vfmadd231ss	%xmm5, %xmm6, %xmm9     # xmm9 = (xmm6 * xmm5) + xmm9
	vfmsub213ss	%xmm7, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm5) - xmm7
	vfmadd231ss	8(%rbx,%rdi), %xmm4, %xmm5 # xmm5 = (xmm4 * mem) + xmm5
	vfmsub213ss	%xmm2, %xmm3, %xmm4     # xmm4 = (xmm3 * xmm4) - xmm2
	vaddss	%xmm7, %xmm8, %xmm6
	vsubss	%xmm7, %xmm6, %xmm10
	vsubss	%xmm10, %xmm6, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm6, %xmm8
	vsubss	%xmm6, %xmm8, %xmm10
	vsubss	%xmm10, %xmm8, %xmm11
	vsubss	%xmm11, %xmm6, %xmm6
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm6, %xmm4
	vfmadd231ss	8(%r8,%rdi), %xmm3, %xmm7 # xmm7 = (xmm3 * mem) + xmm7
	vaddss	%xmm5, %xmm9, %xmm3
	vaddss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm4
	vsubss	%xmm4, %xmm8, %xmm5
	vaddss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm5, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm1, %xmm7
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm7, %xmm5
	vmovshdup	%xmm1, %xmm1            # xmm1 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm6
	vsubss	%xmm1, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vsubss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm5, %xmm6, %xmm2
	vsubss	%xmm6, %xmm2, %xmm7
	vsubss	%xmm7, %xmm2, %xmm8
	vsubss	%xmm8, %xmm6, %xmm6
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vinsertps	$16, %xmm1, %xmm2, %xmm1 # xmm1 = xmm2[0],xmm1[0],xmm2[2,3]
	addq	$12, %rdi
	cmpq	%rdi, %rcx
	jne	.LBB38_9
	jmp	.LBB38_6
.LBB38_10:
	callq	omp_get_wtime
	vsubsd	96(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm13, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	160(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp4332:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp4333:
# %bb.11:
.Ltmp4334:
	movq	%rax, %r12
	movq	144(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp4335:
# %bb.12:
.Ltmp4336:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4337:
# %bb.13:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp4338:
	leaq	112(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4339:
# %bb.14:
.Ltmp4340:
	leaq	112(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp4341:
# %bb.15:
.Ltmp4343:
	callq	mpfr_get_default_rounding_mode
.Ltmp4344:
# %bb.16:
.Ltmp4345:
	leaq	112(%rsp), %rdi
	leaq	160(%rsp), %rsi
	movq	144(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp4346:
# %bb.17:
.Ltmp4348:
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4349:
# %bb.18:
.Ltmp4350:
	movq	%rax, %r12
	movq	144(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp4351:
# %bb.19:
.Ltmp4352:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4353:
# %bb.20:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp4354:
	leaq	16(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4355:
# %bb.21:
.Ltmp4356:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp4357:
# %bb.22:
.Ltmp4359:
	callq	mpfr_get_default_rounding_mode
.Ltmp4360:
# %bb.23:
.Ltmp4361:
	leaq	16(%rsp), %rdi
	leaq	112(%rsp), %rsi
	movq	144(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp4362:
# %bb.24:
.Ltmp4364:
	callq	mpfr_get_default_rounding_mode
.Ltmp4365:
# %bb.25:
.Ltmp4366:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4367:
# %bb.26:
.Ltmp4368:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4369:
# %bb.27:
.Ltmp4370:
	movl	%eax, %r12d
	leaq	48(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4371:
# %bb.28:
.Ltmp4372:
	leaq	48(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4373:
# %bb.29:
.Ltmp4375:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp4376:
# %bb.30:
.Ltmp4378:
	leaq	48(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp4379:
# %bb.31:
	vmovlpd	%xmm0, 208(%rsp)
	vmovss	%xmm1, 216(%rsp)
	cmpq	$0, 72(%rsp)
	je	.LBB38_33
# %bb.32:
.Ltmp4381:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4382:
.LBB38_33:
	cmpq	$0, 40(%rsp)
	je	.LBB38_35
# %bb.34:
.Ltmp4384:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4385:
.LBB38_35:
	cmpq	$0, 136(%rsp)
	je	.LBB38_37
# %bb.36:
.Ltmp4387:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4388:
.LBB38_37:
	cmpq	$0, 184(%rsp)
	je	.LBB38_39
# %bb.38:
.Ltmp4390:
	leaq	160(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4391:
.LBB38_39:
	leaq	440(%rsp), %r15
	movq	%r15, 424(%rsp)
	movl	$544501604, 440(%rsp)           # imm = 0x20746F64
	movw	$32, 444(%rsp)
	movq	$5, 432(%rsp)
.Ltmp4393:
	leaq	424(%rsp), %rdi
	leaq	208(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp4394:
# %bb.40:
	movq	424(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB38_42
# %bb.41:
	callq	_ZdlPv
.LBB38_42:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$120, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vmovupd	%ymm0, 64(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovlpd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 96(%rax)
	xorl	%r15d, %r15d
	vzeroupper
	callq	omp_get_wtime
	vmovsd	%xmm0, 288(%rsp)                # 8-byte Spill
	vxorps	%xmm9, %xmm9, %xmm9
	movl	$2, %r13d
	xorl	%r12d, %r12d
	jmp	.LBB38_45
	.p2align	4, 0x90
.LBB38_43:                              #   in Loop: Header=BB38_45 Depth=1
	vmovss	.LCPI38_1(%rip), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vmulss	%xmm6, %xmm3, %xmm5
	vmulss	%xmm3, %xmm4, %xmm2
	vmulss	%xmm5, %xmm2, %xmm2
	vmovshdup	%xmm4, %xmm4            # xmm4 = xmm4[1,1,3,3]
	vmulss	%xmm3, %xmm4, %xmm4
	vmulss	%xmm5, %xmm4, %xmm4
	vmovss	%xmm4, 192(%rsp)                # 4-byte Spill
	vmulss	%xmm3, %xmm8, %xmm3
	vmulss	%xmm5, %xmm3, %xmm3
	vmovss	%xmm3, 96(%rsp)                 # 4-byte Spill
	vmovd	%xmm7, 244(%rsp)                # 4-byte Folded Spill
	vdivss	%xmm0, %xmm7, %xmm7
	movl	$3, %eax
	vcvtsi2ss	%eax, %xmm12, %xmm0
	vmulss	%xmm6, %xmm0, %xmm5
	vfmsub213ss	%xmm5, %xmm0, %xmm6     # xmm6 = (xmm0 * xmm6) - xmm5
	vmulss	%xmm1, %xmm0, %xmm8
	vxorps	%xmm4, %xmm4, %xmm4
	vxorps	%xmm9, %xmm9, %xmm9
	vfmsub213ss	%xmm8, %xmm0, %xmm9     # xmm9 = (xmm0 * xmm9) - xmm8
	vaddss	%xmm6, %xmm8, %xmm10
	vsubss	%xmm8, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vsubss	%xmm11, %xmm6, %xmm6
	vaddss	%xmm6, %xmm8, %xmm6
	vfmadd231ss	%xmm0, %xmm4, %xmm6     # xmm6 = (xmm4 * xmm0) + xmm6
	vaddss	%xmm6, %xmm9, %xmm0
	vaddss	%xmm0, %xmm10, %xmm8
	vsubss	%xmm8, %xmm10, %xmm6
	vaddss	%xmm0, %xmm6, %xmm0
	vmovss	%xmm0, 224(%rsp)                # 4-byte Spill
	vaddss	%xmm5, %xmm8, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm8, %xmm5
	vmulss	%xmm7, %xmm7, %xmm9
	vmovaps	%xmm7, %xmm8
	vfmsub213ss	%xmm9, %xmm7, %xmm8     # xmm8 = (xmm7 * xmm8) - xmm9
	vmulss	%xmm4, %xmm7, %xmm10
	vxorps	%xmm11, %xmm11, %xmm11
	vfmsub213ss	%xmm10, %xmm7, %xmm11   # xmm11 = (xmm7 * xmm11) - xmm10
	vmulss	%xmm7, %xmm4, %xmm12
	vaddss	%xmm12, %xmm10, %xmm13
	vsubss	%xmm10, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vmovaps	%xmm7, %xmm15
	vfmsub213ss	%xmm12, %xmm4, %xmm15   # xmm15 = (xmm4 * xmm15) - xmm12
	vsubss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm8, %xmm13, %xmm12
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm12, %xmm1
	vsubss	%xmm1, %xmm13, %xmm1
	vsubss	%xmm14, %xmm8, %xmm8
	vaddss	%xmm1, %xmm8, %xmm1
	vfmadd231ss	%xmm7, %xmm4, %xmm11    # xmm11 = (xmm4 * xmm7) + xmm11
	vfmadd231ss	%xmm4, %xmm4, %xmm15    # xmm15 = (xmm4 * xmm4) + xmm15
	vaddss	%xmm15, %xmm11, %xmm8
	vfmadd231ss	%xmm4, %xmm7, %xmm10    # xmm10 = (xmm7 * xmm4) + xmm10
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm1, %xmm12, %xmm10
	vsubss	%xmm10, %xmm12, %xmm8
	vaddss	%xmm1, %xmm8, %xmm8
	vaddss	%xmm10, %xmm9, %xmm1
	vsubss	%xmm1, %xmm9, %xmm9
	vaddss	%xmm10, %xmm9, %xmm9
	vmulss	%xmm2, %xmm9, %xmm10
	vmovss	192(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm0, %xmm11
	vmovaps	%xmm1, %xmm12
	vfmsub213ss	%xmm11, %xmm0, %xmm12   # xmm12 = (xmm0 * xmm12) - xmm11
	vfmadd231ss	%xmm9, %xmm0, %xmm12    # xmm12 = (xmm0 * xmm9) + xmm12
	vfmsub213ss	%xmm10, %xmm2, %xmm9    # xmm9 = (xmm2 * xmm9) - xmm10
	vaddss	%xmm11, %xmm10, %xmm13
	vsubss	%xmm10, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vsubss	%xmm14, %xmm11, %xmm11
	vmulss	%xmm1, %xmm2, %xmm14
	vfmadd231ss	%xmm1, %xmm3, %xmm9     # xmm9 = (xmm3 * xmm1) + xmm9
	vfmsub213ss	%xmm14, %xmm2, %xmm1    # xmm1 = (xmm2 * xmm1) - xmm14
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm1, %xmm13, %xmm11
	vsubss	%xmm13, %xmm11, %xmm15
	vsubss	%xmm15, %xmm11, %xmm3
	vsubss	%xmm3, %xmm13, %xmm3
	vsubss	%xmm15, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm12, %xmm9, %xmm3
	vfmadd231ss	%xmm8, %xmm2, %xmm10    # xmm10 = (xmm2 * xmm8) + xmm10
	vaddss	%xmm3, %xmm10, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm1, %xmm11, %xmm3
	vsubss	%xmm3, %xmm11, %xmm8
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm3, %xmm14, %xmm8
	vsubss	%xmm8, %xmm14, %xmm9
	vaddss	%xmm3, %xmm9, %xmm3
	vbroadcastss	.LCPI38_2(%rip), %xmm0  # xmm0 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm0, %xmm8, %xmm9
	vxorps	%xmm0, %xmm3, %xmm10
	vsubss	%xmm8, %xmm6, %xmm8
	vsubss	%xmm6, %xmm8, %xmm11
	vsubss	%xmm11, %xmm8, %xmm12
	vsubss	%xmm12, %xmm6, %xmm12
	vsubss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm9, %xmm12, %xmm9
	vsubss	%xmm3, %xmm5, %xmm3
	vsubss	%xmm5, %xmm3, %xmm11
	vsubss	%xmm11, %xmm3, %xmm12
	vsubss	%xmm12, %xmm5, %xmm12
	vsubss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm10, %xmm12, %xmm10
	vaddss	%xmm3, %xmm9, %xmm11
	vsubss	%xmm3, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm3, %xmm3
	vsubss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm3, %xmm9, %xmm3
	vmovss	224(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm0, %xmm1
	vaddss	%xmm1, %xmm10, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm11, %xmm3
	vsubss	%xmm3, %xmm11, %xmm9
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm3, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm3, %xmm8, %xmm3
	vmulss	%xmm4, %xmm9, %xmm8
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm8, %xmm9, %xmm10    # xmm10 = (xmm9 * xmm10) - xmm8
	vmulss	%xmm7, %xmm3, %xmm11
	vaddss	%xmm11, %xmm8, %xmm12
	vsubss	%xmm8, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm8, %xmm8
	vmovaps	%xmm7, %xmm14
	vfmsub213ss	%xmm11, %xmm3, %xmm14   # xmm14 = (xmm3 * xmm14) - xmm11
	vsubss	%xmm13, %xmm11, %xmm11
	vfmadd231ss	%xmm1, %xmm7, %xmm10    # xmm10 = (xmm7 * xmm1) + xmm10
	vmulss	%xmm7, %xmm9, %xmm1
	vfmsub213ss	%xmm1, %xmm9, %xmm7     # xmm7 = (xmm9 * xmm7) - xmm1
	vaddss	%xmm11, %xmm8, %xmm8
	vaddss	%xmm7, %xmm12, %xmm11
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm11, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm13, %xmm7, %xmm7
	vaddss	%xmm7, %xmm12, %xmm7
	vfmadd231ss	%xmm3, %xmm4, %xmm14    # xmm14 = (xmm4 * xmm3) + xmm14
	vaddss	%xmm14, %xmm10, %xmm3
	vfmadd231ss	%xmm9, %xmm4, %xmm8     # xmm8 = (xmm4 * xmm9) + xmm8
	vaddss	%xmm3, %xmm8, %xmm3
	vaddss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm11, %xmm9
	vsubss	%xmm9, %xmm11, %xmm7
	vaddss	%xmm3, %xmm7, %xmm4
	vaddss	%xmm1, %xmm9, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vaddss	%xmm1, %xmm9, %xmm9
	vmulss	%xmm9, %xmm8, %xmm1
	vmovaps	%xmm9, %xmm3
	vfmsub213ss	%xmm1, %xmm8, %xmm3     # xmm3 = (xmm8 * xmm3) - xmm1
	vmulss	%xmm8, %xmm9, %xmm10
	vaddss	%xmm1, %xmm10, %xmm11
	vsubss	%xmm1, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm1, %xmm1
	vmovaps	%xmm8, %xmm13
	vfmsub213ss	%xmm10, %xmm9, %xmm13   # xmm13 = (xmm9 * xmm13) - xmm10
	vsubss	%xmm12, %xmm10, %xmm10
	vmulss	%xmm8, %xmm8, %xmm12
	vaddss	%xmm1, %xmm10, %xmm1
	vmovaps	%xmm8, %xmm10
	vfmsub213ss	%xmm12, %xmm8, %xmm10   # xmm10 = (xmm8 * xmm10) - xmm12
	vaddss	%xmm10, %xmm11, %xmm14
	vsubss	%xmm11, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm0
	vsubss	%xmm0, %xmm11, %xmm0
	vsubss	%xmm15, %xmm10, %xmm10
	vaddss	%xmm0, %xmm10, %xmm0
	vmovss	%xmm4, 240(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm8, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm8) + xmm3
	vfmadd231ss	%xmm9, %xmm9, %xmm13    # xmm13 = (xmm9 * xmm9) + xmm13
	vaddss	%xmm3, %xmm13, %xmm3
	vfmadd231ss	%xmm4, %xmm8, %xmm1     # xmm1 = (xmm8 * xmm4) + xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm3
	vaddss	%xmm0, %xmm3, %xmm10
	vaddss	%xmm1, %xmm12, %xmm0
	vsubss	%xmm0, %xmm12, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmulss	%xmm1, %xmm2, %xmm3
	vmovss	192(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm4, %xmm11
	vmovaps	%xmm0, %xmm12
	vfmsub213ss	%xmm11, %xmm4, %xmm12   # xmm12 = (xmm4 * xmm12) - xmm11
	vfmadd231ss	%xmm1, %xmm4, %xmm12    # xmm12 = (xmm4 * xmm1) + xmm12
	vfmsub213ss	%xmm3, %xmm2, %xmm1     # xmm1 = (xmm2 * xmm1) - xmm3
	vaddss	%xmm3, %xmm11, %xmm13
	vsubss	%xmm3, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm3, %xmm3
	vsubss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm3, %xmm11, %xmm3
	vfmadd231ss	96(%rsp), %xmm0, %xmm1  # 4-byte Folded Reload
                                        # xmm1 = (xmm0 * mem) + xmm1
	vmulss	%xmm0, %xmm2, %xmm11
	vfmsub213ss	%xmm11, %xmm2, %xmm0    # xmm0 = (xmm2 * xmm0) - xmm11
	vaddss	%xmm0, %xmm13, %xmm14
	vsubss	%xmm13, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm7
	vsubss	%xmm7, %xmm13, %xmm7
	vsubss	%xmm15, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm1, %xmm12, %xmm1
	vbroadcastss	.LCPI38_2(%rip), %xmm12 # xmm12 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vfmadd231ss	%xmm10, %xmm2, %xmm3    # xmm3 = (xmm2 * xmm10) + xmm3
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm1, %xmm11, %xmm3
	vsubss	%xmm3, %xmm11, %xmm7
	vaddss	%xmm1, %xmm7, %xmm1
	vxorps	%xmm3, %xmm12, %xmm7
	vsubss	%xmm3, %xmm6, %xmm3
	vsubss	%xmm6, %xmm3, %xmm10
	vsubss	%xmm10, %xmm3, %xmm11
	vsubss	%xmm11, %xmm6, %xmm6
	vxorps	%xmm1, %xmm12, %xmm11
	vsubss	%xmm10, %xmm7, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vsubss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm7
	vsubss	%xmm7, %xmm1, %xmm10
	vsubss	%xmm10, %xmm5, %xmm5
	vsubss	%xmm7, %xmm11, %xmm7
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm6, %xmm1, %xmm7
	vsubss	%xmm1, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm1, %xmm1
	vmovss	224(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm5
	vaddss	%xmm0, %xmm5, %xmm6
	vaddss	%xmm1, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm0
	vaddss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm5, %xmm8, %xmm0
	vmulss	%xmm5, %xmm9, %xmm3
	vmulss	%xmm1, %xmm8, %xmm7
	vmovaps	%xmm8, %xmm10
	vfmsub213ss	%xmm7, %xmm1, %xmm10    # xmm10 = (xmm1 * xmm10) - xmm7
	vfmadd231ss	%xmm9, %xmm1, %xmm10    # xmm10 = (xmm1 * xmm9) + xmm10
	vfmsub213ss	%xmm3, %xmm5, %xmm9     # xmm9 = (xmm5 * xmm9) - xmm3
	vfmadd231ss	%xmm6, %xmm8, %xmm9     # xmm9 = (xmm8 * xmm6) + xmm9
	vfmsub213ss	%xmm0, %xmm5, %xmm8     # xmm8 = (xmm5 * xmm8) - xmm0
	vaddss	%xmm7, %xmm3, %xmm1
	vsubss	%xmm3, %xmm1, %xmm6
	vsubss	%xmm6, %xmm1, %xmm11
	vsubss	%xmm11, %xmm3, %xmm3
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm1, %xmm8, %xmm6
	vsubss	%xmm1, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm10, %xmm9, %xmm7
	vfmadd231ss	240(%rsp), %xmm5, %xmm3 # 4-byte Folded Reload
                                        # xmm3 = (xmm5 * mem) + xmm3
	vaddss	%xmm3, %xmm7, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm1, %xmm6, %xmm3
	vsubss	%xmm3, %xmm6, %xmm5
	vaddss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm3, %xmm0, %xmm5
	vsubss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vmulss	%xmm0, %xmm2, %xmm3
	vmulss	%xmm5, %xmm4, %xmm6
	vmovaps	%xmm5, %xmm7
	vfmsub213ss	%xmm6, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm7) - xmm6
	vfmadd231ss	%xmm0, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm0) + xmm7
	vfmsub213ss	%xmm3, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm0) - xmm3
	vaddss	%xmm6, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm8
	vsubss	%xmm8, %xmm4, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vxorps	%xmm9, %xmm9, %xmm9
	vsubss	%xmm8, %xmm6, %xmm6
	vfmadd231ss	96(%rsp), %xmm5, %xmm0  # 4-byte Folded Reload
                                        # xmm0 = (xmm5 * mem) + xmm0
	vmulss	%xmm5, %xmm2, %xmm8
	vfmsub213ss	%xmm8, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm5) - xmm8
	vaddss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm5, %xmm4, %xmm6
	vfmadd231ss	%xmm1, %xmm2, %xmm3     # xmm3 = (xmm2 * xmm1) + xmm3
	vsubss	%xmm4, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm2
	vsubss	%xmm2, %xmm4, %xmm2
	vsubss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm2
	vaddss	%xmm0, %xmm2, %xmm4
	vaddss	%xmm1, %xmm8, %xmm2
	vsubss	%xmm2, %xmm8, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vxorps	%xmm13, %xmm13, %xmm13
	vcvtsi2ss	%r13d, %xmm13, %xmm3
	vmulss	244(%rsp), %xmm3, %xmm3         # 4-byte Folded Reload
	vmulss	%xmm3, %xmm2, %xmm2
	vmulss	%xmm3, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm1[0],xmm2[2,3]
	vmulss	%xmm3, %xmm4, %xmm10
.LBB38_44:                              #   in Loop: Header=BB38_45 Depth=1
	leaq	(%r12,%r12,2), %rax
	vmovlps	%xmm0, (%rbp,%rax,4)
	vmovss	%xmm10, 8(%rbp,%rax,4)
	incq	%r12
	cmpq	$10, %r12
	je	.LBB38_56
.LBB38_45:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB38_47 Depth 2
	vcvtsi2ss	%r15d, %xmm13, %xmm0
	vblendps	$14, .LCPI38_3(%rip), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[1,2,3]
	movq	8(%rsp), %rax                   # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	jle	.LBB38_48
# %bb.46:                               #   in Loop: Header=BB38_45 Depth=1
	shlq	$2, %rax
	leaq	(%rax,%rax,2), %rax
	vxorps	%xmm10, %xmm10, %xmm10
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB38_47:                              #   Parent Loop BB38_45 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rcx), %xmm1              # xmm1 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%rcx), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm1, %xmm11
	vmulss	%xmm2, %xmm1, %xmm3
	vmulss	%xmm1, %xmm2, %xmm4
	vmovaps	%xmm1, %xmm5
	vfmsub213ss	%xmm4, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm5) - xmm4
	vfmadd231ss	%xmm2, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm2) + xmm5
	vfmsub213ss	%xmm3, %xmm1, %xmm2     # xmm2 = (xmm1 * xmm2) - xmm3
	vaddss	%xmm4, %xmm3, %xmm6
	vsubss	%xmm3, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vmovss	8(%rbx,%rcx), %xmm4             # xmm4 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm1, %xmm4, %xmm2     # xmm2 = (xmm4 * xmm1) + xmm2
	vfmadd231ss	%xmm4, %xmm1, %xmm3     # xmm3 = (xmm1 * xmm4) + xmm3
	vfmsub213ss	%xmm11, %xmm1, %xmm1    # xmm1 = (xmm1 * xmm1) - xmm11
	vaddss	%xmm1, %xmm6, %xmm4
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm6, %xmm6
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm6, %xmm1
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm11, %xmm3
	vsubss	%xmm3, %xmm11, %xmm4
	vaddss	%xmm2, %xmm4, %xmm8
	vaddss	%xmm3, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm5, %xmm0, %xmm5
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm5, %xmm3
	vmovshdup	%xmm0, %xmm4            # xmm4 = xmm0[1,1,3,3]
	vaddss	%xmm4, %xmm8, %xmm5
	vsubss	%xmm4, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm4, %xmm4
	vsubss	%xmm6, %xmm8, %xmm0
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm3, %xmm5, %xmm4
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm5, %xmm5
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm1, %xmm10, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm3
	vaddss	%xmm0, %xmm3, %xmm10
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	addq	$12, %rcx
	cmpq	%rcx, %rax
	jne	.LBB38_47
	jmp	.LBB38_49
	.p2align	4, 0x90
.LBB38_48:                              #   in Loop: Header=BB38_45 Depth=1
	vxorps	%xmm10, %xmm10, %xmm10
.LBB38_49:                              #   in Loop: Header=BB38_45 Depth=1
	vucomiss	%xmm9, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB38_44
# %bb.50:                               #   in Loop: Header=BB38_45 Depth=1
	vmovups	%xmm0, 224(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovss	%xmm10, 96(%rsp)                # 4-byte Spill
	callq	sqrtf
	vmovss	96(%rsp), %xmm8                 # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm0
	setnp	%al
	sete	%cl
	vmovd	.LCPI38_0(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vmovdqa	%xmm2, %xmm7
	testb	%al, %cl
	jne	.LBB38_53
# %bb.51:                               #   in Loop: Header=BB38_45 Depth=1
	vmovd	%xmm0, %eax
	andl	$2139095040, %eax               # imm = 0x7F800000
	vmovdqa	%xmm2, %xmm7
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB38_53
# %bb.52:                               #   in Loop: Header=BB38_45 Depth=1
	vmovd	%eax, %xmm7
.LBB38_53:                              #   in Loop: Header=BB38_45 Depth=1
	vucomiss	%xmm1, %xmm0
	setnp	%al
	sete	%cl
	vmovdqa	%xmm2, %xmm3
	testb	%al, %cl
	vmovups	224(%rsp), %xmm4                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	jne	.LBB38_43
# %bb.54:                               #   in Loop: Header=BB38_45 Depth=1
	vmovd	%xmm0, %eax
	andl	$2139095040, %eax               # imm = 0x7F800000
	vmovdqa	%xmm2, %xmm3
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB38_43
# %bb.55:                               #   in Loop: Header=BB38_45 Depth=1
	movl	$2130706432, %ecx               # imm = 0x7F000000
	subl	%eax, %ecx
	vmovd	%ecx, %xmm3
	jmp	.LBB38_43
.LBB38_56:
	callq	omp_get_wtime
	vsubsd	288(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm13, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	160(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp4396:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp4397:
# %bb.57:
	movq	%rax, %r13
	movq	144(%rsp), %rax                 # 8-byte Reload
	leaq	32(%rax), %r14
.Ltmp4398:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp4399:
# %bb.58:
.Ltmp4400:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp4401:
# %bb.59:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp4402:
	leaq	112(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp4403:
# %bb.60:
.Ltmp4404:
	leaq	112(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4405:
# %bb.61:
.Ltmp4407:
	callq	mpfr_get_default_rounding_mode
.Ltmp4408:
# %bb.62:
.Ltmp4409:
	leaq	112(%rsp), %rdi
	leaq	160(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp4410:
# %bb.63:
.Ltmp4412:
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4413:
# %bb.64:
.Ltmp4414:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp4415:
# %bb.65:
.Ltmp4416:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp4417:
# %bb.66:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp4418:
	leaq	16(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp4419:
# %bb.67:
.Ltmp4420:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4421:
# %bb.68:
.Ltmp4423:
	callq	mpfr_get_default_rounding_mode
.Ltmp4424:
# %bb.69:
.Ltmp4425:
	leaq	16(%rsp), %rdi
	leaq	112(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp4426:
# %bb.70:
.Ltmp4428:
	callq	mpfr_get_default_rounding_mode
.Ltmp4429:
# %bb.71:
.Ltmp4430:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4431:
# %bb.72:
.Ltmp4432:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4433:
# %bb.73:
.Ltmp4434:
	movl	%eax, %r12d
	leaq	48(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4435:
# %bb.74:
.Ltmp4436:
	leaq	48(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4437:
# %bb.75:
.Ltmp4439:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp4440:
# %bb.76:
.Ltmp4442:
	leaq	48(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp4443:
# %bb.77:
	vmovlpd	%xmm0, 208(%rsp)
	vmovss	%xmm1, 216(%rsp)
	cmpq	$0, 72(%rsp)
	je	.LBB38_79
# %bb.78:
.Ltmp4445:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4446:
.LBB38_79:
	cmpq	$0, 40(%rsp)
	je	.LBB38_81
# %bb.80:
.Ltmp4448:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4449:
.LBB38_81:
	cmpq	$0, 136(%rsp)
	je	.LBB38_83
# %bb.82:
.Ltmp4451:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4452:
.LBB38_83:
	cmpq	$0, 184(%rsp)
	je	.LBB38_85
# %bb.84:
.Ltmp4454:
	leaq	160(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4455:
.LBB38_85:
	leaq	408(%rsp), %r15
	movq	%r15, 392(%rsp)
	movl	$846033518, 408(%rsp)           # imm = 0x326D726E
	movw	$32, 412(%rsp)
	movq	$5, 400(%rsp)
.Ltmp4457:
	leaq	392(%rsp), %rdi
	leaq	208(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp4458:
# %bb.86:
	movq	392(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB38_88
# %bb.87:
	callq	_ZdlPv
.LBB38_88:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$120, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vmovupd	%ymm0, 64(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovlpd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 96(%rax)
	vzeroupper
	callq	omp_get_wtime
	vxorps	%xmm11, %xmm11, %xmm11
	vmovsd	%xmm0, 96(%rsp)                 # 8-byte Spill
	movq	8(%rsp), %rax                   # 8-byte Reload
	movl	(%rax), %eax
	movq	%rax, %rcx
	shlq	$2, %rcx
	leaq	(%rcx,%rcx,2), %rcx
	xorl	%edx, %edx
	vxorpd	%xmm0, %xmm0, %xmm0
	vbroadcastss	.LCPI38_2(%rip), %xmm1  # xmm1 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	xorl	%esi, %esi
	jmp	.LBB38_90
	.p2align	4, 0x90
.LBB38_89:                              #   in Loop: Header=BB38_90 Depth=1
	leaq	(%rsi,%rsi,2), %rdi
	vmovlps	%xmm3, (%rbp,%rdi,4)
	vmovss	%xmm2, 8(%rbp,%rdi,4)
	incq	%rsi
	cmpq	$10, %rsi
	je	.LBB38_96
.LBB38_90:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB38_94 Depth 2
	vcvtsi2ss	%edx, %xmm12, %xmm2
	vblendps	$1, %xmm2, %xmm11, %xmm3        # xmm3 = xmm2[0],xmm11[1,2,3]
	vxorps	%xmm2, %xmm2, %xmm2
	testl	%eax, %eax
	jle	.LBB38_89
# %bb.91:                               #   in Loop: Header=BB38_90 Depth=1
	xorl	%edi, %edi
	jmp	.LBB38_94
	.p2align	4, 0x90
.LBB38_92:                              #   in Loop: Header=BB38_94 Depth=2
	vmovsd	(%rbx,%rdi), %xmm5              # xmm5 = mem[0],zero
.LBB38_93:                              #   in Loop: Header=BB38_94 Depth=2
	vaddss	%xmm5, %xmm3, %xmm6
	vsubss	%xmm3, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm3, %xmm8
	vsubss	%xmm7, %xmm5, %xmm7
	vaddss	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm3, %xmm3            # xmm3 = xmm3[1,1,3,3]
	vmovshdup	%xmm5, %xmm5            # xmm5 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm3, %xmm8
	vsubss	%xmm3, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm3, %xmm3
	vsubss	%xmm9, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm7, %xmm8, %xmm5
	vsubss	%xmm8, %xmm5, %xmm9
	vsubss	%xmm9, %xmm5, %xmm10
	vsubss	%xmm10, %xmm8, %xmm8
	vsubss	%xmm9, %xmm7, %xmm7
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm3
	vsubss	%xmm3, %xmm5, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vaddss	%xmm3, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm5
	vaddss	%xmm3, %xmm5, %xmm3
	vinsertps	$16, %xmm3, %xmm4, %xmm3 # xmm3 = xmm4[0],xmm3[0],xmm4[2,3]
	addq	$12, %rdi
	cmpq	%rdi, %rcx
	je	.LBB38_89
.LBB38_94:                              #   Parent Loop BB38_90 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rdi), %xmm5              # xmm5 = mem[0],zero,zero,zero
	vmovss	8(%rbx,%rdi), %xmm4             # xmm4 = mem[0],zero,zero,zero
	vcomiss	%xmm5, %xmm0
	jbe	.LBB38_92
# %bb.95:                               #   in Loop: Header=BB38_94 Depth=2
	vinsertps	$16, 4(%rbx,%rdi), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vxorps	%xmm1, %xmm5, %xmm5
	vxorps	%xmm1, %xmm4, %xmm4
	jmp	.LBB38_93
.LBB38_96:
	callq	omp_get_wtime
	vsubsd	96(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm12, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	160(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp4460:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp4461:
# %bb.97:
	movq	%rax, %r13
	movq	144(%rsp), %rax                 # 8-byte Reload
	leaq	64(%rax), %r14
.Ltmp4462:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp4463:
# %bb.98:
.Ltmp4464:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp4465:
# %bb.99:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp4466:
	leaq	112(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp4467:
# %bb.100:
.Ltmp4468:
	leaq	112(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4469:
# %bb.101:
.Ltmp4471:
	callq	mpfr_get_default_rounding_mode
.Ltmp4472:
# %bb.102:
.Ltmp4473:
	leaq	112(%rsp), %rdi
	leaq	160(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp4474:
# %bb.103:
.Ltmp4476:
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4477:
# %bb.104:
.Ltmp4478:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp4479:
# %bb.105:
.Ltmp4480:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp4481:
# %bb.106:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp4482:
	leaq	16(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp4483:
# %bb.107:
.Ltmp4484:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4485:
# %bb.108:
.Ltmp4487:
	callq	mpfr_get_default_rounding_mode
.Ltmp4488:
# %bb.109:
.Ltmp4489:
	leaq	16(%rsp), %rdi
	leaq	112(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp4490:
# %bb.110:
.Ltmp4492:
	callq	mpfr_get_default_rounding_mode
.Ltmp4493:
# %bb.111:
.Ltmp4494:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4495:
# %bb.112:
.Ltmp4496:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4497:
# %bb.113:
.Ltmp4498:
	movl	%eax, %r12d
	leaq	48(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4499:
# %bb.114:
.Ltmp4500:
	leaq	48(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4501:
# %bb.115:
.Ltmp4503:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp4504:
# %bb.116:
.Ltmp4506:
	leaq	48(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp4507:
# %bb.117:
	vmovlpd	%xmm0, 208(%rsp)
	vmovss	%xmm1, 216(%rsp)
	cmpq	$0, 72(%rsp)
	je	.LBB38_119
# %bb.118:
.Ltmp4509:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4510:
.LBB38_119:
	cmpq	$0, 40(%rsp)
	je	.LBB38_121
# %bb.120:
.Ltmp4512:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4513:
.LBB38_121:
	cmpq	$0, 136(%rsp)
	je	.LBB38_123
# %bb.122:
.Ltmp4515:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4516:
.LBB38_123:
	cmpq	$0, 184(%rsp)
	je	.LBB38_125
# %bb.124:
.Ltmp4518:
	leaq	160(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4519:
.LBB38_125:
	leaq	376(%rsp), %r15
	movq	%r15, 360(%rsp)
	movl	$1836413793, 376(%rsp)          # imm = 0x6D757361
	movw	$32, 380(%rsp)
	movq	$5, 368(%rsp)
.Ltmp4521:
	leaq	360(%rsp), %rdi
	leaq	208(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp4522:
# %bb.126:
	movq	360(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB38_128
# %bb.127:
	callq	_ZdlPv
.LBB38_128:
	movq	%rbp, %rdi
	callq	_ZdaPv
	leaq	80(%rsp), %rsi
	movq	8(%rsp), %r13                   # 8-byte Reload
	movq	%r13, %rdi
	movq	%rbx, %rdx
	movq	152(%rsp), %rcx                 # 8-byte Reload
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	48(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	cmpl	$0, (%r13)
	jle	.LBB38_169
# %bb.129:
	movq	144(%rsp), %rax                 # 8-byte Reload
	leaq	256(%rax), %r15
	xorl	%r14d, %r14d
	movq	152(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB38_131
	.p2align	4, 0x90
.LBB38_130:                             #   in Loop: Header=BB38_131 Depth=1
	incq	%r14
	movq	8(%rsp), %rax                   # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r15
	movq	96(%rsp), %rsi                  # 8-byte Reload
	addq	$12, %rsi
	cmpq	%rax, %r14
	jge	.LBB38_169
.LBB38_131:                             # =>This Inner Loop Header: Depth=1
.Ltmp4524:
	leaq	112(%rsp), %rdi
	movq	%rsi, 96(%rsp)                  # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp4525:
# %bb.132:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4527:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp4528:
# %bb.133:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4529:
	movq	%rax, %rbp
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4530:
# %bb.134:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4531:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4532:
# %bb.135:                              #   in Loop: Header=BB38_131 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp4533:
	leaq	16(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4534:
# %bb.136:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4535:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4536:
# %bb.137:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4538:
	callq	mpfr_get_default_rounding_mode
.Ltmp4539:
# %bb.138:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4540:
	leaq	16(%rsp), %rdi
	movq	%r15, %rsi
	leaq	112(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp4541:
# %bb.139:                              #   in Loop: Header=BB38_131 Depth=1
	cmpq	$0, 136(%rsp)
	je	.LBB38_141
# %bb.140:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4543:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4544:
.LBB38_141:                             #   in Loop: Header=BB38_131 Depth=1
.Ltmp4546:
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4547:
# %bb.142:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4548:
	movq	%rax, %rbp
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4549:
# %bb.143:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4550:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4551:
# %bb.144:                              #   in Loop: Header=BB38_131 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp4552:
	leaq	160(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4553:
# %bb.145:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4554:
	leaq	160(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4555:
# %bb.146:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4557:
	callq	mpfr_get_default_rounding_mode
.Ltmp4558:
# %bb.147:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4559:
	leaq	160(%rsp), %rdi
	leaq	16(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp4560:
# %bb.148:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4562:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4563:
# %bb.149:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4564:
	movq	%rax, %rbp
	leaq	160(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4565:
# %bb.150:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4566:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4567:
# %bb.151:                              #   in Loop: Header=BB38_131 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp4568:
	leaq	112(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4569:
# %bb.152:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4570:
	leaq	112(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4571:
# %bb.153:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4573:
	callq	mpfr_get_default_rounding_mode
.Ltmp4574:
	leaq	48(%rsp), %r12
# %bb.154:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4575:
	leaq	112(%rsp), %rdi
	movq	%r12, %rsi
	leaq	160(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp4576:
# %bb.155:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4578:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp4579:
# %bb.156:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4580:
	movq	%rax, %r12
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4581:
# %bb.157:                              #   in Loop: Header=BB38_131 Depth=1
	movq	%rax, %rbp
	cmpq	%rax, %r12
	je	.LBB38_161
# %bb.158:                              #   in Loop: Header=BB38_131 Depth=1
	cmpq	$0, 72(%rsp)
	je	.LBB38_160
# %bb.159:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4582:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4583:
.LBB38_160:                             #   in Loop: Header=BB38_131 Depth=1
.Ltmp4584:
	leaq	48(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp4585:
.LBB38_161:                             #   in Loop: Header=BB38_131 Depth=1
.Ltmp4586:
	callq	mpfr_get_default_rounding_mode
.Ltmp4587:
# %bb.162:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4588:
	leaq	48(%rsp), %rdi
	leaq	112(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp4589:
# %bb.163:                              #   in Loop: Header=BB38_131 Depth=1
	cmpq	$0, 136(%rsp)
	je	.LBB38_165
# %bb.164:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4591:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4592:
.LBB38_165:                             #   in Loop: Header=BB38_131 Depth=1
	cmpq	$0, 184(%rsp)
	je	.LBB38_167
# %bb.166:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4594:
	leaq	160(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4595:
.LBB38_167:                             #   in Loop: Header=BB38_131 Depth=1
	cmpq	$0, 40(%rsp)
	je	.LBB38_130
# %bb.168:                              #   in Loop: Header=BB38_131 Depth=1
.Ltmp4597:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4598:
	jmp	.LBB38_130
.LBB38_169:
.Ltmp4600:
	callq	mpfr_get_default_rounding_mode
.Ltmp4601:
# %bb.170:
.Ltmp4602:
	movl	%eax, %ebp
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4603:
# %bb.171:
.Ltmp4604:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp4605:
# %bb.172:
.Ltmp4606:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp4607:
# %bb.173:
.Ltmp4608:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp4609:
# %bb.174:
.Ltmp4611:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp4612:
# %bb.175:
.Ltmp4614:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4615:
# %bb.176:
.Ltmp4616:
	movq	%rax, %r12
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4617:
# %bb.177:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB38_181
# %bb.178:
	cmpq	$0, 72(%rsp)
	je	.LBB38_180
# %bb.179:
.Ltmp4618:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4619:
.LBB38_180:
.Ltmp4620:
	leaq	48(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp4621:
.LBB38_181:
.Ltmp4622:
	callq	mpfr_get_default_rounding_mode
.Ltmp4623:
# %bb.182:
.Ltmp4624:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp4625:
# %bb.183:
	cmpq	$0, 40(%rsp)
	je	.LBB38_185
# %bb.184:
.Ltmp4627:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4628:
.LBB38_185:
	callq	omp_get_wtime
	vmovsd	%xmm0, 96(%rsp)                 # 8-byte Spill
.Ltmp4630:
	leaq	80(%rsp), %rsi
	movq	8(%rsp), %r15                   # 8-byte Reload
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	152(%rsp), %r14                 # 8-byte Reload
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp4631:
# %bb.186:
.Ltmp4632:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp4633:
# %bb.187:
.Ltmp4634:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp4635:
# %bb.188:
.Ltmp4636:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp4637:
# %bb.189:
.Ltmp4638:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp4639:
# %bb.190:
.Ltmp4640:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp4641:
# %bb.191:
.Ltmp4642:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp4643:
# %bb.192:
.Ltmp4644:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp4645:
# %bb.193:
.Ltmp4646:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp4647:
# %bb.194:
.Ltmp4648:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp4649:
# %bb.195:
	callq	omp_get_wtime
	vsubsd	96(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp4651:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4652:
# %bb.196:
	movq	%rax, %r12
	movq	144(%rsp), %rax                 # 8-byte Reload
	leaq	96(%rax), %r15
.Ltmp4653:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp4654:
# %bb.197:
.Ltmp4655:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4656:
# %bb.198:
	movl	%eax, %ebp
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp4657:
	leaq	16(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4658:
# %bb.199:
.Ltmp4659:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp4660:
# %bb.200:
.Ltmp4662:
	callq	mpfr_get_default_rounding_mode
.Ltmp4663:
# %bb.201:
.Ltmp4664:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movq	%r15, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp4665:
# %bb.202:
.Ltmp4667:
	leaq	16(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp4668:
# %bb.203:
	vmovlpd	%xmm0, 112(%rsp)
	vmovss	%xmm1, 120(%rsp)
	cmpq	$0, 40(%rsp)
	je	.LBB38_205
# %bb.204:
.Ltmp4670:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4671:
.LBB38_205:
	leaq	344(%rsp), %r15
	movq	%r15, 328(%rsp)
	movl	$2037413985, 344(%rsp)          # imm = 0x79707861
	movw	$32, 348(%rsp)
	movq	$5, 336(%rsp)
.Ltmp4673:
	leaq	328(%rsp), %rdi
	leaq	112(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp4674:
# %bb.206:
	movq	328(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB38_208
# %bb.207:
	callq	_ZdlPv
.LBB38_208:
	cmpq	$0, 72(%rsp)
	je	.LBB38_210
# %bb.209:
.Ltmp4676:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4677:
.LBB38_210:
	movslq	4(%rsp), %r15
	movl	$12, %ecx
	movq	%r15, %rax
	mulq	%rcx
	movq	$-1, %rdi
	cmovnoq	%rax, %rdi
	callq	_Znam
	movq	%rax, %rbp
	testq	%r15, %r15
	movq	280(%rsp), %r13                 # 8-byte Reload
	movq	152(%rsp), %r14                 # 8-byte Reload
	je	.LBB38_214
# %bb.211:
	leaq	(%r15,%r15,2), %rax
	leaq	-12(,%rax,4), %rax
	movabsq	$-6148914691236517205, %rcx     # imm = 0xAAAAAAAAAAAAAAAB
	mulq	%rcx
	shrq	$3, %rdx
	leaq	(%rdx,%rdx,2), %rax
	leaq	12(,%rax,4), %rdx
	movq	%rbp, %rdi
	xorl	%esi, %esi
	callq	_intel_fast_memset@PLT
	testl	%r15d, %r15d
	jle	.LBB38_214
# %bb.212:
	xorl	%r15d, %r15d
	xorl	%r12d, %r12d
	.p2align	4, 0x90
.LBB38_213:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, (%r14,%r15)
	vmovss	%xmm1, 8(%r14,%r15)
	movl	8(%r14,%r15), %eax
	movl	%eax, 8(%rbp,%r15)
	movq	(%r14,%r15), %rax
	movq	%rax, (%rbp,%r15)
	incq	%r12
	movslq	4(%rsp), %rax
	addq	$12, %r15
	addq	$32, %r13
	cmpq	%rax, %r12
	jl	.LBB38_213
.LBB38_214:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%rbp, 192(%rsp)                 # 8-byte Spill
	movq	%rbp, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	48(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	cmpl	$0, 4(%rsp)
	jle	.LBB38_255
# %bb.215:
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	leaq	112(%rsp), %rbp
	movq	192(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB38_217
	.p2align	4, 0x90
.LBB38_216:                             #   in Loop: Header=BB38_217 Depth=1
	movq	96(%rsp), %rdx                  # 8-byte Reload
	incq	%rdx
	movslq	4(%rsp), %rax
	movq	224(%rsp), %rsi                 # 8-byte Reload
	addq	$12, %rsi
	movq	%rdx, %rcx
	movq	%rdx, 96(%rsp)                  # 8-byte Spill
	cmpq	%rax, %rdx
	jge	.LBB38_255
.LBB38_217:                             # =>This Inner Loop Header: Depth=1
	movq	8(%rsp), %rax                   # 8-byte Reload
	movl	(%rax), %r15d
.Ltmp4679:
	movq	%rbp, %r14
	movq	%rbp, %rdi
	movq	%rsi, 224(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp4680:
# %bb.218:                              #   in Loop: Header=BB38_217 Depth=1
	movq	96(%rsp), %rax                  # 8-byte Reload
	addl	%r15d, %eax
	addl	$8, %eax
	movslq	%eax, %r12
	shlq	$5, %r12
	addq	144(%rsp), %r12                 # 8-byte Folded Reload
.Ltmp4682:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp4683:
# %bb.219:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4684:
	movq	%rax, %r15
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp4685:
# %bb.220:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4686:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp4687:
# %bb.221:                              #   in Loop: Header=BB38_217 Depth=1
	movl	%eax, %r13d
	cmpq	%rbp, %r15
	cmovgq	%r15, %rbp
.Ltmp4688:
	leaq	16(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp4689:
# %bb.222:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4690:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r13d, %edx
	callq	mpfr_set_si
.Ltmp4691:
# %bb.223:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4693:
	callq	mpfr_get_default_rounding_mode
.Ltmp4694:
# %bb.224:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4695:
	movq	%r14, %rbp
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp4696:
# %bb.225:                              #   in Loop: Header=BB38_217 Depth=1
	cmpq	$0, 136(%rsp)
	je	.LBB38_227
# %bb.226:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4698:
	movq	%rbp, %rdi
	callq	mpfr_clear
.Ltmp4699:
.LBB38_227:                             #   in Loop: Header=BB38_217 Depth=1
.Ltmp4701:
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4702:
# %bb.228:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4703:
	movq	%rax, %r15
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4704:
# %bb.229:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4705:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4706:
# %bb.230:                              #   in Loop: Header=BB38_217 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %r15
	cmovgq	%r15, %r13
.Ltmp4707:
	leaq	160(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4708:
# %bb.231:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4709:
	leaq	160(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4710:
# %bb.232:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4712:
	callq	mpfr_get_default_rounding_mode
.Ltmp4713:
# %bb.233:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4714:
	leaq	160(%rsp), %rdi
	leaq	16(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp4715:
# %bb.234:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4717:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4718:
# %bb.235:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4719:
	movq	%rax, %r15
	leaq	160(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4720:
# %bb.236:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4721:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4722:
# %bb.237:                              #   in Loop: Header=BB38_217 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %r15
	cmovgq	%r15, %r13
.Ltmp4723:
	movq	%rbp, %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4724:
# %bb.238:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4725:
	movq	%rbp, %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4726:
# %bb.239:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4728:
	callq	mpfr_get_default_rounding_mode
.Ltmp4729:
	leaq	48(%rsp), %r14
# %bb.240:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4730:
	movq	%rbp, %rdi
	movq	%r14, %rsi
	leaq	160(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp4731:
# %bb.241:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4733:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp4734:
# %bb.242:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4735:
	movq	%rax, %r15
	movq	%rbp, %rdi
	callq	mpfr_get_prec
.Ltmp4736:
# %bb.243:                              #   in Loop: Header=BB38_217 Depth=1
	movq	%rax, %r12
	cmpq	%rax, %r15
	je	.LBB38_247
# %bb.244:                              #   in Loop: Header=BB38_217 Depth=1
	cmpq	$0, 72(%rsp)
	je	.LBB38_246
# %bb.245:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4737:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4738:
.LBB38_246:                             #   in Loop: Header=BB38_217 Depth=1
.Ltmp4739:
	leaq	48(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp4740:
.LBB38_247:                             #   in Loop: Header=BB38_217 Depth=1
.Ltmp4741:
	callq	mpfr_get_default_rounding_mode
.Ltmp4742:
# %bb.248:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4743:
	leaq	48(%rsp), %rdi
	movq	%rbp, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp4744:
# %bb.249:                              #   in Loop: Header=BB38_217 Depth=1
	cmpq	$0, 136(%rsp)
	je	.LBB38_251
# %bb.250:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4746:
	movq	%rbp, %rdi
	callq	mpfr_clear
.Ltmp4747:
.LBB38_251:                             #   in Loop: Header=BB38_217 Depth=1
	cmpq	$0, 184(%rsp)
	je	.LBB38_253
# %bb.252:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4749:
	leaq	160(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4750:
.LBB38_253:                             #   in Loop: Header=BB38_217 Depth=1
	cmpq	$0, 40(%rsp)
	je	.LBB38_216
# %bb.254:                              #   in Loop: Header=BB38_217 Depth=1
.Ltmp4752:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4753:
	jmp	.LBB38_216
.LBB38_255:
.Ltmp4755:
	callq	mpfr_get_default_rounding_mode
.Ltmp4756:
# %bb.256:
.Ltmp4757:
	movl	%eax, %ebp
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4758:
# %bb.257:
.Ltmp4759:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp4760:
# %bb.258:
.Ltmp4761:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp4762:
# %bb.259:
.Ltmp4763:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp4764:
# %bb.260:
.Ltmp4766:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp4767:
# %bb.261:
.Ltmp4769:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4770:
# %bb.262:
.Ltmp4771:
	movq	%rax, %r12
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4772:
# %bb.263:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB38_267
# %bb.264:
	cmpq	$0, 72(%rsp)
	je	.LBB38_266
# %bb.265:
.Ltmp4773:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4774:
.LBB38_266:
.Ltmp4775:
	leaq	48(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp4776:
.LBB38_267:
.Ltmp4777:
	callq	mpfr_get_default_rounding_mode
.Ltmp4778:
# %bb.268:
.Ltmp4779:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp4780:
# %bb.269:
	cmpq	$0, 40(%rsp)
	je	.LBB38_271
# %bb.270:
.Ltmp4782:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4783:
.LBB38_271:
	callq	omp_get_wtime
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
.Ltmp4785:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	152(%rsp), %r14                 # 8-byte Reload
	movq	%r14, %r8
	movq	192(%rsp), %r15                 # 8-byte Reload
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4786:
# %bb.272:
.Ltmp4787:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4788:
# %bb.273:
.Ltmp4789:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4790:
# %bb.274:
.Ltmp4791:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4792:
# %bb.275:
.Ltmp4793:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4794:
# %bb.276:
.Ltmp4795:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4796:
# %bb.277:
.Ltmp4797:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4798:
# %bb.278:
.Ltmp4799:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4800:
# %bb.279:
.Ltmp4801:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4802:
# %bb.280:
.Ltmp4803:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp4804:
# %bb.281:
	callq	omp_get_wtime
	vsubsd	8(%rsp), %xmm0, %xmm0           # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp4806:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4807:
# %bb.282:
	movq	%rax, %r15
	movq	144(%rsp), %rdi                 # 8-byte Reload
	subq	$-128, %rdi
.Ltmp4808:
	movq	%rdi, %r13
	callq	mpfr_get_prec
.Ltmp4809:
# %bb.283:
.Ltmp4810:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp4811:
# %bb.284:
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp4812:
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp4813:
# %bb.285:
.Ltmp4814:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp4815:
# %bb.286:
.Ltmp4817:
	callq	mpfr_get_default_rounding_mode
.Ltmp4818:
# %bb.287:
.Ltmp4819:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movq	%r13, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp4820:
# %bb.288:
.Ltmp4822:
	leaq	16(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp4823:
# %bb.289:
	vmovlpd	%xmm0, 112(%rsp)
	vmovss	%xmm1, 120(%rsp)
	cmpq	$0, 40(%rsp)
	je	.LBB38_291
# %bb.290:
.Ltmp4825:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4826:
.LBB38_291:
	leaq	312(%rsp), %r15
	movq	%r15, 296(%rsp)
	movl	$1986880871, 312(%rsp)          # imm = 0x766D6567
	movw	$32, 316(%rsp)
	movq	$5, 304(%rsp)
.Ltmp4828:
	leaq	296(%rsp), %rdi
	leaq	112(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp4829:
# %bb.292:
	movq	296(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB38_294
# %bb.293:
	callq	_ZdlPv
.LBB38_294:
	movq	192(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	cmpq	$0, 72(%rsp)
	je	.LBB38_296
# %bb.295:
.Ltmp4831:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4832:
.LBB38_296:
	movq	%rbx, %rdi
	callq	_ZdaPv
	movq	152(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	xorl	%eax, %eax
	vcvtsi2ss	%eax, %xmm2, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vblendps	$1, %xmm0, %xmm1, %xmm2         # xmm2 = xmm0[0],xmm1[1,2,3]
	vxorps	%xmm1, %xmm1, %xmm1
	movl	$2097153, %eax                  # imm = 0x200001
	movl	$1, %ecx
	vxorps	%xmm9, %xmm9, %xmm9
	.p2align	4, 0x90
.LBB38_297:                             # =>This Inner Loop Header: Depth=1
	vmovss	%xmm9, 8(%rsp)                  # 4-byte Spill
	decl	%eax
	vxorps	%xmm15, %xmm15, %xmm15
	vcvtsi2ss	%eax, %xmm15, %xmm3
	vmulss	%xmm1, %xmm3, %xmm4
	vmulss	%xmm3, %xmm3, %xmm5
	vxorps	%xmm6, %xmm6, %xmm6
	vmulss	%xmm3, %xmm1, %xmm7
	vaddss	%xmm7, %xmm4, %xmm8
	vmovaps	%xmm3, %xmm9
	vsubss	%xmm4, %xmm8, %xmm10
	vsubss	%xmm10, %xmm8, %xmm11
	vsubss	%xmm11, %xmm4, %xmm11
	vfmsub213ss	%xmm4, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm6) - xmm4
	vsubss	%xmm10, %xmm7, %xmm4
	vaddss	%xmm4, %xmm11, %xmm4
	vfmadd231ss	%xmm3, %xmm1, %xmm6     # xmm6 = (xmm1 * xmm3) + xmm6
	vfmadd231ss	%xmm3, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm3) + xmm4
	vfmsub213ss	%xmm5, %xmm3, %xmm3     # xmm3 = (xmm3 * xmm3) - xmm5
	vfmsub213ss	%xmm7, %xmm1, %xmm9     # xmm9 = (xmm1 * xmm9) - xmm7
	vaddss	%xmm3, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm10, %xmm3, %xmm3
	vsubss	%xmm11, %xmm8, %xmm8
	vfmadd231ss	%xmm1, %xmm1, %xmm9     # xmm9 = (xmm1 * xmm1) + xmm9
	vaddss	%xmm6, %xmm9, %xmm6
	vaddss	%xmm4, %xmm6, %xmm4
	vaddss	%xmm3, %xmm8, %xmm3
	vaddss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm3
	vaddss	%xmm3, %xmm5, %xmm6
	vsubss	%xmm3, %xmm7, %xmm7
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm3, %xmm5, %xmm5
	vmulss	%xmm5, %xmm6, %xmm8
	vmulss	%xmm6, %xmm6, %xmm3
	vmulss	%xmm6, %xmm5, %xmm9
	vmovaps	%xmm6, %xmm10
	vfmsub213ss	%xmm9, %xmm5, %xmm10    # xmm10 = (xmm5 * xmm10) - xmm9
	vfmadd231ss	%xmm5, %xmm5, %xmm10    # xmm10 = (xmm5 * xmm5) + xmm10
	vaddss	%xmm9, %xmm8, %xmm11
	vsubss	%xmm8, %xmm11, %xmm12
	vfmsub213ss	%xmm8, %xmm6, %xmm5     # xmm5 = (xmm6 * xmm5) - xmm8
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm8, %xmm8
	vsubss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm4, %xmm7, %xmm4
	vaddss	%xmm9, %xmm8, %xmm7
	vfmadd231ss	%xmm6, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm6) + xmm5
	vfmadd231ss	%xmm4, %xmm6, %xmm7     # xmm7 = (xmm6 * xmm4) + xmm7
	vfmsub213ss	%xmm3, %xmm6, %xmm6     # xmm6 = (xmm6 * xmm6) - xmm3
	vaddss	%xmm6, %xmm11, %xmm8
	vsubss	%xmm11, %xmm8, %xmm4
	vsubss	%xmm4, %xmm8, %xmm9
	vsubss	%xmm9, %xmm11, %xmm9
	vsubss	%xmm4, %xmm6, %xmm4
	vaddss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm4, %xmm9, %xmm4
	vaddss	%xmm7, %xmm5, %xmm5
	vcvtsi2ss	%ecx, %xmm14, %xmm6
	vaddss	%xmm4, %xmm5, %xmm5
	vaddss	%xmm5, %xmm8, %xmm9
	vaddss	%xmm3, %xmm9, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vdivss	%xmm7, %xmm6, %xmm10
	vmovaps	%xmm10, %xmm4
	vfnmadd213ss	%xmm6, %xmm7, %xmm4     # xmm4 = -(xmm7 * xmm4) + xmm6
	vaddss	%xmm3, %xmm9, %xmm11
	vaddss	%xmm1, %xmm4, %xmm3
	vfnmadd231ss	%xmm11, %xmm10, %xmm3   # xmm3 = -(xmm10 * xmm11) + xmm3
	vaddss	%xmm7, %xmm11, %xmm4
	vsubss	%xmm9, %xmm8, %xmm8
	vdivss	%xmm4, %xmm3, %xmm9
	vaddss	%xmm9, %xmm10, %xmm3
	vsubss	%xmm3, %xmm10, %xmm10
	vaddss	%xmm5, %xmm8, %xmm12
	vaddss	%xmm9, %xmm10, %xmm5
	vmulss	%xmm7, %xmm3, %xmm8
	vmovaps	%xmm7, %xmm9
	vfmsub213ss	%xmm8, %xmm3, %xmm9     # xmm9 = (xmm3 * xmm9) - xmm8
	vmulss	%xmm3, %xmm11, %xmm10
	vmulss	%xmm7, %xmm5, %xmm13
	vfmsub213ss	%xmm13, %xmm5, %xmm7    # xmm7 = (xmm5 * xmm7) - xmm13
	vfmadd231ss	%xmm11, %xmm5, %xmm7    # xmm7 = (xmm5 * xmm11) + xmm7
	vaddss	%xmm13, %xmm10, %xmm14
	vsubss	%xmm10, %xmm14, %xmm15
	vfmsub213ss	%xmm10, %xmm3, %xmm11   # xmm11 = (xmm3 * xmm11) - xmm10
	vsubss	%xmm15, %xmm14, %xmm0
	vsubss	%xmm0, %xmm10, %xmm0
	vsubss	%xmm15, %xmm13, %xmm10
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm9, %xmm14, %xmm10
	vsubss	%xmm14, %xmm10, %xmm13
	vsubss	%xmm13, %xmm10, %xmm15
	vsubss	%xmm15, %xmm14, %xmm14
	vsubss	%xmm13, %xmm9, %xmm9
	vaddss	%xmm7, %xmm11, %xmm7
	vfmadd231ss	%xmm12, %xmm3, %xmm0    # xmm0 = (xmm3 * xmm12) + xmm0
	vaddss	%xmm9, %xmm14, %xmm9
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm10, %xmm7
	vsubss	%xmm7, %xmm10, %xmm9
	vaddss	%xmm7, %xmm8, %xmm10
	vsubss	%xmm10, %xmm8, %xmm8
	vbroadcastss	.LCPI38_2(%rip), %xmm13 # xmm13 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm13, %xmm10, %xmm11
	vaddss	%xmm7, %xmm8, %xmm7
	vsubss	%xmm10, %xmm6, %xmm8
	vsubss	%xmm6, %xmm8, %xmm10
	vsubss	%xmm10, %xmm8, %xmm12
	vaddss	%xmm0, %xmm9, %xmm0
	vsubss	%xmm12, %xmm6, %xmm6
	vsubss	%xmm10, %xmm11, %xmm9
	vaddss	%xmm6, %xmm9, %xmm6
	vxorps	%xmm7, %xmm13, %xmm9
	vsubss	%xmm7, %xmm1, %xmm7
	vsubss	%xmm1, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm1, %xmm11
	vsubss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm6, %xmm7, %xmm10
	vsubss	%xmm7, %xmm10, %xmm12
	vaddss	%xmm9, %xmm11, %xmm9
	vsubss	%xmm12, %xmm10, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm12, %xmm6, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vsubss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm0, %xmm10, %xmm6
	vsubss	%xmm6, %xmm10, %xmm7
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm6, %xmm8, %xmm7
	vsubss	%xmm7, %xmm8, %xmm8
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm0, %xmm6, %xmm0
	vdivss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm4
	vsubss	%xmm4, %xmm5, %xmm5
	vaddss	%xmm4, %xmm3, %xmm6
	vaddss	%xmm0, %xmm5, %xmm0
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm6, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm7
	vaddss	%xmm4, %xmm3, %xmm3
	vsubss	%xmm7, %xmm5, %xmm4
	vsubss	%xmm4, %xmm2, %xmm4
	vsubss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm6
	vsubss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm4, %xmm6, %xmm7
	vaddss	%xmm3, %xmm2, %xmm2
	vsubss	%xmm6, %xmm7, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vsubss	%xmm3, %xmm7, %xmm3
	vsubss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	8(%rsp), %xmm0, %xmm0           # 4-byte Folded Reload
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm2
	vsubss	%xmm2, %xmm7, %xmm3
	vaddss	%xmm0, %xmm3, %xmm9
	vaddss	%xmm2, %xmm5, %xmm4
	vsubss	%xmm4, %xmm5, %xmm0
	vaddss	%xmm2, %xmm0, %xmm3
	vinsertps	$16, %xmm3, %xmm4, %xmm2 # xmm2 = xmm4[0],xmm3[0],xmm4[2,3]
	cmpl	$1, %eax
	ja	.LBB38_297
# %bb.298:
	movl	$90, %eax
	vcvtsi2ss	%eax, %xmm1, %xmm1
	vmulss	%xmm4, %xmm1, %xmm2
	vfmsub213ss	%xmm2, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm4) - xmm2
	vmulss	%xmm3, %xmm1, %xmm5
	vfmsub213ss	%xmm5, %xmm1, %xmm3     # xmm3 = (xmm1 * xmm3) - xmm5
	vaddss	%xmm4, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm5, %xmm5
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm5, %xmm4
	vfmadd213ss	%xmm4, %xmm1, %xmm9     # xmm9 = (xmm1 * xmm9) + xmm4
	vaddss	%xmm3, %xmm9, %xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm3
	vaddss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm4
	vxorps	%xmm2, %xmm2, %xmm2
	vucomiss	%xmm2, %xmm0
	vaddss	%xmm1, %xmm4, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	je	.LBB38_300
# %bb.299:
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	jmp	.LBB38_309
.LBB38_300:
	vmovups	%xmm1, 192(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovss	%xmm3, 8(%rsp)                  # 4-byte Spill
	vmovups	%xmm0, 96(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm0
	setnp	%cl
	sete	%dl
	vmovd	.LCPI38_0(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	vmovdqa	%xmm4, %xmm8
	testb	%cl, %dl
	jne	.LBB38_304
# %bb.301:
	movl	%eax, %ecx
	andl	$2139095040, %ecx               # imm = 0x7F800000
	cmpl	$2139095040, %ecx               # imm = 0x7F800000
	je	.LBB38_303
# %bb.302:
	vmovd	%ecx, %xmm8
	jmp	.LBB38_304
.LBB38_303:
	vmovd	.LCPI38_0(%rip), %xmm8          # xmm8 = mem[0],zero,zero,zero
.LBB38_304:
	vxorps	%xmm7, %xmm7, %xmm7
	vucomiss	%xmm7, %xmm0
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB38_308
# %bb.305:
	andl	$2139095040, %eax               # imm = 0x7F800000
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB38_307
# %bb.306:
	movl	$2130706432, %ecx               # imm = 0x7F000000
	subl	%eax, %ecx
	vmovd	%ecx, %xmm4
	jmp	.LBB38_308
.LBB38_307:
	vmovd	.LCPI38_0(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
.LBB38_308:
	vmovss	.LCPI38_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm4, %xmm5
	vmulss	96(%rsp), %xmm4, %xmm2          # 16-byte Folded Reload
	vmulss	%xmm5, %xmm2, %xmm2
	vmulss	192(%rsp), %xmm4, %xmm3         # 16-byte Folded Reload
	vmulss	%xmm5, %xmm3, %xmm3
	vmulss	8(%rsp), %xmm4, %xmm4           # 4-byte Folded Reload
	movl	$3, %eax
	vcvtsi2ss	%eax, %xmm14, %xmm6
	vmulss	%xmm5, %xmm4, %xmm4
	vmovss	%xmm4, 8(%rsp)                  # 4-byte Spill
	vmovd	%xmm8, 224(%rsp)                # 4-byte Folded Spill
	vdivss	%xmm0, %xmm8, %xmm8
	vmulss	%xmm1, %xmm6, %xmm5
	vmovaps	%xmm1, %xmm0
	vfmsub213ss	%xmm5, %xmm6, %xmm0     # xmm0 = (xmm6 * xmm0) - xmm5
	vmulss	%xmm7, %xmm6, %xmm9
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm9, %xmm6, %xmm10    # xmm10 = (xmm6 * xmm10) - xmm9
	vaddss	%xmm0, %xmm9, %xmm11
	vsubss	%xmm9, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm9, %xmm9
	vsubss	%xmm12, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vfmadd231ss	%xmm6, %xmm7, %xmm0     # xmm0 = (xmm7 * xmm6) + xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm0, %xmm11, %xmm9
	vsubss	%xmm9, %xmm11, %xmm6
	vaddss	%xmm0, %xmm6, %xmm0
	vmovss	%xmm0, 96(%rsp)                 # 4-byte Spill
	vaddss	%xmm5, %xmm9, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm9, %xmm5
	vmulss	%xmm8, %xmm8, %xmm10
	vmulss	%xmm7, %xmm8, %xmm9
	vxorps	%xmm11, %xmm11, %xmm11
	vfmsub213ss	%xmm9, %xmm8, %xmm11    # xmm11 = (xmm8 * xmm11) - xmm9
	vmulss	%xmm7, %xmm8, %xmm12
	vaddss	%xmm12, %xmm9, %xmm13
	vsubss	%xmm9, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm9, %xmm9
	vmovaps	%xmm8, %xmm15
	vfmsub213ss	%xmm12, %xmm7, %xmm15   # xmm15 = (xmm7 * xmm15) - xmm12
	vsubss	%xmm14, %xmm12, %xmm12
	vmovaps	%xmm8, %xmm14
	vfmsub213ss	%xmm10, %xmm8, %xmm14   # xmm14 = (xmm8 * xmm14) - xmm10
	vaddss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm14, %xmm13, %xmm12
	vsubss	%xmm13, %xmm12, %xmm1
	vsubss	%xmm1, %xmm12, %xmm4
	vsubss	%xmm4, %xmm13, %xmm4
	vsubss	%xmm1, %xmm14, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vfmadd231ss	%xmm8, %xmm7, %xmm11    # xmm11 = (xmm7 * xmm8) + xmm11
	vfmadd231ss	%xmm7, %xmm7, %xmm15    # xmm15 = (xmm7 * xmm7) + xmm15
	vaddss	%xmm15, %xmm11, %xmm4
	vfmadd231ss	%xmm7, %xmm8, %xmm9     # xmm9 = (xmm8 * xmm7) + xmm9
	vaddss	%xmm4, %xmm9, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm1, %xmm12, %xmm4
	vsubss	%xmm4, %xmm12, %xmm9
	vaddss	%xmm1, %xmm9, %xmm9
	vaddss	%xmm4, %xmm10, %xmm1
	vsubss	%xmm1, %xmm10, %xmm10
	vaddss	%xmm4, %xmm10, %xmm4
	vmulss	%xmm4, %xmm2, %xmm10
	vmulss	%xmm1, %xmm3, %xmm11
	vmovaps	%xmm1, %xmm12
	vfmsub213ss	%xmm11, %xmm3, %xmm12   # xmm12 = (xmm3 * xmm12) - xmm11
	vfmadd231ss	%xmm4, %xmm3, %xmm12    # xmm12 = (xmm3 * xmm4) + xmm12
	vfmsub213ss	%xmm10, %xmm2, %xmm4    # xmm4 = (xmm2 * xmm4) - xmm10
	vaddss	%xmm11, %xmm10, %xmm13
	vsubss	%xmm10, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vsubss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm11, %xmm10, %xmm10
	vmulss	%xmm1, %xmm2, %xmm11
	vfmadd231ss	8(%rsp), %xmm1, %xmm4   # 4-byte Folded Reload
                                        # xmm4 = (xmm1 * mem) + xmm4
	vfmsub213ss	%xmm11, %xmm2, %xmm1    # xmm1 = (xmm2 * xmm1) - xmm11
	vaddss	%xmm1, %xmm13, %xmm14
	vsubss	%xmm13, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm0
	vsubss	%xmm0, %xmm13, %xmm0
	vsubss	%xmm15, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm4, %xmm12, %xmm1
	vfmadd231ss	%xmm9, %xmm2, %xmm10    # xmm10 = (xmm2 * xmm9) + xmm10
	vaddss	%xmm1, %xmm10, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm1, %xmm11, %xmm4
	vsubss	%xmm4, %xmm11, %xmm9
	vaddss	%xmm1, %xmm9, %xmm1
	vbroadcastss	.LCPI38_2(%rip), %xmm10 # xmm10 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm4, %xmm10, %xmm9
	vxorps	%xmm1, %xmm10, %xmm10
	vsubss	%xmm4, %xmm6, %xmm4
	vsubss	%xmm6, %xmm4, %xmm11
	vsubss	%xmm11, %xmm4, %xmm12
	vsubss	%xmm12, %xmm6, %xmm12
	vsubss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm9, %xmm12, %xmm9
	vsubss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm11
	vsubss	%xmm11, %xmm1, %xmm12
	vsubss	%xmm12, %xmm5, %xmm12
	vsubss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm10, %xmm12, %xmm10
	vaddss	%xmm1, %xmm9, %xmm11
	vsubss	%xmm1, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm1, %xmm1
	vsubss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm1, %xmm9, %xmm1
	vmovss	96(%rsp), %xmm9                 # 4-byte Reload
                                        # xmm9 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm11, %xmm1
	vsubss	%xmm1, %xmm11, %xmm9
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm1, %xmm4, %xmm9
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vmulss	%xmm7, %xmm9, %xmm4
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm4, %xmm9, %xmm10    # xmm10 = (xmm9 * xmm10) - xmm4
	vmulss	%xmm1, %xmm8, %xmm11
	vaddss	%xmm4, %xmm11, %xmm12
	vsubss	%xmm4, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm4, %xmm4
	vmovaps	%xmm8, %xmm14
	vfmsub213ss	%xmm11, %xmm1, %xmm14   # xmm14 = (xmm1 * xmm14) - xmm11
	vsubss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm4, %xmm11, %xmm4
	vfmadd231ss	%xmm0, %xmm8, %xmm10    # xmm10 = (xmm8 * xmm0) + xmm10
	vmulss	%xmm8, %xmm9, %xmm0
	vfmsub213ss	%xmm0, %xmm9, %xmm8     # xmm8 = (xmm9 * xmm8) - xmm0
	vaddss	%xmm8, %xmm12, %xmm11
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm11, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm13, %xmm8, %xmm8
	vaddss	%xmm8, %xmm12, %xmm8
	vfmadd231ss	%xmm1, %xmm7, %xmm14    # xmm14 = (xmm7 * xmm1) + xmm14
	vaddss	%xmm14, %xmm10, %xmm1
	vfmadd231ss	%xmm7, %xmm9, %xmm4     # xmm4 = (xmm9 * xmm7) + xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm1, %xmm11, %xmm4
	vsubss	%xmm4, %xmm11, %xmm7
	vaddss	%xmm1, %xmm7, %xmm7
	vaddss	%xmm4, %xmm0, %xmm8
	vsubss	%xmm8, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm9
	vmulss	%xmm9, %xmm8, %xmm0
	vmovaps	%xmm9, %xmm1
	vfmsub213ss	%xmm0, %xmm8, %xmm1     # xmm1 = (xmm8 * xmm1) - xmm0
	vmulss	%xmm8, %xmm9, %xmm4
	vaddss	%xmm4, %xmm0, %xmm10
	vsubss	%xmm0, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm8, %xmm12
	vfmsub213ss	%xmm4, %xmm9, %xmm12    # xmm12 = (xmm9 * xmm12) - xmm4
	vsubss	%xmm11, %xmm4, %xmm4
	vmulss	%xmm8, %xmm8, %xmm11
	vaddss	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm8, %xmm4
	vfmsub213ss	%xmm11, %xmm8, %xmm4    # xmm4 = (xmm8 * xmm4) - xmm11
	vaddss	%xmm4, %xmm10, %xmm13
	vsubss	%xmm10, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vsubss	%xmm14, %xmm4, %xmm4
	vaddss	%xmm4, %xmm10, %xmm4
	vmovss	%xmm7, 192(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm8, %xmm7, %xmm1     # xmm1 = (xmm7 * xmm8) + xmm1
	vfmadd231ss	%xmm9, %xmm9, %xmm12    # xmm12 = (xmm9 * xmm9) + xmm12
	vaddss	%xmm1, %xmm12, %xmm1
	vfmadd231ss	%xmm7, %xmm8, %xmm0     # xmm0 = (xmm8 * xmm7) + xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm13, %xmm1
	vsubss	%xmm1, %xmm13, %xmm4
	vaddss	%xmm0, %xmm4, %xmm10
	vaddss	%xmm1, %xmm11, %xmm0
	vsubss	%xmm0, %xmm11, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vmulss	%xmm1, %xmm2, %xmm4
	vmulss	%xmm0, %xmm3, %xmm11
	vmovaps	%xmm0, %xmm12
	vfmsub213ss	%xmm11, %xmm3, %xmm12   # xmm12 = (xmm3 * xmm12) - xmm11
	vfmadd231ss	%xmm1, %xmm3, %xmm12    # xmm12 = (xmm3 * xmm1) + xmm12
	vfmsub213ss	%xmm4, %xmm2, %xmm1     # xmm1 = (xmm2 * xmm1) - xmm4
	vaddss	%xmm4, %xmm11, %xmm13
	vsubss	%xmm4, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm4, %xmm11, %xmm4
	vfmadd231ss	8(%rsp), %xmm0, %xmm1   # 4-byte Folded Reload
                                        # xmm1 = (xmm0 * mem) + xmm1
	vmulss	%xmm0, %xmm2, %xmm11
	vfmsub213ss	%xmm11, %xmm2, %xmm0    # xmm0 = (xmm2 * xmm0) - xmm11
	vaddss	%xmm0, %xmm13, %xmm14
	vsubss	%xmm13, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm7
	vsubss	%xmm7, %xmm13, %xmm7
	vsubss	%xmm15, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm1, %xmm12, %xmm1
	vbroadcastss	.LCPI38_2(%rip), %xmm12 # xmm12 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vfmadd231ss	%xmm10, %xmm2, %xmm4    # xmm4 = (xmm2 * xmm10) + xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm1, %xmm11, %xmm4
	vsubss	%xmm4, %xmm11, %xmm7
	vaddss	%xmm1, %xmm7, %xmm1
	vxorps	%xmm4, %xmm12, %xmm7
	vsubss	%xmm4, %xmm6, %xmm4
	vsubss	%xmm6, %xmm4, %xmm10
	vsubss	%xmm10, %xmm4, %xmm11
	vsubss	%xmm11, %xmm6, %xmm6
	vxorps	%xmm1, %xmm12, %xmm11
	vsubss	%xmm10, %xmm7, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vsubss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm7
	vsubss	%xmm7, %xmm1, %xmm10
	vsubss	%xmm10, %xmm5, %xmm5
	vsubss	%xmm7, %xmm11, %xmm7
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm6, %xmm1, %xmm7
	vsubss	%xmm1, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm1, %xmm1
	vmovss	96(%rsp), %xmm6                 # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm5
	vaddss	%xmm0, %xmm5, %xmm6
	vaddss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm0
	vaddss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm5, %xmm8, %xmm0
	vmulss	%xmm5, %xmm9, %xmm4
	vmulss	%xmm1, %xmm8, %xmm7
	vmovaps	%xmm8, %xmm10
	vfmsub213ss	%xmm7, %xmm1, %xmm10    # xmm10 = (xmm1 * xmm10) - xmm7
	vfmadd231ss	%xmm9, %xmm1, %xmm10    # xmm10 = (xmm1 * xmm9) + xmm10
	vfmsub213ss	%xmm4, %xmm5, %xmm9     # xmm9 = (xmm5 * xmm9) - xmm4
	vfmadd231ss	%xmm6, %xmm8, %xmm9     # xmm9 = (xmm8 * xmm6) + xmm9
	vfmsub213ss	%xmm0, %xmm5, %xmm8     # xmm8 = (xmm5 * xmm8) - xmm0
	vaddss	%xmm7, %xmm4, %xmm1
	vsubss	%xmm4, %xmm1, %xmm6
	vsubss	%xmm6, %xmm1, %xmm11
	vsubss	%xmm11, %xmm4, %xmm4
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm1, %xmm8, %xmm6
	vsubss	%xmm1, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm10, %xmm9, %xmm7
	vfmadd231ss	192(%rsp), %xmm5, %xmm4 # 4-byte Folded Reload
                                        # xmm4 = (xmm5 * mem) + xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm1, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm5
	vaddss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm4, %xmm0, %xmm5
	vsubss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vmulss	%xmm0, %xmm2, %xmm4
	vmulss	%xmm5, %xmm3, %xmm6
	vmovaps	%xmm5, %xmm7
	vfmsub213ss	%xmm6, %xmm3, %xmm7     # xmm7 = (xmm3 * xmm7) - xmm6
	vfmadd231ss	%xmm0, %xmm3, %xmm7     # xmm7 = (xmm3 * xmm0) + xmm7
	vfmsub213ss	%xmm4, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm0) - xmm4
	vaddss	%xmm6, %xmm4, %xmm3
	vsubss	%xmm4, %xmm3, %xmm8
	vsubss	%xmm8, %xmm3, %xmm9
	vsubss	%xmm9, %xmm4, %xmm4
	vsubss	%xmm8, %xmm6, %xmm6
	vfmadd231ss	8(%rsp), %xmm5, %xmm0   # 4-byte Folded Reload
                                        # xmm0 = (xmm5 * mem) + xmm0
	vmulss	%xmm5, %xmm2, %xmm8
	vfmsub213ss	%xmm8, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm5) - xmm8
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm5, %xmm3, %xmm6
	vfmadd231ss	%xmm1, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm1) + xmm4
	vsubss	%xmm3, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm2
	vsubss	%xmm2, %xmm3, %xmm2
	vsubss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm2
	vaddss	%xmm0, %xmm2, %xmm4
	vaddss	%xmm1, %xmm8, %xmm2
	vsubss	%xmm2, %xmm8, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	movl	$2, %eax
	vxorps	%xmm13, %xmm13, %xmm13
	vcvtsi2ss	%eax, %xmm13, %xmm3
	vmulss	224(%rsp), %xmm3, %xmm3         # 4-byte Folded Reload
	vmulss	%xmm3, %xmm2, %xmm2
	vmulss	%xmm3, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm1[0],xmm2[2,3]
	vmulss	%xmm3, %xmm4, %xmm3
	vxorps	%xmm2, %xmm2, %xmm2
.LBB38_309:
	vucomiss	%xmm2, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB38_319
# %bb.310:
	vmovss	%xmm3, 8(%rsp)                  # 4-byte Spill
	vmovups	%xmm0, 96(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vxorps	%xmm2, %xmm2, %xmm2
	vucomiss	%xmm2, %xmm0
	setnp	%cl
	sete	%dl
	vmovd	.LCPI38_0(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	vmovdqa	%xmm4, %xmm7
	testb	%cl, %dl
	jne	.LBB38_314
# %bb.311:
	movl	%eax, %ecx
	andl	$2139095040, %ecx               # imm = 0x7F800000
	cmpl	$2139095040, %ecx               # imm = 0x7F800000
	je	.LBB38_313
# %bb.312:
	vmovd	%ecx, %xmm7
	jmp	.LBB38_314
.LBB38_313:
	vmovd	.LCPI38_0(%rip), %xmm7          # xmm7 = mem[0],zero,zero,zero
.LBB38_314:
	vucomiss	%xmm2, %xmm0
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB38_318
# %bb.315:
	andl	$2139095040, %eax               # imm = 0x7F800000
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB38_317
# %bb.316:
	movl	$2130706432, %ecx               # imm = 0x7F000000
	subl	%eax, %ecx
	vmovd	%ecx, %xmm4
	jmp	.LBB38_318
.LBB38_317:
	vmovd	.LCPI38_0(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
.LBB38_318:
	vmovss	.LCPI38_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm4, %xmm5
	vmovups	96(%rsp), %xmm3                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmulss	%xmm4, %xmm3, %xmm2
	vmulss	%xmm5, %xmm2, %xmm2
	vmovshdup	%xmm3, %xmm3            # xmm3 = xmm3[1,1,3,3]
	vmulss	%xmm4, %xmm3, %xmm3
	vmulss	%xmm5, %xmm3, %xmm3
	vmulss	8(%rsp), %xmm4, %xmm4           # 4-byte Folded Reload
	movl	$3, %eax
	vcvtsi2ss	%eax, %xmm13, %xmm6
	vmulss	%xmm5, %xmm4, %xmm4
	vmovss	%xmm4, 8(%rsp)                  # 4-byte Spill
	vdivss	%xmm0, %xmm7, %xmm8
	vmulss	%xmm1, %xmm6, %xmm5
	vfmsub213ss	%xmm5, %xmm6, %xmm1     # xmm1 = (xmm6 * xmm1) - xmm5
	vmovss	%xmm7, 224(%rsp)                # 4-byte Spill
	vxorps	%xmm7, %xmm7, %xmm7
	vmulss	%xmm7, %xmm6, %xmm0
	vxorps	%xmm9, %xmm9, %xmm9
	vfmsub213ss	%xmm0, %xmm6, %xmm9     # xmm9 = (xmm6 * xmm9) - xmm0
	vaddss	%xmm1, %xmm0, %xmm10
	vsubss	%xmm0, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm0, %xmm0
	vsubss	%xmm11, %xmm1, %xmm11
	vaddss	%xmm0, %xmm11, %xmm0
	vfmadd231ss	%xmm6, %xmm7, %xmm0     # xmm0 = (xmm7 * xmm6) + xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm10, %xmm9
	vsubss	%xmm9, %xmm10, %xmm6
	vaddss	%xmm0, %xmm6, %xmm0
	vmovss	%xmm0, 96(%rsp)                 # 4-byte Spill
	vaddss	%xmm5, %xmm9, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm9, %xmm5
	vmulss	%xmm7, %xmm8, %xmm9
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm9, %xmm8, %xmm10    # xmm10 = (xmm8 * xmm10) - xmm9
	vmulss	%xmm7, %xmm8, %xmm11
	vaddss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm9, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm9, %xmm9
	vmovaps	%xmm8, %xmm14
	vfmsub213ss	%xmm11, %xmm7, %xmm14   # xmm14 = (xmm7 * xmm14) - xmm11
	vsubss	%xmm13, %xmm11, %xmm11
	vmulss	%xmm8, %xmm8, %xmm13
	vaddss	%xmm11, %xmm9, %xmm9
	vmovaps	%xmm8, %xmm11
	vfmsub213ss	%xmm13, %xmm8, %xmm11   # xmm11 = (xmm8 * xmm11) - xmm13
	vaddss	%xmm11, %xmm12, %xmm15
	vsubss	%xmm12, %xmm15, %xmm1
	vsubss	%xmm1, %xmm15, %xmm4
	vsubss	%xmm4, %xmm12, %xmm4
	vsubss	%xmm1, %xmm11, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vfmadd231ss	%xmm8, %xmm7, %xmm10    # xmm10 = (xmm7 * xmm8) + xmm10
	vfmadd231ss	%xmm7, %xmm7, %xmm14    # xmm14 = (xmm7 * xmm7) + xmm14
	vaddss	%xmm14, %xmm10, %xmm4
	vfmadd231ss	%xmm7, %xmm8, %xmm9     # xmm9 = (xmm8 * xmm7) + xmm9
	vaddss	%xmm4, %xmm9, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm1, %xmm15, %xmm4
	vsubss	%xmm4, %xmm15, %xmm9
	vaddss	%xmm1, %xmm9, %xmm9
	vaddss	%xmm4, %xmm13, %xmm1
	vsubss	%xmm1, %xmm13, %xmm10
	vaddss	%xmm4, %xmm10, %xmm4
	vmulss	%xmm4, %xmm2, %xmm10
	vmulss	%xmm1, %xmm3, %xmm11
	vmovaps	%xmm1, %xmm12
	vfmsub213ss	%xmm11, %xmm3, %xmm12   # xmm12 = (xmm3 * xmm12) - xmm11
	vfmadd231ss	%xmm4, %xmm3, %xmm12    # xmm12 = (xmm3 * xmm4) + xmm12
	vfmsub213ss	%xmm10, %xmm2, %xmm4    # xmm4 = (xmm2 * xmm4) - xmm10
	vaddss	%xmm11, %xmm10, %xmm13
	vsubss	%xmm10, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vsubss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm11, %xmm10, %xmm10
	vfmadd231ss	8(%rsp), %xmm1, %xmm4   # 4-byte Folded Reload
                                        # xmm4 = (xmm1 * mem) + xmm4
	vmulss	%xmm1, %xmm2, %xmm11
	vfmsub213ss	%xmm11, %xmm2, %xmm1    # xmm1 = (xmm2 * xmm1) - xmm11
	vaddss	%xmm1, %xmm13, %xmm14
	vsubss	%xmm13, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm0
	vsubss	%xmm0, %xmm13, %xmm0
	vsubss	%xmm15, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm4, %xmm12, %xmm1
	vfmadd231ss	%xmm9, %xmm2, %xmm10    # xmm10 = (xmm2 * xmm9) + xmm10
	vaddss	%xmm1, %xmm10, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm1, %xmm11, %xmm4
	vsubss	%xmm4, %xmm11, %xmm9
	vaddss	%xmm1, %xmm9, %xmm1
	vbroadcastss	.LCPI38_2(%rip), %xmm10 # xmm10 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm4, %xmm10, %xmm9
	vxorps	%xmm1, %xmm10, %xmm10
	vsubss	%xmm4, %xmm6, %xmm4
	vsubss	%xmm6, %xmm4, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vsubss	%xmm11, %xmm4, %xmm11
	vsubss	%xmm11, %xmm6, %xmm11
	vaddss	%xmm9, %xmm11, %xmm9
	vsubss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm11
	vsubss	%xmm11, %xmm10, %xmm10
	vsubss	%xmm11, %xmm1, %xmm11
	vsubss	%xmm11, %xmm5, %xmm11
	vaddss	%xmm10, %xmm11, %xmm10
	vaddss	%xmm1, %xmm9, %xmm11
	vsubss	%xmm1, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm1, %xmm1
	vsubss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm1, %xmm9, %xmm1
	vmovss	96(%rsp), %xmm9                 # 4-byte Reload
                                        # xmm9 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm11, %xmm1
	vsubss	%xmm1, %xmm11, %xmm9
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm1, %xmm4, %xmm9
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vmulss	%xmm7, %xmm9, %xmm4
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm4, %xmm9, %xmm10    # xmm10 = (xmm9 * xmm10) - xmm4
	vmulss	%xmm1, %xmm8, %xmm11
	vaddss	%xmm4, %xmm11, %xmm12
	vsubss	%xmm4, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm4, %xmm4
	vmovaps	%xmm8, %xmm14
	vfmsub213ss	%xmm11, %xmm1, %xmm14   # xmm14 = (xmm1 * xmm14) - xmm11
	vsubss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm4, %xmm11, %xmm4
	vfmadd231ss	%xmm0, %xmm8, %xmm10    # xmm10 = (xmm8 * xmm0) + xmm10
	vmulss	%xmm8, %xmm9, %xmm0
	vfmsub213ss	%xmm0, %xmm9, %xmm8     # xmm8 = (xmm9 * xmm8) - xmm0
	vaddss	%xmm8, %xmm12, %xmm11
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm11, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm13, %xmm8, %xmm8
	vaddss	%xmm8, %xmm12, %xmm8
	vfmadd231ss	%xmm1, %xmm7, %xmm14    # xmm14 = (xmm7 * xmm1) + xmm14
	vaddss	%xmm14, %xmm10, %xmm1
	vfmadd231ss	%xmm7, %xmm9, %xmm4     # xmm4 = (xmm9 * xmm7) + xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm1, %xmm11, %xmm4
	vsubss	%xmm4, %xmm11, %xmm7
	vaddss	%xmm1, %xmm7, %xmm7
	vaddss	%xmm4, %xmm0, %xmm8
	vsubss	%xmm8, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm9
	vmulss	%xmm9, %xmm8, %xmm0
	vmovaps	%xmm9, %xmm1
	vfmsub213ss	%xmm0, %xmm8, %xmm1     # xmm1 = (xmm8 * xmm1) - xmm0
	vmulss	%xmm8, %xmm9, %xmm4
	vaddss	%xmm4, %xmm0, %xmm10
	vsubss	%xmm0, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm8, %xmm12
	vfmsub213ss	%xmm4, %xmm9, %xmm12    # xmm12 = (xmm9 * xmm12) - xmm4
	vsubss	%xmm11, %xmm4, %xmm4
	vaddss	%xmm4, %xmm0, %xmm0
	vmulss	%xmm8, %xmm8, %xmm4
	vmovaps	%xmm8, %xmm11
	vfmsub213ss	%xmm4, %xmm8, %xmm11    # xmm11 = (xmm8 * xmm11) - xmm4
	vaddss	%xmm11, %xmm10, %xmm13
	vsubss	%xmm10, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vsubss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm11, %xmm10, %xmm10
	vmovss	%xmm7, 192(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm8, %xmm7, %xmm1     # xmm1 = (xmm7 * xmm8) + xmm1
	vfmadd231ss	%xmm9, %xmm9, %xmm12    # xmm12 = (xmm9 * xmm9) + xmm12
	vaddss	%xmm1, %xmm12, %xmm1
	vfmadd231ss	%xmm7, %xmm8, %xmm0     # xmm0 = (xmm8 * xmm7) + xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm0, %xmm13, %xmm1
	vsubss	%xmm1, %xmm13, %xmm10
	vaddss	%xmm0, %xmm10, %xmm10
	vaddss	%xmm1, %xmm4, %xmm11
	vsubss	%xmm11, %xmm4, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vmulss	%xmm3, %xmm11, %xmm1
	vmovaps	%xmm11, %xmm4
	vfmsub213ss	%xmm1, %xmm3, %xmm4     # xmm4 = (xmm3 * xmm4) - xmm1
	vfmadd231ss	%xmm0, %xmm3, %xmm4     # xmm4 = (xmm3 * xmm0) + xmm4
	vmulss	%xmm0, %xmm2, %xmm12
	vfmsub213ss	%xmm12, %xmm2, %xmm0    # xmm0 = (xmm2 * xmm0) - xmm12
	vaddss	%xmm1, %xmm12, %xmm13
	vsubss	%xmm12, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm1, %xmm1
	vaddss	%xmm1, %xmm12, %xmm1
	vfmadd231ss	8(%rsp), %xmm11, %xmm0  # 4-byte Folded Reload
                                        # xmm0 = (xmm11 * mem) + xmm0
	vmulss	%xmm2, %xmm11, %xmm12
	vfmsub213ss	%xmm12, %xmm2, %xmm11   # xmm11 = (xmm2 * xmm11) - xmm12
	vaddss	%xmm11, %xmm13, %xmm14
	vsubss	%xmm13, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm7
	vsubss	%xmm7, %xmm13, %xmm7
	vsubss	%xmm15, %xmm11, %xmm11
	vaddss	%xmm7, %xmm11, %xmm7
	vaddss	%xmm4, %xmm0, %xmm0
	vfmadd231ss	%xmm10, %xmm2, %xmm1    # xmm1 = (xmm2 * xmm10) + xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm1, %xmm12, %xmm4
	vsubss	%xmm4, %xmm12, %xmm7
	vaddss	%xmm1, %xmm7, %xmm1
	vsubss	%xmm4, %xmm6, %xmm7
	vsubss	%xmm6, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm6, %xmm6
	vbroadcastss	.LCPI38_2(%rip), %xmm11 # xmm11 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm4, %xmm11, %xmm4
	vxorps	%xmm1, %xmm11, %xmm11
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm6, %xmm4
	vsubss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm6
	vsubss	%xmm6, %xmm1, %xmm10
	vsubss	%xmm10, %xmm5, %xmm5
	vsubss	%xmm6, %xmm11, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm4, %xmm1, %xmm6
	vsubss	%xmm1, %xmm6, %xmm10
	vsubss	%xmm10, %xmm6, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vmovss	96(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm4
	vaddss	%xmm0, %xmm4, %xmm4
	vaddss	%xmm1, %xmm7, %xmm5
	vsubss	%xmm5, %xmm7, %xmm0
	vaddss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm5, %xmm8, %xmm0
	vmulss	%xmm5, %xmm9, %xmm6
	vmulss	%xmm1, %xmm8, %xmm7
	vmovaps	%xmm8, %xmm10
	vfmsub213ss	%xmm7, %xmm1, %xmm10    # xmm10 = (xmm1 * xmm10) - xmm7
	vfmadd231ss	%xmm9, %xmm1, %xmm10    # xmm10 = (xmm1 * xmm9) + xmm10
	vfmsub213ss	%xmm6, %xmm5, %xmm9     # xmm9 = (xmm5 * xmm9) - xmm6
	vfmadd231ss	%xmm4, %xmm8, %xmm9     # xmm9 = (xmm8 * xmm4) + xmm9
	vfmsub213ss	%xmm0, %xmm5, %xmm8     # xmm8 = (xmm5 * xmm8) - xmm0
	vaddss	%xmm7, %xmm6, %xmm1
	vsubss	%xmm6, %xmm1, %xmm4
	vsubss	%xmm4, %xmm1, %xmm11
	vsubss	%xmm11, %xmm6, %xmm6
	vsubss	%xmm4, %xmm7, %xmm4
	vaddss	%xmm4, %xmm6, %xmm4
	vaddss	%xmm1, %xmm8, %xmm6
	vsubss	%xmm1, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm10, %xmm9, %xmm7
	vfmadd231ss	192(%rsp), %xmm5, %xmm4 # 4-byte Folded Reload
                                        # xmm4 = (xmm5 * mem) + xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm1, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm5
	vaddss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm4, %xmm0, %xmm5
	vsubss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vmulss	%xmm0, %xmm2, %xmm4
	vmulss	%xmm5, %xmm3, %xmm6
	vmovaps	%xmm5, %xmm7
	vfmsub213ss	%xmm6, %xmm3, %xmm7     # xmm7 = (xmm3 * xmm7) - xmm6
	vfmadd231ss	%xmm0, %xmm3, %xmm7     # xmm7 = (xmm3 * xmm0) + xmm7
	vfmsub213ss	%xmm4, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm0) - xmm4
	vaddss	%xmm6, %xmm4, %xmm3
	vsubss	%xmm4, %xmm3, %xmm8
	vsubss	%xmm8, %xmm3, %xmm9
	vsubss	%xmm9, %xmm4, %xmm4
	vsubss	%xmm8, %xmm6, %xmm6
	vfmadd231ss	8(%rsp), %xmm5, %xmm0   # 4-byte Folded Reload
                                        # xmm0 = (xmm5 * mem) + xmm0
	vmulss	%xmm5, %xmm2, %xmm8
	vfmsub213ss	%xmm8, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm5) - xmm8
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm5, %xmm3, %xmm6
	vfmadd231ss	%xmm1, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm1) + xmm4
	vsubss	%xmm3, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm2
	vsubss	%xmm2, %xmm3, %xmm2
	vsubss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm2
	vaddss	%xmm0, %xmm2, %xmm4
	vaddss	%xmm1, %xmm8, %xmm2
	vsubss	%xmm2, %xmm8, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	movl	$2, %eax
	vxorps	%xmm13, %xmm13, %xmm13
	vcvtsi2ss	%eax, %xmm13, %xmm3
	vmulss	224(%rsp), %xmm3, %xmm3         # 4-byte Folded Reload
	vmulss	%xmm3, %xmm2, %xmm2
	vmulss	%xmm3, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm1[0],xmm2[2,3]
	vmulss	%xmm3, %xmm4, %xmm3
.LBB38_319:
	vmovlps	%xmm0, 48(%rsp)
	vmovss	%xmm3, 56(%rsp)
	leaq	264(%rsp), %r14
	movq	%r14, 248(%rsp)
	movq	$32, 16(%rsp)
.Ltmp4834:
	leaq	248(%rsp), %rdi
	leaq	16(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp4835:
# %bb.320:
	movq	%rax, 248(%rsp)
	movq	16(%rsp), %rcx
	movq	%rcx, 264(%rsp)
	vmovupd	.L.str.63(%rip), %ymm0
	vmovupd	%ymm0, (%rax)
	movq	%rcx, 256(%rsp)
	movq	248(%rsp), %rax
	movb	$0, (%rax,%rcx)
.Ltmp4837:
	leaq	248(%rsp), %rdi
	leaq	48(%rsp), %rsi
	xorl	%edx, %edx
	vzeroupper
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp4838:
# %bb.321:
	movq	248(%rsp), %rdi
	cmpq	%r14, %rdi
	je	.LBB38_323
# %bb.322:
	callq	_ZdlPv
.LBB38_323:
	addq	$456, %rsp                      # imm = 0x1C8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB38_324:
	.cfi_def_cfa_offset 512
.Ltmp4833:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_325:
.Ltmp4827:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_326:
.Ltmp4784:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_327:
.Ltmp4678:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_328:
.Ltmp4672:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_329:
.Ltmp4629:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_330:
.Ltmp4520:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_331:
.Ltmp4517:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_332:
.Ltmp4514:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_333:
.Ltmp4511:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_334:
.Ltmp4456:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_335:
.Ltmp4453:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_336:
.Ltmp4450:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_337:
.Ltmp4447:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_338:
.Ltmp4392:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_339:
.Ltmp4389:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_340:
.Ltmp4386:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_341:
.Ltmp4383:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_342:
.Ltmp4839:
	movq	%rax, %rbx
	movq	248(%rsp), %rdi
	cmpq	%r14, %rdi
	jne	.LBB38_360
	jmp	.LBB38_361
.LBB38_343:
.Ltmp4836:
	movq	%rax, %rdi
	callq	_Unwind_Resume@PLT
.LBB38_344:
.Ltmp4830:
	movq	%rax, %rbx
	movq	296(%rsp), %rdi
	jmp	.LBB38_348
.LBB38_345:
.Ltmp4824:
	movq	%rax, %rbx
	jmp	.LBB38_420
.LBB38_346:
.Ltmp4768:
	movq	%rax, %rbx
	jmp	.LBB38_420
.LBB38_347:
.Ltmp4675:
	movq	%rax, %rbx
	movq	328(%rsp), %rdi
.LBB38_348:
	cmpq	%r15, %rdi
	je	.LBB38_426
# %bb.349:
	callq	_ZdlPv
	jmp	.LBB38_426
.LBB38_350:
.Ltmp4669:
	movq	%rax, %rbx
	jmp	.LBB38_420
.LBB38_351:
.Ltmp4613:
	movq	%rax, %rbx
	jmp	.LBB38_420
.LBB38_352:
.Ltmp4523:
	movq	%rax, %rbx
	movq	360(%rsp), %rdi
	jmp	.LBB38_359
.LBB38_353:
.Ltmp4508:
	jmp	.LBB38_364
.LBB38_354:
.Ltmp4505:
	jmp	.LBB38_364
.LBB38_355:
.Ltmp4459:
	movq	%rax, %rbx
	movq	392(%rsp), %rdi
	jmp	.LBB38_359
.LBB38_356:
.Ltmp4444:
	jmp	.LBB38_364
.LBB38_357:
.Ltmp4441:
	jmp	.LBB38_364
.LBB38_358:
.Ltmp4395:
	movq	%rax, %rbx
	movq	424(%rsp), %rdi
.LBB38_359:
	cmpq	%r15, %rdi
	je	.LBB38_361
.LBB38_360:
	callq	_ZdlPv
.LBB38_361:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB38_362:
.Ltmp4380:
	jmp	.LBB38_364
.LBB38_363:
.Ltmp4377:
.LBB38_364:
	movq	%rax, %rbx
	leaq	48(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB38_387
.LBB38_365:
.Ltmp4821:
	movq	%rax, %rbx
	jmp	.LBB38_420
.LBB38_366:
.Ltmp4666:
	movq	%rax, %rbx
	jmp	.LBB38_420
.LBB38_367:
.Ltmp4491:
	jmp	.LBB38_386
.LBB38_368:
.Ltmp4475:
	movq	%rax, %rbx
	jmp	.LBB38_388
.LBB38_369:
.Ltmp4427:
	jmp	.LBB38_386
.LBB38_370:
.Ltmp4411:
	movq	%rax, %rbx
	jmp	.LBB38_388
.LBB38_371:
.Ltmp4363:
	jmp	.LBB38_386
.LBB38_372:
.Ltmp4347:
	movq	%rax, %rbx
	jmp	.LBB38_388
.LBB38_373:
.Ltmp4781:
	movq	%rax, %rbx
	jmp	.LBB38_420
.LBB38_374:
.Ltmp4626:
	movq	%rax, %rbx
	jmp	.LBB38_420
.LBB38_375:
.Ltmp4816:
	movq	%rax, %rbx
	jmp	.LBB38_426
.LBB38_376:
.Ltmp4765:
	movq	%rax, %rbx
	jmp	.LBB38_426
.LBB38_377:
.Ltmp4661:
	movq	%rax, %rbx
	jmp	.LBB38_426
.LBB38_378:
.Ltmp4610:
	movq	%rax, %rbx
	jmp	.LBB38_426
.LBB38_379:
.Ltmp4502:
	jmp	.LBB38_386
.LBB38_380:
.Ltmp4486:
	movq	%rax, %rbx
	jmp	.LBB38_388
.LBB38_381:
.Ltmp4470:
	jmp	.LBB38_391
.LBB38_382:
.Ltmp4438:
	jmp	.LBB38_386
.LBB38_383:
.Ltmp4422:
	movq	%rax, %rbx
	jmp	.LBB38_388
.LBB38_384:
.Ltmp4406:
	jmp	.LBB38_391
.LBB38_385:
.Ltmp4374:
.LBB38_386:
	movq	%rax, %rbx
.LBB38_387:
	leaq	16(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB38_388:
	leaq	112(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	leaq	160(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB38_389:
.Ltmp4358:
	movq	%rax, %rbx
	jmp	.LBB38_388
.LBB38_390:
.Ltmp4342:
.LBB38_391:
	movq	%rax, %rbx
	leaq	160(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB38_392:
.Ltmp4805:
	movq	%rax, %rbx
	jmp	.LBB38_426
.LBB38_393:
.Ltmp4650:
	movq	%rax, %rbx
	jmp	.LBB38_426
.LBB38_394:
.Ltmp4754:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_395:
.Ltmp4751:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_396:
.Ltmp4748:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_397:
.Ltmp4700:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_398:
.Ltmp4599:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_399:
.Ltmp4596:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_400:
.Ltmp4593:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_401:
.Ltmp4545:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB38_402:
.Ltmp4681:
	movq	%rax, %rbx
	jmp	.LBB38_426
.LBB38_403:
.Ltmp4526:
	movq	%rax, %rbx
	jmp	.LBB38_426
.LBB38_404:
.Ltmp4732:
	jmp	.LBB38_413
.LBB38_405:
.Ltmp4716:
	jmp	.LBB38_418
.LBB38_406:
.Ltmp4697:
	jmp	.LBB38_410
.LBB38_407:
.Ltmp4577:
	jmp	.LBB38_413
.LBB38_408:
.Ltmp4561:
	jmp	.LBB38_418
.LBB38_409:
.Ltmp4542:
.LBB38_410:
	movq	%rax, %rbx
	leaq	16(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB38_424
.LBB38_411:
.Ltmp4745:
	jmp	.LBB38_413
.LBB38_412:
.Ltmp4590:
.LBB38_413:
	movq	%rax, %rbx
	leaq	112(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB38_419
.LBB38_414:
.Ltmp4727:
	jmp	.LBB38_418
.LBB38_415:
.Ltmp4711:
	movq	%rax, %rbx
	jmp	.LBB38_420
.LBB38_416:
.Ltmp4692:
	jmp	.LBB38_423
.LBB38_417:
.Ltmp4572:
.LBB38_418:
	movq	%rax, %rbx
.LBB38_419:
	leaq	160(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB38_420:
	leaq	16(%rsp), %rdi
	jmp	.LBB38_425
.LBB38_421:
.Ltmp4556:
	movq	%rax, %rbx
	jmp	.LBB38_420
.LBB38_422:
.Ltmp4537:
.LBB38_423:
	movq	%rax, %rbx
.LBB38_424:
	leaq	112(%rsp), %rdi
.LBB38_425:
	callq	_ZN4mpfr6mprealD2Ev
.LBB38_426:
	leaq	48(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end38:
	.size	_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_, .Lfunc_end38-_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.cfi_endproc
	.section	.gcc_except_table._Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"aG",@progbits,_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.p2align	2, 0x0
GCC_except_table38:
.Lexception31:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase20-.Lttbaseref20
.Lttbaseref20:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end31-.Lcst_begin31
.Lcst_begin31:
	.uleb128 .Lfunc_begin31-.Lfunc_begin31  # >> Call Site 1 <<
	.uleb128 .Ltmp4332-.Lfunc_begin31       #   Call between .Lfunc_begin31 and .Ltmp4332
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4332-.Lfunc_begin31       # >> Call Site 2 <<
	.uleb128 .Ltmp4341-.Ltmp4332            #   Call between .Ltmp4332 and .Ltmp4341
	.uleb128 .Ltmp4342-.Lfunc_begin31       #     jumps to .Ltmp4342
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4343-.Lfunc_begin31       # >> Call Site 3 <<
	.uleb128 .Ltmp4346-.Ltmp4343            #   Call between .Ltmp4343 and .Ltmp4346
	.uleb128 .Ltmp4347-.Lfunc_begin31       #     jumps to .Ltmp4347
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4348-.Lfunc_begin31       # >> Call Site 4 <<
	.uleb128 .Ltmp4357-.Ltmp4348            #   Call between .Ltmp4348 and .Ltmp4357
	.uleb128 .Ltmp4358-.Lfunc_begin31       #     jumps to .Ltmp4358
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4359-.Lfunc_begin31       # >> Call Site 5 <<
	.uleb128 .Ltmp4362-.Ltmp4359            #   Call between .Ltmp4359 and .Ltmp4362
	.uleb128 .Ltmp4363-.Lfunc_begin31       #     jumps to .Ltmp4363
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4364-.Lfunc_begin31       # >> Call Site 6 <<
	.uleb128 .Ltmp4373-.Ltmp4364            #   Call between .Ltmp4364 and .Ltmp4373
	.uleb128 .Ltmp4374-.Lfunc_begin31       #     jumps to .Ltmp4374
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4375-.Lfunc_begin31       # >> Call Site 7 <<
	.uleb128 .Ltmp4376-.Ltmp4375            #   Call between .Ltmp4375 and .Ltmp4376
	.uleb128 .Ltmp4377-.Lfunc_begin31       #     jumps to .Ltmp4377
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4378-.Lfunc_begin31       # >> Call Site 8 <<
	.uleb128 .Ltmp4379-.Ltmp4378            #   Call between .Ltmp4378 and .Ltmp4379
	.uleb128 .Ltmp4380-.Lfunc_begin31       #     jumps to .Ltmp4380
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4381-.Lfunc_begin31       # >> Call Site 9 <<
	.uleb128 .Ltmp4382-.Ltmp4381            #   Call between .Ltmp4381 and .Ltmp4382
	.uleb128 .Ltmp4383-.Lfunc_begin31       #     jumps to .Ltmp4383
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4384-.Lfunc_begin31       # >> Call Site 10 <<
	.uleb128 .Ltmp4385-.Ltmp4384            #   Call between .Ltmp4384 and .Ltmp4385
	.uleb128 .Ltmp4386-.Lfunc_begin31       #     jumps to .Ltmp4386
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4387-.Lfunc_begin31       # >> Call Site 11 <<
	.uleb128 .Ltmp4388-.Ltmp4387            #   Call between .Ltmp4387 and .Ltmp4388
	.uleb128 .Ltmp4389-.Lfunc_begin31       #     jumps to .Ltmp4389
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4390-.Lfunc_begin31       # >> Call Site 12 <<
	.uleb128 .Ltmp4391-.Ltmp4390            #   Call between .Ltmp4390 and .Ltmp4391
	.uleb128 .Ltmp4392-.Lfunc_begin31       #     jumps to .Ltmp4392
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4393-.Lfunc_begin31       # >> Call Site 13 <<
	.uleb128 .Ltmp4394-.Ltmp4393            #   Call between .Ltmp4393 and .Ltmp4394
	.uleb128 .Ltmp4395-.Lfunc_begin31       #     jumps to .Ltmp4395
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4394-.Lfunc_begin31       # >> Call Site 14 <<
	.uleb128 .Ltmp4396-.Ltmp4394            #   Call between .Ltmp4394 and .Ltmp4396
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4396-.Lfunc_begin31       # >> Call Site 15 <<
	.uleb128 .Ltmp4405-.Ltmp4396            #   Call between .Ltmp4396 and .Ltmp4405
	.uleb128 .Ltmp4406-.Lfunc_begin31       #     jumps to .Ltmp4406
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4407-.Lfunc_begin31       # >> Call Site 16 <<
	.uleb128 .Ltmp4410-.Ltmp4407            #   Call between .Ltmp4407 and .Ltmp4410
	.uleb128 .Ltmp4411-.Lfunc_begin31       #     jumps to .Ltmp4411
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4412-.Lfunc_begin31       # >> Call Site 17 <<
	.uleb128 .Ltmp4421-.Ltmp4412            #   Call between .Ltmp4412 and .Ltmp4421
	.uleb128 .Ltmp4422-.Lfunc_begin31       #     jumps to .Ltmp4422
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4423-.Lfunc_begin31       # >> Call Site 18 <<
	.uleb128 .Ltmp4426-.Ltmp4423            #   Call between .Ltmp4423 and .Ltmp4426
	.uleb128 .Ltmp4427-.Lfunc_begin31       #     jumps to .Ltmp4427
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4428-.Lfunc_begin31       # >> Call Site 19 <<
	.uleb128 .Ltmp4437-.Ltmp4428            #   Call between .Ltmp4428 and .Ltmp4437
	.uleb128 .Ltmp4438-.Lfunc_begin31       #     jumps to .Ltmp4438
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4439-.Lfunc_begin31       # >> Call Site 20 <<
	.uleb128 .Ltmp4440-.Ltmp4439            #   Call between .Ltmp4439 and .Ltmp4440
	.uleb128 .Ltmp4441-.Lfunc_begin31       #     jumps to .Ltmp4441
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4442-.Lfunc_begin31       # >> Call Site 21 <<
	.uleb128 .Ltmp4443-.Ltmp4442            #   Call between .Ltmp4442 and .Ltmp4443
	.uleb128 .Ltmp4444-.Lfunc_begin31       #     jumps to .Ltmp4444
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4445-.Lfunc_begin31       # >> Call Site 22 <<
	.uleb128 .Ltmp4446-.Ltmp4445            #   Call between .Ltmp4445 and .Ltmp4446
	.uleb128 .Ltmp4447-.Lfunc_begin31       #     jumps to .Ltmp4447
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4448-.Lfunc_begin31       # >> Call Site 23 <<
	.uleb128 .Ltmp4449-.Ltmp4448            #   Call between .Ltmp4448 and .Ltmp4449
	.uleb128 .Ltmp4450-.Lfunc_begin31       #     jumps to .Ltmp4450
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4451-.Lfunc_begin31       # >> Call Site 24 <<
	.uleb128 .Ltmp4452-.Ltmp4451            #   Call between .Ltmp4451 and .Ltmp4452
	.uleb128 .Ltmp4453-.Lfunc_begin31       #     jumps to .Ltmp4453
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4454-.Lfunc_begin31       # >> Call Site 25 <<
	.uleb128 .Ltmp4455-.Ltmp4454            #   Call between .Ltmp4454 and .Ltmp4455
	.uleb128 .Ltmp4456-.Lfunc_begin31       #     jumps to .Ltmp4456
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4457-.Lfunc_begin31       # >> Call Site 26 <<
	.uleb128 .Ltmp4458-.Ltmp4457            #   Call between .Ltmp4457 and .Ltmp4458
	.uleb128 .Ltmp4459-.Lfunc_begin31       #     jumps to .Ltmp4459
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4458-.Lfunc_begin31       # >> Call Site 27 <<
	.uleb128 .Ltmp4460-.Ltmp4458            #   Call between .Ltmp4458 and .Ltmp4460
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4460-.Lfunc_begin31       # >> Call Site 28 <<
	.uleb128 .Ltmp4469-.Ltmp4460            #   Call between .Ltmp4460 and .Ltmp4469
	.uleb128 .Ltmp4470-.Lfunc_begin31       #     jumps to .Ltmp4470
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4471-.Lfunc_begin31       # >> Call Site 29 <<
	.uleb128 .Ltmp4474-.Ltmp4471            #   Call between .Ltmp4471 and .Ltmp4474
	.uleb128 .Ltmp4475-.Lfunc_begin31       #     jumps to .Ltmp4475
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4476-.Lfunc_begin31       # >> Call Site 30 <<
	.uleb128 .Ltmp4485-.Ltmp4476            #   Call between .Ltmp4476 and .Ltmp4485
	.uleb128 .Ltmp4486-.Lfunc_begin31       #     jumps to .Ltmp4486
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4487-.Lfunc_begin31       # >> Call Site 31 <<
	.uleb128 .Ltmp4490-.Ltmp4487            #   Call between .Ltmp4487 and .Ltmp4490
	.uleb128 .Ltmp4491-.Lfunc_begin31       #     jumps to .Ltmp4491
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4492-.Lfunc_begin31       # >> Call Site 32 <<
	.uleb128 .Ltmp4501-.Ltmp4492            #   Call between .Ltmp4492 and .Ltmp4501
	.uleb128 .Ltmp4502-.Lfunc_begin31       #     jumps to .Ltmp4502
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4503-.Lfunc_begin31       # >> Call Site 33 <<
	.uleb128 .Ltmp4504-.Ltmp4503            #   Call between .Ltmp4503 and .Ltmp4504
	.uleb128 .Ltmp4505-.Lfunc_begin31       #     jumps to .Ltmp4505
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4506-.Lfunc_begin31       # >> Call Site 34 <<
	.uleb128 .Ltmp4507-.Ltmp4506            #   Call between .Ltmp4506 and .Ltmp4507
	.uleb128 .Ltmp4508-.Lfunc_begin31       #     jumps to .Ltmp4508
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4509-.Lfunc_begin31       # >> Call Site 35 <<
	.uleb128 .Ltmp4510-.Ltmp4509            #   Call between .Ltmp4509 and .Ltmp4510
	.uleb128 .Ltmp4511-.Lfunc_begin31       #     jumps to .Ltmp4511
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4512-.Lfunc_begin31       # >> Call Site 36 <<
	.uleb128 .Ltmp4513-.Ltmp4512            #   Call between .Ltmp4512 and .Ltmp4513
	.uleb128 .Ltmp4514-.Lfunc_begin31       #     jumps to .Ltmp4514
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4515-.Lfunc_begin31       # >> Call Site 37 <<
	.uleb128 .Ltmp4516-.Ltmp4515            #   Call between .Ltmp4515 and .Ltmp4516
	.uleb128 .Ltmp4517-.Lfunc_begin31       #     jumps to .Ltmp4517
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4518-.Lfunc_begin31       # >> Call Site 38 <<
	.uleb128 .Ltmp4519-.Ltmp4518            #   Call between .Ltmp4518 and .Ltmp4519
	.uleb128 .Ltmp4520-.Lfunc_begin31       #     jumps to .Ltmp4520
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4521-.Lfunc_begin31       # >> Call Site 39 <<
	.uleb128 .Ltmp4522-.Ltmp4521            #   Call between .Ltmp4521 and .Ltmp4522
	.uleb128 .Ltmp4523-.Lfunc_begin31       #     jumps to .Ltmp4523
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4522-.Lfunc_begin31       # >> Call Site 40 <<
	.uleb128 .Ltmp4524-.Ltmp4522            #   Call between .Ltmp4522 and .Ltmp4524
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4524-.Lfunc_begin31       # >> Call Site 41 <<
	.uleb128 .Ltmp4525-.Ltmp4524            #   Call between .Ltmp4524 and .Ltmp4525
	.uleb128 .Ltmp4526-.Lfunc_begin31       #     jumps to .Ltmp4526
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4527-.Lfunc_begin31       # >> Call Site 42 <<
	.uleb128 .Ltmp4536-.Ltmp4527            #   Call between .Ltmp4527 and .Ltmp4536
	.uleb128 .Ltmp4537-.Lfunc_begin31       #     jumps to .Ltmp4537
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4538-.Lfunc_begin31       # >> Call Site 43 <<
	.uleb128 .Ltmp4541-.Ltmp4538            #   Call between .Ltmp4538 and .Ltmp4541
	.uleb128 .Ltmp4542-.Lfunc_begin31       #     jumps to .Ltmp4542
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4543-.Lfunc_begin31       # >> Call Site 44 <<
	.uleb128 .Ltmp4544-.Ltmp4543            #   Call between .Ltmp4543 and .Ltmp4544
	.uleb128 .Ltmp4545-.Lfunc_begin31       #     jumps to .Ltmp4545
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4546-.Lfunc_begin31       # >> Call Site 45 <<
	.uleb128 .Ltmp4555-.Ltmp4546            #   Call between .Ltmp4546 and .Ltmp4555
	.uleb128 .Ltmp4556-.Lfunc_begin31       #     jumps to .Ltmp4556
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4557-.Lfunc_begin31       # >> Call Site 46 <<
	.uleb128 .Ltmp4560-.Ltmp4557            #   Call between .Ltmp4557 and .Ltmp4560
	.uleb128 .Ltmp4561-.Lfunc_begin31       #     jumps to .Ltmp4561
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4562-.Lfunc_begin31       # >> Call Site 47 <<
	.uleb128 .Ltmp4571-.Ltmp4562            #   Call between .Ltmp4562 and .Ltmp4571
	.uleb128 .Ltmp4572-.Lfunc_begin31       #     jumps to .Ltmp4572
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4573-.Lfunc_begin31       # >> Call Site 48 <<
	.uleb128 .Ltmp4576-.Ltmp4573            #   Call between .Ltmp4573 and .Ltmp4576
	.uleb128 .Ltmp4577-.Lfunc_begin31       #     jumps to .Ltmp4577
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4578-.Lfunc_begin31       # >> Call Site 49 <<
	.uleb128 .Ltmp4589-.Ltmp4578            #   Call between .Ltmp4578 and .Ltmp4589
	.uleb128 .Ltmp4590-.Lfunc_begin31       #     jumps to .Ltmp4590
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4591-.Lfunc_begin31       # >> Call Site 50 <<
	.uleb128 .Ltmp4592-.Ltmp4591            #   Call between .Ltmp4591 and .Ltmp4592
	.uleb128 .Ltmp4593-.Lfunc_begin31       #     jumps to .Ltmp4593
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4594-.Lfunc_begin31       # >> Call Site 51 <<
	.uleb128 .Ltmp4595-.Ltmp4594            #   Call between .Ltmp4594 and .Ltmp4595
	.uleb128 .Ltmp4596-.Lfunc_begin31       #     jumps to .Ltmp4596
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4597-.Lfunc_begin31       # >> Call Site 52 <<
	.uleb128 .Ltmp4598-.Ltmp4597            #   Call between .Ltmp4597 and .Ltmp4598
	.uleb128 .Ltmp4599-.Lfunc_begin31       #     jumps to .Ltmp4599
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4600-.Lfunc_begin31       # >> Call Site 53 <<
	.uleb128 .Ltmp4609-.Ltmp4600            #   Call between .Ltmp4600 and .Ltmp4609
	.uleb128 .Ltmp4610-.Lfunc_begin31       #     jumps to .Ltmp4610
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4611-.Lfunc_begin31       # >> Call Site 54 <<
	.uleb128 .Ltmp4612-.Ltmp4611            #   Call between .Ltmp4611 and .Ltmp4612
	.uleb128 .Ltmp4613-.Lfunc_begin31       #     jumps to .Ltmp4613
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4614-.Lfunc_begin31       # >> Call Site 55 <<
	.uleb128 .Ltmp4625-.Ltmp4614            #   Call between .Ltmp4614 and .Ltmp4625
	.uleb128 .Ltmp4626-.Lfunc_begin31       #     jumps to .Ltmp4626
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4627-.Lfunc_begin31       # >> Call Site 56 <<
	.uleb128 .Ltmp4628-.Ltmp4627            #   Call between .Ltmp4627 and .Ltmp4628
	.uleb128 .Ltmp4629-.Lfunc_begin31       #     jumps to .Ltmp4629
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4630-.Lfunc_begin31       # >> Call Site 57 <<
	.uleb128 .Ltmp4649-.Ltmp4630            #   Call between .Ltmp4630 and .Ltmp4649
	.uleb128 .Ltmp4650-.Lfunc_begin31       #     jumps to .Ltmp4650
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4651-.Lfunc_begin31       # >> Call Site 58 <<
	.uleb128 .Ltmp4660-.Ltmp4651            #   Call between .Ltmp4651 and .Ltmp4660
	.uleb128 .Ltmp4661-.Lfunc_begin31       #     jumps to .Ltmp4661
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4662-.Lfunc_begin31       # >> Call Site 59 <<
	.uleb128 .Ltmp4665-.Ltmp4662            #   Call between .Ltmp4662 and .Ltmp4665
	.uleb128 .Ltmp4666-.Lfunc_begin31       #     jumps to .Ltmp4666
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4667-.Lfunc_begin31       # >> Call Site 60 <<
	.uleb128 .Ltmp4668-.Ltmp4667            #   Call between .Ltmp4667 and .Ltmp4668
	.uleb128 .Ltmp4669-.Lfunc_begin31       #     jumps to .Ltmp4669
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4670-.Lfunc_begin31       # >> Call Site 61 <<
	.uleb128 .Ltmp4671-.Ltmp4670            #   Call between .Ltmp4670 and .Ltmp4671
	.uleb128 .Ltmp4672-.Lfunc_begin31       #     jumps to .Ltmp4672
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4673-.Lfunc_begin31       # >> Call Site 62 <<
	.uleb128 .Ltmp4674-.Ltmp4673            #   Call between .Ltmp4673 and .Ltmp4674
	.uleb128 .Ltmp4675-.Lfunc_begin31       #     jumps to .Ltmp4675
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4676-.Lfunc_begin31       # >> Call Site 63 <<
	.uleb128 .Ltmp4677-.Ltmp4676            #   Call between .Ltmp4676 and .Ltmp4677
	.uleb128 .Ltmp4678-.Lfunc_begin31       #     jumps to .Ltmp4678
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4677-.Lfunc_begin31       # >> Call Site 64 <<
	.uleb128 .Ltmp4679-.Ltmp4677            #   Call between .Ltmp4677 and .Ltmp4679
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4679-.Lfunc_begin31       # >> Call Site 65 <<
	.uleb128 .Ltmp4680-.Ltmp4679            #   Call between .Ltmp4679 and .Ltmp4680
	.uleb128 .Ltmp4681-.Lfunc_begin31       #     jumps to .Ltmp4681
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4682-.Lfunc_begin31       # >> Call Site 66 <<
	.uleb128 .Ltmp4691-.Ltmp4682            #   Call between .Ltmp4682 and .Ltmp4691
	.uleb128 .Ltmp4692-.Lfunc_begin31       #     jumps to .Ltmp4692
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4693-.Lfunc_begin31       # >> Call Site 67 <<
	.uleb128 .Ltmp4696-.Ltmp4693            #   Call between .Ltmp4693 and .Ltmp4696
	.uleb128 .Ltmp4697-.Lfunc_begin31       #     jumps to .Ltmp4697
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4698-.Lfunc_begin31       # >> Call Site 68 <<
	.uleb128 .Ltmp4699-.Ltmp4698            #   Call between .Ltmp4698 and .Ltmp4699
	.uleb128 .Ltmp4700-.Lfunc_begin31       #     jumps to .Ltmp4700
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4701-.Lfunc_begin31       # >> Call Site 69 <<
	.uleb128 .Ltmp4710-.Ltmp4701            #   Call between .Ltmp4701 and .Ltmp4710
	.uleb128 .Ltmp4711-.Lfunc_begin31       #     jumps to .Ltmp4711
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4712-.Lfunc_begin31       # >> Call Site 70 <<
	.uleb128 .Ltmp4715-.Ltmp4712            #   Call between .Ltmp4712 and .Ltmp4715
	.uleb128 .Ltmp4716-.Lfunc_begin31       #     jumps to .Ltmp4716
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4717-.Lfunc_begin31       # >> Call Site 71 <<
	.uleb128 .Ltmp4726-.Ltmp4717            #   Call between .Ltmp4717 and .Ltmp4726
	.uleb128 .Ltmp4727-.Lfunc_begin31       #     jumps to .Ltmp4727
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4728-.Lfunc_begin31       # >> Call Site 72 <<
	.uleb128 .Ltmp4731-.Ltmp4728            #   Call between .Ltmp4728 and .Ltmp4731
	.uleb128 .Ltmp4732-.Lfunc_begin31       #     jumps to .Ltmp4732
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4733-.Lfunc_begin31       # >> Call Site 73 <<
	.uleb128 .Ltmp4744-.Ltmp4733            #   Call between .Ltmp4733 and .Ltmp4744
	.uleb128 .Ltmp4745-.Lfunc_begin31       #     jumps to .Ltmp4745
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4746-.Lfunc_begin31       # >> Call Site 74 <<
	.uleb128 .Ltmp4747-.Ltmp4746            #   Call between .Ltmp4746 and .Ltmp4747
	.uleb128 .Ltmp4748-.Lfunc_begin31       #     jumps to .Ltmp4748
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4749-.Lfunc_begin31       # >> Call Site 75 <<
	.uleb128 .Ltmp4750-.Ltmp4749            #   Call between .Ltmp4749 and .Ltmp4750
	.uleb128 .Ltmp4751-.Lfunc_begin31       #     jumps to .Ltmp4751
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4752-.Lfunc_begin31       # >> Call Site 76 <<
	.uleb128 .Ltmp4753-.Ltmp4752            #   Call between .Ltmp4752 and .Ltmp4753
	.uleb128 .Ltmp4754-.Lfunc_begin31       #     jumps to .Ltmp4754
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4755-.Lfunc_begin31       # >> Call Site 77 <<
	.uleb128 .Ltmp4764-.Ltmp4755            #   Call between .Ltmp4755 and .Ltmp4764
	.uleb128 .Ltmp4765-.Lfunc_begin31       #     jumps to .Ltmp4765
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4766-.Lfunc_begin31       # >> Call Site 78 <<
	.uleb128 .Ltmp4767-.Ltmp4766            #   Call between .Ltmp4766 and .Ltmp4767
	.uleb128 .Ltmp4768-.Lfunc_begin31       #     jumps to .Ltmp4768
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4769-.Lfunc_begin31       # >> Call Site 79 <<
	.uleb128 .Ltmp4780-.Ltmp4769            #   Call between .Ltmp4769 and .Ltmp4780
	.uleb128 .Ltmp4781-.Lfunc_begin31       #     jumps to .Ltmp4781
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4782-.Lfunc_begin31       # >> Call Site 80 <<
	.uleb128 .Ltmp4783-.Ltmp4782            #   Call between .Ltmp4782 and .Ltmp4783
	.uleb128 .Ltmp4784-.Lfunc_begin31       #     jumps to .Ltmp4784
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4785-.Lfunc_begin31       # >> Call Site 81 <<
	.uleb128 .Ltmp4804-.Ltmp4785            #   Call between .Ltmp4785 and .Ltmp4804
	.uleb128 .Ltmp4805-.Lfunc_begin31       #     jumps to .Ltmp4805
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4806-.Lfunc_begin31       # >> Call Site 82 <<
	.uleb128 .Ltmp4815-.Ltmp4806            #   Call between .Ltmp4806 and .Ltmp4815
	.uleb128 .Ltmp4816-.Lfunc_begin31       #     jumps to .Ltmp4816
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4817-.Lfunc_begin31       # >> Call Site 83 <<
	.uleb128 .Ltmp4820-.Ltmp4817            #   Call between .Ltmp4817 and .Ltmp4820
	.uleb128 .Ltmp4821-.Lfunc_begin31       #     jumps to .Ltmp4821
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4822-.Lfunc_begin31       # >> Call Site 84 <<
	.uleb128 .Ltmp4823-.Ltmp4822            #   Call between .Ltmp4822 and .Ltmp4823
	.uleb128 .Ltmp4824-.Lfunc_begin31       #     jumps to .Ltmp4824
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4825-.Lfunc_begin31       # >> Call Site 85 <<
	.uleb128 .Ltmp4826-.Ltmp4825            #   Call between .Ltmp4825 and .Ltmp4826
	.uleb128 .Ltmp4827-.Lfunc_begin31       #     jumps to .Ltmp4827
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4828-.Lfunc_begin31       # >> Call Site 86 <<
	.uleb128 .Ltmp4829-.Ltmp4828            #   Call between .Ltmp4828 and .Ltmp4829
	.uleb128 .Ltmp4830-.Lfunc_begin31       #     jumps to .Ltmp4830
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4831-.Lfunc_begin31       # >> Call Site 87 <<
	.uleb128 .Ltmp4832-.Ltmp4831            #   Call between .Ltmp4831 and .Ltmp4832
	.uleb128 .Ltmp4833-.Lfunc_begin31       #     jumps to .Ltmp4833
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4834-.Lfunc_begin31       # >> Call Site 88 <<
	.uleb128 .Ltmp4835-.Ltmp4834            #   Call between .Ltmp4834 and .Ltmp4835
	.uleb128 .Ltmp4836-.Lfunc_begin31       #     jumps to .Ltmp4836
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4837-.Lfunc_begin31       # >> Call Site 89 <<
	.uleb128 .Ltmp4838-.Ltmp4837            #   Call between .Ltmp4837 and .Ltmp4838
	.uleb128 .Ltmp4839-.Lfunc_begin31       #     jumps to .Ltmp4839
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4838-.Lfunc_begin31       # >> Call Site 90 <<
	.uleb128 .Lfunc_end38-.Ltmp4838         #   Call between .Ltmp4838 and .Lfunc_end38
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end31:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase20:
	.p2align	2, 0x0
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2, 0x0                          # -- Begin function _Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.LCPI39_0:
	.long	0x3f800000                      #  1
.LCPI39_1:
	.long	0x3f000000                      #  0.5
.LCPI39_2:
	.long	0x80000000                      #  -0
.LCPI39_3:
	.long	0x3f800001                      #  1.00000012
.LCPI39_4:
	.long	0xbf800001                      #  -1.00000012
.LCPI39_5:
	.long	0x3f7ffffe                      #  0.99999988
.LCPI39_6:
	.long	0x40000000                      #  2
.LCPI39_8:
	.long	0x00000000                      #  0
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4, 0x0
.LCPI39_7:
	.zero	16
	.section	.text._Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"axG",@progbits,_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.weak	_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.p2align	4, 0x90
	.type	_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,@function
_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_: # 
.Lfunc_begin32:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception32
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$472, %rsp                      # imm = 0x1D8
	.cfi_def_cfa_offset 528
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, 176(%rsp)                  # 8-byte Spill
	movq	%rcx, %r15
	movq	%rdx, %r12
	movq	%rsi, 96(%rsp)                  # 8-byte Spill
	movq	%rdi, %rbx
	vcvtsi2sdl	(%rdi), %xmm0, %xmm0
	callq	sqrt
	vcvttsd2si	%xmm0, %eax
	movl	%eax, 4(%rsp)
	movq	%rbx, 8(%rsp)                   # 8-byte Spill
	movslq	(%rbx), %r13
	movl	$12, %ecx
	movq	%r13, %rax
	mulq	%rcx
	movq	%rax, %rbp
	movq	$-1, %r14
	cmovnoq	%rax, %r14
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %rbx
	movabsq	$-6148914691236517205, %rcx     # imm = 0xAAAAAAAAAAAAAAAB
	testq	%r13, %r13
	movq	%r15, 296(%rsp)                 # 8-byte Spill
	je	.LBB39_4
# %bb.1:
	addq	$-12, %rbp
	movq	%rbp, %rax
	mulq	%rcx
	shrq	$3, %rdx
	leaq	(%rdx,%rdx,2), %rax
	leaq	12(,%rax,4), %rbp
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movq	%rbp, %rdx
	callq	_intel_fast_memset@PLT
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 184(%rsp)                 # 8-byte Spill
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%rbp, %rdx
	callq	_intel_fast_memset@PLT
	testl	%r13d, %r13d
	jle	.LBB39_5
# %bb.2:
	movl	$8, %ebp
	movq	%r15, %r13
	xorl	%r15d, %r15d
	movq	184(%rsp), %r14                 # 8-byte Reload
	.p2align	4, 0x90
.LBB39_3:                               # =>This Inner Loop Header: Depth=1
	movq	%r12, %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, -8(%rbx,%rbp)
	vmovss	%xmm1, (%rbx,%rbp)
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, -8(%r14,%rbp)
	vmovss	%xmm1, (%r14,%rbp)
	incq	%r15
	movq	8(%rsp), %rax                   # 8-byte Reload
	movslq	(%rax), %rax
	addq	$12, %rbp
	addq	$32, %r13
	addq	$32, %r12
	cmpq	%rax, %r15
	jl	.LBB39_3
	jmp	.LBB39_5
.LBB39_4:
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 184(%rsp)                 # 8-byte Spill
.LBB39_5:
	movq	96(%rsp), %rdi                  # 8-byte Reload
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, 80(%rsp)
	vmovss	%xmm1, 88(%rsp)
	movl	$120, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vmovupd	%ymm0, 64(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovlpd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 96(%rax)
	vzeroupper
	callq	omp_get_wtime
	vxorps	%xmm15, %xmm15, %xmm15
	vmovsd	%xmm0, 96(%rsp)                 # 8-byte Spill
	movq	8(%rsp), %rax                   # 8-byte Reload
	movl	(%rax), %eax
	movq	%rax, %rcx
	shlq	$2, %rcx
	leaq	(%rcx,%rcx,2), %rcx
	xorl	%edx, %edx
	xorl	%esi, %esi
	movq	184(%rsp), %r8                  # 8-byte Reload
	jmp	.LBB39_7
	.p2align	4, 0x90
.LBB39_6:                               #   in Loop: Header=BB39_7 Depth=1
	leaq	(%rsi,%rsi,2), %rdi
	vmovlps	%xmm1, (%rbp,%rdi,4)
	vmovss	%xmm0, 8(%rbp,%rdi,4)
	incq	%rsi
	cmpq	$10, %rsi
	je	.LBB39_10
.LBB39_7:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB39_9 Depth 2
	vxorps	%xmm9, %xmm9, %xmm9
	vcvtsi2ss	%edx, %xmm9, %xmm0
	vblendps	$1, %xmm0, %xmm15, %xmm1        # xmm1 = xmm0[0],xmm15[1,2,3]
	vxorps	%xmm0, %xmm0, %xmm0
	testl	%eax, %eax
	jle	.LBB39_6
# %bb.8:                                #   in Loop: Header=BB39_7 Depth=1
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB39_9:                               #   Parent Loop BB39_7 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%r8,%rdi), %xmm6               # xmm6 = mem[0],zero,zero,zero
	vmovss	4(%r8,%rdi), %xmm4              # xmm4 = mem[0],zero,zero,zero
	vmovss	(%rbx,%rdi), %xmm3              # xmm3 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%rdi), %xmm5             # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm6, %xmm3, %xmm2
	vmulss	%xmm6, %xmm5, %xmm7
	vmovaps	%xmm6, %xmm8
	vfmsub213ss	%xmm7, %xmm5, %xmm8     # xmm8 = (xmm5 * xmm8) - xmm7
	vfmadd231ss	8(%rbx,%rdi), %xmm6, %xmm8 # xmm8 = (xmm6 * mem) + xmm8
	vfmsub213ss	%xmm2, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm6) - xmm2
	vmulss	%xmm4, %xmm3, %xmm9
	vaddss	%xmm6, %xmm9, %xmm10
	vsubss	%xmm6, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm6, %xmm6
	vaddss	%xmm7, %xmm10, %xmm12
	vsubss	%xmm10, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm10, %xmm10
	vsubss	%xmm13, %xmm7, %xmm7
	vaddss	%xmm7, %xmm10, %xmm7
	vfmadd231ss	%xmm4, %xmm5, %xmm7     # xmm7 = (xmm5 * xmm4) + xmm7
	vfmsub213ss	%xmm9, %xmm3, %xmm4     # xmm4 = (xmm3 * xmm4) - xmm9
	vsubss	%xmm11, %xmm9, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vfmadd231ss	8(%r8,%rdi), %xmm3, %xmm4 # xmm4 = (xmm3 * mem) + xmm4
	vaddss	%xmm7, %xmm5, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vaddss	%xmm3, %xmm12, %xmm4
	vsubss	%xmm4, %xmm12, %xmm5
	vaddss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm5, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm1, %xmm7
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm7, %xmm5
	vmovshdup	%xmm1, %xmm1            # xmm1 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm6
	vsubss	%xmm1, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vsubss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm5, %xmm6, %xmm2
	vsubss	%xmm6, %xmm2, %xmm7
	vsubss	%xmm7, %xmm2, %xmm8
	vsubss	%xmm8, %xmm6, %xmm6
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vinsertps	$16, %xmm1, %xmm2, %xmm1 # xmm1 = xmm2[0],xmm1[0],xmm2[2,3]
	addq	$12, %rdi
	cmpq	%rdi, %rcx
	jne	.LBB39_9
	jmp	.LBB39_6
.LBB39_10:
	callq	omp_get_wtime
	vsubsd	96(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vxorps	%xmm9, %xmm9, %xmm9
	vcvtsi2sd	%eax, %xmm9, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	216(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp4840:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp4841:
# %bb.11:
.Ltmp4842:
	movq	%rax, %r12
	movq	176(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp4843:
# %bb.12:
.Ltmp4844:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4845:
# %bb.13:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp4846:
	leaq	136(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4847:
# %bb.14:
.Ltmp4848:
	leaq	136(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp4849:
# %bb.15:
.Ltmp4851:
	callq	mpfr_get_default_rounding_mode
.Ltmp4852:
# %bb.16:
.Ltmp4853:
	leaq	136(%rsp), %rdi
	leaq	216(%rsp), %rsi
	movq	176(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp4854:
# %bb.17:
.Ltmp4856:
	leaq	136(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4857:
# %bb.18:
.Ltmp4858:
	movq	%rax, %r12
	movq	176(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp4859:
# %bb.19:
.Ltmp4860:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4861:
# %bb.20:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp4862:
	leaq	16(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4863:
# %bb.21:
.Ltmp4864:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp4865:
# %bb.22:
.Ltmp4867:
	callq	mpfr_get_default_rounding_mode
.Ltmp4868:
# %bb.23:
.Ltmp4869:
	leaq	16(%rsp), %rdi
	leaq	136(%rsp), %rsi
	movq	176(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp4870:
# %bb.24:
.Ltmp4872:
	callq	mpfr_get_default_rounding_mode
.Ltmp4873:
# %bb.25:
.Ltmp4874:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4875:
# %bb.26:
.Ltmp4876:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4877:
# %bb.27:
.Ltmp4878:
	movl	%eax, %r12d
	leaq	48(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4879:
# %bb.28:
.Ltmp4880:
	leaq	48(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4881:
# %bb.29:
.Ltmp4883:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp4884:
# %bb.30:
.Ltmp4886:
	leaq	48(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp4887:
# %bb.31:
	vmovlpd	%xmm0, 248(%rsp)
	vmovss	%xmm1, 256(%rsp)
	cmpq	$0, 72(%rsp)
	je	.LBB39_33
# %bb.32:
.Ltmp4889:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4890:
.LBB39_33:
	cmpq	$0, 40(%rsp)
	je	.LBB39_35
# %bb.34:
.Ltmp4892:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4893:
.LBB39_35:
	cmpq	$0, 160(%rsp)
	je	.LBB39_37
# %bb.36:
.Ltmp4895:
	leaq	136(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4896:
.LBB39_37:
	cmpq	$0, 240(%rsp)
	je	.LBB39_39
# %bb.38:
.Ltmp4898:
	leaq	216(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4899:
.LBB39_39:
	leaq	456(%rsp), %r15
	movq	%r15, 440(%rsp)
	movl	$544501604, 456(%rsp)           # imm = 0x20746F64
	movw	$32, 460(%rsp)
	movq	$5, 448(%rsp)
.Ltmp4901:
	leaq	440(%rsp), %rdi
	leaq	248(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp4902:
# %bb.40:
	movq	440(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB39_42
# %bb.41:
	callq	_ZdlPv
.LBB39_42:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$120, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vmovupd	%ymm0, 64(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovlpd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 96(%rax)
	xorl	%r15d, %r15d
	vzeroupper
	callq	omp_get_wtime
	vmovsd	%xmm0, 304(%rsp)                # 8-byte Spill
	vxorps	%xmm10, %xmm10, %xmm10
	movl	$2, %r13d
	xorl	%r12d, %r12d
	jmp	.LBB39_45
	.p2align	4, 0x90
.LBB39_43:                              #   in Loop: Header=BB39_45 Depth=1
	vmovss	.LCPI39_1(%rip), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vmulss	%xmm6, %xmm4, %xmm5
	vmulss	%xmm4, %xmm3, %xmm2
	vmulss	%xmm5, %xmm2, %xmm2
	vmovshdup	%xmm3, %xmm3            # xmm3 = xmm3[1,1,3,3]
	vmulss	%xmm4, %xmm3, %xmm3
	vmulss	%xmm5, %xmm3, %xmm3
	vmovss	%xmm3, 132(%rsp)                # 4-byte Spill
	vmulss	%xmm4, %xmm8, %xmm4
	vmulss	%xmm5, %xmm4, %xmm3
	vmovss	%xmm3, 96(%rsp)                 # 4-byte Spill
	vmovd	%xmm7, 172(%rsp)                # 4-byte Folded Spill
	vdivss	%xmm0, %xmm7, %xmm7
	movl	$3, %eax
	vcvtsi2ss	%eax, %xmm13, %xmm0
	vmulss	%xmm6, %xmm0, %xmm5
	vfmsub213ss	%xmm5, %xmm0, %xmm6     # xmm6 = (xmm0 * xmm6) - xmm5
	vmulss	%xmm1, %xmm0, %xmm8
	vxorps	%xmm9, %xmm9, %xmm9
	vfmsub213ss	%xmm8, %xmm0, %xmm9     # xmm9 = (xmm0 * xmm9) - xmm8
	vaddss	%xmm6, %xmm8, %xmm10
	vsubss	%xmm6, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm6, %xmm6
	vsubss	%xmm11, %xmm8, %xmm8
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm6, %xmm9, %xmm6
	vfmadd231ss	.LCPI39_8(%rip), %xmm0, %xmm6 # xmm6 = (xmm0 * mem) + xmm6
	vaddss	%xmm6, %xmm10, %xmm8
	vsubss	%xmm8, %xmm10, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vmovss	%xmm0, 192(%rsp)                # 4-byte Spill
	vaddss	%xmm5, %xmm8, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm8, %xmm0
	vmovss	%xmm0, 120(%rsp)                # 4-byte Spill
	vmulss	%xmm7, %xmm7, %xmm9
	vmovaps	%xmm7, %xmm8
	vfmsub213ss	%xmm9, %xmm7, %xmm8     # xmm8 = (xmm7 * xmm8) - xmm9
	vmulss	.LCPI39_8(%rip), %xmm7, %xmm10
	vxorps	%xmm11, %xmm11, %xmm11
	vfmsub213ss	%xmm10, %xmm7, %xmm11   # xmm11 = (xmm7 * xmm11) - xmm10
	vmulss	.LCPI39_8(%rip), %xmm7, %xmm12
	vaddss	%xmm10, %xmm8, %xmm13
	vsubss	%xmm8, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm14, %xmm10, %xmm10
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm12, %xmm13, %xmm10
	vsubss	%xmm13, %xmm10, %xmm14
	vsubss	%xmm14, %xmm10, %xmm15
	vsubss	%xmm15, %xmm13, %xmm13
	vmovaps	%xmm7, %xmm15
	vfmsub132ss	.LCPI39_8(%rip), %xmm12, %xmm15 # xmm15 = (xmm15 * mem) - xmm12
	vsubss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm12, %xmm13, %xmm12
	vxorps	%xmm0, %xmm0, %xmm0
	vfmadd231ss	%xmm0, %xmm0, %xmm12    # xmm12 = (xmm0 * xmm0) + xmm12
	vaddss	%xmm12, %xmm8, %xmm8
	vfmadd231ss	.LCPI39_8(%rip), %xmm7, %xmm11 # xmm11 = (xmm7 * mem) + xmm11
	vaddss	%xmm11, %xmm8, %xmm8
	vfmadd231ss	.LCPI39_8(%rip), %xmm7, %xmm15 # xmm15 = (xmm7 * mem) + xmm15
	vaddss	%xmm15, %xmm8, %xmm8
	vaddss	%xmm8, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm8, %xmm10, %xmm8
	vaddss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm11, %xmm9, %xmm10
	vmulss	%xmm2, %xmm12, %xmm9
	vmovss	132(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm12, %xmm11
	vmovaps	%xmm12, %xmm13
	vfmsub213ss	%xmm11, %xmm5, %xmm13   # xmm13 = (xmm5 * xmm13) - xmm11
	vfmadd231ss	%xmm12, %xmm3, %xmm13   # xmm13 = (xmm3 * xmm12) + xmm13
	vfmsub213ss	%xmm9, %xmm2, %xmm12    # xmm12 = (xmm2 * xmm12) - xmm9
	vmulss	%xmm2, %xmm10, %xmm14
	vaddss	%xmm14, %xmm12, %xmm15
	vsubss	%xmm12, %xmm15, %xmm1
	vsubss	%xmm1, %xmm15, %xmm4
	vsubss	%xmm4, %xmm12, %xmm4
	vaddss	%xmm11, %xmm15, %xmm12
	vsubss	%xmm15, %xmm12, %xmm0
	vsubss	%xmm0, %xmm12, %xmm3
	vsubss	%xmm3, %xmm15, %xmm3
	vsubss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vfmadd231ss	%xmm10, %xmm5, %xmm0    # xmm0 = (xmm5 * xmm10) + xmm0
	vmovaps	%xmm5, %xmm15
	vfmsub213ss	%xmm14, %xmm2, %xmm10   # xmm10 = (xmm2 * xmm10) - xmm14
	vsubss	%xmm1, %xmm14, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vfmadd231ss	%xmm8, %xmm2, %xmm10    # xmm10 = (xmm2 * xmm8) + xmm10
	vmovaps	%xmm2, %xmm14
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm0, %xmm12, %xmm1
	vsubss	%xmm1, %xmm12, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm1, %xmm9, %xmm3
	vsubss	%xmm3, %xmm9, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vbroadcastss	.LCPI39_2(%rip), %xmm2  # xmm2 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm3, %xmm4
	vxorps	%xmm2, %xmm1, %xmm8
	vmovss	%xmm6, 260(%rsp)                # 4-byte Spill
	vsubss	%xmm3, %xmm6, %xmm3
	vsubss	%xmm6, %xmm3, %xmm9
	vsubss	%xmm9, %xmm3, %xmm10
	vsubss	%xmm10, %xmm6, %xmm10
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm4, %xmm10, %xmm4
	vmovss	120(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm9
	vsubss	%xmm9, %xmm1, %xmm10
	vsubss	%xmm10, %xmm5, %xmm10
	vsubss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm8, %xmm10, %xmm8
	vaddss	%xmm4, %xmm1, %xmm9
	vsubss	%xmm1, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vmovss	192(%rsp), %xmm2                # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm1
	vsubss	%xmm1, %xmm9, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm1, %xmm3, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vaddss	%xmm1, %xmm3, %xmm10
	vmulss	%xmm7, %xmm9, %xmm8
	vmulss	%xmm7, %xmm10, %xmm1
	vmovaps	%xmm7, %xmm3
	vfmsub213ss	%xmm1, %xmm10, %xmm3    # xmm3 = (xmm10 * xmm3) - xmm1
	vfmadd231ss	%xmm0, %xmm7, %xmm3     # xmm3 = (xmm7 * xmm0) + xmm3
	vfmsub213ss	%xmm8, %xmm9, %xmm7     # xmm7 = (xmm9 * xmm7) - xmm8
	vmulss	.LCPI39_8(%rip), %xmm9, %xmm0
	vaddss	%xmm0, %xmm7, %xmm4
	vsubss	%xmm7, %xmm4, %xmm11
	vsubss	%xmm11, %xmm4, %xmm12
	vsubss	%xmm12, %xmm7, %xmm7
	vxorps	%xmm12, %xmm12, %xmm12
	vfmsub213ss	%xmm0, %xmm9, %xmm12    # xmm12 = (xmm9 * xmm12) - xmm0
	vsubss	%xmm11, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm1, %xmm4, %xmm7
	vsubss	%xmm4, %xmm7, %xmm11
	vsubss	%xmm11, %xmm7, %xmm13
	vsubss	%xmm13, %xmm4, %xmm4
	vsubss	%xmm11, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vfmadd231ss	.LCPI39_8(%rip), %xmm10, %xmm1 # xmm1 = (xmm10 * mem) + xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vfmadd231ss	.LCPI39_8(%rip), %xmm9, %xmm12 # xmm12 = (xmm9 * mem) + xmm12
	vaddss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm3
	vaddss	%xmm0, %xmm3, %xmm2
	vaddss	%xmm1, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm0
	vaddss	%xmm1, %xmm0, %xmm8
	vmulss	%xmm9, %xmm9, %xmm0
	vmovaps	%xmm9, %xmm1
	vfmsub213ss	%xmm0, %xmm9, %xmm1     # xmm1 = (xmm9 * xmm1) - xmm0
	vmulss	%xmm8, %xmm9, %xmm3
	vaddss	%xmm3, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm10
	vsubss	%xmm10, %xmm4, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vmovaps	%xmm8, %xmm11
	vfmsub213ss	%xmm3, %xmm9, %xmm11    # xmm11 = (xmm9 * xmm11) - xmm3
	vsubss	%xmm10, %xmm3, %xmm3
	vmulss	%xmm9, %xmm8, %xmm10
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm4, %xmm10, %xmm3
	vsubss	%xmm4, %xmm3, %xmm12
	vsubss	%xmm12, %xmm3, %xmm13
	vsubss	%xmm13, %xmm4, %xmm4
	vmovaps	%xmm9, %xmm13
	vfmsub213ss	%xmm10, %xmm8, %xmm13   # xmm13 = (xmm8 * xmm13) - xmm10
	vsubss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm4, %xmm10, %xmm4
	vfmadd231ss	%xmm8, %xmm8, %xmm4     # xmm4 = (xmm8 * xmm8) + xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vmovss	%xmm2, 168(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm2, %xmm9, %xmm11    # xmm11 = (xmm9 * xmm2) + xmm11
	vaddss	%xmm1, %xmm11, %xmm1
	vfmadd231ss	%xmm9, %xmm2, %xmm13    # xmm13 = (xmm2 * xmm9) + xmm13
	vaddss	%xmm1, %xmm13, %xmm1
	vaddss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm1, %xmm3, %xmm10
	vaddss	%xmm4, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm12
	vmulss	%xmm1, %xmm14, %xmm11
	vmovaps	%xmm15, %xmm6
	vmulss	%xmm1, %xmm15, %xmm2
	vmovaps	%xmm1, %xmm3
	vfmsub213ss	%xmm2, %xmm15, %xmm3    # xmm3 = (xmm15 * xmm3) - xmm2
	vfmadd231ss	96(%rsp), %xmm1, %xmm3  # 4-byte Folded Reload
                                        # xmm3 = (xmm1 * mem) + xmm3
	vmovaps	%xmm14, %xmm5
	vfmsub213ss	%xmm11, %xmm14, %xmm1   # xmm1 = (xmm14 * xmm1) - xmm11
	vmulss	%xmm12, %xmm14, %xmm4
	vaddss	%xmm4, %xmm1, %xmm13
	vsubss	%xmm1, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm1, %xmm1
	vaddss	%xmm2, %xmm13, %xmm15
	vsubss	%xmm13, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm7
	vsubss	%xmm7, %xmm13, %xmm7
	vbroadcastss	.LCPI39_2(%rip), %xmm13 # xmm13 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vsubss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vfmadd231ss	%xmm12, %xmm6, %xmm0    # xmm0 = (xmm6 * xmm12) + xmm0
	vfmsub213ss	%xmm4, %xmm5, %xmm12    # xmm12 = (xmm5 * xmm12) - xmm4
	vsubss	%xmm14, %xmm4, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vfmadd231ss	%xmm10, %xmm5, %xmm12   # xmm12 = (xmm5 * xmm10) + xmm12
	vmovaps	%xmm5, %xmm14
	vaddss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm15, %xmm1
	vsubss	%xmm1, %xmm15, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm11, %xmm2
	vsubss	%xmm2, %xmm11, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vxorps	%xmm2, %xmm13, %xmm3
	vmovss	260(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm7
	vsubss	%xmm7, %xmm5, %xmm6
	vxorps	%xmm1, %xmm13, %xmm7
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vmovss	120(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm4
	vsubss	%xmm4, %xmm1, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm4, %xmm7, %xmm4
	vaddss	%xmm4, %xmm5, %xmm4
	vaddss	%xmm3, %xmm1, %xmm5
	vsubss	%xmm1, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm1, %xmm1
	vmovss	192(%rsp), %xmm3                # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm3
	vaddss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm1, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm0
	vaddss	%xmm1, %xmm0, %xmm6
	vmulss	%xmm5, %xmm9, %xmm0
	vmulss	%xmm6, %xmm9, %xmm1
	vmovaps	%xmm9, %xmm2
	vfmsub213ss	%xmm1, %xmm6, %xmm2     # xmm2 = (xmm6 * xmm2) - xmm1
	vfmadd231ss	%xmm3, %xmm9, %xmm2     # xmm2 = (xmm9 * xmm3) + xmm2
	vfmsub213ss	%xmm0, %xmm5, %xmm9     # xmm9 = (xmm5 * xmm9) - xmm0
	vmulss	%xmm5, %xmm8, %xmm3
	vaddss	%xmm3, %xmm9, %xmm4
	vsubss	%xmm9, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm10
	vsubss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm1, %xmm4, %xmm10
	vsubss	%xmm4, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm4, %xmm4
	vsubss	%xmm11, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vfmadd231ss	%xmm8, %xmm6, %xmm1     # xmm1 = (xmm6 * xmm8) + xmm1
	vfmsub213ss	%xmm3, %xmm5, %xmm8     # xmm8 = (xmm5 * xmm8) - xmm3
	vsubss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm9, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vfmadd231ss	168(%rsp), %xmm5, %xmm8 # 4-byte Folded Reload
                                        # xmm8 = (xmm5 * mem) + xmm8
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm10, %xmm2
	vsubss	%xmm2, %xmm10, %xmm3
	vxorps	%xmm10, %xmm10, %xmm10
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm2
	vmulss	%xmm3, %xmm14, %xmm0
	vmovss	132(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm6, %xmm4
	vmovaps	%xmm3, %xmm5
	vfmsub213ss	%xmm4, %xmm6, %xmm5     # xmm5 = (xmm6 * xmm5) - xmm4
	vmovaps	%xmm6, %xmm11
	vfmadd231ss	96(%rsp), %xmm3, %xmm5  # 4-byte Folded Reload
                                        # xmm5 = (xmm3 * mem) + xmm5
	vfmsub213ss	%xmm0, %xmm14, %xmm3    # xmm3 = (xmm14 * xmm3) - xmm0
	vmulss	%xmm2, %xmm14, %xmm6
	vaddss	%xmm6, %xmm3, %xmm7
	vaddss	%xmm4, %xmm7, %xmm8
	vsubss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm9, %xmm4, %xmm4
	vsubss	%xmm9, %xmm8, %xmm9
	vsubss	%xmm9, %xmm7, %xmm9
	vaddss	%xmm4, %xmm9, %xmm4
	vfmadd231ss	%xmm2, %xmm11, %xmm4    # xmm4 = (xmm11 * xmm2) + xmm4
	vfmsub213ss	%xmm6, %xmm14, %xmm2    # xmm2 = (xmm14 * xmm2) - xmm6
	vfmadd231ss	%xmm1, %xmm14, %xmm2    # xmm2 = (xmm14 * xmm1) + xmm2
	vsubss	%xmm3, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm1, %xmm6, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm8, %xmm2
	vsubss	%xmm2, %xmm8, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vxorps	%xmm15, %xmm15, %xmm15
	vcvtsi2ss	%r13d, %xmm15, %xmm2
	vmulss	172(%rsp), %xmm2, %xmm2         # 4-byte Folded Reload
	vmulss	%xmm2, %xmm3, %xmm3
	vmulss	%xmm2, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0],xmm0[0],xmm3[2,3]
	vmulss	%xmm2, %xmm1, %xmm11
.LBB39_44:                              #   in Loop: Header=BB39_45 Depth=1
	leaq	(%r12,%r12,2), %rax
	vmovlps	%xmm0, (%rbp,%rax,4)
	vmovss	%xmm11, 8(%rbp,%rax,4)
	incq	%r12
	cmpq	$10, %r12
	je	.LBB39_56
.LBB39_45:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB39_47 Depth 2
	vcvtsi2ss	%r15d, %xmm15, %xmm0
	vblendps	$14, .LCPI39_7(%rip), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[1,2,3]
	movq	8(%rsp), %rax                   # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	jle	.LBB39_48
# %bb.46:                               #   in Loop: Header=BB39_45 Depth=1
	shlq	$2, %rax
	leaq	(%rax,%rax,2), %rax
	vxorps	%xmm11, %xmm11, %xmm11
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB39_47:                              #   Parent Loop BB39_45 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rcx), %xmm3              # xmm3 = mem[0],zero,zero,zero
	vmovss	4(%rbx,%rcx), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm3, %xmm12
	vmulss	%xmm1, %xmm3, %xmm4
	vmovaps	%xmm1, %xmm2
	vfmsub213ss	%xmm4, %xmm3, %xmm2     # xmm2 = (xmm3 * xmm2) - xmm4
	vmulss	%xmm3, %xmm1, %xmm5
	vmovaps	%xmm3, %xmm6
	vfmsub213ss	%xmm5, %xmm1, %xmm6     # xmm6 = (xmm1 * xmm6) - xmm5
	vmovss	8(%rbx,%rcx), %xmm7             # xmm7 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm7, %xmm3, %xmm2     # xmm2 = (xmm3 * xmm7) + xmm2
	vfmadd231ss	%xmm3, %xmm7, %xmm6     # xmm6 = (xmm7 * xmm3) + xmm6
	vfmsub213ss	%xmm12, %xmm3, %xmm3    # xmm3 = (xmm3 * xmm3) - xmm12
	vaddss	%xmm4, %xmm3, %xmm7
	vsubss	%xmm3, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vsubss	%xmm8, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm5, %xmm7, %xmm4
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm4, %xmm9
	vsubss	%xmm9, %xmm7, %xmm7
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm7, %xmm5
	vfmadd231ss	%xmm1, %xmm1, %xmm5     # xmm5 = (xmm1 * xmm1) + xmm5
	vaddss	%xmm5, %xmm3, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm12, %xmm3
	vsubss	%xmm3, %xmm12, %xmm4
	vaddss	%xmm2, %xmm4, %xmm8
	vaddss	%xmm3, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm5, %xmm0, %xmm5
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm5, %xmm3
	vmovshdup	%xmm0, %xmm4            # xmm4 = xmm0[1,1,3,3]
	vaddss	%xmm4, %xmm8, %xmm5
	vsubss	%xmm4, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm4, %xmm4
	vsubss	%xmm6, %xmm8, %xmm0
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm3, %xmm5, %xmm4
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm5, %xmm5
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm1, %xmm11, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm3
	vaddss	%xmm0, %xmm3, %xmm11
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	addq	$12, %rcx
	cmpq	%rcx, %rax
	jne	.LBB39_47
	jmp	.LBB39_49
	.p2align	4, 0x90
.LBB39_48:                              #   in Loop: Header=BB39_45 Depth=1
	vxorps	%xmm11, %xmm11, %xmm11
.LBB39_49:                              #   in Loop: Header=BB39_45 Depth=1
	vucomiss	%xmm10, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB39_44
# %bb.50:                               #   in Loop: Header=BB39_45 Depth=1
	vmovups	%xmm0, 192(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovss	%xmm11, 96(%rsp)                # 4-byte Spill
	callq	sqrtf
	vmovss	96(%rsp), %xmm8                 # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm0
	setnp	%al
	sete	%cl
	vmovd	.LCPI39_0(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vmovdqa	%xmm2, %xmm7
	testb	%al, %cl
	jne	.LBB39_53
# %bb.51:                               #   in Loop: Header=BB39_45 Depth=1
	vmovd	%xmm0, %eax
	andl	$2139095040, %eax               # imm = 0x7F800000
	vmovdqa	%xmm2, %xmm7
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB39_53
# %bb.52:                               #   in Loop: Header=BB39_45 Depth=1
	vmovd	%eax, %xmm7
.LBB39_53:                              #   in Loop: Header=BB39_45 Depth=1
	vucomiss	%xmm1, %xmm0
	setnp	%al
	sete	%cl
	vmovdqa	%xmm2, %xmm4
	testb	%al, %cl
	vmovups	192(%rsp), %xmm3                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	jne	.LBB39_43
# %bb.54:                               #   in Loop: Header=BB39_45 Depth=1
	vmovd	%xmm0, %eax
	andl	$2139095040, %eax               # imm = 0x7F800000
	vmovdqa	%xmm2, %xmm4
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB39_43
# %bb.55:                               #   in Loop: Header=BB39_45 Depth=1
	movl	$2130706432, %ecx               # imm = 0x7F000000
	subl	%eax, %ecx
	vmovd	%ecx, %xmm4
	jmp	.LBB39_43
.LBB39_56:
	callq	omp_get_wtime
	vsubsd	304(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm13, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	216(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp4904:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp4905:
# %bb.57:
	movq	%rax, %r13
	movq	176(%rsp), %rax                 # 8-byte Reload
	leaq	32(%rax), %r14
.Ltmp4906:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp4907:
# %bb.58:
.Ltmp4908:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp4909:
# %bb.59:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp4910:
	leaq	136(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp4911:
# %bb.60:
.Ltmp4912:
	leaq	136(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4913:
# %bb.61:
.Ltmp4915:
	callq	mpfr_get_default_rounding_mode
.Ltmp4916:
# %bb.62:
.Ltmp4917:
	leaq	136(%rsp), %rdi
	leaq	216(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp4918:
# %bb.63:
.Ltmp4920:
	leaq	136(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4921:
# %bb.64:
.Ltmp4922:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp4923:
# %bb.65:
.Ltmp4924:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp4925:
# %bb.66:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp4926:
	leaq	16(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp4927:
# %bb.67:
.Ltmp4928:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4929:
# %bb.68:
.Ltmp4931:
	callq	mpfr_get_default_rounding_mode
.Ltmp4932:
# %bb.69:
.Ltmp4933:
	leaq	16(%rsp), %rdi
	leaq	136(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp4934:
# %bb.70:
.Ltmp4936:
	callq	mpfr_get_default_rounding_mode
.Ltmp4937:
# %bb.71:
.Ltmp4938:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4939:
# %bb.72:
.Ltmp4940:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp4941:
# %bb.73:
.Ltmp4942:
	movl	%eax, %r12d
	leaq	48(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp4943:
# %bb.74:
.Ltmp4944:
	leaq	48(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4945:
# %bb.75:
.Ltmp4947:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp4948:
# %bb.76:
.Ltmp4950:
	leaq	48(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp4951:
# %bb.77:
	vmovlpd	%xmm0, 248(%rsp)
	vmovss	%xmm1, 256(%rsp)
	cmpq	$0, 72(%rsp)
	je	.LBB39_79
# %bb.78:
.Ltmp4953:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4954:
.LBB39_79:
	cmpq	$0, 40(%rsp)
	je	.LBB39_81
# %bb.80:
.Ltmp4956:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4957:
.LBB39_81:
	cmpq	$0, 160(%rsp)
	je	.LBB39_83
# %bb.82:
.Ltmp4959:
	leaq	136(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4960:
.LBB39_83:
	cmpq	$0, 240(%rsp)
	je	.LBB39_85
# %bb.84:
.Ltmp4962:
	leaq	216(%rsp), %rdi
	callq	mpfr_clear
.Ltmp4963:
.LBB39_85:
	leaq	424(%rsp), %r15
	movq	%r15, 408(%rsp)
	movl	$846033518, 424(%rsp)           # imm = 0x326D726E
	movw	$32, 428(%rsp)
	movq	$5, 416(%rsp)
.Ltmp4965:
	leaq	408(%rsp), %rdi
	leaq	248(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp4966:
# %bb.86:
	movq	408(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB39_88
# %bb.87:
	callq	_ZdlPv
.LBB39_88:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$120, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%ymm0, (%rax)
	vmovupd	%ymm0, 32(%rax)
	vmovupd	%ymm0, 64(%rax)
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovlpd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 96(%rax)
	vzeroupper
	callq	omp_get_wtime
	vxorps	%xmm11, %xmm11, %xmm11
	vmovsd	%xmm0, 96(%rsp)                 # 8-byte Spill
	movq	8(%rsp), %rax                   # 8-byte Reload
	movl	(%rax), %eax
	movq	%rax, %rcx
	shlq	$2, %rcx
	leaq	(%rcx,%rcx,2), %rcx
	xorl	%edx, %edx
	vxorpd	%xmm0, %xmm0, %xmm0
	vbroadcastss	.LCPI39_2(%rip), %xmm1  # xmm1 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	xorl	%esi, %esi
	jmp	.LBB39_90
	.p2align	4, 0x90
.LBB39_89:                              #   in Loop: Header=BB39_90 Depth=1
	leaq	(%rsi,%rsi,2), %rdi
	vmovlps	%xmm3, (%rbp,%rdi,4)
	vmovss	%xmm2, 8(%rbp,%rdi,4)
	incq	%rsi
	cmpq	$10, %rsi
	je	.LBB39_96
.LBB39_90:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB39_94 Depth 2
	vcvtsi2ss	%edx, %xmm12, %xmm2
	vblendps	$1, %xmm2, %xmm11, %xmm3        # xmm3 = xmm2[0],xmm11[1,2,3]
	vxorps	%xmm2, %xmm2, %xmm2
	testl	%eax, %eax
	jle	.LBB39_89
# %bb.91:                               #   in Loop: Header=BB39_90 Depth=1
	xorl	%edi, %edi
	jmp	.LBB39_94
	.p2align	4, 0x90
.LBB39_92:                              #   in Loop: Header=BB39_94 Depth=2
	vmovsd	(%rbx,%rdi), %xmm5              # xmm5 = mem[0],zero
.LBB39_93:                              #   in Loop: Header=BB39_94 Depth=2
	vaddss	%xmm5, %xmm3, %xmm6
	vsubss	%xmm3, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm3, %xmm8
	vsubss	%xmm7, %xmm5, %xmm7
	vaddss	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm3, %xmm3            # xmm3 = xmm3[1,1,3,3]
	vmovshdup	%xmm5, %xmm5            # xmm5 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm3, %xmm8
	vsubss	%xmm3, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm3, %xmm3
	vsubss	%xmm9, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm7, %xmm8, %xmm5
	vsubss	%xmm8, %xmm5, %xmm9
	vsubss	%xmm9, %xmm5, %xmm10
	vsubss	%xmm10, %xmm8, %xmm8
	vsubss	%xmm9, %xmm7, %xmm7
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm3
	vsubss	%xmm3, %xmm5, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vaddss	%xmm3, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm5
	vaddss	%xmm3, %xmm5, %xmm3
	vinsertps	$16, %xmm3, %xmm4, %xmm3 # xmm3 = xmm4[0],xmm3[0],xmm4[2,3]
	addq	$12, %rdi
	cmpq	%rdi, %rcx
	je	.LBB39_89
.LBB39_94:                              #   Parent Loop BB39_90 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rdi), %xmm5              # xmm5 = mem[0],zero,zero,zero
	vmovss	8(%rbx,%rdi), %xmm4             # xmm4 = mem[0],zero,zero,zero
	vcomiss	%xmm5, %xmm0
	jbe	.LBB39_92
# %bb.95:                               #   in Loop: Header=BB39_94 Depth=2
	vinsertps	$16, 4(%rbx,%rdi), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vxorps	%xmm1, %xmm5, %xmm5
	vxorps	%xmm1, %xmm4, %xmm4
	jmp	.LBB39_93
.LBB39_96:
	callq	omp_get_wtime
	vsubsd	96(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm12, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	216(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp4968:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp4969:
# %bb.97:
	movq	%rax, %r13
	movq	176(%rsp), %rax                 # 8-byte Reload
	leaq	64(%rax), %r14
.Ltmp4970:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp4971:
# %bb.98:
.Ltmp4972:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp4973:
# %bb.99:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp4974:
	leaq	136(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp4975:
# %bb.100:
.Ltmp4976:
	leaq	136(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4977:
# %bb.101:
.Ltmp4979:
	callq	mpfr_get_default_rounding_mode
.Ltmp4980:
# %bb.102:
.Ltmp4981:
	leaq	136(%rsp), %rdi
	leaq	216(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp4982:
# %bb.103:
.Ltmp4984:
	leaq	136(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp4985:
# %bb.104:
.Ltmp4986:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp4987:
# %bb.105:
.Ltmp4988:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp4989:
# %bb.106:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp4990:
	leaq	16(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp4991:
# %bb.107:
.Ltmp4992:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp4993:
# %bb.108:
.Ltmp4995:
	callq	mpfr_get_default_rounding_mode
.Ltmp4996:
# %bb.109:
.Ltmp4997:
	leaq	16(%rsp), %rdi
	leaq	136(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp4998:
# %bb.110:
.Ltmp5000:
	callq	mpfr_get_default_rounding_mode
.Ltmp5001:
# %bb.111:
.Ltmp5002:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5003:
# %bb.112:
.Ltmp5004:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5005:
# %bb.113:
.Ltmp5006:
	movl	%eax, %r12d
	leaq	48(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5007:
# %bb.114:
.Ltmp5008:
	leaq	48(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5009:
# %bb.115:
.Ltmp5011:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp5012:
# %bb.116:
.Ltmp5014:
	leaq	48(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp5015:
# %bb.117:
	vmovlpd	%xmm0, 248(%rsp)
	vmovss	%xmm1, 256(%rsp)
	cmpq	$0, 72(%rsp)
	je	.LBB39_119
# %bb.118:
.Ltmp5017:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5018:
.LBB39_119:
	cmpq	$0, 40(%rsp)
	je	.LBB39_121
# %bb.120:
.Ltmp5020:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5021:
.LBB39_121:
	cmpq	$0, 160(%rsp)
	je	.LBB39_123
# %bb.122:
.Ltmp5023:
	leaq	136(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5024:
.LBB39_123:
	cmpq	$0, 240(%rsp)
	je	.LBB39_125
# %bb.124:
.Ltmp5026:
	leaq	216(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5027:
.LBB39_125:
	leaq	392(%rsp), %r15
	movq	%r15, 376(%rsp)
	movl	$1836413793, 392(%rsp)          # imm = 0x6D757361
	movw	$32, 396(%rsp)
	movq	$5, 384(%rsp)
.Ltmp5029:
	leaq	376(%rsp), %rdi
	leaq	248(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp5030:
# %bb.126:
	movq	376(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB39_128
# %bb.127:
	callq	_ZdlPv
.LBB39_128:
	movq	%rbp, %rdi
	callq	_ZdaPv
	leaq	80(%rsp), %rsi
	movq	8(%rsp), %r13                   # 8-byte Reload
	movq	%r13, %rdi
	movq	%rbx, %rdx
	movq	184(%rsp), %rcx                 # 8-byte Reload
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	48(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	cmpl	$0, (%r13)
	jle	.LBB39_169
# %bb.129:
	movq	176(%rsp), %rax                 # 8-byte Reload
	leaq	256(%rax), %r15
	xorl	%r14d, %r14d
	movq	184(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB39_131
	.p2align	4, 0x90
.LBB39_130:                             #   in Loop: Header=BB39_131 Depth=1
	incq	%r14
	movq	8(%rsp), %rax                   # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r15
	movq	96(%rsp), %rsi                  # 8-byte Reload
	addq	$12, %rsi
	cmpq	%rax, %r14
	jge	.LBB39_169
.LBB39_131:                             # =>This Inner Loop Header: Depth=1
.Ltmp5032:
	leaq	136(%rsp), %rdi
	movq	%rsi, 96(%rsp)                  # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp5033:
# %bb.132:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5035:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp5036:
# %bb.133:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5037:
	movq	%rax, %rbp
	leaq	136(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5038:
# %bb.134:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5039:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5040:
# %bb.135:                              #   in Loop: Header=BB39_131 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp5041:
	leaq	16(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5042:
# %bb.136:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5043:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5044:
# %bb.137:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5046:
	callq	mpfr_get_default_rounding_mode
.Ltmp5047:
# %bb.138:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5048:
	leaq	16(%rsp), %rdi
	movq	%r15, %rsi
	leaq	136(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp5049:
# %bb.139:                              #   in Loop: Header=BB39_131 Depth=1
	cmpq	$0, 160(%rsp)
	je	.LBB39_141
# %bb.140:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5051:
	leaq	136(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5052:
.LBB39_141:                             #   in Loop: Header=BB39_131 Depth=1
.Ltmp5054:
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5055:
# %bb.142:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5056:
	movq	%rax, %rbp
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5057:
# %bb.143:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5058:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5059:
# %bb.144:                              #   in Loop: Header=BB39_131 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp5060:
	leaq	216(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5061:
# %bb.145:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5062:
	leaq	216(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5063:
# %bb.146:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5065:
	callq	mpfr_get_default_rounding_mode
.Ltmp5066:
# %bb.147:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5067:
	leaq	216(%rsp), %rdi
	leaq	16(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp5068:
# %bb.148:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5070:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5071:
# %bb.149:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5072:
	movq	%rax, %rbp
	leaq	216(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5073:
# %bb.150:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5074:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5075:
# %bb.151:                              #   in Loop: Header=BB39_131 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp5076:
	leaq	136(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5077:
# %bb.152:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5078:
	leaq	136(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5079:
# %bb.153:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5081:
	callq	mpfr_get_default_rounding_mode
.Ltmp5082:
	leaq	48(%rsp), %r12
# %bb.154:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5083:
	leaq	136(%rsp), %rdi
	movq	%r12, %rsi
	leaq	216(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp5084:
# %bb.155:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5086:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp5087:
# %bb.156:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5088:
	movq	%rax, %r12
	leaq	136(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5089:
# %bb.157:                              #   in Loop: Header=BB39_131 Depth=1
	movq	%rax, %rbp
	cmpq	%rax, %r12
	je	.LBB39_161
# %bb.158:                              #   in Loop: Header=BB39_131 Depth=1
	cmpq	$0, 72(%rsp)
	je	.LBB39_160
# %bb.159:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5090:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5091:
.LBB39_160:                             #   in Loop: Header=BB39_131 Depth=1
.Ltmp5092:
	leaq	48(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp5093:
.LBB39_161:                             #   in Loop: Header=BB39_131 Depth=1
.Ltmp5094:
	callq	mpfr_get_default_rounding_mode
.Ltmp5095:
# %bb.162:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5096:
	leaq	48(%rsp), %rdi
	leaq	136(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp5097:
# %bb.163:                              #   in Loop: Header=BB39_131 Depth=1
	cmpq	$0, 160(%rsp)
	je	.LBB39_165
# %bb.164:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5099:
	leaq	136(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5100:
.LBB39_165:                             #   in Loop: Header=BB39_131 Depth=1
	cmpq	$0, 240(%rsp)
	je	.LBB39_167
# %bb.166:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5102:
	leaq	216(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5103:
.LBB39_167:                             #   in Loop: Header=BB39_131 Depth=1
	cmpq	$0, 40(%rsp)
	je	.LBB39_130
# %bb.168:                              #   in Loop: Header=BB39_131 Depth=1
.Ltmp5105:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5106:
	jmp	.LBB39_130
.LBB39_169:
.Ltmp5108:
	callq	mpfr_get_default_rounding_mode
.Ltmp5109:
# %bb.170:
.Ltmp5110:
	movl	%eax, %ebp
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5111:
# %bb.171:
.Ltmp5112:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp5113:
# %bb.172:
.Ltmp5114:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp5115:
# %bb.173:
.Ltmp5116:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp5117:
# %bb.174:
.Ltmp5119:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp5120:
# %bb.175:
.Ltmp5122:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5123:
# %bb.176:
.Ltmp5124:
	movq	%rax, %r12
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5125:
# %bb.177:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB39_181
# %bb.178:
	cmpq	$0, 72(%rsp)
	je	.LBB39_180
# %bb.179:
.Ltmp5126:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5127:
.LBB39_180:
.Ltmp5128:
	leaq	48(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp5129:
.LBB39_181:
.Ltmp5130:
	callq	mpfr_get_default_rounding_mode
.Ltmp5131:
# %bb.182:
.Ltmp5132:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp5133:
# %bb.183:
	cmpq	$0, 40(%rsp)
	je	.LBB39_185
# %bb.184:
.Ltmp5135:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5136:
.LBB39_185:
	callq	omp_get_wtime
	vmovsd	%xmm0, 96(%rsp)                 # 8-byte Spill
.Ltmp5138:
	leaq	80(%rsp), %rsi
	movq	8(%rsp), %r15                   # 8-byte Reload
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	184(%rsp), %r14                 # 8-byte Reload
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp5139:
# %bb.186:
.Ltmp5140:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp5141:
# %bb.187:
.Ltmp5142:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp5143:
# %bb.188:
.Ltmp5144:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp5145:
# %bb.189:
.Ltmp5146:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp5147:
# %bb.190:
.Ltmp5148:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp5149:
# %bb.191:
.Ltmp5150:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp5151:
# %bb.192:
.Ltmp5152:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp5153:
# %bb.193:
.Ltmp5154:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp5155:
# %bb.194:
.Ltmp5156:
	leaq	80(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp5157:
# %bb.195:
	callq	omp_get_wtime
	vsubsd	96(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp5159:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5160:
# %bb.196:
	movq	%rax, %r12
	movq	176(%rsp), %rax                 # 8-byte Reload
	leaq	96(%rax), %r15
.Ltmp5161:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp5162:
# %bb.197:
.Ltmp5163:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5164:
# %bb.198:
	movl	%eax, %ebp
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp5165:
	leaq	16(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5166:
# %bb.199:
.Ltmp5167:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp5168:
# %bb.200:
.Ltmp5170:
	callq	mpfr_get_default_rounding_mode
.Ltmp5171:
# %bb.201:
.Ltmp5172:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movq	%r15, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp5173:
# %bb.202:
.Ltmp5175:
	leaq	16(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp5176:
# %bb.203:
	vmovlpd	%xmm0, 136(%rsp)
	vmovss	%xmm1, 144(%rsp)
	cmpq	$0, 40(%rsp)
	je	.LBB39_205
# %bb.204:
.Ltmp5178:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5179:
.LBB39_205:
	leaq	360(%rsp), %r15
	movq	%r15, 344(%rsp)
	movl	$2037413985, 360(%rsp)          # imm = 0x79707861
	movw	$32, 364(%rsp)
	movq	$5, 352(%rsp)
.Ltmp5181:
	leaq	344(%rsp), %rdi
	leaq	136(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp5182:
# %bb.206:
	movq	344(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB39_208
# %bb.207:
	callq	_ZdlPv
.LBB39_208:
	cmpq	$0, 72(%rsp)
	je	.LBB39_210
# %bb.209:
.Ltmp5184:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5185:
.LBB39_210:
	movslq	4(%rsp), %r15
	movl	$12, %ecx
	movq	%r15, %rax
	mulq	%rcx
	movq	$-1, %rdi
	cmovnoq	%rax, %rdi
	callq	_Znam
	movq	%rax, %rbp
	testq	%r15, %r15
	movq	296(%rsp), %r13                 # 8-byte Reload
	movq	184(%rsp), %r14                 # 8-byte Reload
	je	.LBB39_214
# %bb.211:
	leaq	(%r15,%r15,2), %rax
	leaq	-12(,%rax,4), %rax
	movabsq	$-6148914691236517205, %rcx     # imm = 0xAAAAAAAAAAAAAAAB
	mulq	%rcx
	shrq	$3, %rdx
	leaq	(%rdx,%rdx,2), %rax
	leaq	12(,%rax,4), %rdx
	movq	%rbp, %rdi
	xorl	%esi, %esi
	callq	_intel_fast_memset@PLT
	testl	%r15d, %r15d
	jle	.LBB39_214
# %bb.212:
	xorl	%r15d, %r15d
	xorl	%r12d, %r12d
	.p2align	4, 0x90
.LBB39_213:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovlpd	%xmm0, (%r14,%r15)
	vmovss	%xmm1, 8(%r14,%r15)
	movl	8(%r14,%r15), %eax
	movl	%eax, 8(%rbp,%r15)
	movq	(%r14,%r15), %rax
	movq	%rax, (%rbp,%r15)
	incq	%r12
	movslq	4(%rsp), %rax
	addq	$12, %r15
	addq	$32, %r13
	cmpq	%rax, %r12
	jl	.LBB39_213
.LBB39_214:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%rbp, 120(%rsp)                 # 8-byte Spill
	movq	%rbp, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	48(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	cmpl	$0, 4(%rsp)
	jle	.LBB39_255
# %bb.215:
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	leaq	136(%rsp), %rbp
	movq	120(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB39_217
	.p2align	4, 0x90
.LBB39_216:                             #   in Loop: Header=BB39_217 Depth=1
	movq	96(%rsp), %rdx                  # 8-byte Reload
	incq	%rdx
	movslq	4(%rsp), %rax
	movq	192(%rsp), %rsi                 # 8-byte Reload
	addq	$12, %rsi
	movq	%rdx, %rcx
	movq	%rdx, 96(%rsp)                  # 8-byte Spill
	cmpq	%rax, %rdx
	jge	.LBB39_255
.LBB39_217:                             # =>This Inner Loop Header: Depth=1
	movq	8(%rsp), %rax                   # 8-byte Reload
	movl	(%rax), %r15d
.Ltmp5187:
	movq	%rbp, %r14
	movq	%rbp, %rdi
	movq	%rsi, 192(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp5188:
# %bb.218:                              #   in Loop: Header=BB39_217 Depth=1
	movq	96(%rsp), %rax                  # 8-byte Reload
	addl	%r15d, %eax
	addl	$8, %eax
	movslq	%eax, %r12
	shlq	$5, %r12
	addq	176(%rsp), %r12                 # 8-byte Folded Reload
.Ltmp5190:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp5191:
# %bb.219:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5192:
	movq	%rax, %r15
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp5193:
# %bb.220:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5194:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp5195:
# %bb.221:                              #   in Loop: Header=BB39_217 Depth=1
	movl	%eax, %r13d
	cmpq	%rbp, %r15
	cmovgq	%r15, %rbp
.Ltmp5196:
	leaq	16(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp5197:
# %bb.222:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5198:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r13d, %edx
	callq	mpfr_set_si
.Ltmp5199:
# %bb.223:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5201:
	callq	mpfr_get_default_rounding_mode
.Ltmp5202:
# %bb.224:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5203:
	movq	%r14, %rbp
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp5204:
# %bb.225:                              #   in Loop: Header=BB39_217 Depth=1
	cmpq	$0, 160(%rsp)
	je	.LBB39_227
# %bb.226:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5206:
	movq	%rbp, %rdi
	callq	mpfr_clear
.Ltmp5207:
.LBB39_227:                             #   in Loop: Header=BB39_217 Depth=1
.Ltmp5209:
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5210:
# %bb.228:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5211:
	movq	%rax, %r15
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5212:
# %bb.229:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5213:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5214:
# %bb.230:                              #   in Loop: Header=BB39_217 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %r15
	cmovgq	%r15, %r13
.Ltmp5215:
	leaq	216(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5216:
# %bb.231:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5217:
	leaq	216(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5218:
# %bb.232:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5220:
	callq	mpfr_get_default_rounding_mode
.Ltmp5221:
# %bb.233:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5222:
	leaq	216(%rsp), %rdi
	leaq	16(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp5223:
# %bb.234:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5225:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5226:
# %bb.235:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5227:
	movq	%rax, %r15
	leaq	216(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5228:
# %bb.236:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5229:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5230:
# %bb.237:                              #   in Loop: Header=BB39_217 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %r15
	cmovgq	%r15, %r13
.Ltmp5231:
	movq	%rbp, %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5232:
# %bb.238:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5233:
	movq	%rbp, %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5234:
# %bb.239:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5236:
	callq	mpfr_get_default_rounding_mode
.Ltmp5237:
	leaq	48(%rsp), %r14
# %bb.240:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5238:
	movq	%rbp, %rdi
	movq	%r14, %rsi
	leaq	216(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp5239:
# %bb.241:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5241:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp5242:
# %bb.242:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5243:
	movq	%rax, %r15
	movq	%rbp, %rdi
	callq	mpfr_get_prec
.Ltmp5244:
# %bb.243:                              #   in Loop: Header=BB39_217 Depth=1
	movq	%rax, %r12
	cmpq	%rax, %r15
	je	.LBB39_247
# %bb.244:                              #   in Loop: Header=BB39_217 Depth=1
	cmpq	$0, 72(%rsp)
	je	.LBB39_246
# %bb.245:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5245:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5246:
.LBB39_246:                             #   in Loop: Header=BB39_217 Depth=1
.Ltmp5247:
	leaq	48(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp5248:
.LBB39_247:                             #   in Loop: Header=BB39_217 Depth=1
.Ltmp5249:
	callq	mpfr_get_default_rounding_mode
.Ltmp5250:
# %bb.248:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5251:
	leaq	48(%rsp), %rdi
	movq	%rbp, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp5252:
# %bb.249:                              #   in Loop: Header=BB39_217 Depth=1
	cmpq	$0, 160(%rsp)
	je	.LBB39_251
# %bb.250:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5254:
	movq	%rbp, %rdi
	callq	mpfr_clear
.Ltmp5255:
.LBB39_251:                             #   in Loop: Header=BB39_217 Depth=1
	cmpq	$0, 240(%rsp)
	je	.LBB39_253
# %bb.252:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5257:
	leaq	216(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5258:
.LBB39_253:                             #   in Loop: Header=BB39_217 Depth=1
	cmpq	$0, 40(%rsp)
	je	.LBB39_216
# %bb.254:                              #   in Loop: Header=BB39_217 Depth=1
.Ltmp5260:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5261:
	jmp	.LBB39_216
.LBB39_255:
.Ltmp5263:
	callq	mpfr_get_default_rounding_mode
.Ltmp5264:
# %bb.256:
.Ltmp5265:
	movl	%eax, %ebp
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5266:
# %bb.257:
.Ltmp5267:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp5268:
# %bb.258:
.Ltmp5269:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp5270:
# %bb.259:
.Ltmp5271:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp5272:
# %bb.260:
.Ltmp5274:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp5275:
# %bb.261:
.Ltmp5277:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5278:
# %bb.262:
.Ltmp5279:
	movq	%rax, %r12
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5280:
# %bb.263:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB39_267
# %bb.264:
	cmpq	$0, 72(%rsp)
	je	.LBB39_266
# %bb.265:
.Ltmp5281:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5282:
.LBB39_266:
.Ltmp5283:
	leaq	48(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp5284:
.LBB39_267:
.Ltmp5285:
	callq	mpfr_get_default_rounding_mode
.Ltmp5286:
# %bb.268:
.Ltmp5287:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp5288:
# %bb.269:
	cmpq	$0, 40(%rsp)
	je	.LBB39_271
# %bb.270:
.Ltmp5290:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5291:
.LBB39_271:
	callq	omp_get_wtime
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
.Ltmp5293:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	184(%rsp), %r14                 # 8-byte Reload
	movq	%r14, %r8
	movq	120(%rsp), %r15                 # 8-byte Reload
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5294:
# %bb.272:
.Ltmp5295:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5296:
# %bb.273:
.Ltmp5297:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5298:
# %bb.274:
.Ltmp5299:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5300:
# %bb.275:
.Ltmp5301:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5302:
# %bb.276:
.Ltmp5303:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5304:
# %bb.277:
.Ltmp5305:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5306:
# %bb.278:
.Ltmp5307:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5308:
# %bb.279:
.Ltmp5309:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5310:
# %bb.280:
.Ltmp5311:
	leaq	4(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5312:
# %bb.281:
	callq	omp_get_wtime
	vsubsd	8(%rsp), %xmm0, %xmm0           # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp5314:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5315:
# %bb.282:
	movq	%rax, %r15
	movq	176(%rsp), %rdi                 # 8-byte Reload
	subq	$-128, %rdi
.Ltmp5316:
	movq	%rdi, %r13
	callq	mpfr_get_prec
.Ltmp5317:
# %bb.283:
.Ltmp5318:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp5319:
# %bb.284:
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp5320:
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp5321:
# %bb.285:
.Ltmp5322:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp5323:
# %bb.286:
.Ltmp5325:
	callq	mpfr_get_default_rounding_mode
.Ltmp5326:
# %bb.287:
.Ltmp5327:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movq	%r13, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp5328:
# %bb.288:
.Ltmp5330:
	leaq	16(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7tX_real7tx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp5331:
# %bb.289:
	vmovlpd	%xmm0, 136(%rsp)
	vmovss	%xmm1, 144(%rsp)
	cmpq	$0, 40(%rsp)
	je	.LBB39_291
# %bb.290:
.Ltmp5333:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5334:
.LBB39_291:
	leaq	328(%rsp), %r15
	movq	%r15, 312(%rsp)
	movl	$1986880871, 328(%rsp)          # imm = 0x766D6567
	movw	$32, 332(%rsp)
	movq	$5, 320(%rsp)
.Ltmp5336:
	leaq	312(%rsp), %rdi
	leaq	136(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp5337:
# %bb.292:
	movq	312(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB39_294
# %bb.293:
	callq	_ZdlPv
.LBB39_294:
	movq	120(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	cmpq	$0, 72(%rsp)
	je	.LBB39_296
# %bb.295:
.Ltmp5339:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5340:
.LBB39_296:
	movq	%rbx, %rdi
	callq	_ZdaPv
	movq	184(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	xorl	%eax, %eax
	vcvtsi2ss	%eax, %xmm2, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vblendps	$1, %xmm0, %xmm1, %xmm0         # xmm0 = xmm0[0],xmm1[1,2,3]
	movl	$2097153, %eax                  # imm = 0x200001
	movl	$1, %ecx
	vxorps	%xmm9, %xmm9, %xmm9
	.p2align	4, 0x90
.LBB39_297:                             # =>This Inner Loop Header: Depth=1
	vmovups	%xmm0, 96(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovss	%xmm9, 8(%rsp)                  # 4-byte Spill
	decl	%eax
	vcvtsi2ss	%eax, %xmm13, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vmulss	%xmm1, %xmm2, %xmm7
	vxorps	%xmm8, %xmm8, %xmm8
	vfmsub213ss	%xmm7, %xmm2, %xmm8     # xmm8 = (xmm2 * xmm8) - xmm7
	vmulss	%xmm2, %xmm1, %xmm9
	vmovaps	%xmm2, %xmm10
	vmulss	%xmm2, %xmm2, %xmm11
	vfmsub213ss	%xmm9, %xmm1, %xmm10    # xmm10 = (xmm1 * xmm10) - xmm9
	vfmadd231ss	%xmm1, %xmm2, %xmm8     # xmm8 = (xmm2 * xmm1) + xmm8
	vfmadd231ss	%xmm2, %xmm1, %xmm10    # xmm10 = (xmm1 * xmm2) + xmm10
	vfmsub213ss	%xmm11, %xmm2, %xmm2    # xmm2 = (xmm2 * xmm2) - xmm11
	vaddss	%xmm7, %xmm2, %xmm12
	vsubss	%xmm2, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm13, %xmm7, %xmm7
	vaddss	%xmm9, %xmm12, %xmm13
	vsubss	%xmm12, %xmm13, %xmm15
	vsubss	%xmm14, %xmm2, %xmm2
	vsubss	%xmm15, %xmm13, %xmm14
	vsubss	%xmm14, %xmm12, %xmm12
	vsubss	%xmm15, %xmm9, %xmm9
	vaddss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm9, %xmm12, %xmm7
	vfmadd231ss	%xmm1, %xmm1, %xmm7     # xmm7 = (xmm1 * xmm1) + xmm7
	vaddss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm2, %xmm10, %xmm2
	vaddss	%xmm2, %xmm13, %xmm7
	vsubss	%xmm7, %xmm13, %xmm8
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm7, %xmm11, %xmm8
	vsubss	%xmm8, %xmm11, %xmm9
	vaddss	%xmm7, %xmm9, %xmm9
	vmulss	%xmm8, %xmm8, %xmm7
	vmulss	%xmm9, %xmm8, %xmm10
	vmovaps	%xmm9, %xmm11
	vmulss	%xmm8, %xmm9, %xmm12
	vfmsub213ss	%xmm10, %xmm8, %xmm11   # xmm11 = (xmm8 * xmm11) - xmm10
	vmovaps	%xmm8, %xmm13
	vfmsub213ss	%xmm12, %xmm9, %xmm13   # xmm13 = (xmm9 * xmm13) - xmm12
	vfmadd231ss	%xmm2, %xmm8, %xmm11    # xmm11 = (xmm8 * xmm2) + xmm11
	vfmadd231ss	%xmm8, %xmm2, %xmm13    # xmm13 = (xmm2 * xmm8) + xmm13
	vfmsub213ss	%xmm7, %xmm8, %xmm8     # xmm8 = (xmm8 * xmm8) - xmm7
	vaddss	%xmm10, %xmm8, %xmm2
	vsubss	%xmm8, %xmm2, %xmm14
	vsubss	%xmm14, %xmm2, %xmm15
	vsubss	%xmm14, %xmm10, %xmm10
	vaddss	%xmm2, %xmm12, %xmm14
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm2, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm3
	vsubss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm10, %xmm8, %xmm3
	vsubss	%xmm15, %xmm12, %xmm8
	vaddss	%xmm2, %xmm8, %xmm2
	vfmadd231ss	%xmm9, %xmm9, %xmm2     # xmm2 = (xmm9 * xmm9) + xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm2, %xmm11, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vcvtsi2ss	%ecx, %xmm5, %xmm9
	vaddss	%xmm2, %xmm13, %xmm2
	vaddss	%xmm2, %xmm14, %xmm3
	vaddss	%xmm3, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm7
	vmovss	.LCPI39_3(%rip), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vdivss	%xmm8, %xmm0, %xmm11
	vmovaps	%xmm8, %xmm12
	vfmadd213ss	.LCPI39_4(%rip), %xmm11, %xmm12 # xmm12 = (xmm11 * xmm12) + mem
	vaddss	%xmm3, %xmm7, %xmm5
	vfmadd231ss	%xmm5, %xmm11, %xmm12   # xmm12 = (xmm11 * xmm5) + xmm12
	vmovss	.LCPI39_5(%rip), %xmm15         # xmm15 = mem[0],zero,zero,zero
	vmulss	%xmm15, %xmm11, %xmm7
	vsubss	%xmm3, %xmm14, %xmm3
	vfmsub213ss	%xmm7, %xmm11, %xmm15   # xmm15 = (xmm11 * xmm15) - xmm7
	vfnmadd231ss	%xmm12, %xmm11, %xmm15  # xmm15 = -(xmm11 * xmm12) + xmm15
	vaddss	%xmm7, %xmm15, %xmm12
	vaddss	%xmm2, %xmm3, %xmm13
	vsubss	%xmm12, %xmm7, %xmm2
	vmulss	%xmm12, %xmm9, %xmm0
	vmovss	%xmm0, 192(%rsp)                # 4-byte Spill
	vmulss	%xmm1, %xmm12, %xmm14
	vaddss	%xmm2, %xmm15, %xmm11
	vmovss	%xmm5, 120(%rsp)                # 4-byte Spill
	vmulss	%xmm5, %xmm12, %xmm7
	vmovaps	%xmm12, %xmm15
	vfmsub213ss	%xmm7, %xmm5, %xmm15    # xmm15 = (xmm5 * xmm15) - xmm7
	vfmadd231ss	%xmm13, %xmm12, %xmm15  # xmm15 = (xmm12 * xmm13) + xmm15
	vmovaps	%xmm12, %xmm3
	vfmsub213ss	%xmm0, %xmm9, %xmm3     # xmm3 = (xmm9 * xmm3) - xmm0
	vmulss	%xmm11, %xmm9, %xmm13
	vaddss	%xmm3, %xmm13, %xmm4
	vsubss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm12, %xmm3
	vfmsub213ss	%xmm14, %xmm1, %xmm3    # xmm3 = (xmm1 * xmm3) - xmm14
	vsubss	%xmm5, %xmm13, %xmm5
	vaddss	%xmm5, %xmm0, %xmm10
	vaddss	%xmm4, %xmm14, %xmm5
	vsubss	%xmm4, %xmm5, %xmm6
	vsubss	%xmm6, %xmm14, %xmm14
	vsubss	%xmm6, %xmm5, %xmm6
	vsubss	%xmm6, %xmm4, %xmm4
	vfmadd231ss	%xmm12, %xmm1, %xmm3    # xmm3 = (xmm1 * xmm12) + xmm3
	vaddss	%xmm4, %xmm14, %xmm4
	vmulss	%xmm12, %xmm8, %xmm6
	vfmsub213ss	%xmm6, %xmm8, %xmm12    # xmm12 = (xmm8 * xmm12) - xmm6
	vmulss	%xmm11, %xmm8, %xmm14
	vaddss	%xmm14, %xmm12, %xmm1
	vaddss	%xmm7, %xmm1, %xmm0
	vsubss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm2, %xmm7, %xmm7
	vsubss	%xmm2, %xmm0, %xmm2
	vsubss	%xmm2, %xmm1, %xmm2
	vaddss	%xmm7, %xmm2, %xmm2
	vfmadd231ss	120(%rsp), %xmm11, %xmm2 # 4-byte Folded Reload
                                        # xmm2 = (xmm11 * mem) + xmm2
	vmovaps	%xmm11, %xmm7
	vfmsub213ss	%xmm13, %xmm9, %xmm7    # xmm7 = (xmm9 * xmm7) - xmm13
	vxorps	%xmm13, %xmm13, %xmm13
	vfmadd231ss	%xmm11, %xmm13, %xmm4   # xmm4 = (xmm13 * xmm11) + xmm4
	vaddss	%xmm4, %xmm10, %xmm4
	vfmadd231ss	%xmm9, %xmm13, %xmm7    # xmm7 = (xmm13 * xmm9) + xmm7
	vxorps	%xmm10, %xmm10, %xmm10
	vsubss	%xmm12, %xmm1, %xmm9
	vsubss	%xmm9, %xmm1, %xmm1
	vsubss	%xmm1, %xmm12, %xmm1
	vfmsub213ss	%xmm14, %xmm8, %xmm11   # xmm11 = (xmm8 * xmm11) - xmm14
	vsubss	%xmm9, %xmm14, %xmm9
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm7, %xmm4, %xmm2
	vfmadd231ss	%xmm8, %xmm10, %xmm11   # xmm11 = (xmm10 * xmm8) + xmm11
	vaddss	%xmm1, %xmm11, %xmm1
	vaddss	%xmm1, %xmm15, %xmm1
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm1, %xmm0, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm5, %xmm1
	vaddss	%xmm3, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm6
	vbroadcastss	.LCPI39_2(%rip), %xmm10 # xmm10 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm4, %xmm10, %xmm7
	vaddss	%xmm3, %xmm6, %xmm3
	vmovss	.LCPI39_6(%rip), %xmm9          # xmm9 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm9, %xmm4
	vsubss	%xmm9, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm8
	vsubss	%xmm1, %xmm5, %xmm5
	vsubss	%xmm8, %xmm9, %xmm8
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm6, %xmm8, %xmm6
	vxorps	%xmm3, %xmm10, %xmm7
	vxorps	%xmm10, %xmm10, %xmm10
	vsubss	%xmm3, %xmm10, %xmm3
	vsubss	%xmm10, %xmm3, %xmm8
	vsubss	%xmm8, %xmm3, %xmm9
	vsubss	%xmm9, %xmm10, %xmm9
	vxorps	%xmm12, %xmm12, %xmm12
	vsubss	%xmm8, %xmm7, %xmm7
	vaddss	%xmm6, %xmm3, %xmm10
	vsubss	%xmm3, %xmm10, %xmm11
	vaddss	%xmm7, %xmm9, %xmm7
	vsubss	%xmm11, %xmm10, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vmovss	192(%rsp), %xmm9                # 4-byte Reload
                                        # xmm9 = mem[0],zero,zero,zero
	vaddss	%xmm1, %xmm9, %xmm8
	vsubss	%xmm8, %xmm9, %xmm9
	vsubss	%xmm11, %xmm6, %xmm6
	vaddss	%xmm6, %xmm3, %xmm3
	vsubss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm10, %xmm3
	vaddss	%xmm2, %xmm5, %xmm5
	vaddss	%xmm3, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm4
	vmulss	%xmm6, %xmm8, %xmm2
	vaddss	%xmm3, %xmm4, %xmm4
	vmulss	%xmm6, %xmm1, %xmm7
	vmovaps	%xmm6, %xmm9
	vfmsub213ss	%xmm7, %xmm1, %xmm9     # xmm9 = (xmm1 * xmm9) - xmm7
	vsubss	%xmm3, %xmm10, %xmm3
	vfmadd231ss	%xmm5, %xmm6, %xmm9     # xmm9 = (xmm6 * xmm5) + xmm9
	vfmsub213ss	%xmm2, %xmm8, %xmm6     # xmm6 = (xmm8 * xmm6) - xmm2
	vaddss	%xmm0, %xmm3, %xmm0
	vmulss	%xmm4, %xmm8, %xmm3
	vaddss	%xmm3, %xmm6, %xmm5
	vaddss	%xmm7, %xmm5, %xmm10
	vsubss	%xmm5, %xmm10, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm11, %xmm10, %xmm11
	vsubss	%xmm11, %xmm5, %xmm11
	vaddss	%xmm7, %xmm11, %xmm7
	vfmadd231ss	%xmm4, %xmm1, %xmm7     # xmm7 = (xmm1 * xmm4) + xmm7
	vsubss	%xmm6, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm5
	vsubss	%xmm5, %xmm6, %xmm5
	vfmsub213ss	%xmm3, %xmm8, %xmm4     # xmm4 = (xmm8 * xmm4) - xmm3
	vsubss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vfmadd231ss	%xmm0, %xmm8, %xmm4     # xmm4 = (xmm8 * xmm0) + xmm4
	vaddss	%xmm4, %xmm1, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm10, %xmm1
	vsubss	%xmm1, %xmm10, %xmm3
	vaddss	%xmm1, %xmm2, %xmm4
	vaddss	%xmm0, %xmm3, %xmm0
	vsubss	%xmm4, %xmm2, %xmm2
	vmovups	96(%rsp), %xmm6                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm4, %xmm6, %xmm3
	vsubss	%xmm6, %xmm3, %xmm5
	vaddss	%xmm1, %xmm2, %xmm1
	vsubss	%xmm5, %xmm3, %xmm2
	vsubss	%xmm2, %xmm6, %xmm2
	vsubss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vmovshdup	%xmm6, %xmm4            # xmm4 = xmm6[1,1,3,3]
	vaddss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm4, %xmm4
	vsubss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm2, %xmm5, %xmm6
	vaddss	%xmm1, %xmm4, %xmm1
	vsubss	%xmm5, %xmm6, %xmm4
	vsubss	%xmm4, %xmm2, %xmm2
	vsubss	%xmm4, %xmm6, %xmm4
	vsubss	%xmm4, %xmm5, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vaddss	8(%rsp), %xmm0, %xmm0           # 4-byte Folded Reload
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm2
	vaddss	%xmm0, %xmm2, %xmm9
	vaddss	%xmm1, %xmm3, %xmm7
	vsubss	%xmm7, %xmm3, %xmm0
	vaddss	%xmm1, %xmm0, %xmm2
	vinsertps	$16, %xmm2, %xmm7, %xmm0 # xmm0 = xmm7[0],xmm2[0],xmm7[2,3]
	cmpl	$1, %eax
	ja	.LBB39_297
# %bb.298:
	movl	$90, %eax
	vcvtsi2ss	%eax, %xmm14, %xmm1
	vmulss	%xmm7, %xmm1, %xmm3
	vfmsub213ss	%xmm3, %xmm1, %xmm7     # xmm7 = (xmm1 * xmm7) - xmm3
	vmulss	%xmm2, %xmm1, %xmm4
	vfmsub213ss	%xmm4, %xmm1, %xmm2     # xmm2 = (xmm1 * xmm2) - xmm4
	vaddss	%xmm4, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm8
	vsubss	%xmm8, %xmm7, %xmm7
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vfmadd213ss	%xmm2, %xmm1, %xmm9     # xmm9 = (xmm1 * xmm9) + xmm2
	vaddss	%xmm5, %xmm9, %xmm1
	vsubss	%xmm1, %xmm5, %xmm2
	vaddss	%xmm2, %xmm9, %xmm4
	vaddss	%xmm1, %xmm3, %xmm0
	vsubss	%xmm0, %xmm3, %xmm3
	vxorps	%xmm2, %xmm2, %xmm2
	vucomiss	%xmm2, %xmm0
	vaddss	%xmm1, %xmm3, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	je	.LBB39_300
# %bb.299:
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	jmp	.LBB39_309
.LBB39_300:
	vmovups	%xmm1, 192(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovss	%xmm4, 8(%rsp)                  # 4-byte Spill
	vmovups	%xmm0, 96(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm0
	setnp	%cl
	sete	%dl
	vmovd	.LCPI39_0(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	vmovdqa	%xmm4, %xmm8
	testb	%cl, %dl
	jne	.LBB39_304
# %bb.301:
	movl	%eax, %ecx
	andl	$2139095040, %ecx               # imm = 0x7F800000
	cmpl	$2139095040, %ecx               # imm = 0x7F800000
	je	.LBB39_303
# %bb.302:
	vmovd	%ecx, %xmm8
	jmp	.LBB39_304
.LBB39_303:
	vmovd	.LCPI39_0(%rip), %xmm8          # xmm8 = mem[0],zero,zero,zero
.LBB39_304:
	vxorps	%xmm7, %xmm7, %xmm7
	vucomiss	%xmm7, %xmm0
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB39_308
# %bb.305:
	andl	$2139095040, %eax               # imm = 0x7F800000
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB39_307
# %bb.306:
	movl	$2130706432, %ecx               # imm = 0x7F000000
	subl	%eax, %ecx
	vmovd	%ecx, %xmm4
	jmp	.LBB39_308
.LBB39_307:
	vmovd	.LCPI39_0(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
.LBB39_308:
	vmovss	.LCPI39_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm4, %xmm5
	vmulss	96(%rsp), %xmm4, %xmm2          # 16-byte Folded Reload
	vmulss	%xmm5, %xmm2, %xmm2
	vmulss	192(%rsp), %xmm4, %xmm3         # 16-byte Folded Reload
	vmulss	%xmm5, %xmm3, %xmm3
	vmovss	%xmm3, 120(%rsp)                # 4-byte Spill
	vmulss	8(%rsp), %xmm4, %xmm4           # 4-byte Folded Reload
	movl	$3, %eax
	vcvtsi2ss	%eax, %xmm12, %xmm6
	vmulss	%xmm5, %xmm4, %xmm3
	vmovss	%xmm3, 8(%rsp)                  # 4-byte Spill
	vmovd	%xmm8, 132(%rsp)                # 4-byte Folded Spill
	vdivss	%xmm0, %xmm8, %xmm8
	vmulss	%xmm1, %xmm6, %xmm5
	vmovaps	%xmm1, %xmm0
	vfmsub213ss	%xmm5, %xmm6, %xmm0     # xmm0 = (xmm6 * xmm0) - xmm5
	vmulss	%xmm7, %xmm6, %xmm9
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm9, %xmm6, %xmm10    # xmm10 = (xmm6 * xmm10) - xmm9
	vaddss	%xmm0, %xmm9, %xmm11
	vsubss	%xmm0, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm0, %xmm0
	vsubss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vfmadd231ss	%xmm6, %xmm7, %xmm0     # xmm0 = (xmm7 * xmm6) + xmm0
	vaddss	%xmm0, %xmm11, %xmm9
	vsubss	%xmm9, %xmm11, %xmm6
	vaddss	%xmm0, %xmm6, %xmm0
	vmovss	%xmm0, 96(%rsp)                 # 4-byte Spill
	vaddss	%xmm5, %xmm9, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm9, %xmm0
	vmovss	%xmm0, 192(%rsp)                # 4-byte Spill
	vmulss	%xmm8, %xmm8, %xmm10
	vmovaps	%xmm8, %xmm9
	vfmsub213ss	%xmm10, %xmm8, %xmm9    # xmm9 = (xmm8 * xmm9) - xmm10
	vmulss	%xmm7, %xmm8, %xmm11
	vxorps	%xmm12, %xmm12, %xmm12
	vfmsub213ss	%xmm11, %xmm8, %xmm12   # xmm12 = (xmm8 * xmm12) - xmm11
	vaddss	%xmm11, %xmm9, %xmm13
	vsubss	%xmm9, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm9, %xmm9
	vmulss	%xmm7, %xmm8, %xmm15
	vsubss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm15, %xmm13, %xmm11
	vsubss	%xmm13, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm1
	vsubss	%xmm1, %xmm13, %xmm1
	vmovaps	%xmm8, %xmm13
	vfmsub213ss	%xmm15, %xmm7, %xmm13   # xmm13 = (xmm7 * xmm13) - xmm15
	vsubss	%xmm14, %xmm15, %xmm14
	vaddss	%xmm1, %xmm14, %xmm1
	vfmadd231ss	%xmm7, %xmm7, %xmm1     # xmm1 = (xmm7 * xmm7) + xmm1
	vaddss	%xmm1, %xmm9, %xmm1
	vfmadd231ss	%xmm7, %xmm8, %xmm12    # xmm12 = (xmm8 * xmm7) + xmm12
	vaddss	%xmm1, %xmm12, %xmm1
	vfmadd231ss	%xmm8, %xmm7, %xmm13    # xmm13 = (xmm7 * xmm8) + xmm13
	vaddss	%xmm1, %xmm13, %xmm1
	vaddss	%xmm1, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm9
	vaddss	%xmm1, %xmm9, %xmm9
	vaddss	%xmm12, %xmm10, %xmm1
	vsubss	%xmm1, %xmm10, %xmm10
	vaddss	%xmm12, %xmm10, %xmm11
	vmulss	%xmm1, %xmm2, %xmm10
	vmovss	120(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm5, %xmm12
	vmovaps	%xmm1, %xmm13
	vfmsub213ss	%xmm12, %xmm5, %xmm13   # xmm13 = (xmm5 * xmm13) - xmm12
	vfmadd231ss	%xmm1, %xmm3, %xmm13    # xmm13 = (xmm3 * xmm1) + xmm13
	vfmsub213ss	%xmm10, %xmm2, %xmm1    # xmm1 = (xmm2 * xmm1) - xmm10
	vmulss	%xmm2, %xmm11, %xmm14
	vaddss	%xmm1, %xmm14, %xmm15
	vsubss	%xmm1, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm4
	vsubss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm12, %xmm15, %xmm4
	vsubss	%xmm15, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm3
	vsubss	%xmm3, %xmm15, %xmm3
	vsubss	%xmm7, %xmm12, %xmm7
	vaddss	%xmm7, %xmm3, %xmm3
	vfmadd231ss	%xmm11, %xmm5, %xmm3    # xmm3 = (xmm5 * xmm11) + xmm3
	vmovaps	%xmm5, %xmm15
	vfmsub213ss	%xmm14, %xmm2, %xmm11   # xmm11 = (xmm2 * xmm11) - xmm14
	vsubss	%xmm0, %xmm14, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vfmadd231ss	%xmm9, %xmm2, %xmm11    # xmm11 = (xmm2 * xmm9) + xmm11
	vmovaps	%xmm2, %xmm14
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm1, %xmm10, %xmm3
	vsubss	%xmm3, %xmm10, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vbroadcastss	.LCPI39_2(%rip), %xmm2  # xmm2 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm3, %xmm4
	vxorps	%xmm2, %xmm1, %xmm7
	vmovss	%xmm6, 168(%rsp)                # 4-byte Spill
	vsubss	%xmm3, %xmm6, %xmm3
	vsubss	%xmm6, %xmm3, %xmm9
	vsubss	%xmm9, %xmm3, %xmm10
	vsubss	%xmm10, %xmm6, %xmm10
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm4, %xmm10, %xmm4
	vmovss	192(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm9
	vsubss	%xmm9, %xmm1, %xmm10
	vsubss	%xmm10, %xmm5, %xmm10
	vsubss	%xmm9, %xmm7, %xmm7
	vaddss	%xmm7, %xmm10, %xmm7
	vaddss	%xmm4, %xmm1, %xmm9
	vsubss	%xmm1, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vmovss	96(%rsp), %xmm2                 # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm1
	vsubss	%xmm1, %xmm9, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm1, %xmm3, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vaddss	%xmm1, %xmm3, %xmm11
	vmulss	%xmm8, %xmm9, %xmm10
	vmulss	%xmm8, %xmm11, %xmm1
	vmovaps	%xmm8, %xmm3
	vfmsub213ss	%xmm1, %xmm11, %xmm3    # xmm3 = (xmm11 * xmm3) - xmm1
	vfmadd231ss	%xmm0, %xmm8, %xmm3     # xmm3 = (xmm8 * xmm0) + xmm3
	vfmsub213ss	%xmm10, %xmm9, %xmm8    # xmm8 = (xmm9 * xmm8) - xmm10
	vxorps	%xmm2, %xmm2, %xmm2
	vmulss	%xmm2, %xmm9, %xmm0
	vaddss	%xmm0, %xmm8, %xmm4
	vsubss	%xmm8, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vxorps	%xmm12, %xmm12, %xmm12
	vfmsub213ss	%xmm0, %xmm9, %xmm12    # xmm12 = (xmm9 * xmm12) - xmm0
	vsubss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm1, %xmm4, %xmm7
	vsubss	%xmm4, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm13
	vsubss	%xmm13, %xmm4, %xmm4
	vsubss	%xmm8, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vfmadd231ss	%xmm11, %xmm2, %xmm1    # xmm1 = (xmm2 * xmm11) + xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vfmadd231ss	%xmm2, %xmm9, %xmm12    # xmm12 = (xmm9 * xmm2) + xmm12
	vaddss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm3
	vaddss	%xmm0, %xmm3, %xmm2
	vaddss	%xmm1, %xmm10, %xmm9
	vsubss	%xmm9, %xmm10, %xmm0
	vaddss	%xmm1, %xmm0, %xmm8
	vmulss	%xmm9, %xmm9, %xmm0
	vmovaps	%xmm9, %xmm1
	vfmsub213ss	%xmm0, %xmm9, %xmm1     # xmm1 = (xmm9 * xmm1) - xmm0
	vmulss	%xmm8, %xmm9, %xmm3
	vaddss	%xmm3, %xmm1, %xmm4
	vsubss	%xmm1, %xmm4, %xmm10
	vsubss	%xmm10, %xmm4, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vmovaps	%xmm8, %xmm11
	vfmsub213ss	%xmm3, %xmm9, %xmm11    # xmm11 = (xmm9 * xmm11) - xmm3
	vsubss	%xmm10, %xmm3, %xmm3
	vmulss	%xmm9, %xmm8, %xmm10
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm4, %xmm10, %xmm3
	vsubss	%xmm4, %xmm3, %xmm12
	vsubss	%xmm12, %xmm3, %xmm13
	vsubss	%xmm13, %xmm4, %xmm4
	vmovaps	%xmm9, %xmm13
	vfmsub213ss	%xmm10, %xmm8, %xmm13   # xmm13 = (xmm8 * xmm13) - xmm10
	vsubss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm4, %xmm10, %xmm4
	vfmadd231ss	%xmm8, %xmm8, %xmm4     # xmm4 = (xmm8 * xmm8) + xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vmovss	%xmm2, 172(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm2, %xmm9, %xmm11    # xmm11 = (xmm9 * xmm2) + xmm11
	vaddss	%xmm1, %xmm11, %xmm1
	vfmadd231ss	%xmm9, %xmm2, %xmm13    # xmm13 = (xmm2 * xmm9) + xmm13
	vaddss	%xmm1, %xmm13, %xmm1
	vaddss	%xmm1, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm1, %xmm3, %xmm10
	vaddss	%xmm4, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm12
	vmulss	%xmm1, %xmm14, %xmm11
	vmovaps	%xmm15, %xmm6
	vmulss	%xmm1, %xmm15, %xmm2
	vmovaps	%xmm1, %xmm3
	vfmsub213ss	%xmm2, %xmm15, %xmm3    # xmm3 = (xmm15 * xmm3) - xmm2
	vfmadd231ss	8(%rsp), %xmm1, %xmm3   # 4-byte Folded Reload
                                        # xmm3 = (xmm1 * mem) + xmm3
	vmovaps	%xmm14, %xmm5
	vfmsub213ss	%xmm11, %xmm14, %xmm1   # xmm1 = (xmm14 * xmm1) - xmm11
	vmulss	%xmm12, %xmm14, %xmm4
	vaddss	%xmm4, %xmm1, %xmm13
	vsubss	%xmm1, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm1, %xmm1
	vaddss	%xmm2, %xmm13, %xmm15
	vsubss	%xmm13, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm7
	vsubss	%xmm7, %xmm13, %xmm7
	vbroadcastss	.LCPI39_2(%rip), %xmm13 # xmm13 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vsubss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vfmadd231ss	%xmm12, %xmm6, %xmm0    # xmm0 = (xmm6 * xmm12) + xmm0
	vfmsub213ss	%xmm4, %xmm5, %xmm12    # xmm12 = (xmm5 * xmm12) - xmm4
	vsubss	%xmm14, %xmm4, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vfmadd231ss	%xmm10, %xmm5, %xmm12   # xmm12 = (xmm5 * xmm10) + xmm12
	vmovaps	%xmm5, %xmm14
	vaddss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm15, %xmm1
	vsubss	%xmm1, %xmm15, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm11, %xmm2
	vsubss	%xmm2, %xmm11, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vxorps	%xmm2, %xmm13, %xmm3
	vmovss	168(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm7
	vsubss	%xmm7, %xmm5, %xmm6
	vxorps	%xmm1, %xmm13, %xmm7
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vmovss	192(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm4
	vsubss	%xmm4, %xmm1, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm4, %xmm7, %xmm4
	vaddss	%xmm4, %xmm5, %xmm4
	vaddss	%xmm3, %xmm1, %xmm5
	vsubss	%xmm1, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm1, %xmm1
	vmovss	96(%rsp), %xmm3                 # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm3
	vaddss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm1, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm0
	vaddss	%xmm1, %xmm0, %xmm6
	vmulss	%xmm5, %xmm9, %xmm0
	vmulss	%xmm6, %xmm9, %xmm1
	vmovaps	%xmm9, %xmm2
	vfmsub213ss	%xmm1, %xmm6, %xmm2     # xmm2 = (xmm6 * xmm2) - xmm1
	vfmadd231ss	%xmm3, %xmm9, %xmm2     # xmm2 = (xmm9 * xmm3) + xmm2
	vfmsub213ss	%xmm0, %xmm5, %xmm9     # xmm9 = (xmm5 * xmm9) - xmm0
	vmulss	%xmm5, %xmm8, %xmm3
	vaddss	%xmm3, %xmm9, %xmm4
	vsubss	%xmm9, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm10
	vsubss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm1, %xmm4, %xmm10
	vsubss	%xmm4, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm4, %xmm4
	vsubss	%xmm11, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vfmadd231ss	%xmm8, %xmm6, %xmm1     # xmm1 = (xmm6 * xmm8) + xmm1
	vfmsub213ss	%xmm3, %xmm5, %xmm8     # xmm8 = (xmm5 * xmm8) - xmm3
	vsubss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm9, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vfmadd231ss	172(%rsp), %xmm5, %xmm8 # 4-byte Folded Reload
                                        # xmm8 = (xmm5 * mem) + xmm8
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm10, %xmm2
	vsubss	%xmm2, %xmm10, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm2
	vmulss	%xmm3, %xmm14, %xmm0
	vmovss	120(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm6, %xmm4
	vmovaps	%xmm3, %xmm5
	vfmsub213ss	%xmm4, %xmm6, %xmm5     # xmm5 = (xmm6 * xmm5) - xmm4
	vmovaps	%xmm6, %xmm10
	vfmadd231ss	8(%rsp), %xmm3, %xmm5   # 4-byte Folded Reload
                                        # xmm5 = (xmm3 * mem) + xmm5
	vfmsub213ss	%xmm0, %xmm14, %xmm3    # xmm3 = (xmm14 * xmm3) - xmm0
	vmulss	%xmm2, %xmm14, %xmm6
	vaddss	%xmm6, %xmm3, %xmm7
	vaddss	%xmm4, %xmm7, %xmm8
	vsubss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm9, %xmm4, %xmm4
	vsubss	%xmm9, %xmm8, %xmm9
	vsubss	%xmm9, %xmm7, %xmm9
	vaddss	%xmm4, %xmm9, %xmm4
	vfmadd231ss	%xmm2, %xmm10, %xmm4    # xmm4 = (xmm10 * xmm2) + xmm4
	vfmsub213ss	%xmm6, %xmm14, %xmm2    # xmm2 = (xmm14 * xmm2) - xmm6
	vfmadd231ss	%xmm1, %xmm14, %xmm2    # xmm2 = (xmm14 * xmm1) + xmm2
	vsubss	%xmm3, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm1, %xmm6, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm8, %xmm2
	vsubss	%xmm2, %xmm8, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	movl	$2, %eax
	vxorps	%xmm15, %xmm15, %xmm15
	vcvtsi2ss	%eax, %xmm15, %xmm2
	vmulss	132(%rsp), %xmm2, %xmm2         # 4-byte Folded Reload
	vmulss	%xmm2, %xmm3, %xmm3
	vmulss	%xmm2, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0],xmm0[0],xmm3[2,3]
	vmulss	%xmm2, %xmm1, %xmm4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB39_309:
	vucomiss	%xmm2, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB39_319
# %bb.310:
	vmovss	%xmm4, 8(%rsp)                  # 4-byte Spill
	vmovups	%xmm0, 96(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vxorps	%xmm2, %xmm2, %xmm2
	vucomiss	%xmm2, %xmm0
	setnp	%cl
	sete	%dl
	vmovd	.LCPI39_0(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	vmovdqa	%xmm4, %xmm8
	testb	%cl, %dl
	jne	.LBB39_314
# %bb.311:
	movl	%eax, %ecx
	andl	$2139095040, %ecx               # imm = 0x7F800000
	cmpl	$2139095040, %ecx               # imm = 0x7F800000
	je	.LBB39_313
# %bb.312:
	vmovd	%ecx, %xmm8
	jmp	.LBB39_314
.LBB39_313:
	vmovd	.LCPI39_0(%rip), %xmm8          # xmm8 = mem[0],zero,zero,zero
.LBB39_314:
	vucomiss	%xmm2, %xmm0
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB39_318
# %bb.315:
	andl	$2139095040, %eax               # imm = 0x7F800000
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB39_317
# %bb.316:
	movl	$2130706432, %ecx               # imm = 0x7F000000
	subl	%eax, %ecx
	vmovd	%ecx, %xmm4
	jmp	.LBB39_318
.LBB39_317:
	vmovd	.LCPI39_0(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
.LBB39_318:
	vmovss	.LCPI39_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm4, %xmm5
	vmovups	96(%rsp), %xmm3                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmulss	%xmm4, %xmm3, %xmm2
	vmulss	%xmm5, %xmm2, %xmm7
	vmovshdup	%xmm3, %xmm3            # xmm3 = xmm3[1,1,3,3]
	vmulss	%xmm4, %xmm3, %xmm3
	vmulss	%xmm5, %xmm3, %xmm2
	vmovss	%xmm2, 168(%rsp)                # 4-byte Spill
	vmulss	8(%rsp), %xmm4, %xmm4           # 4-byte Folded Reload
	movl	$3, %eax
	vcvtsi2ss	%eax, %xmm13, %xmm6
	vmulss	%xmm5, %xmm4, %xmm2
	vmovss	%xmm2, 8(%rsp)                  # 4-byte Spill
	vmovd	%xmm8, 120(%rsp)                # 4-byte Folded Spill
	vdivss	%xmm0, %xmm8, %xmm8
	vmulss	%xmm1, %xmm6, %xmm5
	vfmsub213ss	%xmm5, %xmm6, %xmm1     # xmm1 = (xmm6 * xmm1) - xmm5
	vxorps	%xmm3, %xmm3, %xmm3
	vmulss	%xmm3, %xmm6, %xmm0
	vxorps	%xmm9, %xmm9, %xmm9
	vfmsub213ss	%xmm0, %xmm6, %xmm9     # xmm9 = (xmm6 * xmm9) - xmm0
	vaddss	%xmm0, %xmm1, %xmm10
	vsubss	%xmm1, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm1, %xmm12
	vsubss	%xmm11, %xmm0, %xmm0
	vaddss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vfmadd231ss	%xmm6, %xmm3, %xmm0     # xmm0 = (xmm3 * xmm6) + xmm0
	vaddss	%xmm0, %xmm10, %xmm9
	vsubss	%xmm9, %xmm10, %xmm6
	vaddss	%xmm0, %xmm6, %xmm0
	vmovss	%xmm0, 96(%rsp)                 # 4-byte Spill
	vaddss	%xmm5, %xmm9, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm9, %xmm0
	vmovss	%xmm0, 192(%rsp)                # 4-byte Spill
	vmulss	%xmm8, %xmm8, %xmm10
	vmovaps	%xmm8, %xmm9
	vfmsub213ss	%xmm10, %xmm8, %xmm9    # xmm9 = (xmm8 * xmm9) - xmm10
	vmulss	%xmm3, %xmm8, %xmm11
	vaddss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm9, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm9, %xmm9
	vxorps	%xmm14, %xmm14, %xmm14
	vfmsub213ss	%xmm11, %xmm8, %xmm14   # xmm14 = (xmm8 * xmm14) - xmm11
	vsubss	%xmm13, %xmm11, %xmm11
	vmulss	%xmm3, %xmm8, %xmm13
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm13, %xmm12, %xmm11
	vsubss	%xmm12, %xmm11, %xmm15
	vsubss	%xmm15, %xmm11, %xmm1
	vsubss	%xmm1, %xmm12, %xmm1
	vmovaps	%xmm8, %xmm12
	vfmsub213ss	%xmm13, %xmm3, %xmm12   # xmm12 = (xmm3 * xmm12) - xmm13
	vsubss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm1, %xmm13, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vfmadd231ss	%xmm0, %xmm0, %xmm1     # xmm1 = (xmm0 * xmm0) + xmm1
	vaddss	%xmm1, %xmm9, %xmm1
	vfmadd231ss	%xmm0, %xmm8, %xmm14    # xmm14 = (xmm8 * xmm0) + xmm14
	vaddss	%xmm1, %xmm14, %xmm1
	vfmadd231ss	%xmm8, %xmm0, %xmm12    # xmm12 = (xmm0 * xmm8) + xmm12
	vaddss	%xmm1, %xmm12, %xmm1
	vaddss	%xmm1, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm9
	vaddss	%xmm1, %xmm9, %xmm9
	vaddss	%xmm12, %xmm10, %xmm1
	vsubss	%xmm1, %xmm10, %xmm10
	vaddss	%xmm12, %xmm10, %xmm11
	vmulss	%xmm1, %xmm7, %xmm10
	vmovss	168(%rsp), %xmm3                # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm3, %xmm12
	vmovaps	%xmm1, %xmm13
	vfmsub213ss	%xmm12, %xmm3, %xmm13   # xmm13 = (xmm3 * xmm13) - xmm12
	vfmadd231ss	%xmm1, %xmm2, %xmm13    # xmm13 = (xmm2 * xmm1) + xmm13
	vfmsub213ss	%xmm10, %xmm7, %xmm1    # xmm1 = (xmm7 * xmm1) - xmm10
	vmulss	%xmm7, %xmm11, %xmm14
	vaddss	%xmm1, %xmm14, %xmm15
	vsubss	%xmm1, %xmm15, %xmm2
	vsubss	%xmm2, %xmm15, %xmm4
	vsubss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm12, %xmm15, %xmm4
	vsubss	%xmm15, %xmm4, %xmm0
	vsubss	%xmm0, %xmm4, %xmm5
	vsubss	%xmm5, %xmm15, %xmm5
	vsubss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm0, %xmm5, %xmm0
	vfmadd231ss	%xmm11, %xmm3, %xmm0    # xmm0 = (xmm3 * xmm11) + xmm0
	vmovaps	%xmm3, %xmm15
	vfmsub213ss	%xmm14, %xmm7, %xmm11   # xmm11 = (xmm7 * xmm11) - xmm14
	vsubss	%xmm2, %xmm14, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vfmadd231ss	%xmm9, %xmm7, %xmm11    # xmm11 = (xmm7 * xmm9) + xmm11
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm10, %xmm2
	vsubss	%xmm2, %xmm10, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vbroadcastss	.LCPI39_2(%rip), %xmm5  # xmm5 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm5, %xmm2, %xmm4
	vxorps	%xmm5, %xmm1, %xmm5
	vmovss	%xmm6, 172(%rsp)                # 4-byte Spill
	vsubss	%xmm2, %xmm6, %xmm2
	vsubss	%xmm6, %xmm2, %xmm9
	vsubss	%xmm9, %xmm4, %xmm4
	vsubss	%xmm9, %xmm2, %xmm9
	vsubss	%xmm9, %xmm6, %xmm9
	vaddss	%xmm4, %xmm9, %xmm4
	vmovss	192(%rsp), %xmm3                # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm3, %xmm1
	vsubss	%xmm3, %xmm1, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vsubss	%xmm9, %xmm1, %xmm9
	vsubss	%xmm9, %xmm3, %xmm9
	vaddss	%xmm5, %xmm9, %xmm5
	vaddss	%xmm4, %xmm1, %xmm9
	vsubss	%xmm1, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vmovss	96(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm1
	vsubss	%xmm1, %xmm9, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm1, %xmm2, %xmm9
	vsubss	%xmm9, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm11
	vmulss	%xmm8, %xmm9, %xmm10
	vmulss	%xmm8, %xmm11, %xmm1
	vmovaps	%xmm8, %xmm2
	vfmsub213ss	%xmm1, %xmm11, %xmm2    # xmm2 = (xmm11 * xmm2) - xmm1
	vfmadd231ss	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm0) + xmm2
	vfmsub213ss	%xmm10, %xmm9, %xmm8    # xmm8 = (xmm9 * xmm8) - xmm10
	vxorps	%xmm3, %xmm3, %xmm3
	vmulss	%xmm3, %xmm9, %xmm0
	vaddss	%xmm0, %xmm8, %xmm4
	vsubss	%xmm8, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vxorps	%xmm12, %xmm12, %xmm12
	vfmsub213ss	%xmm0, %xmm9, %xmm12    # xmm12 = (xmm9 * xmm12) - xmm0
	vsubss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm8
	vsubss	%xmm8, %xmm5, %xmm13
	vsubss	%xmm13, %xmm4, %xmm4
	vsubss	%xmm8, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vfmadd231ss	%xmm11, %xmm3, %xmm1    # xmm1 = (xmm3 * xmm11) + xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vfmadd231ss	%xmm3, %xmm9, %xmm12    # xmm12 = (xmm9 * xmm3) + xmm12
	vaddss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm2
	vsubss	%xmm2, %xmm5, %xmm1
	vaddss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm2, %xmm10, %xmm9
	vsubss	%xmm9, %xmm10, %xmm0
	vaddss	%xmm2, %xmm0, %xmm8
	vmulss	%xmm9, %xmm9, %xmm0
	vmovaps	%xmm9, %xmm2
	vfmsub213ss	%xmm0, %xmm9, %xmm2     # xmm2 = (xmm9 * xmm2) - xmm0
	vmulss	%xmm8, %xmm9, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm10
	vsubss	%xmm10, %xmm2, %xmm2
	vmovaps	%xmm8, %xmm10
	vfmsub213ss	%xmm3, %xmm9, %xmm10    # xmm10 = (xmm9 * xmm10) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmulss	%xmm9, %xmm8, %xmm3
	vaddss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm11
	vsubss	%xmm11, %xmm5, %xmm12
	vsubss	%xmm12, %xmm4, %xmm4
	vmovaps	%xmm9, %xmm12
	vfmsub213ss	%xmm3, %xmm8, %xmm12    # xmm12 = (xmm8 * xmm12) - xmm3
	vsubss	%xmm11, %xmm3, %xmm3
	vaddss	%xmm3, %xmm4, %xmm3
	vfmadd231ss	%xmm8, %xmm8, %xmm3     # xmm3 = (xmm8 * xmm8) + xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm1, 132(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm1, %xmm9, %xmm10    # xmm10 = (xmm9 * xmm1) + xmm10
	vaddss	%xmm2, %xmm10, %xmm2
	vfmadd231ss	%xmm9, %xmm1, %xmm12    # xmm12 = (xmm1 * xmm9) + xmm12
	vaddss	%xmm2, %xmm12, %xmm2
	vaddss	%xmm2, %xmm5, %xmm3
	vsubss	%xmm3, %xmm5, %xmm4
	vaddss	%xmm2, %xmm4, %xmm10
	vaddss	%xmm3, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm11
	vmovaps	%xmm15, %xmm6
	vmulss	%xmm2, %xmm15, %xmm0
	vmovaps	%xmm2, %xmm3
	vfmsub213ss	%xmm0, %xmm15, %xmm3    # xmm3 = (xmm15 * xmm3) - xmm0
	vfmadd231ss	8(%rsp), %xmm2, %xmm3   # 4-byte Folded Reload
                                        # xmm3 = (xmm2 * mem) + xmm3
	vmulss	%xmm2, %xmm7, %xmm4
	vfmsub213ss	%xmm4, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm4
	vmulss	%xmm7, %xmm11, %xmm5
	vaddss	%xmm5, %xmm2, %xmm12
	vsubss	%xmm2, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm0, %xmm12, %xmm14
	vsubss	%xmm12, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm1
	vsubss	%xmm1, %xmm12, %xmm1
	vsubss	%xmm15, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm6, %xmm15
	vfmadd231ss	%xmm11, %xmm6, %xmm0    # xmm0 = (xmm6 * xmm11) + xmm0
	vfmsub213ss	%xmm5, %xmm7, %xmm11    # xmm11 = (xmm7 * xmm11) - xmm5
	vsubss	%xmm13, %xmm5, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vfmadd231ss	%xmm10, %xmm7, %xmm11   # xmm11 = (xmm7 * xmm10) + xmm11
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	172(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm6, %xmm3
	vsubss	%xmm6, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm6, %xmm5
	vbroadcastss	.LCPI39_2(%rip), %xmm6  # xmm6 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm6, %xmm2, %xmm2
	vxorps	%xmm6, %xmm1, %xmm6
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vmovss	192(%rsp), %xmm10               # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm10, %xmm1
	vsubss	%xmm10, %xmm1, %xmm4
	vsubss	%xmm4, %xmm1, %xmm5
	vsubss	%xmm5, %xmm10, %xmm5
	vsubss	%xmm4, %xmm6, %xmm4
	vaddss	%xmm4, %xmm5, %xmm4
	vaddss	%xmm2, %xmm1, %xmm5
	vsubss	%xmm1, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm10
	vsubss	%xmm10, %xmm1, %xmm1
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vmovss	96(%rsp), %xmm2                 # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm2
	vaddss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm0
	vaddss	%xmm1, %xmm0, %xmm6
	vmulss	%xmm5, %xmm9, %xmm0
	vmulss	%xmm6, %xmm9, %xmm1
	vmovaps	%xmm9, %xmm3
	vfmsub213ss	%xmm1, %xmm6, %xmm3     # xmm3 = (xmm6 * xmm3) - xmm1
	vfmadd231ss	%xmm2, %xmm9, %xmm3     # xmm3 = (xmm9 * xmm2) + xmm3
	vfmsub213ss	%xmm0, %xmm5, %xmm9     # xmm9 = (xmm5 * xmm9) - xmm0
	vmulss	%xmm5, %xmm8, %xmm2
	vaddss	%xmm2, %xmm9, %xmm4
	vsubss	%xmm9, %xmm4, %xmm10
	vsubss	%xmm10, %xmm4, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm1, %xmm4, %xmm11
	vsubss	%xmm4, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm4, %xmm4
	vsubss	%xmm12, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vfmadd231ss	%xmm8, %xmm6, %xmm1     # xmm1 = (xmm6 * xmm8) + xmm1
	vfmsub213ss	%xmm2, %xmm5, %xmm8     # xmm8 = (xmm5 * xmm8) - xmm2
	vsubss	%xmm10, %xmm2, %xmm2
	vaddss	%xmm2, %xmm9, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vfmadd231ss	132(%rsp), %xmm5, %xmm8 # 4-byte Folded Reload
                                        # xmm8 = (xmm5 * mem) + xmm8
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm11, %xmm2
	vsubss	%xmm2, %xmm11, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm2
	vmulss	%xmm3, %xmm7, %xmm0
	vmovaps	%xmm15, %xmm6
	vmulss	%xmm3, %xmm15, %xmm4
	vmovaps	%xmm3, %xmm5
	vfmsub213ss	%xmm4, %xmm15, %xmm5    # xmm5 = (xmm15 * xmm5) - xmm4
	vmovaps	%xmm15, %xmm11
	vfmadd231ss	8(%rsp), %xmm3, %xmm5   # 4-byte Folded Reload
                                        # xmm5 = (xmm3 * mem) + xmm5
	vfmsub213ss	%xmm0, %xmm7, %xmm3     # xmm3 = (xmm7 * xmm3) - xmm0
	vmulss	%xmm2, %xmm7, %xmm6
	vaddss	%xmm6, %xmm3, %xmm8
	vaddss	%xmm4, %xmm8, %xmm9
	vsubss	%xmm8, %xmm9, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vsubss	%xmm10, %xmm9, %xmm10
	vsubss	%xmm10, %xmm8, %xmm10
	vaddss	%xmm4, %xmm10, %xmm4
	vfmadd231ss	%xmm2, %xmm15, %xmm4    # xmm4 = (xmm15 * xmm2) + xmm4
	vfmsub213ss	%xmm6, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm6
	vfmadd231ss	%xmm1, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm1) + xmm2
	vsubss	%xmm3, %xmm8, %xmm1
	vsubss	%xmm1, %xmm8, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm1, %xmm6, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm9, %xmm2
	vsubss	%xmm2, %xmm9, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	movl	$2, %eax
	vxorps	%xmm14, %xmm14, %xmm14
	vcvtsi2ss	%eax, %xmm14, %xmm2
	vmulss	120(%rsp), %xmm2, %xmm2         # 4-byte Folded Reload
	vmulss	%xmm2, %xmm3, %xmm3
	vmulss	%xmm2, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0],xmm0[0],xmm3[2,3]
	vmulss	%xmm2, %xmm1, %xmm4
.LBB39_319:
	vmovlps	%xmm0, 48(%rsp)
	vmovss	%xmm4, 56(%rsp)
	leaq	280(%rsp), %r14
	movq	%r14, 264(%rsp)
	movq	$32, 16(%rsp)
.Ltmp5342:
	leaq	264(%rsp), %rdi
	leaq	16(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp5343:
# %bb.320:
	movq	%rax, 264(%rsp)
	movq	16(%rsp), %rcx
	movq	%rcx, 280(%rsp)
	vmovupd	.L.str.63(%rip), %ymm0
	vmovupd	%ymm0, (%rax)
	movq	%rcx, 272(%rsp)
	movq	264(%rsp), %rax
	movb	$0, (%rax,%rcx)
.Ltmp5345:
	leaq	264(%rsp), %rdi
	leaq	48(%rsp), %rsi
	xorl	%edx, %edx
	vzeroupper
	callq	_Z5printIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp5346:
# %bb.321:
	movq	264(%rsp), %rdi
	cmpq	%r14, %rdi
	je	.LBB39_323
# %bb.322:
	callq	_ZdlPv
.LBB39_323:
	addq	$472, %rsp                      # imm = 0x1D8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB39_324:
	.cfi_def_cfa_offset 528
.Ltmp5341:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_325:
.Ltmp5335:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_326:
.Ltmp5292:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_327:
.Ltmp5186:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_328:
.Ltmp5180:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_329:
.Ltmp5137:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_330:
.Ltmp5028:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_331:
.Ltmp5025:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_332:
.Ltmp5022:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_333:
.Ltmp5019:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_334:
.Ltmp4964:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_335:
.Ltmp4961:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_336:
.Ltmp4958:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_337:
.Ltmp4955:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_338:
.Ltmp4900:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_339:
.Ltmp4897:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_340:
.Ltmp4894:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_341:
.Ltmp4891:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_342:
.Ltmp5347:
	movq	%rax, %rbx
	movq	264(%rsp), %rdi
	cmpq	%r14, %rdi
	jne	.LBB39_360
	jmp	.LBB39_361
.LBB39_343:
.Ltmp5344:
	movq	%rax, %rdi
	callq	_Unwind_Resume@PLT
.LBB39_344:
.Ltmp5338:
	movq	%rax, %rbx
	movq	312(%rsp), %rdi
	jmp	.LBB39_348
.LBB39_345:
.Ltmp5332:
	movq	%rax, %rbx
	jmp	.LBB39_420
.LBB39_346:
.Ltmp5276:
	movq	%rax, %rbx
	jmp	.LBB39_420
.LBB39_347:
.Ltmp5183:
	movq	%rax, %rbx
	movq	344(%rsp), %rdi
.LBB39_348:
	cmpq	%r15, %rdi
	je	.LBB39_426
# %bb.349:
	callq	_ZdlPv
	jmp	.LBB39_426
.LBB39_350:
.Ltmp5177:
	movq	%rax, %rbx
	jmp	.LBB39_420
.LBB39_351:
.Ltmp5121:
	movq	%rax, %rbx
	jmp	.LBB39_420
.LBB39_352:
.Ltmp5031:
	movq	%rax, %rbx
	movq	376(%rsp), %rdi
	jmp	.LBB39_359
.LBB39_353:
.Ltmp5016:
	jmp	.LBB39_364
.LBB39_354:
.Ltmp5013:
	jmp	.LBB39_364
.LBB39_355:
.Ltmp4967:
	movq	%rax, %rbx
	movq	408(%rsp), %rdi
	jmp	.LBB39_359
.LBB39_356:
.Ltmp4952:
	jmp	.LBB39_364
.LBB39_357:
.Ltmp4949:
	jmp	.LBB39_364
.LBB39_358:
.Ltmp4903:
	movq	%rax, %rbx
	movq	440(%rsp), %rdi
.LBB39_359:
	cmpq	%r15, %rdi
	je	.LBB39_361
.LBB39_360:
	callq	_ZdlPv
.LBB39_361:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB39_362:
.Ltmp4888:
	jmp	.LBB39_364
.LBB39_363:
.Ltmp4885:
.LBB39_364:
	movq	%rax, %rbx
	leaq	48(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB39_387
.LBB39_365:
.Ltmp5329:
	movq	%rax, %rbx
	jmp	.LBB39_420
.LBB39_366:
.Ltmp5174:
	movq	%rax, %rbx
	jmp	.LBB39_420
.LBB39_367:
.Ltmp4999:
	jmp	.LBB39_386
.LBB39_368:
.Ltmp4983:
	movq	%rax, %rbx
	jmp	.LBB39_388
.LBB39_369:
.Ltmp4935:
	jmp	.LBB39_386
.LBB39_370:
.Ltmp4919:
	movq	%rax, %rbx
	jmp	.LBB39_388
.LBB39_371:
.Ltmp4871:
	jmp	.LBB39_386
.LBB39_372:
.Ltmp4855:
	movq	%rax, %rbx
	jmp	.LBB39_388
.LBB39_373:
.Ltmp5289:
	movq	%rax, %rbx
	jmp	.LBB39_420
.LBB39_374:
.Ltmp5134:
	movq	%rax, %rbx
	jmp	.LBB39_420
.LBB39_375:
.Ltmp5324:
	movq	%rax, %rbx
	jmp	.LBB39_426
.LBB39_376:
.Ltmp5273:
	movq	%rax, %rbx
	jmp	.LBB39_426
.LBB39_377:
.Ltmp5169:
	movq	%rax, %rbx
	jmp	.LBB39_426
.LBB39_378:
.Ltmp5118:
	movq	%rax, %rbx
	jmp	.LBB39_426
.LBB39_379:
.Ltmp5010:
	jmp	.LBB39_386
.LBB39_380:
.Ltmp4994:
	movq	%rax, %rbx
	jmp	.LBB39_388
.LBB39_381:
.Ltmp4978:
	jmp	.LBB39_391
.LBB39_382:
.Ltmp4946:
	jmp	.LBB39_386
.LBB39_383:
.Ltmp4930:
	movq	%rax, %rbx
	jmp	.LBB39_388
.LBB39_384:
.Ltmp4914:
	jmp	.LBB39_391
.LBB39_385:
.Ltmp4882:
.LBB39_386:
	movq	%rax, %rbx
.LBB39_387:
	leaq	16(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB39_388:
	leaq	136(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	leaq	216(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB39_389:
.Ltmp4866:
	movq	%rax, %rbx
	jmp	.LBB39_388
.LBB39_390:
.Ltmp4850:
.LBB39_391:
	movq	%rax, %rbx
	leaq	216(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB39_392:
.Ltmp5313:
	movq	%rax, %rbx
	jmp	.LBB39_426
.LBB39_393:
.Ltmp5158:
	movq	%rax, %rbx
	jmp	.LBB39_426
.LBB39_394:
.Ltmp5262:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_395:
.Ltmp5259:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_396:
.Ltmp5256:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_397:
.Ltmp5208:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_398:
.Ltmp5107:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_399:
.Ltmp5104:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_400:
.Ltmp5101:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_401:
.Ltmp5053:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB39_402:
.Ltmp5189:
	movq	%rax, %rbx
	jmp	.LBB39_426
.LBB39_403:
.Ltmp5034:
	movq	%rax, %rbx
	jmp	.LBB39_426
.LBB39_404:
.Ltmp5240:
	jmp	.LBB39_413
.LBB39_405:
.Ltmp5224:
	jmp	.LBB39_418
.LBB39_406:
.Ltmp5205:
	jmp	.LBB39_410
.LBB39_407:
.Ltmp5085:
	jmp	.LBB39_413
.LBB39_408:
.Ltmp5069:
	jmp	.LBB39_418
.LBB39_409:
.Ltmp5050:
.LBB39_410:
	movq	%rax, %rbx
	leaq	16(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB39_424
.LBB39_411:
.Ltmp5253:
	jmp	.LBB39_413
.LBB39_412:
.Ltmp5098:
.LBB39_413:
	movq	%rax, %rbx
	leaq	136(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB39_419
.LBB39_414:
.Ltmp5235:
	jmp	.LBB39_418
.LBB39_415:
.Ltmp5219:
	movq	%rax, %rbx
	jmp	.LBB39_420
.LBB39_416:
.Ltmp5200:
	jmp	.LBB39_423
.LBB39_417:
.Ltmp5080:
.LBB39_418:
	movq	%rax, %rbx
.LBB39_419:
	leaq	216(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB39_420:
	leaq	16(%rsp), %rdi
	jmp	.LBB39_425
.LBB39_421:
.Ltmp5064:
	movq	%rax, %rbx
	jmp	.LBB39_420
.LBB39_422:
.Ltmp5045:
.LBB39_423:
	movq	%rax, %rbx
.LBB39_424:
	leaq	136(%rsp), %rdi
.LBB39_425:
	callq	_ZN4mpfr6mprealD2Ev
.LBB39_426:
	leaq	48(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end39:
	.size	_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_, .Lfunc_end39-_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.cfi_endproc
	.section	.gcc_except_table._Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"aG",@progbits,_Z6verifyIN7mX_real7tX_real7tx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.p2align	2, 0x0
GCC_except_table39:
.Lexception32:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase21-.Lttbaseref21
.Lttbaseref21:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end32-.Lcst_begin32
.Lcst_begin32:
	.uleb128 .Lfunc_begin32-.Lfunc_begin32  # >> Call Site 1 <<
	.uleb128 .Ltmp4840-.Lfunc_begin32       #   Call between .Lfunc_begin32 and .Ltmp4840
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4840-.Lfunc_begin32       # >> Call Site 2 <<
	.uleb128 .Ltmp4849-.Ltmp4840            #   Call between .Ltmp4840 and .Ltmp4849
	.uleb128 .Ltmp4850-.Lfunc_begin32       #     jumps to .Ltmp4850
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4851-.Lfunc_begin32       # >> Call Site 3 <<
	.uleb128 .Ltmp4854-.Ltmp4851            #   Call between .Ltmp4851 and .Ltmp4854
	.uleb128 .Ltmp4855-.Lfunc_begin32       #     jumps to .Ltmp4855
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4856-.Lfunc_begin32       # >> Call Site 4 <<
	.uleb128 .Ltmp4865-.Ltmp4856            #   Call between .Ltmp4856 and .Ltmp4865
	.uleb128 .Ltmp4866-.Lfunc_begin32       #     jumps to .Ltmp4866
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4867-.Lfunc_begin32       # >> Call Site 5 <<
	.uleb128 .Ltmp4870-.Ltmp4867            #   Call between .Ltmp4867 and .Ltmp4870
	.uleb128 .Ltmp4871-.Lfunc_begin32       #     jumps to .Ltmp4871
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4872-.Lfunc_begin32       # >> Call Site 6 <<
	.uleb128 .Ltmp4881-.Ltmp4872            #   Call between .Ltmp4872 and .Ltmp4881
	.uleb128 .Ltmp4882-.Lfunc_begin32       #     jumps to .Ltmp4882
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4883-.Lfunc_begin32       # >> Call Site 7 <<
	.uleb128 .Ltmp4884-.Ltmp4883            #   Call between .Ltmp4883 and .Ltmp4884
	.uleb128 .Ltmp4885-.Lfunc_begin32       #     jumps to .Ltmp4885
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4886-.Lfunc_begin32       # >> Call Site 8 <<
	.uleb128 .Ltmp4887-.Ltmp4886            #   Call between .Ltmp4886 and .Ltmp4887
	.uleb128 .Ltmp4888-.Lfunc_begin32       #     jumps to .Ltmp4888
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4889-.Lfunc_begin32       # >> Call Site 9 <<
	.uleb128 .Ltmp4890-.Ltmp4889            #   Call between .Ltmp4889 and .Ltmp4890
	.uleb128 .Ltmp4891-.Lfunc_begin32       #     jumps to .Ltmp4891
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4892-.Lfunc_begin32       # >> Call Site 10 <<
	.uleb128 .Ltmp4893-.Ltmp4892            #   Call between .Ltmp4892 and .Ltmp4893
	.uleb128 .Ltmp4894-.Lfunc_begin32       #     jumps to .Ltmp4894
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4895-.Lfunc_begin32       # >> Call Site 11 <<
	.uleb128 .Ltmp4896-.Ltmp4895            #   Call between .Ltmp4895 and .Ltmp4896
	.uleb128 .Ltmp4897-.Lfunc_begin32       #     jumps to .Ltmp4897
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4898-.Lfunc_begin32       # >> Call Site 12 <<
	.uleb128 .Ltmp4899-.Ltmp4898            #   Call between .Ltmp4898 and .Ltmp4899
	.uleb128 .Ltmp4900-.Lfunc_begin32       #     jumps to .Ltmp4900
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4901-.Lfunc_begin32       # >> Call Site 13 <<
	.uleb128 .Ltmp4902-.Ltmp4901            #   Call between .Ltmp4901 and .Ltmp4902
	.uleb128 .Ltmp4903-.Lfunc_begin32       #     jumps to .Ltmp4903
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4902-.Lfunc_begin32       # >> Call Site 14 <<
	.uleb128 .Ltmp4904-.Ltmp4902            #   Call between .Ltmp4902 and .Ltmp4904
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4904-.Lfunc_begin32       # >> Call Site 15 <<
	.uleb128 .Ltmp4913-.Ltmp4904            #   Call between .Ltmp4904 and .Ltmp4913
	.uleb128 .Ltmp4914-.Lfunc_begin32       #     jumps to .Ltmp4914
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4915-.Lfunc_begin32       # >> Call Site 16 <<
	.uleb128 .Ltmp4918-.Ltmp4915            #   Call between .Ltmp4915 and .Ltmp4918
	.uleb128 .Ltmp4919-.Lfunc_begin32       #     jumps to .Ltmp4919
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4920-.Lfunc_begin32       # >> Call Site 17 <<
	.uleb128 .Ltmp4929-.Ltmp4920            #   Call between .Ltmp4920 and .Ltmp4929
	.uleb128 .Ltmp4930-.Lfunc_begin32       #     jumps to .Ltmp4930
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4931-.Lfunc_begin32       # >> Call Site 18 <<
	.uleb128 .Ltmp4934-.Ltmp4931            #   Call between .Ltmp4931 and .Ltmp4934
	.uleb128 .Ltmp4935-.Lfunc_begin32       #     jumps to .Ltmp4935
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4936-.Lfunc_begin32       # >> Call Site 19 <<
	.uleb128 .Ltmp4945-.Ltmp4936            #   Call between .Ltmp4936 and .Ltmp4945
	.uleb128 .Ltmp4946-.Lfunc_begin32       #     jumps to .Ltmp4946
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4947-.Lfunc_begin32       # >> Call Site 20 <<
	.uleb128 .Ltmp4948-.Ltmp4947            #   Call between .Ltmp4947 and .Ltmp4948
	.uleb128 .Ltmp4949-.Lfunc_begin32       #     jumps to .Ltmp4949
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4950-.Lfunc_begin32       # >> Call Site 21 <<
	.uleb128 .Ltmp4951-.Ltmp4950            #   Call between .Ltmp4950 and .Ltmp4951
	.uleb128 .Ltmp4952-.Lfunc_begin32       #     jumps to .Ltmp4952
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4953-.Lfunc_begin32       # >> Call Site 22 <<
	.uleb128 .Ltmp4954-.Ltmp4953            #   Call between .Ltmp4953 and .Ltmp4954
	.uleb128 .Ltmp4955-.Lfunc_begin32       #     jumps to .Ltmp4955
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4956-.Lfunc_begin32       # >> Call Site 23 <<
	.uleb128 .Ltmp4957-.Ltmp4956            #   Call between .Ltmp4956 and .Ltmp4957
	.uleb128 .Ltmp4958-.Lfunc_begin32       #     jumps to .Ltmp4958
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4959-.Lfunc_begin32       # >> Call Site 24 <<
	.uleb128 .Ltmp4960-.Ltmp4959            #   Call between .Ltmp4959 and .Ltmp4960
	.uleb128 .Ltmp4961-.Lfunc_begin32       #     jumps to .Ltmp4961
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4962-.Lfunc_begin32       # >> Call Site 25 <<
	.uleb128 .Ltmp4963-.Ltmp4962            #   Call between .Ltmp4962 and .Ltmp4963
	.uleb128 .Ltmp4964-.Lfunc_begin32       #     jumps to .Ltmp4964
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp4965-.Lfunc_begin32       # >> Call Site 26 <<
	.uleb128 .Ltmp4966-.Ltmp4965            #   Call between .Ltmp4965 and .Ltmp4966
	.uleb128 .Ltmp4967-.Lfunc_begin32       #     jumps to .Ltmp4967
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4966-.Lfunc_begin32       # >> Call Site 27 <<
	.uleb128 .Ltmp4968-.Ltmp4966            #   Call between .Ltmp4966 and .Ltmp4968
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4968-.Lfunc_begin32       # >> Call Site 28 <<
	.uleb128 .Ltmp4977-.Ltmp4968            #   Call between .Ltmp4968 and .Ltmp4977
	.uleb128 .Ltmp4978-.Lfunc_begin32       #     jumps to .Ltmp4978
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4979-.Lfunc_begin32       # >> Call Site 29 <<
	.uleb128 .Ltmp4982-.Ltmp4979            #   Call between .Ltmp4979 and .Ltmp4982
	.uleb128 .Ltmp4983-.Lfunc_begin32       #     jumps to .Ltmp4983
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4984-.Lfunc_begin32       # >> Call Site 30 <<
	.uleb128 .Ltmp4993-.Ltmp4984            #   Call between .Ltmp4984 and .Ltmp4993
	.uleb128 .Ltmp4994-.Lfunc_begin32       #     jumps to .Ltmp4994
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp4995-.Lfunc_begin32       # >> Call Site 31 <<
	.uleb128 .Ltmp4998-.Ltmp4995            #   Call between .Ltmp4995 and .Ltmp4998
	.uleb128 .Ltmp4999-.Lfunc_begin32       #     jumps to .Ltmp4999
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5000-.Lfunc_begin32       # >> Call Site 32 <<
	.uleb128 .Ltmp5009-.Ltmp5000            #   Call between .Ltmp5000 and .Ltmp5009
	.uleb128 .Ltmp5010-.Lfunc_begin32       #     jumps to .Ltmp5010
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5011-.Lfunc_begin32       # >> Call Site 33 <<
	.uleb128 .Ltmp5012-.Ltmp5011            #   Call between .Ltmp5011 and .Ltmp5012
	.uleb128 .Ltmp5013-.Lfunc_begin32       #     jumps to .Ltmp5013
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5014-.Lfunc_begin32       # >> Call Site 34 <<
	.uleb128 .Ltmp5015-.Ltmp5014            #   Call between .Ltmp5014 and .Ltmp5015
	.uleb128 .Ltmp5016-.Lfunc_begin32       #     jumps to .Ltmp5016
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5017-.Lfunc_begin32       # >> Call Site 35 <<
	.uleb128 .Ltmp5018-.Ltmp5017            #   Call between .Ltmp5017 and .Ltmp5018
	.uleb128 .Ltmp5019-.Lfunc_begin32       #     jumps to .Ltmp5019
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5020-.Lfunc_begin32       # >> Call Site 36 <<
	.uleb128 .Ltmp5021-.Ltmp5020            #   Call between .Ltmp5020 and .Ltmp5021
	.uleb128 .Ltmp5022-.Lfunc_begin32       #     jumps to .Ltmp5022
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5023-.Lfunc_begin32       # >> Call Site 37 <<
	.uleb128 .Ltmp5024-.Ltmp5023            #   Call between .Ltmp5023 and .Ltmp5024
	.uleb128 .Ltmp5025-.Lfunc_begin32       #     jumps to .Ltmp5025
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5026-.Lfunc_begin32       # >> Call Site 38 <<
	.uleb128 .Ltmp5027-.Ltmp5026            #   Call between .Ltmp5026 and .Ltmp5027
	.uleb128 .Ltmp5028-.Lfunc_begin32       #     jumps to .Ltmp5028
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5029-.Lfunc_begin32       # >> Call Site 39 <<
	.uleb128 .Ltmp5030-.Ltmp5029            #   Call between .Ltmp5029 and .Ltmp5030
	.uleb128 .Ltmp5031-.Lfunc_begin32       #     jumps to .Ltmp5031
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5030-.Lfunc_begin32       # >> Call Site 40 <<
	.uleb128 .Ltmp5032-.Ltmp5030            #   Call between .Ltmp5030 and .Ltmp5032
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5032-.Lfunc_begin32       # >> Call Site 41 <<
	.uleb128 .Ltmp5033-.Ltmp5032            #   Call between .Ltmp5032 and .Ltmp5033
	.uleb128 .Ltmp5034-.Lfunc_begin32       #     jumps to .Ltmp5034
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5035-.Lfunc_begin32       # >> Call Site 42 <<
	.uleb128 .Ltmp5044-.Ltmp5035            #   Call between .Ltmp5035 and .Ltmp5044
	.uleb128 .Ltmp5045-.Lfunc_begin32       #     jumps to .Ltmp5045
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5046-.Lfunc_begin32       # >> Call Site 43 <<
	.uleb128 .Ltmp5049-.Ltmp5046            #   Call between .Ltmp5046 and .Ltmp5049
	.uleb128 .Ltmp5050-.Lfunc_begin32       #     jumps to .Ltmp5050
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5051-.Lfunc_begin32       # >> Call Site 44 <<
	.uleb128 .Ltmp5052-.Ltmp5051            #   Call between .Ltmp5051 and .Ltmp5052
	.uleb128 .Ltmp5053-.Lfunc_begin32       #     jumps to .Ltmp5053
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5054-.Lfunc_begin32       # >> Call Site 45 <<
	.uleb128 .Ltmp5063-.Ltmp5054            #   Call between .Ltmp5054 and .Ltmp5063
	.uleb128 .Ltmp5064-.Lfunc_begin32       #     jumps to .Ltmp5064
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5065-.Lfunc_begin32       # >> Call Site 46 <<
	.uleb128 .Ltmp5068-.Ltmp5065            #   Call between .Ltmp5065 and .Ltmp5068
	.uleb128 .Ltmp5069-.Lfunc_begin32       #     jumps to .Ltmp5069
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5070-.Lfunc_begin32       # >> Call Site 47 <<
	.uleb128 .Ltmp5079-.Ltmp5070            #   Call between .Ltmp5070 and .Ltmp5079
	.uleb128 .Ltmp5080-.Lfunc_begin32       #     jumps to .Ltmp5080
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5081-.Lfunc_begin32       # >> Call Site 48 <<
	.uleb128 .Ltmp5084-.Ltmp5081            #   Call between .Ltmp5081 and .Ltmp5084
	.uleb128 .Ltmp5085-.Lfunc_begin32       #     jumps to .Ltmp5085
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5086-.Lfunc_begin32       # >> Call Site 49 <<
	.uleb128 .Ltmp5097-.Ltmp5086            #   Call between .Ltmp5086 and .Ltmp5097
	.uleb128 .Ltmp5098-.Lfunc_begin32       #     jumps to .Ltmp5098
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5099-.Lfunc_begin32       # >> Call Site 50 <<
	.uleb128 .Ltmp5100-.Ltmp5099            #   Call between .Ltmp5099 and .Ltmp5100
	.uleb128 .Ltmp5101-.Lfunc_begin32       #     jumps to .Ltmp5101
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5102-.Lfunc_begin32       # >> Call Site 51 <<
	.uleb128 .Ltmp5103-.Ltmp5102            #   Call between .Ltmp5102 and .Ltmp5103
	.uleb128 .Ltmp5104-.Lfunc_begin32       #     jumps to .Ltmp5104
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5105-.Lfunc_begin32       # >> Call Site 52 <<
	.uleb128 .Ltmp5106-.Ltmp5105            #   Call between .Ltmp5105 and .Ltmp5106
	.uleb128 .Ltmp5107-.Lfunc_begin32       #     jumps to .Ltmp5107
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5108-.Lfunc_begin32       # >> Call Site 53 <<
	.uleb128 .Ltmp5117-.Ltmp5108            #   Call between .Ltmp5108 and .Ltmp5117
	.uleb128 .Ltmp5118-.Lfunc_begin32       #     jumps to .Ltmp5118
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5119-.Lfunc_begin32       # >> Call Site 54 <<
	.uleb128 .Ltmp5120-.Ltmp5119            #   Call between .Ltmp5119 and .Ltmp5120
	.uleb128 .Ltmp5121-.Lfunc_begin32       #     jumps to .Ltmp5121
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5122-.Lfunc_begin32       # >> Call Site 55 <<
	.uleb128 .Ltmp5133-.Ltmp5122            #   Call between .Ltmp5122 and .Ltmp5133
	.uleb128 .Ltmp5134-.Lfunc_begin32       #     jumps to .Ltmp5134
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5135-.Lfunc_begin32       # >> Call Site 56 <<
	.uleb128 .Ltmp5136-.Ltmp5135            #   Call between .Ltmp5135 and .Ltmp5136
	.uleb128 .Ltmp5137-.Lfunc_begin32       #     jumps to .Ltmp5137
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5138-.Lfunc_begin32       # >> Call Site 57 <<
	.uleb128 .Ltmp5157-.Ltmp5138            #   Call between .Ltmp5138 and .Ltmp5157
	.uleb128 .Ltmp5158-.Lfunc_begin32       #     jumps to .Ltmp5158
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5159-.Lfunc_begin32       # >> Call Site 58 <<
	.uleb128 .Ltmp5168-.Ltmp5159            #   Call between .Ltmp5159 and .Ltmp5168
	.uleb128 .Ltmp5169-.Lfunc_begin32       #     jumps to .Ltmp5169
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5170-.Lfunc_begin32       # >> Call Site 59 <<
	.uleb128 .Ltmp5173-.Ltmp5170            #   Call between .Ltmp5170 and .Ltmp5173
	.uleb128 .Ltmp5174-.Lfunc_begin32       #     jumps to .Ltmp5174
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5175-.Lfunc_begin32       # >> Call Site 60 <<
	.uleb128 .Ltmp5176-.Ltmp5175            #   Call between .Ltmp5175 and .Ltmp5176
	.uleb128 .Ltmp5177-.Lfunc_begin32       #     jumps to .Ltmp5177
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5178-.Lfunc_begin32       # >> Call Site 61 <<
	.uleb128 .Ltmp5179-.Ltmp5178            #   Call between .Ltmp5178 and .Ltmp5179
	.uleb128 .Ltmp5180-.Lfunc_begin32       #     jumps to .Ltmp5180
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5181-.Lfunc_begin32       # >> Call Site 62 <<
	.uleb128 .Ltmp5182-.Ltmp5181            #   Call between .Ltmp5181 and .Ltmp5182
	.uleb128 .Ltmp5183-.Lfunc_begin32       #     jumps to .Ltmp5183
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5184-.Lfunc_begin32       # >> Call Site 63 <<
	.uleb128 .Ltmp5185-.Ltmp5184            #   Call between .Ltmp5184 and .Ltmp5185
	.uleb128 .Ltmp5186-.Lfunc_begin32       #     jumps to .Ltmp5186
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5185-.Lfunc_begin32       # >> Call Site 64 <<
	.uleb128 .Ltmp5187-.Ltmp5185            #   Call between .Ltmp5185 and .Ltmp5187
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5187-.Lfunc_begin32       # >> Call Site 65 <<
	.uleb128 .Ltmp5188-.Ltmp5187            #   Call between .Ltmp5187 and .Ltmp5188
	.uleb128 .Ltmp5189-.Lfunc_begin32       #     jumps to .Ltmp5189
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5190-.Lfunc_begin32       # >> Call Site 66 <<
	.uleb128 .Ltmp5199-.Ltmp5190            #   Call between .Ltmp5190 and .Ltmp5199
	.uleb128 .Ltmp5200-.Lfunc_begin32       #     jumps to .Ltmp5200
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5201-.Lfunc_begin32       # >> Call Site 67 <<
	.uleb128 .Ltmp5204-.Ltmp5201            #   Call between .Ltmp5201 and .Ltmp5204
	.uleb128 .Ltmp5205-.Lfunc_begin32       #     jumps to .Ltmp5205
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5206-.Lfunc_begin32       # >> Call Site 68 <<
	.uleb128 .Ltmp5207-.Ltmp5206            #   Call between .Ltmp5206 and .Ltmp5207
	.uleb128 .Ltmp5208-.Lfunc_begin32       #     jumps to .Ltmp5208
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5209-.Lfunc_begin32       # >> Call Site 69 <<
	.uleb128 .Ltmp5218-.Ltmp5209            #   Call between .Ltmp5209 and .Ltmp5218
	.uleb128 .Ltmp5219-.Lfunc_begin32       #     jumps to .Ltmp5219
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5220-.Lfunc_begin32       # >> Call Site 70 <<
	.uleb128 .Ltmp5223-.Ltmp5220            #   Call between .Ltmp5220 and .Ltmp5223
	.uleb128 .Ltmp5224-.Lfunc_begin32       #     jumps to .Ltmp5224
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5225-.Lfunc_begin32       # >> Call Site 71 <<
	.uleb128 .Ltmp5234-.Ltmp5225            #   Call between .Ltmp5225 and .Ltmp5234
	.uleb128 .Ltmp5235-.Lfunc_begin32       #     jumps to .Ltmp5235
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5236-.Lfunc_begin32       # >> Call Site 72 <<
	.uleb128 .Ltmp5239-.Ltmp5236            #   Call between .Ltmp5236 and .Ltmp5239
	.uleb128 .Ltmp5240-.Lfunc_begin32       #     jumps to .Ltmp5240
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5241-.Lfunc_begin32       # >> Call Site 73 <<
	.uleb128 .Ltmp5252-.Ltmp5241            #   Call between .Ltmp5241 and .Ltmp5252
	.uleb128 .Ltmp5253-.Lfunc_begin32       #     jumps to .Ltmp5253
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5254-.Lfunc_begin32       # >> Call Site 74 <<
	.uleb128 .Ltmp5255-.Ltmp5254            #   Call between .Ltmp5254 and .Ltmp5255
	.uleb128 .Ltmp5256-.Lfunc_begin32       #     jumps to .Ltmp5256
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5257-.Lfunc_begin32       # >> Call Site 75 <<
	.uleb128 .Ltmp5258-.Ltmp5257            #   Call between .Ltmp5257 and .Ltmp5258
	.uleb128 .Ltmp5259-.Lfunc_begin32       #     jumps to .Ltmp5259
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5260-.Lfunc_begin32       # >> Call Site 76 <<
	.uleb128 .Ltmp5261-.Ltmp5260            #   Call between .Ltmp5260 and .Ltmp5261
	.uleb128 .Ltmp5262-.Lfunc_begin32       #     jumps to .Ltmp5262
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5263-.Lfunc_begin32       # >> Call Site 77 <<
	.uleb128 .Ltmp5272-.Ltmp5263            #   Call between .Ltmp5263 and .Ltmp5272
	.uleb128 .Ltmp5273-.Lfunc_begin32       #     jumps to .Ltmp5273
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5274-.Lfunc_begin32       # >> Call Site 78 <<
	.uleb128 .Ltmp5275-.Ltmp5274            #   Call between .Ltmp5274 and .Ltmp5275
	.uleb128 .Ltmp5276-.Lfunc_begin32       #     jumps to .Ltmp5276
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5277-.Lfunc_begin32       # >> Call Site 79 <<
	.uleb128 .Ltmp5288-.Ltmp5277            #   Call between .Ltmp5277 and .Ltmp5288
	.uleb128 .Ltmp5289-.Lfunc_begin32       #     jumps to .Ltmp5289
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5290-.Lfunc_begin32       # >> Call Site 80 <<
	.uleb128 .Ltmp5291-.Ltmp5290            #   Call between .Ltmp5290 and .Ltmp5291
	.uleb128 .Ltmp5292-.Lfunc_begin32       #     jumps to .Ltmp5292
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5293-.Lfunc_begin32       # >> Call Site 81 <<
	.uleb128 .Ltmp5312-.Ltmp5293            #   Call between .Ltmp5293 and .Ltmp5312
	.uleb128 .Ltmp5313-.Lfunc_begin32       #     jumps to .Ltmp5313
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5314-.Lfunc_begin32       # >> Call Site 82 <<
	.uleb128 .Ltmp5323-.Ltmp5314            #   Call between .Ltmp5314 and .Ltmp5323
	.uleb128 .Ltmp5324-.Lfunc_begin32       #     jumps to .Ltmp5324
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5325-.Lfunc_begin32       # >> Call Site 83 <<
	.uleb128 .Ltmp5328-.Ltmp5325            #   Call between .Ltmp5325 and .Ltmp5328
	.uleb128 .Ltmp5329-.Lfunc_begin32       #     jumps to .Ltmp5329
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5330-.Lfunc_begin32       # >> Call Site 84 <<
	.uleb128 .Ltmp5331-.Ltmp5330            #   Call between .Ltmp5330 and .Ltmp5331
	.uleb128 .Ltmp5332-.Lfunc_begin32       #     jumps to .Ltmp5332
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5333-.Lfunc_begin32       # >> Call Site 85 <<
	.uleb128 .Ltmp5334-.Ltmp5333            #   Call between .Ltmp5333 and .Ltmp5334
	.uleb128 .Ltmp5335-.Lfunc_begin32       #     jumps to .Ltmp5335
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5336-.Lfunc_begin32       # >> Call Site 86 <<
	.uleb128 .Ltmp5337-.Ltmp5336            #   Call between .Ltmp5336 and .Ltmp5337
	.uleb128 .Ltmp5338-.Lfunc_begin32       #     jumps to .Ltmp5338
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5339-.Lfunc_begin32       # >> Call Site 87 <<
	.uleb128 .Ltmp5340-.Ltmp5339            #   Call between .Ltmp5339 and .Ltmp5340
	.uleb128 .Ltmp5341-.Lfunc_begin32       #     jumps to .Ltmp5341
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5342-.Lfunc_begin32       # >> Call Site 88 <<
	.uleb128 .Ltmp5343-.Ltmp5342            #   Call between .Ltmp5342 and .Ltmp5343
	.uleb128 .Ltmp5344-.Lfunc_begin32       #     jumps to .Ltmp5344
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5345-.Lfunc_begin32       # >> Call Site 89 <<
	.uleb128 .Ltmp5346-.Ltmp5345            #   Call between .Ltmp5345 and .Ltmp5346
	.uleb128 .Ltmp5347-.Lfunc_begin32       #     jumps to .Ltmp5347
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5346-.Lfunc_begin32       # >> Call Site 90 <<
	.uleb128 .Lfunc_end39-.Ltmp5346         #   Call between .Ltmp5346 and .Lfunc_end39
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end32:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase21:
	.p2align	2, 0x0
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2, 0x0                          # -- Begin function _Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.LCPI40_0:
	.long	0x7fc00000                      #  NaN
.LCPI40_1:
	.long	0x80000000                      #  -0
.LCPI40_3:
	.long	0x00000000                      #  0
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4, 0x0
.LCPI40_2:
	.zero	16
	.section	.text._Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"axG",@progbits,_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.weak	_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.p2align	4, 0x90
	.type	_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,@function
_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_: # 
.Lfunc_begin33:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception33
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$536, %rsp                      # imm = 0x218
	.cfi_def_cfa_offset 592
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, 256(%rsp)                  # 8-byte Spill
	movq	%rcx, 312(%rsp)                 # 8-byte Spill
	movq	%rdx, %r12
	movq	%rsi, %rbp
	movq	%rdi, %rbx
	vcvtsi2sdl	(%rdi), %xmm0, %xmm0
	callq	sqrt
	vcvttsd2si	%xmm0, %eax
	movl	%eax, 12(%rsp)
	movq	%rbx, 192(%rsp)                 # 8-byte Spill
	movslq	(%rbx), %r15
	movq	%r15, %r13
	shlq	$4, %r13
	testq	%r15, %r15
	movq	$-1, %r14
	cmovnsq	%r13, %r14
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %rbx
	movq	%rax, 240(%rsp)                 # 8-byte Spill
	testq	%r15, %r15
	je	.LBB40_5
# %bb.1:
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movq	%r13, %rdx
	callq	_intel_fast_memset@PLT
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 104(%rsp)                 # 8-byte Spill
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%r13, %rdx
	callq	_intel_fast_memset@PLT
	testl	%r15d, %r15d
	jle	.LBB40_6
# %bb.2:
	movq	%rbp, 80(%rsp)                  # 8-byte Spill
	movl	$8, %ebp
	xorl	%r15d, %r15d
	movq	312(%rsp), %r13                 # 8-byte Reload
	movq	104(%rsp), %r14                 # 8-byte Reload
	.p2align	4, 0x90
.LBB40_3:                               # =>This Inner Loop Header: Depth=1
	movq	%r12, %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, -8(%rbx,%rbp)
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, -8(%r14,%rbp)
	incq	%r15
	movq	192(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$16, %rbp
	addq	$32, %r13
	addq	$32, %r12
	cmpq	%rax, %r15
	jl	.LBB40_3
# %bb.4:
	movq	80(%rsp), %rbp                  # 8-byte Reload
	jmp	.LBB40_6
.LBB40_5:
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 104(%rsp)                 # 8-byte Spill
.LBB40_6:
	movq	%rbp, %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 144(%rsp)
	movl	$160, %edi
	callq	_Znam
	movq	%rax, %r15
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rax)
	vmovupd	%xmm0, 16(%rax)
	vmovupd	%xmm0, 32(%rax)
	vmovupd	%xmm0, 48(%rax)
	vmovupd	%xmm0, 64(%rax)
	vmovupd	%xmm0, 80(%rax)
	vmovupd	%xmm0, 96(%rax)
	vmovupd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 128(%rax)
	vmovupd	%xmm0, 144(%rax)
	callq	omp_get_wtime
	vmovsd	%xmm0, 248(%rsp)                # 8-byte Spill
	movq	192(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %ecx
	xorl	%eax, %eax
	xorl	%edx, %edx
	movq	104(%rsp), %r9                  # 8-byte Reload
	jmp	.LBB40_9
	.p2align	4, 0x90
.LBB40_7:                               #   in Loop: Header=BB40_9 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
.LBB40_8:                               #   in Loop: Header=BB40_9 Depth=1
	vmovlhps	%xmm0, %xmm1, %xmm0             # xmm0 = xmm1[0],xmm0[0]
	movq	%rdx, %rsi
	shlq	$4, %rsi
	vmovups	%xmm0, (%r15,%rsi)
	incq	%rdx
	cmpq	$10, %rdx
	je	.LBB40_12
.LBB40_9:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB40_11 Depth 2
	vxorps	%xmm15, %xmm15, %xmm15
	vcvtsi2ss	%eax, %xmm15, %xmm0
	vblendps	$14, .LCPI40_2(%rip), %xmm0, %xmm1 # xmm1 = xmm0[0],mem[1,2,3]
	testl	%ecx, %ecx
	jle	.LBB40_7
# %bb.10:                               #   in Loop: Header=BB40_9 Depth=1
	movq	192(%rsp), %rcx                 # 8-byte Reload
	movl	(%rcx), %ecx
	movslq	%ecx, %rsi
	vxorps	%xmm0, %xmm0, %xmm0
	movl	$12, %edi
	xorl	%r8d, %r8d
	.p2align	4, 0x90
.LBB40_11:                              #   Parent Loop BB40_9 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	-12(%r9,%rdi), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmovss	-8(%r9,%rdi), %xmm5             # xmm5 = mem[0],zero,zero,zero
	vmovss	-12(%rbx,%rdi), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vmovss	-8(%rbx,%rdi), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm7, %xmm2
	vmovss	%xmm2, 176(%rsp)                # 4-byte Spill
	vmulss	%xmm5, %xmm7, %xmm12
	vmulss	%xmm4, %xmm6, %xmm13
	vmovaps	%xmm4, %xmm3
	vmulss	%xmm5, %xmm6, %xmm11
	vmovaps	%xmm4, %xmm9
	vmovaps	%xmm5, %xmm8
	vfmsub213ss	%xmm13, %xmm6, %xmm3    # xmm3 = (xmm6 * xmm3) - xmm13
	vmovss	%xmm3, 80(%rsp)                 # 4-byte Spill
	vaddss	%xmm13, %xmm12, %xmm10
	vfmsub213ss	%xmm12, %xmm7, %xmm8    # xmm8 = (xmm7 * xmm8) - xmm12
	vsubss	%xmm12, %xmm10, %xmm14
	vsubss	%xmm14, %xmm10, %xmm15
	vsubss	%xmm14, %xmm13, %xmm13
	vsubss	%xmm15, %xmm12, %xmm14
	vmovss	-4(%r9,%rdi), %xmm12            # xmm12 = mem[0],zero,zero,zero
	vmulss	(%r9,%rdi), %xmm7, %xmm15
	vaddss	%xmm13, %xmm14, %xmm3
	vmovss	%xmm3, 160(%rsp)                # 4-byte Spill
	vmulss	%xmm7, %xmm12, %xmm14
	vfmadd231ss	%xmm12, %xmm6, %xmm15   # xmm15 = (xmm6 * xmm12) + xmm15
	vfmsub213ss	%xmm2, %xmm7, %xmm9     # xmm9 = (xmm7 * xmm9) - xmm2
	vfmsub213ss	%xmm14, %xmm7, %xmm12   # xmm12 = (xmm7 * xmm12) - xmm14
	vmovss	-4(%rbx,%rdi), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm5, %xmm7, %xmm15    # xmm15 = (xmm7 * xmm5) + xmm15
	vfmsub213ss	%xmm11, %xmm6, %xmm5    # xmm5 = (xmm6 * xmm5) - xmm11
	vmulss	%xmm4, %xmm7, %xmm6
	vfmadd231ss	(%rbx,%rdi), %xmm4, %xmm15 # xmm15 = (xmm4 * mem) + xmm15
	vfmsub213ss	%xmm6, %xmm7, %xmm4     # xmm4 = (xmm7 * xmm4) - xmm6
	vaddss	%xmm11, %xmm14, %xmm7
	vmovaps	%xmm0, %xmm13
	vsubss	%xmm14, %xmm7, %xmm0
	vsubss	%xmm0, %xmm7, %xmm2
	vsubss	%xmm2, %xmm14, %xmm2
	vsubss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm9, %xmm10, %xmm11
	vsubss	%xmm10, %xmm11, %xmm14
	vsubss	%xmm14, %xmm9, %xmm9
	vsubss	%xmm14, %xmm11, %xmm14
	vsubss	%xmm14, %xmm10, %xmm10
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm6, %xmm7, %xmm2
	vsubss	%xmm7, %xmm2, %xmm14
	vmovaps	%xmm1, %xmm3
	vsubss	%xmm14, %xmm2, %xmm1
	vsubss	%xmm1, %xmm7, %xmm1
	vsubss	%xmm14, %xmm6, %xmm6
	vaddss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm2, %xmm8, %xmm6
	vsubss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm14
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm9, %xmm10, %xmm9
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm12, %xmm15, %xmm8
	vmovss	80(%rsp), %xmm12                # 4-byte Reload
                                        # xmm12 = mem[0],zero,zero,zero
	vaddss	%xmm6, %xmm12, %xmm10
	vaddss	%xmm7, %xmm2, %xmm2
	vsubss	%xmm6, %xmm10, %xmm7
	vsubss	%xmm7, %xmm12, %xmm12
	vsubss	%xmm7, %xmm10, %xmm7
	vsubss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm4, %xmm5, %xmm4
	vmovss	160(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vaddss	%xmm7, %xmm10, %xmm5
	vaddss	%xmm6, %xmm12, %xmm8
	vsubss	%xmm10, %xmm5, %xmm6
	vsubss	%xmm6, %xmm7, %xmm7
	vsubss	%xmm6, %xmm5, %xmm6
	vsubss	%xmm6, %xmm10, %xmm6
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	176(%rsp), %xmm10               # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vaddss	%xmm3, %xmm10, %xmm1
	vaddss	%xmm7, %xmm6, %xmm4
	vsubss	%xmm3, %xmm1, %xmm6
	vsubss	%xmm6, %xmm1, %xmm7
	vsubss	%xmm7, %xmm3, %xmm7
	vaddss	%xmm2, %xmm0, %xmm0
	vsubss	%xmm6, %xmm10, %xmm2
	vaddss	%xmm2, %xmm7, %xmm2
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm6, %xmm11, %xmm3
	vsubss	%xmm6, %xmm3, %xmm7
	vsubss	%xmm7, %xmm3, %xmm8
	vaddss	%xmm5, %xmm9, %xmm10
	vsubss	%xmm8, %xmm6, %xmm6
	vsubss	%xmm5, %xmm10, %xmm8
	vsubss	%xmm8, %xmm9, %xmm9
	vsubss	%xmm8, %xmm10, %xmm8
	vsubss	%xmm7, %xmm11, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm2, %xmm3, %xmm7
	vsubss	%xmm8, %xmm5, %xmm5
	vsubss	%xmm3, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm11
	vsubss	%xmm11, %xmm3, %xmm3
	vaddss	%xmm4, %xmm0, %xmm0
	vsubss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm10, %xmm13, %xmm3
	vaddss	%xmm5, %xmm9, %xmm4
	vsubss	%xmm13, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm8
	vsubss	%xmm8, %xmm13, %xmm8
	vaddss	%xmm4, %xmm0, %xmm0
	vsubss	%xmm5, %xmm10, %xmm4
	vaddss	%xmm6, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm9
	vaddss	%xmm4, %xmm8, %xmm4
	vsubss	%xmm9, %xmm5, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm9, %xmm6, %xmm6
	vaddss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vsubss	%xmm8, %xmm2, %xmm2
	vmovshdup	%xmm13, %xmm8           # xmm8 = xmm13[1,1,3,3]
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm2, %xmm5, %xmm2
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vinsertps	$16, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm7[0],xmm1[2,3]
	vinsertps	$16, %xmm0, %xmm6, %xmm0 # xmm0 = xmm6[0],xmm0[0],xmm6[2,3]
	incq	%r8
	addq	$16, %rdi
	cmpq	%rsi, %r8
	jl	.LBB40_11
	jmp	.LBB40_8
.LBB40_12:
	callq	omp_get_wtime
	vsubsd	248(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vxorps	%xmm15, %xmm15, %xmm15
	vcvtsi2sd	%eax, %xmm15, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	208(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp5348:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp5349:
# %bb.13:
.Ltmp5350:
	movq	%rax, %r13
	movq	256(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp5351:
# %bb.14:
.Ltmp5352:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp5353:
# %bb.15:
	movl	%eax, %r12d
	cmpq	%rbp, %r13
	cmovgq	%r13, %rbp
.Ltmp5354:
	leaq	112(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp5355:
# %bb.16:
.Ltmp5356:
	leaq	112(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5357:
# %bb.17:
.Ltmp5359:
	callq	mpfr_get_default_rounding_mode
.Ltmp5360:
# %bb.18:
.Ltmp5361:
	leaq	112(%rsp), %rdi
	leaq	208(%rsp), %rsi
	movq	256(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp5362:
# %bb.19:
.Ltmp5364:
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5365:
# %bb.20:
.Ltmp5366:
	movq	%rax, %r13
	movq	256(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp5367:
# %bb.21:
.Ltmp5368:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp5369:
# %bb.22:
	movl	%eax, %r12d
	cmpq	%rbp, %r13
	cmovgq	%r13, %rbp
.Ltmp5370:
	leaq	16(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp5371:
# %bb.23:
.Ltmp5372:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5373:
# %bb.24:
.Ltmp5375:
	callq	mpfr_get_default_rounding_mode
.Ltmp5376:
# %bb.25:
.Ltmp5377:
	leaq	16(%rsp), %rdi
	leaq	112(%rsp), %rsi
	movq	256(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp5378:
# %bb.26:
.Ltmp5380:
	callq	mpfr_get_default_rounding_mode
.Ltmp5381:
# %bb.27:
.Ltmp5382:
	movl	%eax, %r12d
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5383:
# %bb.28:
.Ltmp5384:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp5385:
# %bb.29:
.Ltmp5386:
	movl	%eax, %r13d
	leaq	48(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp5387:
# %bb.30:
.Ltmp5388:
	leaq	48(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r13d, %edx
	callq	mpfr_set_si
.Ltmp5389:
# %bb.31:
.Ltmp5391:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%r12d, %edx
	callq	mpfr_abs
.Ltmp5392:
# %bb.32:
.Ltmp5394:
	leaq	48(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp5395:
# %bb.33:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 320(%rsp)
	cmpq	$0, 72(%rsp)
	je	.LBB40_35
# %bb.34:
.Ltmp5397:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5398:
.LBB40_35:
	cmpq	$0, 40(%rsp)
	je	.LBB40_37
# %bb.36:
.Ltmp5400:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5401:
.LBB40_37:
	cmpq	$0, 136(%rsp)
	je	.LBB40_39
# %bb.38:
.Ltmp5403:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5404:
.LBB40_39:
	cmpq	$0, 232(%rsp)
	je	.LBB40_41
# %bb.40:
.Ltmp5406:
	leaq	208(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5407:
.LBB40_41:
	leaq	520(%rsp), %r12
	movq	%r12, 504(%rsp)
	movl	$544501604, 520(%rsp)           # imm = 0x20746F64
	movw	$32, 524(%rsp)
	movq	$5, 512(%rsp)
.Ltmp5409:
	leaq	504(%rsp), %rdi
	leaq	320(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp5410:
# %bb.42:
	movq	504(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB40_44
# %bb.43:
	callq	_ZdlPv
.LBB40_44:
	movq	%r15, %rdi
	callq	_ZdaPv
	movl	$160, %edi
	callq	_Znam
	movq	%rax, %r15
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rax)
	vmovupd	%xmm0, 16(%rax)
	vmovupd	%xmm0, 32(%rax)
	vmovupd	%xmm0, 48(%rax)
	vmovupd	%xmm0, 64(%rax)
	vmovupd	%xmm0, 80(%rax)
	vmovupd	%xmm0, 96(%rax)
	vmovupd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 128(%rax)
	vmovupd	%xmm0, 144(%rax)
	callq	omp_get_wtime
	vmovsd	%xmm0, 368(%rsp)                # 8-byte Spill
	movq	240(%rsp), %rbp                 # 8-byte Reload
	addq	$12, %rbp
	xorl	%r12d, %r12d
	xorl	%r13d, %r13d
	jmp	.LBB40_47
	.p2align	4, 0x90
.LBB40_45:                              #   in Loop: Header=BB40_47 Depth=1
	vxorps	%xmm2, %xmm2, %xmm2
	vblendps	$1, %xmm4, %xmm2, %xmm1         # xmm1 = xmm4[0],xmm2[1,2,3]
.LBB40_46:                              #   in Loop: Header=BB40_47 Depth=1
	vmovlhps	%xmm2, %xmm1, %xmm0             # xmm0 = xmm1[0],xmm2[0]
	movq	%r13, %rax
	shlq	$4, %rax
	vmovups	%xmm0, (%r15,%rax)
	incq	%r13
	cmpq	$10, %r13
	je	.LBB40_53
.LBB40_47:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB40_49 Depth 2
	vxorps	%xmm13, %xmm13, %xmm13
	vcvtsi2ss	%r12d, %xmm13, %xmm0
	vblendps	$14, .LCPI40_2(%rip), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[1,2,3]
	movq	192(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	vxorps	%xmm3, %xmm3, %xmm3
	testq	%rax, %rax
	jle	.LBB40_50
# %bb.48:                               #   in Loop: Header=BB40_47 Depth=1
	movq	%rbp, %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB40_49:                              #   Parent Loop BB40_47 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	%xmm0, 176(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm3, 80(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovss	-12(%rcx), %xmm0                # xmm0 = mem[0],zero,zero,zero
	vmovss	-8(%rcx), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm0, %xmm3
	vmulss	%xmm0, %xmm1, %xmm4
	vmovss	-4(%rcx), %xmm6                 # xmm6 = mem[0],zero,zero,zero
	vmovaps	%xmm0, %xmm2
	vmulss	%xmm1, %xmm1, %xmm5
	vmovss	(%rcx), %xmm7                   # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm7, %xmm0, %xmm8
	vfmsub213ss	%xmm4, %xmm1, %xmm2     # xmm2 = (xmm1 * xmm2) - xmm4
	vmovss	%xmm2, 160(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm6, %xmm1, %xmm8     # xmm8 = (xmm1 * xmm6) + xmm8
	vfmadd231ss	%xmm1, %xmm6, %xmm8     # xmm8 = (xmm6 * xmm1) + xmm8
	vmovaps	%xmm1, %xmm9
	vfmsub213ss	%xmm3, %xmm0, %xmm9     # xmm9 = (xmm0 * xmm9) - xmm3
	vaddss	%xmm4, %xmm3, %xmm10
	vsubss	%xmm3, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm3, %xmm3
	vsubss	%xmm11, %xmm4, %xmm2
	vmulss	%xmm6, %xmm0, %xmm11
	vmulss	%xmm0, %xmm6, %xmm12
	vmovaps	%xmm0, %xmm13
	vfmsub213ss	%xmm12, %xmm6, %xmm13   # xmm13 = (xmm6 * xmm13) - xmm12
	vfmsub213ss	%xmm11, %xmm0, %xmm6    # xmm6 = (xmm0 * xmm6) - xmm11
	vmovaps	%xmm5, %xmm4
	vaddss	%xmm5, %xmm11, %xmm14
	vsubss	%xmm11, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm5
	vsubss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm2, %xmm3, %xmm3
	vfmsub213ss	%xmm4, %xmm1, %xmm1     # xmm1 = (xmm1 * xmm1) - xmm4
	vsubss	%xmm15, %xmm4, %xmm4
	vaddss	%xmm4, %xmm5, %xmm4
	vaddss	%xmm12, %xmm14, %xmm5
	vsubss	%xmm14, %xmm5, %xmm11
	vsubss	%xmm11, %xmm5, %xmm15
	vsubss	%xmm15, %xmm14, %xmm14
	vsubss	%xmm11, %xmm12, %xmm11
	vfmadd231ss	%xmm0, %xmm7, %xmm8     # xmm8 = (xmm7 * xmm0) + xmm8
	vmulss	%xmm0, %xmm0, %xmm7
	vfmsub213ss	%xmm7, %xmm0, %xmm0     # xmm0 = (xmm0 * xmm0) - xmm7
	vaddss	%xmm11, %xmm14, %xmm11
	vaddss	%xmm5, %xmm9, %xmm12
	vaddss	%xmm6, %xmm8, %xmm6
	vsubss	%xmm5, %xmm12, %xmm8
	vsubss	%xmm8, %xmm9, %xmm9
	vsubss	%xmm8, %xmm12, %xmm8
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm0, %xmm10, %xmm8
	vaddss	%xmm1, %xmm6, %xmm1
	vmovss	160(%rsp), %xmm2                # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm12, %xmm6
	vaddss	%xmm5, %xmm9, %xmm5
	vsubss	%xmm12, %xmm6, %xmm9
	vsubss	%xmm9, %xmm2, %xmm2
	vsubss	%xmm9, %xmm6, %xmm9
	vsubss	%xmm9, %xmm12, %xmm9
	vsubss	%xmm10, %xmm8, %xmm12
	vsubss	%xmm12, %xmm0, %xmm0
	vsubss	%xmm12, %xmm8, %xmm12
	vsubss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm1, %xmm13, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm3, %xmm6, %xmm4
	vaddss	%xmm2, %xmm9, %xmm2
	vsubss	%xmm6, %xmm4, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vsubss	%xmm9, %xmm4, %xmm9
	vsubss	%xmm9, %xmm6, %xmm6
	vaddss	%xmm1, %xmm11, %xmm1
	vmovups	176(%rsp), %xmm12               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm7, %xmm12, %xmm9
	vsubss	%xmm12, %xmm9, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm11, %xmm9, %xmm11
	vsubss	%xmm11, %xmm12, %xmm11
	vaddss	%xmm7, %xmm11, %xmm7
	vaddss	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm12, %xmm10          # xmm10 = xmm12[1,1,3,3]
	vaddss	%xmm8, %xmm10, %xmm11
	vsubss	%xmm10, %xmm11, %xmm12
	vaddss	%xmm3, %xmm6, %xmm3
	vsubss	%xmm12, %xmm11, %xmm6
	vsubss	%xmm6, %xmm10, %xmm6
	vsubss	%xmm12, %xmm8, %xmm8
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm6, %xmm8, %xmm5
	vaddss	%xmm7, %xmm11, %xmm6
	vsubss	%xmm11, %xmm6, %xmm8
	vaddss	%xmm2, %xmm1, %xmm1
	vsubss	%xmm8, %xmm6, %xmm2
	vsubss	%xmm2, %xmm11, %xmm2
	vaddss	%xmm0, %xmm4, %xmm10
	vaddss	%xmm3, %xmm1, %xmm1
	vsubss	%xmm4, %xmm10, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm3, %xmm10, %xmm3
	vsubss	%xmm3, %xmm4, %xmm3
	vsubss	%xmm8, %xmm7, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vmovups	80(%rsp), %xmm8                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm10, %xmm8, %xmm4
	vaddss	%xmm0, %xmm3, %xmm0
	vsubss	%xmm8, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vmovaps	%xmm8, %xmm11
	vaddss	%xmm0, %xmm1, %xmm0
	vsubss	%xmm3, %xmm10, %xmm1
	vaddss	%xmm5, %xmm4, %xmm3
	vsubss	%xmm4, %xmm3, %xmm8
	vaddss	%xmm1, %xmm7, %xmm1
	vsubss	%xmm8, %xmm3, %xmm7
	vsubss	%xmm7, %xmm4, %xmm4
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm2, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm2, %xmm2
	vmovshdup	%xmm11, %xmm7           # xmm7 = xmm11[1,1,3,3]
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm1
	vinsertps	$16, %xmm6, %xmm9, %xmm0 # xmm0 = xmm9[0],xmm6[0],xmm9[2,3]
	vinsertps	$16, %xmm1, %xmm5, %xmm3 # xmm3 = xmm5[0],xmm1[0],xmm5[2,3]
	incq	%rdx
	addq	$16, %rcx
	cmpq	%rax, %rdx
	jl	.LBB40_49
.LBB40_50:                              #   in Loop: Header=BB40_47 Depth=1
	vmovshdup	%xmm0, %xmm6            # xmm6 = xmm0[1,1,3,3]
	vaddss	%xmm3, %xmm6, %xmm1
	vmovshdup	%xmm3, %xmm5            # xmm5 = xmm3[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm4
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm4
	setp	%al
	setne	%cl
	orb	%al, %cl
	je	.LBB40_45
# %bb.51:                               #   in Loop: Header=BB40_47 Depth=1
	vcomiss	%xmm4, %xmm1
	vbroadcastss	.LCPI40_0(%rip), %xmm2  # xmm2 = [NaN,NaN,NaN,NaN]
	vmovaps	%xmm2, %xmm1
	ja	.LBB40_46
# %bb.52:                               #   in Loop: Header=BB40_47 Depth=1
	vmovups	%xmm3, 80(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm0, 176(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm5, 160(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm6, 288(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovaps	%xmm0, %xmm3
	vmovups	176(%rsp), %xmm14               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vfnmadd213ss	%xmm14, %xmm0, %xmm0    # xmm0 = -(xmm0 * xmm0) + xmm14
	vmovups	288(%rsp), %xmm13               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm3, %xmm3, %xmm1
	vdivss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm3, %xmm3, %xmm2
	vbroadcastss	.LCPI40_1(%rip), %xmm4  # xmm4 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm4, %xmm2, %xmm0
	vmovaps	%xmm4, %xmm12
	vmovaps	%xmm3, %xmm6
	vfmsub213ss	%xmm2, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm6) - xmm2
	vmulss	%xmm1, %xmm3, %xmm7
	vmovaps	%xmm1, %xmm4
	vfmsub213ss	%xmm7, %xmm3, %xmm4     # xmm4 = (xmm3 * xmm4) - xmm7
	vmulss	%xmm3, %xmm1, %xmm8
	vmovaps	%xmm3, %xmm5
	vfmsub213ss	%xmm8, %xmm1, %xmm5     # xmm5 = (xmm1 * xmm5) - xmm8
	vaddss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm7, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm7, %xmm8, %xmm10
	vmovss	%xmm10, 240(%rsp)               # 4-byte Spill
	vaddss	%xmm6, %xmm9, %xmm7
	vsubss	%xmm9, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vsubss	%xmm8, %xmm6, %xmm6
	vaddss	%xmm6, %xmm9, %xmm8
	vmovss	%xmm8, 248(%rsp)                # 4-byte Spill
	vmovaps	%xmm1, %xmm6
	vfmadd213ss	%xmm5, %xmm1, %xmm6     # xmm6 = (xmm1 * xmm6) + xmm5
	vaddss	%xmm6, %xmm4, %xmm6
	vaddss	%xmm6, %xmm10, %xmm6
	vaddss	%xmm6, %xmm8, %xmm6
	vmovaps	%xmm14, %xmm9
	vsubss	%xmm2, %xmm14, %xmm2
	vsubss	%xmm14, %xmm2, %xmm8
	vsubss	%xmm8, %xmm2, %xmm11
	vsubss	%xmm11, %xmm14, %xmm11
	vxorps	%xmm7, %xmm12, %xmm12
	vsubss	%xmm8, %xmm0, %xmm0
	vaddss	%xmm0, %xmm11, %xmm0
	vmovaps	%xmm13, %xmm9
	vsubss	%xmm7, %xmm13, %xmm7
	vsubss	%xmm13, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm11
	vsubss	%xmm11, %xmm13, %xmm11
	vsubss	%xmm8, %xmm12, %xmm8
	vaddss	%xmm8, %xmm11, %xmm8
	vaddss	%xmm0, %xmm7, %xmm11
	vsubss	%xmm7, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm7, %xmm7
	vsubss	%xmm12, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm7
	vmovss	%xmm7, 280(%rsp)                # 4-byte Spill
	vmovups	80(%rsp), %xmm0                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm2, %xmm11, %xmm2
	vmovss	%xmm2, 176(%rsp)                # 4-byte Spill
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm15, %xmm6
	vmovss	%xmm6, 288(%rsp)                # 4-byte Spill
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm3, %xmm2
	vmovss	%xmm2, 284(%rsp)                # 4-byte Spill
	vmulss	%xmm2, %xmm6, %xmm2
	vdivss	%xmm2, %xmm0, %xmm11
	vmulss	%xmm3, %xmm11, %xmm0
	vmovaps	%xmm11, %xmm12
	vfmsub213ss	%xmm0, %xmm3, %xmm12    # xmm12 = (xmm3 * xmm12) - xmm0
	vmulss	%xmm1, %xmm1, %xmm2
	vaddss	%xmm2, %xmm0, %xmm14
	vsubss	%xmm0, %xmm14, %xmm6
	vsubss	%xmm6, %xmm14, %xmm13
	vsubss	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm1, %xmm13
	vfmsub213ss	%xmm2, %xmm1, %xmm13    # xmm13 = (xmm1 * xmm13) - xmm2
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vmovss	%xmm0, 276(%rsp)                # 4-byte Spill
	vmulss	%xmm3, %xmm11, %xmm2
	vaddss	%xmm2, %xmm14, %xmm15
	vsubss	%xmm14, %xmm15, %xmm6
	vsubss	%xmm6, %xmm15, %xmm0
	vsubss	%xmm0, %xmm14, %xmm14
	vxorps	%xmm9, %xmm9, %xmm9
	vmulss	%xmm3, %xmm9, %xmm7
	vfmadd231ss	%xmm11, %xmm1, %xmm7    # xmm7 = (xmm1 * xmm11) + xmm7
	vfmadd231ss	%xmm1, %xmm11, %xmm7    # xmm7 = (xmm11 * xmm1) + xmm7
	vinsertps	$16, %xmm1, %xmm3, %xmm1 # xmm1 = xmm3[0],xmm1[0],xmm3[2,3]
	vfmadd231ss	%xmm3, %xmm9, %xmm7     # xmm7 = (xmm9 * xmm3) + xmm7
	vfmsub213ss	%xmm2, %xmm11, %xmm3    # xmm3 = (xmm11 * xmm3) - xmm2
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm14, %xmm2
	vaddss	%xmm4, %xmm15, %xmm6
	vsubss	%xmm15, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm10
	vsubss	%xmm10, %xmm15, %xmm10
	vsubss	%xmm14, %xmm4, %xmm4
	vaddss	%xmm4, %xmm10, %xmm4
	vaddss	%xmm5, %xmm6, %xmm10
	vsubss	%xmm6, %xmm10, %xmm14
	vsubss	%xmm14, %xmm10, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm14, %xmm5, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vmovss	240(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm10, %xmm6
	vsubss	%xmm10, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vsubss	%xmm14, %xmm0, %xmm14
	vaddss	%xmm14, %xmm10, %xmm10
	vmovss	248(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm6, %xmm14
	vsubss	%xmm6, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm9
	vsubss	%xmm9, %xmm6, %xmm6
	vsubss	%xmm15, %xmm0, %xmm9
	vaddss	%xmm6, %xmm9, %xmm6
	vaddss	%xmm7, %xmm12, %xmm0
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	276(%rsp), %xmm0, %xmm0         # 4-byte Folded Reload
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vbroadcastss	.LCPI40_1(%rip), %xmm2  # xmm2 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm14, %xmm2
	vmovups	80(%rsp), %xmm6                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm14, %xmm6, %xmm3
	vsubss	%xmm6, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm6, %xmm5
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vaddss	%xmm3, %xmm8, %xmm4
	vsubss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm3, %xmm3
	vsubss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vmovss	280(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vaddss	%xmm7, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vmovups	160(%rsp), %xmm6                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	176(%rsp), %xmm5, %xmm2         # 4-byte Folded Reload
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	284(%rsp), %xmm11, %xmm2        # 4-byte Folded Reload
	vmulss	288(%rsp), %xmm2, %xmm2         # 4-byte Folded Reload
	vdivss	%xmm2, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm11, %xmm2 # xmm2 = xmm11[0],xmm0[0],xmm11[2,3]
	jmp	.LBB40_46
.LBB40_53:
	callq	omp_get_wtime
	vsubsd	368(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vxorps	%xmm13, %xmm13, %xmm13
	vcvtsi2sd	%eax, %xmm13, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	208(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp5412:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp5413:
# %bb.54:
	movq	%rax, %r13
	movq	256(%rsp), %rax                 # 8-byte Reload
	leaq	32(%rax), %r14
.Ltmp5414:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp5415:
# %bb.55:
.Ltmp5416:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp5417:
# %bb.56:
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp5418:
	leaq	112(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp5419:
# %bb.57:
.Ltmp5420:
	leaq	112(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp5421:
# %bb.58:
.Ltmp5423:
	callq	mpfr_get_default_rounding_mode
.Ltmp5424:
# %bb.59:
.Ltmp5425:
	leaq	112(%rsp), %rdi
	leaq	208(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp5426:
# %bb.60:
.Ltmp5428:
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5429:
# %bb.61:
.Ltmp5430:
	movq	%rax, %r12
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp5431:
# %bb.62:
.Ltmp5432:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5433:
# %bb.63:
	movl	%eax, %ebp
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp5434:
	leaq	16(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5435:
# %bb.64:
.Ltmp5436:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp5437:
# %bb.65:
.Ltmp5439:
	callq	mpfr_get_default_rounding_mode
.Ltmp5440:
# %bb.66:
.Ltmp5441:
	leaq	16(%rsp), %rdi
	leaq	112(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp5442:
# %bb.67:
.Ltmp5444:
	callq	mpfr_get_default_rounding_mode
.Ltmp5445:
# %bb.68:
.Ltmp5446:
	movl	%eax, %ebp
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5447:
# %bb.69:
.Ltmp5448:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5449:
# %bb.70:
.Ltmp5450:
	movl	%eax, %r12d
	leaq	48(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5451:
# %bb.71:
.Ltmp5452:
	leaq	48(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5453:
# %bb.72:
.Ltmp5455:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_abs
.Ltmp5456:
# %bb.73:
.Ltmp5458:
	leaq	48(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp5459:
# %bb.74:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 320(%rsp)
	cmpq	$0, 72(%rsp)
	je	.LBB40_76
# %bb.75:
.Ltmp5461:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5462:
.LBB40_76:
	cmpq	$0, 40(%rsp)
	je	.LBB40_78
# %bb.77:
.Ltmp5464:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5465:
.LBB40_78:
	cmpq	$0, 136(%rsp)
	je	.LBB40_80
# %bb.79:
.Ltmp5467:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5468:
.LBB40_80:
	cmpq	$0, 232(%rsp)
	je	.LBB40_82
# %bb.81:
.Ltmp5470:
	leaq	208(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5471:
.LBB40_82:
	leaq	488(%rsp), %r12
	movq	%r12, 472(%rsp)
	movl	$846033518, 488(%rsp)           # imm = 0x326D726E
	movw	$32, 492(%rsp)
	movq	$5, 480(%rsp)
.Ltmp5473:
	leaq	472(%rsp), %rdi
	leaq	320(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp5474:
# %bb.83:
	movq	472(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB40_85
# %bb.84:
	callq	_ZdlPv
.LBB40_85:
	movq	%r15, %rdi
	callq	_ZdaPv
	movl	$160, %edi
	callq	_Znam
	movq	%rax, %r15
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rax)
	vmovupd	%xmm0, 16(%rax)
	vmovupd	%xmm0, 32(%rax)
	vmovupd	%xmm0, 48(%rax)
	vmovupd	%xmm0, 64(%rax)
	vmovupd	%xmm0, 80(%rax)
	vmovupd	%xmm0, 96(%rax)
	vmovupd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 128(%rax)
	vmovupd	%xmm0, 144(%rax)
	callq	omp_get_wtime
	vxorps	%xmm12, %xmm12, %xmm12
	vmovsd	%xmm0, 80(%rsp)                 # 8-byte Spill
	movq	192(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	movq	%rax, %rcx
	shlq	$4, %rcx
	xorl	%edx, %edx
	vxorpd	%xmm0, %xmm0, %xmm0
	vbroadcastss	.LCPI40_1(%rip), %xmm1  # xmm1 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	xorl	%esi, %esi
	jmp	.LBB40_87
	.p2align	4, 0x90
.LBB40_86:                              #   in Loop: Header=BB40_87 Depth=1
	vmovlhps	%xmm3, %xmm2, %xmm2             # xmm2 = xmm2[0],xmm3[0]
	movq	%rsi, %rdi
	shlq	$4, %rdi
	vmovups	%xmm2, (%r15,%rdi)
	incq	%rsi
	cmpq	$10, %rsi
	je	.LBB40_93
.LBB40_87:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB40_91 Depth 2
	vcvtsi2ss	%edx, %xmm13, %xmm2
	vblendps	$1, %xmm2, %xmm12, %xmm2        # xmm2 = xmm2[0],xmm12[1,2,3]
	vxorps	%xmm3, %xmm3, %xmm3
	testl	%eax, %eax
	jle	.LBB40_86
# %bb.88:                               #   in Loop: Header=BB40_87 Depth=1
	xorl	%edi, %edi
	jmp	.LBB40_91
	.p2align	4, 0x90
.LBB40_89:                              #   in Loop: Header=BB40_91 Depth=2
	vmovups	(%rbx,%rdi), %xmm5
	vmovddup	8(%rbx,%rdi), %xmm4             # xmm4 = mem[0,0]
.LBB40_90:                              #   in Loop: Header=BB40_91 Depth=2
	vaddss	%xmm5, %xmm2, %xmm6
	vsubss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm2, %xmm8
	vsubss	%xmm7, %xmm5, %xmm7
	vaddss	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vmovshdup	%xmm5, %xmm5            # xmm5 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm2, %xmm8
	vsubss	%xmm2, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm2, %xmm2
	vsubss	%xmm9, %xmm5, %xmm5
	vaddss	%xmm5, %xmm2, %xmm5
	vaddss	%xmm7, %xmm8, %xmm9
	vinsertps	$16, %xmm9, %xmm6, %xmm2 # xmm2 = xmm6[0],xmm9[0],xmm6[2,3]
	vsubss	%xmm8, %xmm9, %xmm6
	vsubss	%xmm6, %xmm9, %xmm9
	vsubss	%xmm9, %xmm8, %xmm8
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm4, %xmm3, %xmm7
	vsubss	%xmm3, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm3, %xmm9
	vsubss	%xmm8, %xmm4, %xmm8
	vaddss	%xmm8, %xmm9, %xmm8
	vaddss	%xmm5, %xmm7, %xmm9
	vsubss	%xmm7, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm7, %xmm5
	vaddss	%xmm6, %xmm9, %xmm7
	vsubss	%xmm9, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm9, %xmm6
	vmovshdup	%xmm3, %xmm3            # xmm3 = xmm3[1,1,3,3]
	vmovshdup	%xmm4, %xmm4            # xmm4 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm6, %xmm3, %xmm3
	vinsertps	$16, %xmm3, %xmm7, %xmm3 # xmm3 = xmm7[0],xmm3[0],xmm7[2,3]
	addq	$16, %rdi
	cmpq	%rdi, %rcx
	je	.LBB40_86
.LBB40_91:                              #   Parent Loop BB40_87 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	(%rbx,%rdi), %xmm4              # xmm4 = mem[0],zero
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vmovss	8(%rbx,%rdi), %xmm6             # xmm6 = mem[0],zero,zero,zero
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	12(%rbx,%rdi), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm5
	vcomiss	%xmm5, %xmm0
	jbe	.LBB40_89
# %bb.92:                               #   in Loop: Header=BB40_91 Depth=2
	vxorps	%xmm1, %xmm4, %xmm5
	vinsertps	$16, %xmm7, %xmm6, %xmm4 # xmm4 = xmm6[0],xmm7[0],xmm6[2,3]
	vxorps	%xmm1, %xmm4, %xmm4
	jmp	.LBB40_90
.LBB40_93:
	callq	omp_get_wtime
	vsubsd	80(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm13, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	208(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp5476:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp5477:
# %bb.94:
	movq	%rax, %r13
	movq	256(%rsp), %rax                 # 8-byte Reload
	leaq	64(%rax), %r14
.Ltmp5478:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp5479:
# %bb.95:
.Ltmp5480:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp5481:
# %bb.96:
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp5482:
	leaq	112(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp5483:
# %bb.97:
.Ltmp5484:
	leaq	112(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp5485:
# %bb.98:
.Ltmp5487:
	callq	mpfr_get_default_rounding_mode
.Ltmp5488:
# %bb.99:
.Ltmp5489:
	leaq	112(%rsp), %rdi
	leaq	208(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp5490:
# %bb.100:
.Ltmp5492:
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5493:
# %bb.101:
.Ltmp5494:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp5495:
# %bb.102:
.Ltmp5496:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp5497:
# %bb.103:
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp5498:
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp5499:
# %bb.104:
.Ltmp5500:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp5501:
# %bb.105:
.Ltmp5503:
	callq	mpfr_get_default_rounding_mode
.Ltmp5504:
# %bb.106:
.Ltmp5505:
	leaq	16(%rsp), %rdi
	leaq	112(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp5506:
# %bb.107:
.Ltmp5508:
	callq	mpfr_get_default_rounding_mode
.Ltmp5509:
# %bb.108:
.Ltmp5510:
	movl	%eax, %ebp
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5511:
# %bb.109:
.Ltmp5512:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5513:
# %bb.110:
.Ltmp5514:
	movl	%eax, %r12d
	leaq	48(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5515:
# %bb.111:
.Ltmp5516:
	leaq	48(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5517:
# %bb.112:
.Ltmp5519:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_abs
.Ltmp5520:
# %bb.113:
.Ltmp5522:
	leaq	48(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp5523:
# %bb.114:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 320(%rsp)
	cmpq	$0, 72(%rsp)
	je	.LBB40_116
# %bb.115:
.Ltmp5525:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5526:
.LBB40_116:
	cmpq	$0, 40(%rsp)
	je	.LBB40_118
# %bb.117:
.Ltmp5528:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5529:
.LBB40_118:
	cmpq	$0, 136(%rsp)
	je	.LBB40_120
# %bb.119:
.Ltmp5531:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5532:
.LBB40_120:
	cmpq	$0, 232(%rsp)
	je	.LBB40_122
# %bb.121:
.Ltmp5534:
	leaq	208(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5535:
.LBB40_122:
	leaq	456(%rsp), %r12
	movq	%r12, 440(%rsp)
	movl	$1836413793, 456(%rsp)          # imm = 0x6D757361
	movw	$32, 460(%rsp)
	movq	$5, 448(%rsp)
.Ltmp5537:
	leaq	440(%rsp), %rdi
	leaq	320(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp5538:
# %bb.123:
	movq	440(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB40_125
# %bb.124:
	callq	_ZdlPv
.LBB40_125:
	movq	%r15, %rdi
	callq	_ZdaPv
	leaq	144(%rsp), %rsi
	movq	192(%rsp), %r13                 # 8-byte Reload
	movq	%r13, %rdi
	movq	%rbx, %rdx
	movq	104(%rsp), %rcx                 # 8-byte Reload
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	48(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	cmpl	$0, (%r13)
	jle	.LBB40_166
# %bb.126:
	movq	256(%rsp), %rax                 # 8-byte Reload
	leaq	256(%rax), %r15
	xorl	%r14d, %r14d
	movq	104(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB40_128
	.p2align	4, 0x90
.LBB40_127:                             #   in Loop: Header=BB40_128 Depth=1
	incq	%r14
	movq	192(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r15
	movq	80(%rsp), %rsi                  # 8-byte Reload
	addq	$16, %rsi
	cmpq	%rax, %r14
	jge	.LBB40_166
.LBB40_128:                             # =>This Inner Loop Header: Depth=1
.Ltmp5540:
	leaq	112(%rsp), %rdi
	movq	%rsi, 80(%rsp)                  # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp5541:
# %bb.129:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5543:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp5544:
# %bb.130:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5545:
	movq	%rax, %rbp
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5546:
# %bb.131:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5547:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5548:
# %bb.132:                              #   in Loop: Header=BB40_128 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp5549:
	leaq	16(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5550:
# %bb.133:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5551:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5552:
# %bb.134:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5554:
	callq	mpfr_get_default_rounding_mode
.Ltmp5555:
# %bb.135:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5556:
	leaq	16(%rsp), %rdi
	movq	%r15, %rsi
	leaq	112(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp5557:
# %bb.136:                              #   in Loop: Header=BB40_128 Depth=1
	cmpq	$0, 136(%rsp)
	je	.LBB40_138
# %bb.137:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5559:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5560:
.LBB40_138:                             #   in Loop: Header=BB40_128 Depth=1
.Ltmp5562:
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5563:
# %bb.139:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5564:
	movq	%rax, %rbp
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5565:
# %bb.140:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5566:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5567:
# %bb.141:                              #   in Loop: Header=BB40_128 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp5568:
	leaq	208(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5569:
# %bb.142:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5570:
	leaq	208(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5571:
# %bb.143:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5573:
	callq	mpfr_get_default_rounding_mode
.Ltmp5574:
# %bb.144:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5575:
	leaq	208(%rsp), %rdi
	leaq	16(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp5576:
# %bb.145:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5578:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5579:
# %bb.146:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5580:
	movq	%rax, %rbp
	leaq	208(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5581:
# %bb.147:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5582:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5583:
# %bb.148:                              #   in Loop: Header=BB40_128 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp5584:
	leaq	112(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5585:
# %bb.149:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5586:
	leaq	112(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5587:
# %bb.150:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5589:
	callq	mpfr_get_default_rounding_mode
.Ltmp5590:
	leaq	48(%rsp), %r12
# %bb.151:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5591:
	leaq	112(%rsp), %rdi
	movq	%r12, %rsi
	leaq	208(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp5592:
# %bb.152:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5594:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp5595:
# %bb.153:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5596:
	movq	%rax, %r12
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5597:
# %bb.154:                              #   in Loop: Header=BB40_128 Depth=1
	movq	%rax, %rbp
	cmpq	%rax, %r12
	je	.LBB40_158
# %bb.155:                              #   in Loop: Header=BB40_128 Depth=1
	cmpq	$0, 72(%rsp)
	je	.LBB40_157
# %bb.156:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5598:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5599:
.LBB40_157:                             #   in Loop: Header=BB40_128 Depth=1
.Ltmp5600:
	leaq	48(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp5601:
.LBB40_158:                             #   in Loop: Header=BB40_128 Depth=1
.Ltmp5602:
	callq	mpfr_get_default_rounding_mode
.Ltmp5603:
# %bb.159:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5604:
	leaq	48(%rsp), %rdi
	leaq	112(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp5605:
# %bb.160:                              #   in Loop: Header=BB40_128 Depth=1
	cmpq	$0, 136(%rsp)
	je	.LBB40_162
# %bb.161:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5607:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5608:
.LBB40_162:                             #   in Loop: Header=BB40_128 Depth=1
	cmpq	$0, 232(%rsp)
	je	.LBB40_164
# %bb.163:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5610:
	leaq	208(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5611:
.LBB40_164:                             #   in Loop: Header=BB40_128 Depth=1
	cmpq	$0, 40(%rsp)
	je	.LBB40_127
# %bb.165:                              #   in Loop: Header=BB40_128 Depth=1
.Ltmp5613:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5614:
	jmp	.LBB40_127
.LBB40_166:
.Ltmp5616:
	callq	mpfr_get_default_rounding_mode
.Ltmp5617:
# %bb.167:
.Ltmp5618:
	movl	%eax, %ebp
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5619:
# %bb.168:
.Ltmp5620:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp5621:
# %bb.169:
.Ltmp5622:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp5623:
# %bb.170:
.Ltmp5624:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp5625:
# %bb.171:
.Ltmp5627:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp5628:
# %bb.172:
.Ltmp5630:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5631:
# %bb.173:
.Ltmp5632:
	movq	%rax, %r12
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5633:
# %bb.174:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB40_178
# %bb.175:
	cmpq	$0, 72(%rsp)
	je	.LBB40_177
# %bb.176:
.Ltmp5634:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5635:
.LBB40_177:
.Ltmp5636:
	leaq	48(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp5637:
.LBB40_178:
.Ltmp5638:
	callq	mpfr_get_default_rounding_mode
.Ltmp5639:
# %bb.179:
.Ltmp5640:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp5641:
# %bb.180:
	cmpq	$0, 40(%rsp)
	je	.LBB40_182
# %bb.181:
.Ltmp5643:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5644:
.LBB40_182:
	callq	omp_get_wtime
	vmovsd	%xmm0, 80(%rsp)                 # 8-byte Spill
.Ltmp5646:
	leaq	144(%rsp), %rsi
	movq	192(%rsp), %r15                 # 8-byte Reload
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	104(%rsp), %r14                 # 8-byte Reload
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp5647:
# %bb.183:
.Ltmp5648:
	leaq	144(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp5649:
# %bb.184:
.Ltmp5650:
	leaq	144(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp5651:
# %bb.185:
.Ltmp5652:
	leaq	144(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp5653:
# %bb.186:
.Ltmp5654:
	leaq	144(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp5655:
# %bb.187:
.Ltmp5656:
	leaq	144(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp5657:
# %bb.188:
.Ltmp5658:
	leaq	144(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp5659:
# %bb.189:
.Ltmp5660:
	leaq	144(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp5661:
# %bb.190:
.Ltmp5662:
	leaq	144(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp5663:
# %bb.191:
.Ltmp5664:
	leaq	144(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKT_PS8_PS7_
.Ltmp5665:
# %bb.192:
	callq	omp_get_wtime
	vsubsd	80(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp5667:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5668:
# %bb.193:
	movq	%rax, %r12
	movq	256(%rsp), %rax                 # 8-byte Reload
	leaq	96(%rax), %r15
.Ltmp5669:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp5670:
# %bb.194:
.Ltmp5671:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5672:
# %bb.195:
	movl	%eax, %ebp
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp5673:
	leaq	16(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5674:
# %bb.196:
.Ltmp5675:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp5676:
# %bb.197:
.Ltmp5678:
	callq	mpfr_get_default_rounding_mode
.Ltmp5679:
# %bb.198:
.Ltmp5680:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movq	%r15, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp5681:
# %bb.199:
.Ltmp5683:
	leaq	16(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp5684:
# %bb.200:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 112(%rsp)
	cmpq	$0, 40(%rsp)
	je	.LBB40_202
# %bb.201:
.Ltmp5686:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5687:
.LBB40_202:
	leaq	424(%rsp), %r15
	movq	%r15, 408(%rsp)
	movl	$2037413985, 424(%rsp)          # imm = 0x79707861
	movw	$32, 428(%rsp)
	movq	$5, 416(%rsp)
.Ltmp5689:
	leaq	408(%rsp), %rdi
	leaq	112(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp5690:
# %bb.203:
	movq	408(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB40_205
# %bb.204:
	callq	_ZdlPv
.LBB40_205:
	cmpq	$0, 72(%rsp)
	je	.LBB40_207
# %bb.206:
.Ltmp5692:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5693:
.LBB40_207:
	movslq	12(%rsp), %r12
	movq	%r12, %r15
	shlq	$4, %r15
	testq	%r12, %r12
	movq	$-1, %rdi
	cmovnsq	%r15, %rdi
	callq	_Znam
	movq	%rax, %rbp
	testq	%r12, %r12
	movq	104(%rsp), %r14                 # 8-byte Reload
	je	.LBB40_211
# %bb.208:
	movq	%rbp, %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_intel_fast_memset@PLT
	movq	312(%rsp), %r13                 # 8-byte Reload
	testl	%r12d, %r12d
	jle	.LBB40_211
# %bb.209:
	xorl	%r15d, %r15d
	xorl	%r12d, %r12d
	.p2align	4, 0x90
.LBB40_210:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, (%r14,%r15)
	vmovupd	(%r14,%r15), %xmm0
	vmovupd	%xmm0, (%rbp,%r15)
	incq	%r12
	movslq	12(%rsp), %rax
	addq	$16, %r15
	addq	$32, %r13
	cmpq	%rax, %r12
	jl	.LBB40_210
.LBB40_211:
	leaq	12(%rsp), %rdi
	leaq	144(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%rbp, 160(%rsp)                 # 8-byte Spill
	movq	%rbp, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	48(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	cmpl	$0, 12(%rsp)
	jle	.LBB40_252
# %bb.212:
	xorl	%eax, %eax
	movq	%rax, 80(%rsp)                  # 8-byte Spill
	leaq	112(%rsp), %r12
	movq	160(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB40_214
	.p2align	4, 0x90
.LBB40_213:                             #   in Loop: Header=BB40_214 Depth=1
	movq	80(%rsp), %rdx                  # 8-byte Reload
	incq	%rdx
	movslq	12(%rsp), %rax
	movq	176(%rsp), %rsi                 # 8-byte Reload
	addq	$16, %rsi
	movq	%rdx, %rcx
	movq	%rdx, 80(%rsp)                  # 8-byte Spill
	cmpq	%rax, %rdx
	jge	.LBB40_252
.LBB40_214:                             # =>This Inner Loop Header: Depth=1
	movq	192(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %r15d
.Ltmp5695:
	movq	%r12, %r14
	movq	%r12, %rdi
	movq	%rsi, 176(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp5696:
# %bb.215:                              #   in Loop: Header=BB40_214 Depth=1
	movq	80(%rsp), %rax                  # 8-byte Reload
	addl	%r15d, %eax
	addl	$8, %eax
	movslq	%eax, %rbp
	shlq	$5, %rbp
	addq	256(%rsp), %rbp                 # 8-byte Folded Reload
.Ltmp5698:
	movq	%rbp, %rdi
	callq	mpfr_get_prec
.Ltmp5699:
# %bb.216:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5700:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp5701:
# %bb.217:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5702:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp5703:
# %bb.218:                              #   in Loop: Header=BB40_214 Depth=1
	movl	%eax, %r15d
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp5704:
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp5705:
# %bb.219:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5706:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp5707:
# %bb.220:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5709:
	callq	mpfr_get_default_rounding_mode
.Ltmp5710:
# %bb.221:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5711:
	movq	%r14, %r12
	leaq	16(%rsp), %rdi
	movq	%rbp, %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp5712:
# %bb.222:                              #   in Loop: Header=BB40_214 Depth=1
	cmpq	$0, 136(%rsp)
	je	.LBB40_224
# %bb.223:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5714:
	movq	%r12, %rdi
	callq	mpfr_clear
.Ltmp5715:
.LBB40_224:                             #   in Loop: Header=BB40_214 Depth=1
.Ltmp5717:
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5718:
# %bb.225:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5719:
	movq	%rax, %r13
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5720:
# %bb.226:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5721:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp5722:
# %bb.227:                              #   in Loop: Header=BB40_214 Depth=1
	movl	%eax, %ebp
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp5723:
	leaq	208(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp5724:
# %bb.228:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5725:
	leaq	208(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp5726:
# %bb.229:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5728:
	callq	mpfr_get_default_rounding_mode
.Ltmp5729:
# %bb.230:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5730:
	leaq	208(%rsp), %rdi
	leaq	16(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp5731:
# %bb.231:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5733:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5734:
# %bb.232:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5735:
	movq	%rax, %r13
	leaq	208(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5736:
# %bb.233:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5737:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp5738:
# %bb.234:                              #   in Loop: Header=BB40_214 Depth=1
	movl	%eax, %ebp
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp5739:
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp5740:
# %bb.235:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5741:
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp5742:
# %bb.236:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5744:
	callq	mpfr_get_default_rounding_mode
.Ltmp5745:
	leaq	48(%rsp), %r14
# %bb.237:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5746:
	movq	%r12, %rdi
	movq	%r14, %rsi
	leaq	208(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp5747:
# %bb.238:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5749:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp5750:
# %bb.239:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5751:
	movq	%rax, %r15
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp5752:
# %bb.240:                              #   in Loop: Header=BB40_214 Depth=1
	movq	%rax, %rbp
	cmpq	%rax, %r15
	je	.LBB40_244
# %bb.241:                              #   in Loop: Header=BB40_214 Depth=1
	cmpq	$0, 72(%rsp)
	je	.LBB40_243
# %bb.242:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5753:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5754:
.LBB40_243:                             #   in Loop: Header=BB40_214 Depth=1
.Ltmp5755:
	leaq	48(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp5756:
.LBB40_244:                             #   in Loop: Header=BB40_214 Depth=1
.Ltmp5757:
	callq	mpfr_get_default_rounding_mode
.Ltmp5758:
# %bb.245:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5759:
	leaq	48(%rsp), %rdi
	movq	%r12, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp5760:
# %bb.246:                              #   in Loop: Header=BB40_214 Depth=1
	cmpq	$0, 136(%rsp)
	je	.LBB40_248
# %bb.247:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5762:
	movq	%r12, %rdi
	callq	mpfr_clear
.Ltmp5763:
.LBB40_248:                             #   in Loop: Header=BB40_214 Depth=1
	cmpq	$0, 232(%rsp)
	je	.LBB40_250
# %bb.249:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5765:
	leaq	208(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5766:
.LBB40_250:                             #   in Loop: Header=BB40_214 Depth=1
	cmpq	$0, 40(%rsp)
	je	.LBB40_213
# %bb.251:                              #   in Loop: Header=BB40_214 Depth=1
.Ltmp5768:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5769:
	jmp	.LBB40_213
.LBB40_252:
.Ltmp5771:
	callq	mpfr_get_default_rounding_mode
.Ltmp5772:
# %bb.253:
.Ltmp5773:
	movl	%eax, %ebp
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5774:
# %bb.254:
.Ltmp5775:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp5776:
# %bb.255:
.Ltmp5777:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp5778:
# %bb.256:
.Ltmp5779:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp5780:
# %bb.257:
.Ltmp5782:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp5783:
# %bb.258:
.Ltmp5785:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5786:
# %bb.259:
.Ltmp5787:
	movq	%rax, %r12
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5788:
# %bb.260:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB40_264
# %bb.261:
	cmpq	$0, 72(%rsp)
	je	.LBB40_263
# %bb.262:
.Ltmp5789:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5790:
.LBB40_263:
.Ltmp5791:
	leaq	48(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp5792:
.LBB40_264:
.Ltmp5793:
	callq	mpfr_get_default_rounding_mode
.Ltmp5794:
# %bb.265:
.Ltmp5795:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp5796:
# %bb.266:
	cmpq	$0, 40(%rsp)
	je	.LBB40_268
# %bb.267:
.Ltmp5798:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5799:
.LBB40_268:
	callq	omp_get_wtime
	vmovsd	%xmm0, 80(%rsp)                 # 8-byte Spill
.Ltmp5801:
	leaq	12(%rsp), %rdi
	leaq	144(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	104(%rsp), %r14                 # 8-byte Reload
	movq	%r14, %r8
	movq	160(%rsp), %r15                 # 8-byte Reload
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5802:
# %bb.269:
.Ltmp5803:
	leaq	12(%rsp), %rdi
	leaq	144(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5804:
# %bb.270:
.Ltmp5805:
	leaq	12(%rsp), %rdi
	leaq	144(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5806:
# %bb.271:
.Ltmp5807:
	leaq	12(%rsp), %rdi
	leaq	144(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5808:
# %bb.272:
.Ltmp5809:
	leaq	12(%rsp), %rdi
	leaq	144(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5810:
# %bb.273:
.Ltmp5811:
	leaq	12(%rsp), %rdi
	leaq	144(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5812:
# %bb.274:
.Ltmp5813:
	leaq	12(%rsp), %rdi
	leaq	144(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5814:
# %bb.275:
.Ltmp5815:
	leaq	12(%rsp), %rdi
	leaq	144(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5816:
# %bb.276:
.Ltmp5817:
	leaq	12(%rsp), %rdi
	leaq	144(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5818:
# %bb.277:
.Ltmp5819:
	leaq	12(%rsp), %rdi
	leaq	144(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp5820:
# %bb.278:
	callq	omp_get_wtime
	vsubsd	80(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp5822:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5823:
# %bb.279:
	movq	%rax, %r15
	movq	256(%rsp), %rdi                 # 8-byte Reload
	subq	$-128, %rdi
.Ltmp5824:
	movq	%rdi, %r13
	callq	mpfr_get_prec
.Ltmp5825:
# %bb.280:
.Ltmp5826:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp5827:
# %bb.281:
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp5828:
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp5829:
# %bb.282:
.Ltmp5830:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp5831:
# %bb.283:
.Ltmp5833:
	callq	mpfr_get_default_rounding_mode
.Ltmp5834:
# %bb.284:
.Ltmp5835:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movq	%r13, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp5836:
# %bb.285:
.Ltmp5838:
	leaq	16(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp5839:
# %bb.286:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 112(%rsp)
	cmpq	$0, 40(%rsp)
	je	.LBB40_288
# %bb.287:
.Ltmp5841:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5842:
.LBB40_288:
	leaq	392(%rsp), %r15
	movq	%r15, 376(%rsp)
	movl	$1986880871, 392(%rsp)          # imm = 0x766D6567
	movw	$32, 396(%rsp)
	movq	$5, 384(%rsp)
.Ltmp5844:
	leaq	376(%rsp), %rdi
	leaq	112(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp5845:
# %bb.289:
	movq	376(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB40_291
# %bb.290:
	callq	_ZdlPv
.LBB40_291:
	movq	160(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	cmpq	$0, 72(%rsp)
	je	.LBB40_293
# %bb.292:
.Ltmp5847:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5848:
.LBB40_293:
	movq	%rbx, %rdi
	callq	_ZdaPv
	movq	104(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	xorl	%eax, %eax
	vcvtsi2ss	%eax, %xmm2, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vblendps	$1, %xmm0, %xmm1, %xmm10        # xmm10 = xmm0[0],xmm1[1,2,3]
	movl	$2097153, %ebp                  # imm = 0x200001
	movl	$1, %r15d
	leaq	48(%rsp), %rbx
	leaq	16(%rsp), %r14
	.p2align	4, 0x90
.LBB40_294:                             # =>This Inner Loop Header: Depth=1
	vmovups	%xmm10, 176(%rsp)               # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm1, 80(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	decl	%ebp
	vcvtsi2ss	%ebp, %xmm13, %xmm1
	vxorps	%xmm4, %xmm4, %xmm4
	vmulss	%xmm4, %xmm1, %xmm2
	vmulss	%xmm1, %xmm1, %xmm0
	vxorps	%xmm3, %xmm3, %xmm3
	vmulss	%xmm1, %xmm4, %xmm5
	vxorps	%xmm8, %xmm8, %xmm8
	vaddss	%xmm5, %xmm2, %xmm4
	vfmsub213ss	%xmm2, %xmm1, %xmm3     # xmm3 = (xmm1 * xmm3) - xmm2
	vsubss	%xmm2, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm6
	vaddss	%xmm2, %xmm8, %xmm8
	vsubss	%xmm6, %xmm2, %xmm10
	vsubss	%xmm2, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm6
	vsubss	%xmm6, %xmm2, %xmm11
	vmovaps	%xmm1, %xmm6
	vxorps	%xmm13, %xmm13, %xmm13
	vfmadd231ss	%xmm13, %xmm13, %xmm2   # xmm2 = (xmm13 * xmm13) + xmm2
	vfmadd231ss	%xmm13, %xmm13, %xmm2   # xmm2 = (xmm13 * xmm13) + xmm2
	vfmadd231ss	%xmm1, %xmm13, %xmm2    # xmm2 = (xmm13 * xmm1) + xmm2
	vmovaps	%xmm1, %xmm12
	vfmsub213ss	%xmm0, %xmm1, %xmm12    # xmm12 = (xmm1 * xmm12) - xmm0
	vfmsub213ss	%xmm5, %xmm13, %xmm6    # xmm6 = (xmm13 * xmm6) - xmm5
	vsubss	%xmm7, %xmm5, %xmm1
	vaddss	%xmm1, %xmm10, %xmm7
	vaddss	%xmm4, %xmm12, %xmm1
	vsubss	%xmm4, %xmm1, %xmm10
	vsubss	%xmm9, %xmm13, %xmm9
	vsubss	%xmm10, %xmm1, %xmm13
	vaddss	%xmm9, %xmm11, %xmm9
	vaddss	%xmm5, %xmm8, %xmm11
	vsubss	%xmm8, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm14, %xmm5, %xmm5
	vaddss	%xmm3, %xmm11, %xmm14
	vaddss	%xmm5, %xmm8, %xmm8
	vsubss	%xmm11, %xmm14, %xmm5
	vsubss	%xmm5, %xmm14, %xmm15
	vsubss	%xmm5, %xmm3, %xmm5
	vsubss	%xmm15, %xmm11, %xmm11
	vaddss	%xmm5, %xmm11, %xmm11
	vaddss	%xmm6, %xmm14, %xmm5
	vsubss	%xmm14, %xmm5, %xmm15
	vsubss	%xmm10, %xmm12, %xmm10
	vsubss	%xmm15, %xmm5, %xmm12
	vsubss	%xmm12, %xmm14, %xmm12
	vsubss	%xmm15, %xmm6, %xmm14
	vsubss	%xmm13, %xmm4, %xmm4
	vaddss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm7, %xmm5, %xmm13
	vsubss	%xmm5, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm14, %xmm7, %xmm7
	vaddss	%xmm7, %xmm5, %xmm7
	vaddss	%xmm4, %xmm10, %xmm4
	vaddss	%xmm4, %xmm13, %xmm5
	vsubss	%xmm13, %xmm5, %xmm10
	vsubss	%xmm10, %xmm5, %xmm14
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	.LCPI40_3(%rip), %xmm2, %xmm2
	vaddss	%xmm6, %xmm2, %xmm2
	vsubss	%xmm14, %xmm13, %xmm3
	vaddss	%xmm2, %xmm9, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm2, %xmm11, %xmm2
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm2, %xmm12, %xmm2
	vaddss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm6
	vmulss	%xmm0, %xmm0, %xmm2
	vmulss	%xmm1, %xmm0, %xmm7
	vmulss	%xmm0, %xmm1, %xmm8
	vmovaps	%xmm0, %xmm3
	vfmsub213ss	%xmm8, %xmm1, %xmm3     # xmm3 = (xmm1 * xmm3) - xmm8
	vmulss	%xmm5, %xmm0, %xmm9
	vmulss	%xmm1, %xmm1, %xmm11
	vmovaps	%xmm1, %xmm4
	vmulss	%xmm0, %xmm5, %xmm10
	vmulss	%xmm6, %xmm0, %xmm12
	vfmadd231ss	%xmm5, %xmm1, %xmm12    # xmm12 = (xmm1 * xmm5) + xmm12
	vfmadd231ss	%xmm1, %xmm5, %xmm12    # xmm12 = (xmm5 * xmm1) + xmm12
	vmovaps	%xmm0, %xmm13
	vfmsub213ss	%xmm10, %xmm5, %xmm13   # xmm13 = (xmm5 * xmm13) - xmm10
	vfmsub213ss	%xmm7, %xmm0, %xmm1     # xmm1 = (xmm0 * xmm1) - xmm7
	vfmsub213ss	%xmm9, %xmm0, %xmm5     # xmm5 = (xmm0 * xmm5) - xmm9
	vfmadd231ss	%xmm0, %xmm6, %xmm12    # xmm12 = (xmm6 * xmm0) + xmm12
	vfmsub213ss	%xmm2, %xmm0, %xmm0     # xmm0 = (xmm0 * xmm0) - xmm2
	vaddss	%xmm7, %xmm8, %xmm6
	vsubss	%xmm7, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm14, %xmm8, %xmm8
	vfmsub213ss	%xmm11, %xmm4, %xmm4    # xmm4 = (xmm4 * xmm4) - xmm11
	vaddss	%xmm11, %xmm9, %xmm14
	vaddss	%xmm7, %xmm8, %xmm7
	vsubss	%xmm9, %xmm14, %xmm8
	vsubss	%xmm8, %xmm14, %xmm15
	vsubss	%xmm8, %xmm11, %xmm8
	vsubss	%xmm15, %xmm9, %xmm9
	vaddss	%xmm8, %xmm9, %xmm8
	vmovss	%xmm8, 160(%rsp)                # 4-byte Spill
	vaddss	%xmm10, %xmm14, %xmm9
	vsubss	%xmm14, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm15
	vsubss	%xmm15, %xmm14, %xmm14
	vsubss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm10, %xmm14, %xmm10
	vaddss	%xmm1, %xmm9, %xmm11
	vsubss	%xmm9, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm15
	vsubss	%xmm15, %xmm9, %xmm9
	vaddss	%xmm0, %xmm6, %xmm15
	vinsertps	$16, %xmm15, %xmm2, %xmm8 # xmm8 = xmm2[0],xmm15[0],xmm2[2,3]
	vsubss	%xmm6, %xmm15, %xmm2
	vsubss	%xmm2, %xmm15, %xmm15
	vsubss	%xmm2, %xmm0, %xmm0
	vsubss	%xmm14, %xmm1, %xmm1
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm3, %xmm11, %xmm2
	vsubss	%xmm11, %xmm2, %xmm9
	vsubss	%xmm9, %xmm2, %xmm14
	vsubss	%xmm14, %xmm11, %xmm11
	vsubss	%xmm9, %xmm3, %xmm3
	vsubss	%xmm15, %xmm6, %xmm6
	vaddss	%xmm3, %xmm11, %xmm3
	vaddss	%xmm7, %xmm2, %xmm9
	vsubss	%xmm2, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm14
	vsubss	%xmm14, %xmm2, %xmm2
	vsubss	%xmm11, %xmm7, %xmm7
	vaddss	%xmm5, %xmm12, %xmm5
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm4, %xmm5, %xmm5
	vaddss	%xmm5, %xmm13, %xmm5
	vaddss	%xmm0, %xmm9, %xmm6
	vaddss	%xmm7, %xmm2, %xmm2
	vsubss	%xmm9, %xmm6, %xmm7
	vsubss	%xmm7, %xmm0, %xmm0
	vsubss	%xmm7, %xmm6, %xmm7
	vsubss	%xmm7, %xmm9, %xmm7
	vaddss	160(%rsp), %xmm5, %xmm5         # 4-byte Folded Reload
	vaddss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vinsertps	$16, %xmm0, %xmm6, %xmm0 # xmm0 = xmm6[0],xmm0[0],xmm6[2,3]
	vxorps	%xmm13, %xmm13, %xmm13
	vcvtsi2ss	%r15d, %xmm13, %xmm1
	vmovss	%xmm1, 48(%rsp)
	vmovlhps	%xmm0, %xmm8, %xmm0             # xmm0 = xmm8[0],xmm0[0]
	vmovups	%xmm0, 16(%rsp)
	movl	$0, 60(%rsp)
	vxorps	%xmm0, %xmm0, %xmm0
	vmovlps	%xmm0, 52(%rsp)
	movq	%rbx, %rdi
	movq	%r14, %rsi
	callq	_ZN7mX_real7qX_real12operator_divIfLNS_9AlgorithmE2ELS2_2ELS2_2EEEDaRKNS0_7qx_realIT_XT0_EEERKNS3_IS4_XT1_EEE
	vmovups	176(%rsp), %xmm5                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm0, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm5, %xmm4
	vsubss	%xmm3, %xmm0, %xmm3
	vaddss	%xmm3, %xmm4, %xmm3
	vmovshdup	%xmm5, %xmm4            # xmm4 = xmm5[1,1,3,3]
	vmovshdup	%xmm0, %xmm0            # xmm0 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm4, %xmm4
	vsubss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm4
	vaddss	%xmm3, %xmm5, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm10 # xmm10 = xmm2[0],xmm0[0],xmm2[2,3]
	vsubss	%xmm5, %xmm0, %xmm6
	vsubss	%xmm6, %xmm0, %xmm7
	vsubss	%xmm7, %xmm5, %xmm5
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm5, %xmm5
	vmovups	80(%rsp), %xmm8                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm1, %xmm8, %xmm3
	vsubss	%xmm8, %xmm3, %xmm6
	vsubss	%xmm6, %xmm3, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vmovaps	%xmm8, %xmm11
	vsubss	%xmm6, %xmm1, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm4, %xmm3, %xmm7
	vsubss	%xmm3, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vsubss	%xmm8, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm4
	vaddss	%xmm5, %xmm7, %xmm3
	vsubss	%xmm7, %xmm3, %xmm8
	vsubss	%xmm8, %xmm3, %xmm9
	vsubss	%xmm9, %xmm7, %xmm7
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm7, %xmm5
	vmovshdup	%xmm11, %xmm7           # xmm7 = xmm11[1,1,3,3]
	vmovshdup	%xmm1, %xmm1            # xmm1 = xmm1[1,1,3,3]
	vaddss	%xmm1, %xmm7, %xmm1
	vaddss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm4
	vinsertps	$16, %xmm4, %xmm3, %xmm1 # xmm1 = xmm3[0],xmm4[0],xmm3[2,3]
	cmpl	$1, %ebp
	ja	.LBB40_294
# %bb.295:
	movl	$90, %eax
	vcvtsi2ss	%eax, %xmm13, %xmm5
	vmulss	%xmm2, %xmm5, %xmm1
	vmovss	%xmm1, 176(%rsp)                # 4-byte Spill
	vmovaps	%xmm2, %xmm6
	vfmsub213ss	%xmm1, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm6) - xmm1
	vmulss	%xmm0, %xmm5, %xmm7
	vmovaps	%xmm0, %xmm11
	vfmsub213ss	%xmm7, %xmm5, %xmm11    # xmm11 = (xmm5 * xmm11) - xmm7
	vxorps	%xmm1, %xmm1, %xmm1
	vmulss	%xmm2, %xmm1, %xmm12
	vaddss	%xmm7, %xmm12, %xmm8
	vsubss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm7, %xmm7
	vsubss	%xmm9, %xmm12, %xmm9
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm6, %xmm8, %xmm1
	vmovss	%xmm1, 80(%rsp)                 # 4-byte Spill
	vsubss	%xmm8, %xmm1, %xmm9
	vsubss	%xmm9, %xmm1, %xmm10
	vsubss	%xmm10, %xmm8, %xmm8
	vsubss	%xmm9, %xmm6, %xmm6
	vaddss	%xmm6, %xmm8, %xmm8
	vmulss	%xmm3, %xmm5, %xmm9
	vmovaps	%xmm3, %xmm6
	vfmsub213ss	%xmm9, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm6) - xmm9
	vxorps	%xmm1, %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm13
	vaddss	%xmm13, %xmm9, %xmm14
	vsubss	%xmm9, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm10
	vsubss	%xmm10, %xmm9, %xmm9
	vmovaps	%xmm0, %xmm10
	vfmsub213ss	%xmm13, %xmm1, %xmm10   # xmm10 = (xmm1 * xmm10) - xmm13
	vsubss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm13, %xmm9, %xmm1
	vmovss	%xmm1, 160(%rsp)                # 4-byte Spill
	vaddss	%xmm12, %xmm14, %xmm15
	vsubss	%xmm14, %xmm15, %xmm1
	vsubss	%xmm1, %xmm15, %xmm13
	vsubss	%xmm13, %xmm14, %xmm14
	vmovaps	%xmm2, %xmm13
	vfmsub132ss	.LCPI40_3(%rip), %xmm12, %xmm13 # xmm13 = (xmm13 * mem) - xmm12
	vsubss	%xmm1, %xmm12, %xmm1
	vaddss	%xmm1, %xmm14, %xmm1
	vmovss	%xmm1, 192(%rsp)                # 4-byte Spill
	vaddss	%xmm11, %xmm15, %xmm1
	vsubss	%xmm15, %xmm1, %xmm14
	vsubss	%xmm14, %xmm1, %xmm9
	vsubss	%xmm9, %xmm15, %xmm9
	vsubss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm1, %xmm13, %xmm11
	vsubss	%xmm1, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm15
	vsubss	%xmm15, %xmm1, %xmm1
	vsubss	%xmm14, %xmm13, %xmm14
	vaddss	%xmm1, %xmm14, %xmm1
	vaddss	%xmm7, %xmm11, %xmm14
	vsubss	%xmm11, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm12
	vsubss	%xmm12, %xmm11, %xmm11
	vsubss	%xmm15, %xmm7, %xmm7
	vaddss	%xmm7, %xmm11, %xmm7
	vaddss	%xmm8, %xmm14, %xmm15
	vsubss	%xmm14, %xmm15, %xmm11
	vsubss	%xmm11, %xmm15, %xmm12
	vsubss	%xmm12, %xmm14, %xmm12
	vsubss	%xmm11, %xmm8, %xmm8
	vaddss	%xmm8, %xmm12, %xmm8
	vmulss	%xmm4, %xmm5, %xmm4
	vfmadd231ss	.LCPI40_3(%rip), %xmm3, %xmm4 # xmm4 = (xmm3 * mem) + xmm4
	vxorps	%xmm3, %xmm3, %xmm3
	vfmadd231ss	%xmm0, %xmm3, %xmm4     # xmm4 = (xmm3 * xmm0) + xmm4
	vfmadd231ss	%xmm2, %xmm3, %xmm4     # xmm4 = (xmm3 * xmm2) + xmm4
	vaddss	%xmm6, %xmm4, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	160(%rsp), %xmm0, %xmm0         # 4-byte Folded Reload
	vaddss	192(%rsp), %xmm0, %xmm0         # 4-byte Folded Reload
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm1
	vaddss	80(%rsp), %xmm15, %xmm0         # 4-byte Folded Reload
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	176(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm0
	vucomiss	%xmm3, %xmm0
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB40_297
# %bb.296:
	vxorps	%xmm2, %xmm2, %xmm2
	vblendps	$1, %xmm0, %xmm2, %xmm7         # xmm7 = xmm0[0],xmm2[1,2,3]
	jmp	.LBB40_300
.LBB40_297:
	vcomiss	%xmm0, %xmm3
	jbe	.LBB40_299
# %bb.298:
	vbroadcastss	.LCPI40_0(%rip), %xmm7  # xmm7 = [NaN,NaN,NaN,NaN]
	vmovaps	%xmm7, %xmm2
	jmp	.LBB40_300
.LBB40_299:
	vmovaps	%xmm4, %xmm0
	vmovss	%xmm15, 160(%rsp)               # 4-byte Spill
	vmovss	%xmm1, 192(%rsp)                # 4-byte Spill
	callq	sqrtf
	vmovaps	%xmm0, %xmm3
	vmovss	176(%rsp), %xmm14               # 4-byte Reload
                                        # xmm14 = mem[0],zero,zero,zero
	vfnmadd213ss	%xmm14, %xmm0, %xmm0    # xmm0 = -(xmm0 * xmm0) + xmm14
	vmovss	80(%rsp), %xmm13                # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm3, %xmm3, %xmm1
	vdivss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm3, %xmm3, %xmm2
	vbroadcastss	.LCPI40_1(%rip), %xmm4  # xmm4 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm4, %xmm2, %xmm0
	vmovaps	%xmm3, %xmm6
	vfmsub213ss	%xmm2, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm6) - xmm2
	vmulss	%xmm1, %xmm3, %xmm7
	vmovaps	%xmm1, %xmm5
	vfmsub213ss	%xmm7, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm5) - xmm7
	vmovaps	%xmm5, %xmm12
	vmovups	%xmm5, 256(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmulss	%xmm3, %xmm1, %xmm8
	vmovaps	%xmm3, %xmm5
	vfmsub213ss	%xmm8, %xmm1, %xmm5     # xmm5 = (xmm1 * xmm5) - xmm8
	vaddss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm7, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm7, %xmm8, %xmm10
	vaddss	%xmm6, %xmm9, %xmm7
	vsubss	%xmm9, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vsubss	%xmm8, %xmm6, %xmm6
	vaddss	%xmm6, %xmm9, %xmm9
	vmovaps	%xmm1, %xmm6
	vfmadd213ss	%xmm5, %xmm1, %xmm6     # xmm6 = (xmm1 * xmm6) + xmm5
	vaddss	%xmm6, %xmm12, %xmm6
	vaddss	%xmm6, %xmm10, %xmm6
	vaddss	%xmm6, %xmm9, %xmm6
	vmovaps	%xmm14, %xmm11
	vsubss	%xmm2, %xmm14, %xmm2
	vsubss	%xmm14, %xmm2, %xmm8
	vmovaps	%xmm14, %xmm12
	vsubss	%xmm8, %xmm2, %xmm11
	vsubss	%xmm11, %xmm14, %xmm11
	vxorps	%xmm4, %xmm7, %xmm12
	vsubss	%xmm8, %xmm0, %xmm0
	vaddss	%xmm0, %xmm11, %xmm0
	vsubss	%xmm7, %xmm13, %xmm7
	vsubss	%xmm13, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm11
	vsubss	%xmm11, %xmm13, %xmm11
	vsubss	%xmm8, %xmm12, %xmm8
	vaddss	%xmm8, %xmm11, %xmm4
	vmovss	%xmm4, 248(%rsp)                # 4-byte Spill
	vaddss	%xmm0, %xmm7, %xmm11
	vsubss	%xmm7, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm7, %xmm7
	vsubss	%xmm12, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm8
	vmovss	160(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm2, %xmm11, %xmm2
	vmovss	%xmm2, 80(%rsp)                 # 4-byte Spill
	vaddss	%xmm0, %xmm2, %xmm0
	movl	$2, %eax
	vxorps	%xmm15, %xmm15, %xmm15
	vcvtsi2ss	%eax, %xmm15, %xmm2
	vmovss	%xmm2, 176(%rsp)                # 4-byte Spill
	vaddss	%xmm1, %xmm3, %xmm4
	vmovss	%xmm4, 104(%rsp)                # 4-byte Spill
	vmulss	%xmm4, %xmm2, %xmm2
	vdivss	%xmm2, %xmm0, %xmm11
	vmulss	%xmm3, %xmm11, %xmm0
	vmovaps	%xmm11, %xmm12
	vfmsub213ss	%xmm0, %xmm3, %xmm12    # xmm12 = (xmm3 * xmm12) - xmm0
	vmulss	%xmm1, %xmm1, %xmm2
	vaddss	%xmm2, %xmm0, %xmm14
	vsubss	%xmm0, %xmm14, %xmm6
	vsubss	%xmm6, %xmm14, %xmm13
	vsubss	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm1, %xmm13
	vfmsub213ss	%xmm2, %xmm1, %xmm13    # xmm13 = (xmm1 * xmm13) - xmm2
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vmovss	%xmm0, 240(%rsp)                # 4-byte Spill
	vmulss	%xmm3, %xmm11, %xmm4
	vaddss	%xmm4, %xmm14, %xmm15
	vsubss	%xmm14, %xmm15, %xmm6
	vsubss	%xmm6, %xmm15, %xmm0
	vsubss	%xmm0, %xmm14, %xmm14
	vxorps	%xmm2, %xmm2, %xmm2
	vmulss	%xmm2, %xmm3, %xmm0
	vfmadd231ss	%xmm11, %xmm1, %xmm0    # xmm0 = (xmm1 * xmm11) + xmm0
	vfmadd231ss	%xmm1, %xmm11, %xmm0    # xmm0 = (xmm11 * xmm1) + xmm0
	vfmadd231ss	%xmm2, %xmm3, %xmm0     # xmm0 = (xmm3 * xmm2) + xmm0
	vinsertps	$16, %xmm1, %xmm3, %xmm7 # xmm7 = xmm3[0],xmm1[0],xmm3[2,3]
	vfmsub213ss	%xmm4, %xmm11, %xmm3    # xmm3 = (xmm11 * xmm3) - xmm4
	vsubss	%xmm6, %xmm4, %xmm1
	vaddss	%xmm1, %xmm14, %xmm1
	vmovups	256(%rsp), %xmm14               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm14, %xmm15, %xmm2
	vsubss	%xmm15, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm6
	vsubss	%xmm6, %xmm15, %xmm6
	vsubss	%xmm4, %xmm14, %xmm4
	vaddss	%xmm4, %xmm6, %xmm4
	vaddss	%xmm5, %xmm2, %xmm6
	vsubss	%xmm2, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm2, %xmm2
	vsubss	%xmm14, %xmm5, %xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm6, %xmm10, %xmm5
	vsubss	%xmm6, %xmm5, %xmm14
	vsubss	%xmm14, %xmm5, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm14, %xmm10, %xmm10
	vaddss	%xmm6, %xmm10, %xmm6
	vaddss	%xmm5, %xmm9, %xmm10
	vsubss	%xmm5, %xmm10, %xmm14
	vsubss	%xmm14, %xmm10, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm14, %xmm9, %xmm9
	vaddss	%xmm5, %xmm9, %xmm5
	vaddss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	240(%rsp), %xmm0, %xmm0         # 4-byte Folded Reload
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vbroadcastss	.LCPI40_1(%rip), %xmm1  # xmm1 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm1, %xmm10, %xmm1
	vmovss	160(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm10, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm5, %xmm4
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vmovss	248(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vaddss	%xmm6, %xmm2, %xmm3
	vsubss	%xmm2, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm2, %xmm2
	vsubss	%xmm4, %xmm6, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm3, %xmm8, %xmm4
	vsubss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm3, %xmm3
	vsubss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vmovss	192(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm2
	vaddss	80(%rsp), %xmm4, %xmm1          # 4-byte Folded Reload
	vaddss	%xmm2, %xmm1, %xmm2
	vaddss	104(%rsp), %xmm11, %xmm1        # 4-byte Folded Reload
	vmulss	176(%rsp), %xmm1, %xmm1         # 4-byte Folded Reload
	vdivss	%xmm1, %xmm2, %xmm1
	vinsertps	$16, %xmm1, %xmm11, %xmm2 # xmm2 = xmm11[0],xmm1[0],xmm11[2,3]
.LBB40_300:
	vmovshdup	%xmm7, %xmm5            # xmm5 = xmm7[1,1,3,3]
	vaddss	%xmm2, %xmm5, %xmm1
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm7, %xmm1
	vxorps	%xmm4, %xmm4, %xmm4
	vucomiss	%xmm4, %xmm1
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB40_302
# %bb.301:
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         # xmm1 = xmm1[0],xmm0[1,2,3]
	jmp	.LBB40_305
.LBB40_302:
	vcomiss	%xmm1, %xmm4
	jbe	.LBB40_304
# %bb.303:
	vbroadcastss	.LCPI40_0(%rip), %xmm1  # xmm1 = [NaN,NaN,NaN,NaN]
	vmovaps	%xmm1, %xmm0
	jmp	.LBB40_305
.LBB40_304:
	vmovaps	%xmm7, %xmm0
	vmovups	%xmm2, 80(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm3, 176(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm7, 192(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm5, 160(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovaps	%xmm0, %xmm3
	vmovups	192(%rsp), %xmm15               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vfnmadd213ss	%xmm15, %xmm0, %xmm0    # xmm0 = -(xmm0 * xmm0) + xmm15
	vmovups	160(%rsp), %xmm12               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm3, %xmm3, %xmm1
	vdivss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm3, %xmm3, %xmm0
	vmovaps	%xmm3, %xmm2
	vfmsub213ss	%xmm0, %xmm3, %xmm2     # xmm2 = (xmm3 * xmm2) - xmm0
	vmulss	%xmm1, %xmm3, %xmm6
	vmovaps	%xmm1, %xmm4
	vfmsub213ss	%xmm6, %xmm3, %xmm4     # xmm4 = (xmm3 * xmm4) - xmm6
	vmovaps	%xmm4, %xmm13
	vmulss	%xmm3, %xmm1, %xmm7
	vmovaps	%xmm3, %xmm5
	vfmsub213ss	%xmm7, %xmm1, %xmm5     # xmm5 = (xmm1 * xmm5) - xmm7
	vaddss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm6, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm6, %xmm6
	vsubss	%xmm9, %xmm7, %xmm7
	vaddss	%xmm7, %xmm6, %xmm10
	vmovss	%xmm10, 240(%rsp)               # 4-byte Spill
	vaddss	%xmm2, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm9
	vsubss	%xmm9, %xmm8, %xmm8
	vbroadcastss	.LCPI40_1(%rip), %xmm4  # xmm4 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm4, %xmm0, %xmm11
	vsubss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm14
	vmovss	%xmm14, 248(%rsp)               # 4-byte Spill
	vmovaps	%xmm15, %xmm7
	vsubss	%xmm0, %xmm15, %xmm0
	vsubss	%xmm15, %xmm0, %xmm2
	vmovaps	%xmm15, %xmm8
	vsubss	%xmm2, %xmm0, %xmm7
	vsubss	%xmm7, %xmm15, %xmm7
	vxorps	%xmm4, %xmm6, %xmm8
	vsubss	%xmm2, %xmm11, %xmm2
	vaddss	%xmm2, %xmm7, %xmm2
	vmovaps	%xmm12, %xmm4
	vsubss	%xmm6, %xmm12, %xmm6
	vsubss	%xmm12, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm11
	vsubss	%xmm11, %xmm12, %xmm11
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm7, %xmm11, %xmm4
	vmovss	%xmm4, 104(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm6, %xmm11
	vsubss	%xmm6, %xmm11, %xmm8
	vsubss	%xmm8, %xmm11, %xmm12
	vsubss	%xmm12, %xmm6, %xmm6
	vmovaps	%xmm1, %xmm12
	vfmadd213ss	%xmm5, %xmm1, %xmm12    # xmm12 = (xmm1 * xmm12) + xmm5
	vaddss	%xmm12, %xmm13, %xmm12
	vmovaps	%xmm13, %xmm9
	vaddss	%xmm10, %xmm12, %xmm12
	vaddss	%xmm14, %xmm12, %xmm12
	vsubss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm8
	vmovups	80(%rsp), %xmm2                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm12, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm0, %xmm11, %xmm0
	vmovss	%xmm0, 160(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm0, %xmm0
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm3, %xmm2
	vmovss	%xmm2, 192(%rsp)                # 4-byte Spill
	vaddss	%xmm1, %xmm3, %xmm4
	vmovss	%xmm4, 256(%rsp)                # 4-byte Spill
	vmulss	%xmm4, %xmm2, %xmm2
	vdivss	%xmm2, %xmm0, %xmm11
	vmulss	%xmm3, %xmm11, %xmm0
	vmovaps	%xmm11, %xmm12
	vfmsub213ss	%xmm0, %xmm3, %xmm12    # xmm12 = (xmm3 * xmm12) - xmm0
	vmulss	%xmm1, %xmm1, %xmm2
	vaddss	%xmm2, %xmm0, %xmm14
	vsubss	%xmm0, %xmm14, %xmm6
	vsubss	%xmm6, %xmm14, %xmm13
	vsubss	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm1, %xmm13
	vfmsub213ss	%xmm2, %xmm1, %xmm13    # xmm13 = (xmm1 * xmm13) - xmm2
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vmovss	%xmm0, 288(%rsp)                # 4-byte Spill
	vmulss	%xmm3, %xmm11, %xmm4
	vaddss	%xmm4, %xmm14, %xmm15
	vsubss	%xmm14, %xmm15, %xmm6
	vsubss	%xmm6, %xmm15, %xmm0
	vsubss	%xmm0, %xmm14, %xmm14
	vxorps	%xmm2, %xmm2, %xmm2
	vmulss	%xmm2, %xmm3, %xmm7
	vfmadd231ss	%xmm11, %xmm1, %xmm7    # xmm7 = (xmm1 * xmm11) + xmm7
	vfmadd231ss	%xmm1, %xmm11, %xmm7    # xmm7 = (xmm11 * xmm1) + xmm7
	vfmadd231ss	%xmm2, %xmm3, %xmm7     # xmm7 = (xmm3 * xmm2) + xmm7
	vinsertps	$16, %xmm1, %xmm3, %xmm1 # xmm1 = xmm3[0],xmm1[0],xmm3[2,3]
	vfmsub213ss	%xmm4, %xmm11, %xmm3    # xmm3 = (xmm11 * xmm3) - xmm4
	vsubss	%xmm6, %xmm4, %xmm2
	vaddss	%xmm2, %xmm14, %xmm2
	vaddss	%xmm9, %xmm15, %xmm4
	vsubss	%xmm15, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm14
	vsubss	%xmm14, %xmm15, %xmm14
	vsubss	%xmm6, %xmm9, %xmm6
	vaddss	%xmm6, %xmm14, %xmm6
	vaddss	%xmm5, %xmm4, %xmm14
	vsubss	%xmm4, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vsubss	%xmm15, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vmovss	240(%rsp), %xmm9                # 4-byte Reload
                                        # xmm9 = mem[0],zero,zero,zero
	vaddss	%xmm9, %xmm14, %xmm5
	vsubss	%xmm14, %xmm5, %xmm10
	vsubss	%xmm10, %xmm5, %xmm15
	vsubss	%xmm15, %xmm14, %xmm14
	vsubss	%xmm10, %xmm9, %xmm10
	vaddss	%xmm10, %xmm14, %xmm10
	vmovss	248(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm5, %xmm14
	vsubss	%xmm5, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vsubss	%xmm15, %xmm0, %xmm9
	vaddss	%xmm5, %xmm9, %xmm5
	vaddss	%xmm7, %xmm12, %xmm0
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	288(%rsp), %xmm0, %xmm0         # 4-byte Folded Reload
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vbroadcastss	.LCPI40_1(%rip), %xmm2  # xmm2 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm14, %xmm2
	vmovups	80(%rsp), %xmm6                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm14, %xmm6, %xmm3
	vsubss	%xmm6, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm6, %xmm5
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vmovss	104(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vaddss	%xmm7, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm3, %xmm3
	vsubss	%xmm5, %xmm7, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm4, %xmm8, %xmm5
	vsubss	%xmm4, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm4, %xmm4
	vsubss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vmovups	176(%rsp), %xmm6                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	160(%rsp), %xmm5, %xmm2         # 4-byte Folded Reload
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	256(%rsp), %xmm11, %xmm2        # 4-byte Folded Reload
	vmulss	192(%rsp), %xmm2, %xmm2         # 4-byte Folded Reload
	vdivss	%xmm2, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm11, %xmm0 # xmm0 = xmm11[0],xmm0[0],xmm11[2,3]
.LBB40_305:
	vmovlhps	%xmm0, %xmm1, %xmm0             # xmm0 = xmm1[0],xmm0[0]
	vmovups	%xmm0, 48(%rsp)
	leaq	352(%rsp), %r14
	movq	%r14, 336(%rsp)
	movq	$32, 16(%rsp)
.Ltmp5850:
	leaq	336(%rsp), %rdi
	leaq	16(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp5851:
# %bb.306:
	movq	%rax, 336(%rsp)
	movq	16(%rsp), %rcx
	movq	%rcx, 352(%rsp)
	vmovupd	.L.str.63(%rip), %ymm0
	vmovupd	%ymm0, (%rax)
	movq	%rcx, 344(%rsp)
	movq	336(%rsp), %rax
	movb	$0, (%rax,%rcx)
.Ltmp5853:
	leaq	336(%rsp), %rdi
	leaq	48(%rsp), %rsi
	xorl	%edx, %edx
	vzeroupper
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp5854:
# %bb.307:
	movq	336(%rsp), %rdi
	cmpq	%r14, %rdi
	je	.LBB40_309
# %bb.308:
	callq	_ZdlPv
.LBB40_309:
	addq	$536, %rsp                      # imm = 0x218
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB40_310:
	.cfi_def_cfa_offset 592
.Ltmp5849:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_311:
.Ltmp5843:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_312:
.Ltmp5800:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_313:
.Ltmp5694:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_314:
.Ltmp5688:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_315:
.Ltmp5645:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_316:
.Ltmp5536:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_317:
.Ltmp5533:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_318:
.Ltmp5530:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_319:
.Ltmp5527:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_320:
.Ltmp5472:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_321:
.Ltmp5469:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_322:
.Ltmp5466:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_323:
.Ltmp5463:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_324:
.Ltmp5408:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_325:
.Ltmp5405:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_326:
.Ltmp5402:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_327:
.Ltmp5399:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_328:
.Ltmp5855:
	movq	%rax, %rbx
	movq	336(%rsp), %rdi
	cmpq	%r14, %rdi
	jne	.LBB40_346
	jmp	.LBB40_347
.LBB40_329:
.Ltmp5852:
	movq	%rax, %rdi
	callq	_Unwind_Resume@PLT
.LBB40_330:
.Ltmp5846:
	movq	%rax, %rbx
	movq	376(%rsp), %rdi
	jmp	.LBB40_334
.LBB40_331:
.Ltmp5840:
	movq	%rax, %rbx
	jmp	.LBB40_406
.LBB40_332:
.Ltmp5784:
	movq	%rax, %rbx
	jmp	.LBB40_406
.LBB40_333:
.Ltmp5691:
	movq	%rax, %rbx
	movq	408(%rsp), %rdi
.LBB40_334:
	cmpq	%r15, %rdi
	je	.LBB40_412
# %bb.335:
	callq	_ZdlPv
	jmp	.LBB40_412
.LBB40_336:
.Ltmp5685:
	movq	%rax, %rbx
	jmp	.LBB40_406
.LBB40_337:
.Ltmp5629:
	movq	%rax, %rbx
	jmp	.LBB40_406
.LBB40_338:
.Ltmp5539:
	movq	%rax, %rbx
	movq	440(%rsp), %rdi
	jmp	.LBB40_345
.LBB40_339:
.Ltmp5524:
	jmp	.LBB40_350
.LBB40_340:
.Ltmp5521:
	jmp	.LBB40_350
.LBB40_341:
.Ltmp5475:
	movq	%rax, %rbx
	movq	472(%rsp), %rdi
	jmp	.LBB40_345
.LBB40_342:
.Ltmp5460:
	jmp	.LBB40_350
.LBB40_343:
.Ltmp5457:
	jmp	.LBB40_350
.LBB40_344:
.Ltmp5411:
	movq	%rax, %rbx
	movq	504(%rsp), %rdi
.LBB40_345:
	cmpq	%r12, %rdi
	je	.LBB40_347
.LBB40_346:
	callq	_ZdlPv
.LBB40_347:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB40_348:
.Ltmp5396:
	jmp	.LBB40_350
.LBB40_349:
.Ltmp5393:
.LBB40_350:
	movq	%rax, %rbx
	leaq	48(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB40_373
.LBB40_351:
.Ltmp5837:
	movq	%rax, %rbx
	jmp	.LBB40_406
.LBB40_352:
.Ltmp5682:
	movq	%rax, %rbx
	jmp	.LBB40_406
.LBB40_353:
.Ltmp5507:
	jmp	.LBB40_372
.LBB40_354:
.Ltmp5491:
	movq	%rax, %rbx
	jmp	.LBB40_374
.LBB40_355:
.Ltmp5443:
	jmp	.LBB40_372
.LBB40_356:
.Ltmp5427:
	movq	%rax, %rbx
	jmp	.LBB40_374
.LBB40_357:
.Ltmp5379:
	jmp	.LBB40_372
.LBB40_358:
.Ltmp5363:
	movq	%rax, %rbx
	jmp	.LBB40_374
.LBB40_359:
.Ltmp5797:
	movq	%rax, %rbx
	jmp	.LBB40_406
.LBB40_360:
.Ltmp5642:
	movq	%rax, %rbx
	jmp	.LBB40_406
.LBB40_361:
.Ltmp5832:
	movq	%rax, %rbx
	jmp	.LBB40_412
.LBB40_362:
.Ltmp5781:
	movq	%rax, %rbx
	jmp	.LBB40_412
.LBB40_363:
.Ltmp5677:
	movq	%rax, %rbx
	jmp	.LBB40_412
.LBB40_364:
.Ltmp5626:
	movq	%rax, %rbx
	jmp	.LBB40_412
.LBB40_365:
.Ltmp5518:
	jmp	.LBB40_372
.LBB40_366:
.Ltmp5502:
	movq	%rax, %rbx
	jmp	.LBB40_374
.LBB40_367:
.Ltmp5486:
	jmp	.LBB40_377
.LBB40_368:
.Ltmp5454:
	jmp	.LBB40_372
.LBB40_369:
.Ltmp5438:
	movq	%rax, %rbx
	jmp	.LBB40_374
.LBB40_370:
.Ltmp5422:
	jmp	.LBB40_377
.LBB40_371:
.Ltmp5390:
.LBB40_372:
	movq	%rax, %rbx
.LBB40_373:
	leaq	16(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB40_374:
	leaq	112(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	leaq	208(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB40_375:
.Ltmp5374:
	movq	%rax, %rbx
	jmp	.LBB40_374
.LBB40_376:
.Ltmp5358:
.LBB40_377:
	movq	%rax, %rbx
	leaq	208(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB40_378:
.Ltmp5821:
	movq	%rax, %rbx
	jmp	.LBB40_412
.LBB40_379:
.Ltmp5666:
	movq	%rax, %rbx
	jmp	.LBB40_412
.LBB40_380:
.Ltmp5770:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_381:
.Ltmp5767:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_382:
.Ltmp5764:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_383:
.Ltmp5716:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_384:
.Ltmp5615:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_385:
.Ltmp5612:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_386:
.Ltmp5609:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_387:
.Ltmp5561:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB40_388:
.Ltmp5697:
	movq	%rax, %rbx
	jmp	.LBB40_412
.LBB40_389:
.Ltmp5542:
	movq	%rax, %rbx
	jmp	.LBB40_412
.LBB40_390:
.Ltmp5748:
	jmp	.LBB40_399
.LBB40_391:
.Ltmp5732:
	jmp	.LBB40_404
.LBB40_392:
.Ltmp5713:
	jmp	.LBB40_396
.LBB40_393:
.Ltmp5593:
	jmp	.LBB40_399
.LBB40_394:
.Ltmp5577:
	jmp	.LBB40_404
.LBB40_395:
.Ltmp5558:
.LBB40_396:
	movq	%rax, %rbx
	leaq	16(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB40_410
.LBB40_397:
.Ltmp5761:
	jmp	.LBB40_399
.LBB40_398:
.Ltmp5606:
.LBB40_399:
	movq	%rax, %rbx
	leaq	112(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB40_405
.LBB40_400:
.Ltmp5743:
	jmp	.LBB40_404
.LBB40_401:
.Ltmp5727:
	movq	%rax, %rbx
	jmp	.LBB40_406
.LBB40_402:
.Ltmp5708:
	jmp	.LBB40_409
.LBB40_403:
.Ltmp5588:
.LBB40_404:
	movq	%rax, %rbx
.LBB40_405:
	leaq	208(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB40_406:
	leaq	16(%rsp), %rdi
	jmp	.LBB40_411
.LBB40_407:
.Ltmp5572:
	movq	%rax, %rbx
	jmp	.LBB40_406
.LBB40_408:
.Ltmp5553:
.LBB40_409:
	movq	%rax, %rbx
.LBB40_410:
	leaq	112(%rsp), %rdi
.LBB40_411:
	callq	_ZN4mpfr6mprealD2Ev
.LBB40_412:
	leaq	48(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end40:
	.size	_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_, .Lfunc_end40-_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.cfi_endproc
	.section	.gcc_except_table._Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"aG",@progbits,_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.p2align	2, 0x0
GCC_except_table40:
.Lexception33:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase22-.Lttbaseref22
.Lttbaseref22:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end33-.Lcst_begin33
.Lcst_begin33:
	.uleb128 .Lfunc_begin33-.Lfunc_begin33  # >> Call Site 1 <<
	.uleb128 .Ltmp5348-.Lfunc_begin33       #   Call between .Lfunc_begin33 and .Ltmp5348
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5348-.Lfunc_begin33       # >> Call Site 2 <<
	.uleb128 .Ltmp5357-.Ltmp5348            #   Call between .Ltmp5348 and .Ltmp5357
	.uleb128 .Ltmp5358-.Lfunc_begin33       #     jumps to .Ltmp5358
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5359-.Lfunc_begin33       # >> Call Site 3 <<
	.uleb128 .Ltmp5362-.Ltmp5359            #   Call between .Ltmp5359 and .Ltmp5362
	.uleb128 .Ltmp5363-.Lfunc_begin33       #     jumps to .Ltmp5363
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5364-.Lfunc_begin33       # >> Call Site 4 <<
	.uleb128 .Ltmp5373-.Ltmp5364            #   Call between .Ltmp5364 and .Ltmp5373
	.uleb128 .Ltmp5374-.Lfunc_begin33       #     jumps to .Ltmp5374
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5375-.Lfunc_begin33       # >> Call Site 5 <<
	.uleb128 .Ltmp5378-.Ltmp5375            #   Call between .Ltmp5375 and .Ltmp5378
	.uleb128 .Ltmp5379-.Lfunc_begin33       #     jumps to .Ltmp5379
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5380-.Lfunc_begin33       # >> Call Site 6 <<
	.uleb128 .Ltmp5389-.Ltmp5380            #   Call between .Ltmp5380 and .Ltmp5389
	.uleb128 .Ltmp5390-.Lfunc_begin33       #     jumps to .Ltmp5390
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5391-.Lfunc_begin33       # >> Call Site 7 <<
	.uleb128 .Ltmp5392-.Ltmp5391            #   Call between .Ltmp5391 and .Ltmp5392
	.uleb128 .Ltmp5393-.Lfunc_begin33       #     jumps to .Ltmp5393
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5394-.Lfunc_begin33       # >> Call Site 8 <<
	.uleb128 .Ltmp5395-.Ltmp5394            #   Call between .Ltmp5394 and .Ltmp5395
	.uleb128 .Ltmp5396-.Lfunc_begin33       #     jumps to .Ltmp5396
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5397-.Lfunc_begin33       # >> Call Site 9 <<
	.uleb128 .Ltmp5398-.Ltmp5397            #   Call between .Ltmp5397 and .Ltmp5398
	.uleb128 .Ltmp5399-.Lfunc_begin33       #     jumps to .Ltmp5399
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5400-.Lfunc_begin33       # >> Call Site 10 <<
	.uleb128 .Ltmp5401-.Ltmp5400            #   Call between .Ltmp5400 and .Ltmp5401
	.uleb128 .Ltmp5402-.Lfunc_begin33       #     jumps to .Ltmp5402
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5403-.Lfunc_begin33       # >> Call Site 11 <<
	.uleb128 .Ltmp5404-.Ltmp5403            #   Call between .Ltmp5403 and .Ltmp5404
	.uleb128 .Ltmp5405-.Lfunc_begin33       #     jumps to .Ltmp5405
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5406-.Lfunc_begin33       # >> Call Site 12 <<
	.uleb128 .Ltmp5407-.Ltmp5406            #   Call between .Ltmp5406 and .Ltmp5407
	.uleb128 .Ltmp5408-.Lfunc_begin33       #     jumps to .Ltmp5408
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5409-.Lfunc_begin33       # >> Call Site 13 <<
	.uleb128 .Ltmp5410-.Ltmp5409            #   Call between .Ltmp5409 and .Ltmp5410
	.uleb128 .Ltmp5411-.Lfunc_begin33       #     jumps to .Ltmp5411
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5410-.Lfunc_begin33       # >> Call Site 14 <<
	.uleb128 .Ltmp5412-.Ltmp5410            #   Call between .Ltmp5410 and .Ltmp5412
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5412-.Lfunc_begin33       # >> Call Site 15 <<
	.uleb128 .Ltmp5421-.Ltmp5412            #   Call between .Ltmp5412 and .Ltmp5421
	.uleb128 .Ltmp5422-.Lfunc_begin33       #     jumps to .Ltmp5422
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5423-.Lfunc_begin33       # >> Call Site 16 <<
	.uleb128 .Ltmp5426-.Ltmp5423            #   Call between .Ltmp5423 and .Ltmp5426
	.uleb128 .Ltmp5427-.Lfunc_begin33       #     jumps to .Ltmp5427
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5428-.Lfunc_begin33       # >> Call Site 17 <<
	.uleb128 .Ltmp5437-.Ltmp5428            #   Call between .Ltmp5428 and .Ltmp5437
	.uleb128 .Ltmp5438-.Lfunc_begin33       #     jumps to .Ltmp5438
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5439-.Lfunc_begin33       # >> Call Site 18 <<
	.uleb128 .Ltmp5442-.Ltmp5439            #   Call between .Ltmp5439 and .Ltmp5442
	.uleb128 .Ltmp5443-.Lfunc_begin33       #     jumps to .Ltmp5443
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5444-.Lfunc_begin33       # >> Call Site 19 <<
	.uleb128 .Ltmp5453-.Ltmp5444            #   Call between .Ltmp5444 and .Ltmp5453
	.uleb128 .Ltmp5454-.Lfunc_begin33       #     jumps to .Ltmp5454
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5455-.Lfunc_begin33       # >> Call Site 20 <<
	.uleb128 .Ltmp5456-.Ltmp5455            #   Call between .Ltmp5455 and .Ltmp5456
	.uleb128 .Ltmp5457-.Lfunc_begin33       #     jumps to .Ltmp5457
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5458-.Lfunc_begin33       # >> Call Site 21 <<
	.uleb128 .Ltmp5459-.Ltmp5458            #   Call between .Ltmp5458 and .Ltmp5459
	.uleb128 .Ltmp5460-.Lfunc_begin33       #     jumps to .Ltmp5460
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5461-.Lfunc_begin33       # >> Call Site 22 <<
	.uleb128 .Ltmp5462-.Ltmp5461            #   Call between .Ltmp5461 and .Ltmp5462
	.uleb128 .Ltmp5463-.Lfunc_begin33       #     jumps to .Ltmp5463
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5464-.Lfunc_begin33       # >> Call Site 23 <<
	.uleb128 .Ltmp5465-.Ltmp5464            #   Call between .Ltmp5464 and .Ltmp5465
	.uleb128 .Ltmp5466-.Lfunc_begin33       #     jumps to .Ltmp5466
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5467-.Lfunc_begin33       # >> Call Site 24 <<
	.uleb128 .Ltmp5468-.Ltmp5467            #   Call between .Ltmp5467 and .Ltmp5468
	.uleb128 .Ltmp5469-.Lfunc_begin33       #     jumps to .Ltmp5469
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5470-.Lfunc_begin33       # >> Call Site 25 <<
	.uleb128 .Ltmp5471-.Ltmp5470            #   Call between .Ltmp5470 and .Ltmp5471
	.uleb128 .Ltmp5472-.Lfunc_begin33       #     jumps to .Ltmp5472
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5473-.Lfunc_begin33       # >> Call Site 26 <<
	.uleb128 .Ltmp5474-.Ltmp5473            #   Call between .Ltmp5473 and .Ltmp5474
	.uleb128 .Ltmp5475-.Lfunc_begin33       #     jumps to .Ltmp5475
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5474-.Lfunc_begin33       # >> Call Site 27 <<
	.uleb128 .Ltmp5476-.Ltmp5474            #   Call between .Ltmp5474 and .Ltmp5476
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5476-.Lfunc_begin33       # >> Call Site 28 <<
	.uleb128 .Ltmp5485-.Ltmp5476            #   Call between .Ltmp5476 and .Ltmp5485
	.uleb128 .Ltmp5486-.Lfunc_begin33       #     jumps to .Ltmp5486
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5487-.Lfunc_begin33       # >> Call Site 29 <<
	.uleb128 .Ltmp5490-.Ltmp5487            #   Call between .Ltmp5487 and .Ltmp5490
	.uleb128 .Ltmp5491-.Lfunc_begin33       #     jumps to .Ltmp5491
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5492-.Lfunc_begin33       # >> Call Site 30 <<
	.uleb128 .Ltmp5501-.Ltmp5492            #   Call between .Ltmp5492 and .Ltmp5501
	.uleb128 .Ltmp5502-.Lfunc_begin33       #     jumps to .Ltmp5502
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5503-.Lfunc_begin33       # >> Call Site 31 <<
	.uleb128 .Ltmp5506-.Ltmp5503            #   Call between .Ltmp5503 and .Ltmp5506
	.uleb128 .Ltmp5507-.Lfunc_begin33       #     jumps to .Ltmp5507
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5508-.Lfunc_begin33       # >> Call Site 32 <<
	.uleb128 .Ltmp5517-.Ltmp5508            #   Call between .Ltmp5508 and .Ltmp5517
	.uleb128 .Ltmp5518-.Lfunc_begin33       #     jumps to .Ltmp5518
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5519-.Lfunc_begin33       # >> Call Site 33 <<
	.uleb128 .Ltmp5520-.Ltmp5519            #   Call between .Ltmp5519 and .Ltmp5520
	.uleb128 .Ltmp5521-.Lfunc_begin33       #     jumps to .Ltmp5521
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5522-.Lfunc_begin33       # >> Call Site 34 <<
	.uleb128 .Ltmp5523-.Ltmp5522            #   Call between .Ltmp5522 and .Ltmp5523
	.uleb128 .Ltmp5524-.Lfunc_begin33       #     jumps to .Ltmp5524
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5525-.Lfunc_begin33       # >> Call Site 35 <<
	.uleb128 .Ltmp5526-.Ltmp5525            #   Call between .Ltmp5525 and .Ltmp5526
	.uleb128 .Ltmp5527-.Lfunc_begin33       #     jumps to .Ltmp5527
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5528-.Lfunc_begin33       # >> Call Site 36 <<
	.uleb128 .Ltmp5529-.Ltmp5528            #   Call between .Ltmp5528 and .Ltmp5529
	.uleb128 .Ltmp5530-.Lfunc_begin33       #     jumps to .Ltmp5530
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5531-.Lfunc_begin33       # >> Call Site 37 <<
	.uleb128 .Ltmp5532-.Ltmp5531            #   Call between .Ltmp5531 and .Ltmp5532
	.uleb128 .Ltmp5533-.Lfunc_begin33       #     jumps to .Ltmp5533
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5534-.Lfunc_begin33       # >> Call Site 38 <<
	.uleb128 .Ltmp5535-.Ltmp5534            #   Call between .Ltmp5534 and .Ltmp5535
	.uleb128 .Ltmp5536-.Lfunc_begin33       #     jumps to .Ltmp5536
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5537-.Lfunc_begin33       # >> Call Site 39 <<
	.uleb128 .Ltmp5538-.Ltmp5537            #   Call between .Ltmp5537 and .Ltmp5538
	.uleb128 .Ltmp5539-.Lfunc_begin33       #     jumps to .Ltmp5539
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5538-.Lfunc_begin33       # >> Call Site 40 <<
	.uleb128 .Ltmp5540-.Ltmp5538            #   Call between .Ltmp5538 and .Ltmp5540
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5540-.Lfunc_begin33       # >> Call Site 41 <<
	.uleb128 .Ltmp5541-.Ltmp5540            #   Call between .Ltmp5540 and .Ltmp5541
	.uleb128 .Ltmp5542-.Lfunc_begin33       #     jumps to .Ltmp5542
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5543-.Lfunc_begin33       # >> Call Site 42 <<
	.uleb128 .Ltmp5552-.Ltmp5543            #   Call between .Ltmp5543 and .Ltmp5552
	.uleb128 .Ltmp5553-.Lfunc_begin33       #     jumps to .Ltmp5553
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5554-.Lfunc_begin33       # >> Call Site 43 <<
	.uleb128 .Ltmp5557-.Ltmp5554            #   Call between .Ltmp5554 and .Ltmp5557
	.uleb128 .Ltmp5558-.Lfunc_begin33       #     jumps to .Ltmp5558
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5559-.Lfunc_begin33       # >> Call Site 44 <<
	.uleb128 .Ltmp5560-.Ltmp5559            #   Call between .Ltmp5559 and .Ltmp5560
	.uleb128 .Ltmp5561-.Lfunc_begin33       #     jumps to .Ltmp5561
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5562-.Lfunc_begin33       # >> Call Site 45 <<
	.uleb128 .Ltmp5571-.Ltmp5562            #   Call between .Ltmp5562 and .Ltmp5571
	.uleb128 .Ltmp5572-.Lfunc_begin33       #     jumps to .Ltmp5572
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5573-.Lfunc_begin33       # >> Call Site 46 <<
	.uleb128 .Ltmp5576-.Ltmp5573            #   Call between .Ltmp5573 and .Ltmp5576
	.uleb128 .Ltmp5577-.Lfunc_begin33       #     jumps to .Ltmp5577
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5578-.Lfunc_begin33       # >> Call Site 47 <<
	.uleb128 .Ltmp5587-.Ltmp5578            #   Call between .Ltmp5578 and .Ltmp5587
	.uleb128 .Ltmp5588-.Lfunc_begin33       #     jumps to .Ltmp5588
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5589-.Lfunc_begin33       # >> Call Site 48 <<
	.uleb128 .Ltmp5592-.Ltmp5589            #   Call between .Ltmp5589 and .Ltmp5592
	.uleb128 .Ltmp5593-.Lfunc_begin33       #     jumps to .Ltmp5593
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5594-.Lfunc_begin33       # >> Call Site 49 <<
	.uleb128 .Ltmp5605-.Ltmp5594            #   Call between .Ltmp5594 and .Ltmp5605
	.uleb128 .Ltmp5606-.Lfunc_begin33       #     jumps to .Ltmp5606
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5607-.Lfunc_begin33       # >> Call Site 50 <<
	.uleb128 .Ltmp5608-.Ltmp5607            #   Call between .Ltmp5607 and .Ltmp5608
	.uleb128 .Ltmp5609-.Lfunc_begin33       #     jumps to .Ltmp5609
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5610-.Lfunc_begin33       # >> Call Site 51 <<
	.uleb128 .Ltmp5611-.Ltmp5610            #   Call between .Ltmp5610 and .Ltmp5611
	.uleb128 .Ltmp5612-.Lfunc_begin33       #     jumps to .Ltmp5612
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5613-.Lfunc_begin33       # >> Call Site 52 <<
	.uleb128 .Ltmp5614-.Ltmp5613            #   Call between .Ltmp5613 and .Ltmp5614
	.uleb128 .Ltmp5615-.Lfunc_begin33       #     jumps to .Ltmp5615
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5616-.Lfunc_begin33       # >> Call Site 53 <<
	.uleb128 .Ltmp5625-.Ltmp5616            #   Call between .Ltmp5616 and .Ltmp5625
	.uleb128 .Ltmp5626-.Lfunc_begin33       #     jumps to .Ltmp5626
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5627-.Lfunc_begin33       # >> Call Site 54 <<
	.uleb128 .Ltmp5628-.Ltmp5627            #   Call between .Ltmp5627 and .Ltmp5628
	.uleb128 .Ltmp5629-.Lfunc_begin33       #     jumps to .Ltmp5629
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5630-.Lfunc_begin33       # >> Call Site 55 <<
	.uleb128 .Ltmp5641-.Ltmp5630            #   Call between .Ltmp5630 and .Ltmp5641
	.uleb128 .Ltmp5642-.Lfunc_begin33       #     jumps to .Ltmp5642
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5643-.Lfunc_begin33       # >> Call Site 56 <<
	.uleb128 .Ltmp5644-.Ltmp5643            #   Call between .Ltmp5643 and .Ltmp5644
	.uleb128 .Ltmp5645-.Lfunc_begin33       #     jumps to .Ltmp5645
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5646-.Lfunc_begin33       # >> Call Site 57 <<
	.uleb128 .Ltmp5665-.Ltmp5646            #   Call between .Ltmp5646 and .Ltmp5665
	.uleb128 .Ltmp5666-.Lfunc_begin33       #     jumps to .Ltmp5666
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5667-.Lfunc_begin33       # >> Call Site 58 <<
	.uleb128 .Ltmp5676-.Ltmp5667            #   Call between .Ltmp5667 and .Ltmp5676
	.uleb128 .Ltmp5677-.Lfunc_begin33       #     jumps to .Ltmp5677
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5678-.Lfunc_begin33       # >> Call Site 59 <<
	.uleb128 .Ltmp5681-.Ltmp5678            #   Call between .Ltmp5678 and .Ltmp5681
	.uleb128 .Ltmp5682-.Lfunc_begin33       #     jumps to .Ltmp5682
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5683-.Lfunc_begin33       # >> Call Site 60 <<
	.uleb128 .Ltmp5684-.Ltmp5683            #   Call between .Ltmp5683 and .Ltmp5684
	.uleb128 .Ltmp5685-.Lfunc_begin33       #     jumps to .Ltmp5685
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5686-.Lfunc_begin33       # >> Call Site 61 <<
	.uleb128 .Ltmp5687-.Ltmp5686            #   Call between .Ltmp5686 and .Ltmp5687
	.uleb128 .Ltmp5688-.Lfunc_begin33       #     jumps to .Ltmp5688
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5689-.Lfunc_begin33       # >> Call Site 62 <<
	.uleb128 .Ltmp5690-.Ltmp5689            #   Call between .Ltmp5689 and .Ltmp5690
	.uleb128 .Ltmp5691-.Lfunc_begin33       #     jumps to .Ltmp5691
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5692-.Lfunc_begin33       # >> Call Site 63 <<
	.uleb128 .Ltmp5693-.Ltmp5692            #   Call between .Ltmp5692 and .Ltmp5693
	.uleb128 .Ltmp5694-.Lfunc_begin33       #     jumps to .Ltmp5694
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5693-.Lfunc_begin33       # >> Call Site 64 <<
	.uleb128 .Ltmp5695-.Ltmp5693            #   Call between .Ltmp5693 and .Ltmp5695
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5695-.Lfunc_begin33       # >> Call Site 65 <<
	.uleb128 .Ltmp5696-.Ltmp5695            #   Call between .Ltmp5695 and .Ltmp5696
	.uleb128 .Ltmp5697-.Lfunc_begin33       #     jumps to .Ltmp5697
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5698-.Lfunc_begin33       # >> Call Site 66 <<
	.uleb128 .Ltmp5707-.Ltmp5698            #   Call between .Ltmp5698 and .Ltmp5707
	.uleb128 .Ltmp5708-.Lfunc_begin33       #     jumps to .Ltmp5708
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5709-.Lfunc_begin33       # >> Call Site 67 <<
	.uleb128 .Ltmp5712-.Ltmp5709            #   Call between .Ltmp5709 and .Ltmp5712
	.uleb128 .Ltmp5713-.Lfunc_begin33       #     jumps to .Ltmp5713
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5714-.Lfunc_begin33       # >> Call Site 68 <<
	.uleb128 .Ltmp5715-.Ltmp5714            #   Call between .Ltmp5714 and .Ltmp5715
	.uleb128 .Ltmp5716-.Lfunc_begin33       #     jumps to .Ltmp5716
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5717-.Lfunc_begin33       # >> Call Site 69 <<
	.uleb128 .Ltmp5726-.Ltmp5717            #   Call between .Ltmp5717 and .Ltmp5726
	.uleb128 .Ltmp5727-.Lfunc_begin33       #     jumps to .Ltmp5727
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5728-.Lfunc_begin33       # >> Call Site 70 <<
	.uleb128 .Ltmp5731-.Ltmp5728            #   Call between .Ltmp5728 and .Ltmp5731
	.uleb128 .Ltmp5732-.Lfunc_begin33       #     jumps to .Ltmp5732
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5733-.Lfunc_begin33       # >> Call Site 71 <<
	.uleb128 .Ltmp5742-.Ltmp5733            #   Call between .Ltmp5733 and .Ltmp5742
	.uleb128 .Ltmp5743-.Lfunc_begin33       #     jumps to .Ltmp5743
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5744-.Lfunc_begin33       # >> Call Site 72 <<
	.uleb128 .Ltmp5747-.Ltmp5744            #   Call between .Ltmp5744 and .Ltmp5747
	.uleb128 .Ltmp5748-.Lfunc_begin33       #     jumps to .Ltmp5748
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5749-.Lfunc_begin33       # >> Call Site 73 <<
	.uleb128 .Ltmp5760-.Ltmp5749            #   Call between .Ltmp5749 and .Ltmp5760
	.uleb128 .Ltmp5761-.Lfunc_begin33       #     jumps to .Ltmp5761
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5762-.Lfunc_begin33       # >> Call Site 74 <<
	.uleb128 .Ltmp5763-.Ltmp5762            #   Call between .Ltmp5762 and .Ltmp5763
	.uleb128 .Ltmp5764-.Lfunc_begin33       #     jumps to .Ltmp5764
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5765-.Lfunc_begin33       # >> Call Site 75 <<
	.uleb128 .Ltmp5766-.Ltmp5765            #   Call between .Ltmp5765 and .Ltmp5766
	.uleb128 .Ltmp5767-.Lfunc_begin33       #     jumps to .Ltmp5767
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5768-.Lfunc_begin33       # >> Call Site 76 <<
	.uleb128 .Ltmp5769-.Ltmp5768            #   Call between .Ltmp5768 and .Ltmp5769
	.uleb128 .Ltmp5770-.Lfunc_begin33       #     jumps to .Ltmp5770
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5771-.Lfunc_begin33       # >> Call Site 77 <<
	.uleb128 .Ltmp5780-.Ltmp5771            #   Call between .Ltmp5771 and .Ltmp5780
	.uleb128 .Ltmp5781-.Lfunc_begin33       #     jumps to .Ltmp5781
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5782-.Lfunc_begin33       # >> Call Site 78 <<
	.uleb128 .Ltmp5783-.Ltmp5782            #   Call between .Ltmp5782 and .Ltmp5783
	.uleb128 .Ltmp5784-.Lfunc_begin33       #     jumps to .Ltmp5784
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5785-.Lfunc_begin33       # >> Call Site 79 <<
	.uleb128 .Ltmp5796-.Ltmp5785            #   Call between .Ltmp5785 and .Ltmp5796
	.uleb128 .Ltmp5797-.Lfunc_begin33       #     jumps to .Ltmp5797
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5798-.Lfunc_begin33       # >> Call Site 80 <<
	.uleb128 .Ltmp5799-.Ltmp5798            #   Call between .Ltmp5798 and .Ltmp5799
	.uleb128 .Ltmp5800-.Lfunc_begin33       #     jumps to .Ltmp5800
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5801-.Lfunc_begin33       # >> Call Site 81 <<
	.uleb128 .Ltmp5820-.Ltmp5801            #   Call between .Ltmp5801 and .Ltmp5820
	.uleb128 .Ltmp5821-.Lfunc_begin33       #     jumps to .Ltmp5821
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5822-.Lfunc_begin33       # >> Call Site 82 <<
	.uleb128 .Ltmp5831-.Ltmp5822            #   Call between .Ltmp5822 and .Ltmp5831
	.uleb128 .Ltmp5832-.Lfunc_begin33       #     jumps to .Ltmp5832
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5833-.Lfunc_begin33       # >> Call Site 83 <<
	.uleb128 .Ltmp5836-.Ltmp5833            #   Call between .Ltmp5833 and .Ltmp5836
	.uleb128 .Ltmp5837-.Lfunc_begin33       #     jumps to .Ltmp5837
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5838-.Lfunc_begin33       # >> Call Site 84 <<
	.uleb128 .Ltmp5839-.Ltmp5838            #   Call between .Ltmp5838 and .Ltmp5839
	.uleb128 .Ltmp5840-.Lfunc_begin33       #     jumps to .Ltmp5840
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5841-.Lfunc_begin33       # >> Call Site 85 <<
	.uleb128 .Ltmp5842-.Ltmp5841            #   Call between .Ltmp5841 and .Ltmp5842
	.uleb128 .Ltmp5843-.Lfunc_begin33       #     jumps to .Ltmp5843
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5844-.Lfunc_begin33       # >> Call Site 86 <<
	.uleb128 .Ltmp5845-.Ltmp5844            #   Call between .Ltmp5844 and .Ltmp5845
	.uleb128 .Ltmp5846-.Lfunc_begin33       #     jumps to .Ltmp5846
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5847-.Lfunc_begin33       # >> Call Site 87 <<
	.uleb128 .Ltmp5848-.Ltmp5847            #   Call between .Ltmp5847 and .Ltmp5848
	.uleb128 .Ltmp5849-.Lfunc_begin33       #     jumps to .Ltmp5849
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5848-.Lfunc_begin33       # >> Call Site 88 <<
	.uleb128 .Ltmp5850-.Ltmp5848            #   Call between .Ltmp5848 and .Ltmp5850
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5850-.Lfunc_begin33       # >> Call Site 89 <<
	.uleb128 .Ltmp5851-.Ltmp5850            #   Call between .Ltmp5850 and .Ltmp5851
	.uleb128 .Ltmp5852-.Lfunc_begin33       #     jumps to .Ltmp5852
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5853-.Lfunc_begin33       # >> Call Site 90 <<
	.uleb128 .Ltmp5854-.Ltmp5853            #   Call between .Ltmp5853 and .Ltmp5854
	.uleb128 .Ltmp5855-.Lfunc_begin33       #     jumps to .Ltmp5855
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5854-.Lfunc_begin33       # >> Call Site 91 <<
	.uleb128 .Lfunc_end40-.Ltmp5854         #   Call between .Ltmp5854 and .Lfunc_end40
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end33:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase22:
	.p2align	2, 0x0
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2, 0x0                          # -- Begin function _Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.LCPI41_0:
	.long	0x7fc00000                      #  NaN
.LCPI41_1:
	.long	0x3f800000                      #  1
.LCPI41_2:
	.long	0x3f000000                      #  0.5
.LCPI41_3:
	.long	0x80000000                      #  -0
.LCPI41_4:
	.long	0x7f800000                      #  +Inf
.LCPI41_5:
	.long	0xff800000                      #  -Inf
.LCPI41_6:
	.long	0x00000000                      #  0
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4, 0x0
.LCPI41_7:
	.zero	16
	.section	.text._Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"axG",@progbits,_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.weak	_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.p2align	4, 0x90
	.type	_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,@function
_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_: # 
.Lfunc_begin34:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception34
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$568, %rsp                      # imm = 0x238
	.cfi_def_cfa_offset 624
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, 8(%rsp)                    # 8-byte Spill
	movq	%rcx, 232(%rsp)                 # 8-byte Spill
	movq	%rdx, %r12
	movq	%rsi, %rbp
	movq	%rdi, %rbx
	vcvtsi2sdl	(%rdi), %xmm0, %xmm0
	callq	sqrt
	vcvttsd2si	%xmm0, %eax
	movl	%eax, 4(%rsp)
	movq	%rbx, 192(%rsp)                 # 8-byte Spill
	movslq	(%rbx), %r15
	movq	%r15, %r13
	shlq	$4, %r13
	testq	%r15, %r15
	movq	$-1, %r14
	cmovnsq	%r13, %r14
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %rbx
	movq	%rax, 152(%rsp)                 # 8-byte Spill
	testq	%r15, %r15
	je	.LBB41_5
# %bb.1:
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movq	%r13, %rdx
	callq	_intel_fast_memset@PLT
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%r13, %rdx
	callq	_intel_fast_memset@PLT
	testl	%r15d, %r15d
	jle	.LBB41_6
# %bb.2:
	movq	%rbp, 96(%rsp)                  # 8-byte Spill
	movl	$8, %ebp
	xorl	%r15d, %r15d
	movq	232(%rsp), %r13                 # 8-byte Reload
	movq	16(%rsp), %r14                  # 8-byte Reload
	.p2align	4, 0x90
.LBB41_3:                               # =>This Inner Loop Header: Depth=1
	movq	%r12, %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, -8(%rbx,%rbp)
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, -8(%r14,%rbp)
	incq	%r15
	movq	192(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$16, %rbp
	addq	$32, %r13
	addq	$32, %r12
	cmpq	%rax, %r15
	jl	.LBB41_3
# %bb.4:
	movq	96(%rsp), %rbp                  # 8-byte Reload
	jmp	.LBB41_6
.LBB41_5:
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 16(%rsp)                  # 8-byte Spill
.LBB41_6:
	movq	%rbp, %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 216(%rsp)
	movl	$160, %edi
	callq	_Znam
	movq	%rax, %r15
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rax)
	vmovupd	%xmm0, 16(%rax)
	vmovupd	%xmm0, 32(%rax)
	vmovupd	%xmm0, 48(%rax)
	vmovupd	%xmm0, 64(%rax)
	vmovupd	%xmm0, 80(%rax)
	vmovupd	%xmm0, 96(%rax)
	vmovupd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 128(%rax)
	vmovupd	%xmm0, 144(%rax)
	callq	omp_get_wtime
	vmovsd	%xmm0, 56(%rsp)                 # 8-byte Spill
	movq	192(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %ecx
	xorl	%eax, %eax
	xorl	%edx, %edx
	movq	16(%rsp), %r9                   # 8-byte Reload
	jmp	.LBB41_9
	.p2align	4, 0x90
.LBB41_7:                               #   in Loop: Header=BB41_9 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
.LBB41_8:                               #   in Loop: Header=BB41_9 Depth=1
	vmovlhps	%xmm0, %xmm1, %xmm0             # xmm0 = xmm1[0],xmm0[0]
	movq	%rdx, %rsi
	shlq	$4, %rsi
	vmovups	%xmm0, (%r15,%rsi)
	incq	%rdx
	cmpq	$10, %rdx
	je	.LBB41_12
.LBB41_9:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB41_11 Depth 2
	vcvtsi2ss	%eax, %xmm13, %xmm0
	vblendps	$14, .LCPI41_7(%rip), %xmm0, %xmm1 # xmm1 = xmm0[0],mem[1,2,3]
	testl	%ecx, %ecx
	jle	.LBB41_7
# %bb.10:                               #   in Loop: Header=BB41_9 Depth=1
	movq	192(%rsp), %rcx                 # 8-byte Reload
	movl	(%rcx), %ecx
	movslq	%ecx, %rsi
	vxorps	%xmm0, %xmm0, %xmm0
	movl	$12, %edi
	xorl	%r8d, %r8d
	.p2align	4, 0x90
.LBB41_11:                              #   Parent Loop BB41_9 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	%xmm0, 160(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovss	-12(%r9,%rdi), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmovss	-8(%r9,%rdi), %xmm6             # xmm6 = mem[0],zero,zero,zero
	vmovss	-12(%rbx,%rdi), %xmm8           # xmm8 = mem[0],zero,zero,zero
	vmovss	-8(%rbx,%rdi), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm8, %xmm2
	vmovss	%xmm2, 96(%rsp)                 # 4-byte Spill
	vmulss	%xmm4, %xmm7, %xmm11
	vmovaps	%xmm4, %xmm3
	vmovaps	%xmm4, %xmm9
	vmulss	%xmm6, %xmm8, %xmm14
	vmovaps	%xmm6, %xmm5
	vfmsub213ss	%xmm11, %xmm7, %xmm3    # xmm3 = (xmm7 * xmm3) - xmm11
	vmovss	%xmm3, 240(%rsp)                # 4-byte Spill
	vfmsub213ss	%xmm2, %xmm8, %xmm9     # xmm9 = (xmm8 * xmm9) - xmm2
	vaddss	%xmm11, %xmm14, %xmm10
	vsubss	%xmm14, %xmm10, %xmm12
	vsubss	%xmm12, %xmm10, %xmm15
	vfmsub213ss	%xmm14, %xmm8, %xmm5    # xmm5 = (xmm8 * xmm5) - xmm14
	vsubss	%xmm12, %xmm11, %xmm12
	vmovss	-4(%r9,%rdi), %xmm13            # xmm13 = mem[0],zero,zero,zero
	vmulss	%xmm13, %xmm8, %xmm11
	vsubss	%xmm15, %xmm14, %xmm14
	vmulss	(%r9,%rdi), %xmm8, %xmm15
	vfmadd231ss	%xmm13, %xmm7, %xmm15   # xmm15 = (xmm7 * xmm13) + xmm15
	vaddss	%xmm12, %xmm14, %xmm12
	vfmsub213ss	%xmm11, %xmm8, %xmm13   # xmm13 = (xmm8 * xmm13) - xmm11
	vmovss	-4(%rbx,%rdi), %xmm8            # xmm8 = mem[0],zero,zero,zero
	vmulss	%xmm6, %xmm7, %xmm14
	vfmadd231ss	%xmm6, %xmm8, %xmm15    # xmm15 = (xmm8 * xmm6) + xmm15
	vfmsub213ss	%xmm14, %xmm7, %xmm6    # xmm6 = (xmm7 * xmm6) - xmm14
	vaddss	%xmm9, %xmm10, %xmm7
	vsubss	%xmm10, %xmm7, %xmm0
	vsubss	%xmm0, %xmm9, %xmm9
	vsubss	%xmm0, %xmm7, %xmm0
	vsubss	%xmm0, %xmm10, %xmm3
	vfmadd231ss	(%rbx,%rdi), %xmm4, %xmm15 # xmm15 = (xmm4 * mem) + xmm15
	vmulss	%xmm4, %xmm8, %xmm10
	vfmsub213ss	%xmm10, %xmm8, %xmm4    # xmm4 = (xmm8 * xmm4) - xmm10
	vaddss	%xmm14, %xmm11, %xmm8
	vmovaps	%xmm1, %xmm0
	vsubss	%xmm11, %xmm8, %xmm1
	vsubss	%xmm1, %xmm8, %xmm2
	vsubss	%xmm2, %xmm11, %xmm2
	vsubss	%xmm1, %xmm14, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm10, %xmm8, %xmm2
	vsubss	%xmm8, %xmm2, %xmm11
	vsubss	%xmm11, %xmm2, %xmm14
	vsubss	%xmm14, %xmm8, %xmm8
	vaddss	%xmm3, %xmm9, %xmm14
	vsubss	%xmm11, %xmm10, %xmm9
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm5, %xmm2, %xmm9
	vsubss	%xmm2, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm2, %xmm2
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vmovss	240(%rsp), %xmm3                # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vaddss	%xmm3, %xmm9, %xmm5
	vsubss	%xmm9, %xmm5, %xmm10
	vsubss	%xmm10, %xmm5, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vsubss	%xmm10, %xmm3, %xmm3
	vaddss	%xmm13, %xmm15, %xmm10
	vaddss	%xmm5, %xmm12, %xmm11
	vaddss	%xmm3, %xmm9, %xmm3
	vsubss	%xmm5, %xmm11, %xmm9
	vsubss	%xmm9, %xmm12, %xmm12
	vsubss	%xmm9, %xmm11, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vaddss	%xmm6, %xmm10, %xmm6
	vaddss	%xmm4, %xmm6, %xmm4
	vaddss	%xmm14, %xmm11, %xmm6
	vaddss	%xmm5, %xmm12, %xmm5
	vsubss	%xmm11, %xmm6, %xmm9
	vsubss	%xmm9, %xmm14, %xmm10
	vsubss	%xmm9, %xmm6, %xmm9
	vsubss	%xmm9, %xmm11, %xmm9
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm10, %xmm9, %xmm2
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm11
	vaddss	%xmm6, %xmm11, %xmm1
	vaddss	%xmm1, %xmm7, %xmm3
	vmovss	96(%rsp), %xmm2                 # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm5
	vaddss	%xmm4, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm8
	vsubss	%xmm8, %xmm2, %xmm9
	vsubss	%xmm9, %xmm0, %xmm9
	vsubss	%xmm8, %xmm4, %xmm4
	vaddss	%xmm4, %xmm9, %xmm4
	vmovshdup	%xmm0, %xmm8            # xmm8 = xmm0[1,1,3,3]
	vsubss	%xmm3, %xmm7, %xmm3
	vaddss	%xmm5, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm10
	vsubss	%xmm10, %xmm8, %xmm8
	vsubss	%xmm9, %xmm5, %xmm5
	vaddss	%xmm4, %xmm7, %xmm9
	vsubss	%xmm7, %xmm9, %xmm10
	vaddss	%xmm5, %xmm8, %xmm5
	vsubss	%xmm10, %xmm9, %xmm8
	vsubss	%xmm8, %xmm7, %xmm7
	vaddss	%xmm1, %xmm3, %xmm3
	vsubss	%xmm1, %xmm6, %xmm1
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vmovups	160(%rsp), %xmm8                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm3, %xmm8, %xmm6
	vaddss	%xmm1, %xmm11, %xmm0
	vsubss	%xmm8, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm7
	vsubss	%xmm1, %xmm3, %xmm1
	vsubss	%xmm7, %xmm8, %xmm3
	vmovaps	%xmm8, %xmm10
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm5, %xmm6, %xmm3
	vsubss	%xmm6, %xmm3, %xmm7
	vsubss	%xmm7, %xmm3, %xmm8
	vsubss	%xmm8, %xmm6, %xmm6
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm4, %xmm3, %xmm7
	vaddss	%xmm5, %xmm6, %xmm5
	vsubss	%xmm3, %xmm7, %xmm6
	vsubss	%xmm6, %xmm4, %xmm4
	vsubss	%xmm6, %xmm7, %xmm6
	vsubss	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm10, %xmm6           # xmm6 = xmm10[1,1,3,3]
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm4, %xmm3, %xmm1
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm1, %xmm9, %xmm3
	vsubss	%xmm3, %xmm9, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	vinsertps	$16, %xmm2, %xmm4, %xmm1 # xmm1 = xmm4[0],xmm2[0],xmm4[2,3]
	incq	%r8
	addq	$16, %rdi
	cmpq	%rsi, %r8
	jl	.LBB41_11
	jmp	.LBB41_8
.LBB41_12:
	callq	omp_get_wtime
	vsubsd	56(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm13, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	264(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp5856:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp5857:
# %bb.13:
.Ltmp5858:
	movq	%rax, %r13
	movq	8(%rsp), %rdi                   # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp5859:
# %bb.14:
.Ltmp5860:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp5861:
# %bb.15:
	movl	%eax, %r12d
	cmpq	%rbp, %r13
	cmovgq	%r13, %rbp
.Ltmp5862:
	leaq	120(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp5863:
# %bb.16:
.Ltmp5864:
	leaq	120(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5865:
# %bb.17:
.Ltmp5867:
	callq	mpfr_get_default_rounding_mode
.Ltmp5868:
# %bb.18:
.Ltmp5869:
	leaq	120(%rsp), %rdi
	leaq	264(%rsp), %rsi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp5870:
# %bb.19:
.Ltmp5872:
	leaq	120(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5873:
# %bb.20:
.Ltmp5874:
	movq	%rax, %r13
	movq	8(%rsp), %rdi                   # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp5875:
# %bb.21:
.Ltmp5876:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp5877:
# %bb.22:
	movl	%eax, %r12d
	cmpq	%rbp, %r13
	cmovgq	%r13, %rbp
.Ltmp5878:
	leaq	24(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp5879:
# %bb.23:
.Ltmp5880:
	leaq	24(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5881:
# %bb.24:
.Ltmp5883:
	callq	mpfr_get_default_rounding_mode
.Ltmp5884:
# %bb.25:
.Ltmp5885:
	leaq	24(%rsp), %rdi
	leaq	120(%rsp), %rsi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp5886:
# %bb.26:
.Ltmp5888:
	callq	mpfr_get_default_rounding_mode
.Ltmp5889:
# %bb.27:
.Ltmp5890:
	movl	%eax, %r12d
	leaq	24(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5891:
# %bb.28:
.Ltmp5892:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp5893:
# %bb.29:
.Ltmp5894:
	movl	%eax, %r13d
	leaq	64(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp5895:
# %bb.30:
.Ltmp5896:
	leaq	64(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r13d, %edx
	callq	mpfr_set_si
.Ltmp5897:
# %bb.31:
.Ltmp5899:
	leaq	64(%rsp), %rdi
	leaq	24(%rsp), %rsi
	movl	%r12d, %edx
	callq	mpfr_abs
.Ltmp5900:
# %bb.32:
.Ltmp5902:
	leaq	64(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp5903:
# %bb.33:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 336(%rsp)
	cmpq	$0, 88(%rsp)
	je	.LBB41_35
# %bb.34:
.Ltmp5905:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5906:
.LBB41_35:
	cmpq	$0, 48(%rsp)
	je	.LBB41_37
# %bb.36:
.Ltmp5908:
	leaq	24(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5909:
.LBB41_37:
	cmpq	$0, 144(%rsp)
	je	.LBB41_39
# %bb.38:
.Ltmp5911:
	leaq	120(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5912:
.LBB41_39:
	cmpq	$0, 288(%rsp)
	je	.LBB41_41
# %bb.40:
.Ltmp5914:
	leaq	264(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5915:
.LBB41_41:
	leaq	528(%rsp), %r12
	movq	%r12, 512(%rsp)
	movl	$544501604, 528(%rsp)           # imm = 0x20746F64
	movw	$32, 532(%rsp)
	movq	$5, 520(%rsp)
.Ltmp5917:
	leaq	512(%rsp), %rdi
	leaq	336(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp5918:
# %bb.42:
	movq	512(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB41_44
# %bb.43:
	callq	_ZdlPv
.LBB41_44:
	movq	%r15, %rdi
	callq	_ZdaPv
	movl	$160, %edi
	callq	_Znam
	movq	%rax, %r15
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rax)
	vmovupd	%xmm0, 16(%rax)
	vmovupd	%xmm0, 32(%rax)
	vmovupd	%xmm0, 48(%rax)
	vmovupd	%xmm0, 64(%rax)
	vmovupd	%xmm0, 80(%rax)
	vmovupd	%xmm0, 96(%rax)
	vmovupd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 128(%rax)
	vmovupd	%xmm0, 144(%rax)
	callq	omp_get_wtime
	vmovsd	%xmm0, 296(%rsp)                # 8-byte Spill
	movq	152(%rsp), %r13                 # 8-byte Reload
	addq	$12, %r13
	xorl	%r12d, %r12d
	xorl	%ebp, %ebp
	jmp	.LBB41_47
	.p2align	4, 0x90
.LBB41_45:                              #   in Loop: Header=BB41_47 Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
	vblendps	$2, %xmm1, %xmm0, %xmm3         # xmm3 = xmm0[0],xmm1[1],xmm0[2,3]
.LBB41_46:                              #   in Loop: Header=BB41_47 Depth=1
	vmovlhps	%xmm1, %xmm3, %xmm0             # xmm0 = xmm3[0],xmm1[0]
	movq	%rbp, %rax
	shlq	$4, %rax
	vmovups	%xmm0, (%r15,%rax)
	incq	%rbp
	cmpq	$10, %rbp
	je	.LBB41_59
.LBB41_47:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB41_49 Depth 2
	vcvtsi2ss	%r12d, %xmm13, %xmm0
	vblendps	$14, .LCPI41_7(%rip), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[1,2,3]
	movq	192(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	vxorps	%xmm2, %xmm2, %xmm2
	testq	%rax, %rax
	jle	.LBB41_50
# %bb.48:                               #   in Loop: Header=BB41_47 Depth=1
	movq	%r13, %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB41_49:                              #   Parent Loop BB41_47 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	%xmm0, 240(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm2, 96(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovss	-12(%rcx), %xmm0                # xmm0 = mem[0],zero,zero,zero
	vmovss	-8(%rcx), %xmm2                 # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm0, %xmm3
	vmulss	%xmm0, %xmm2, %xmm4
	vmovaps	%xmm0, %xmm1
	vmovss	-4(%rcx), %xmm5                 # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm2, %xmm6
	vfmsub213ss	%xmm4, %xmm2, %xmm1     # xmm1 = (xmm2 * xmm1) - xmm4
	vmovss	(%rcx), %xmm7                   # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm7, %xmm0, %xmm8
	vfmadd231ss	%xmm5, %xmm2, %xmm8     # xmm8 = (xmm2 * xmm5) + xmm8
	vfmadd231ss	%xmm2, %xmm5, %xmm8     # xmm8 = (xmm5 * xmm2) + xmm8
	vmovaps	%xmm2, %xmm9
	vaddss	%xmm4, %xmm3, %xmm10
	vsubss	%xmm3, %xmm10, %xmm11
	vfmsub213ss	%xmm3, %xmm0, %xmm9     # xmm9 = (xmm0 * xmm9) - xmm3
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm11, %xmm4, %xmm4
	vmulss	%xmm5, %xmm0, %xmm11
	vsubss	%xmm12, %xmm3, %xmm3
	vmulss	%xmm0, %xmm5, %xmm12
	vmovaps	%xmm0, %xmm13
	vfmsub213ss	%xmm12, %xmm5, %xmm13   # xmm13 = (xmm5 * xmm13) - xmm12
	vaddss	%xmm4, %xmm3, %xmm3
	vfmsub213ss	%xmm11, %xmm0, %xmm5    # xmm5 = (xmm0 * xmm5) - xmm11
	vaddss	%xmm6, %xmm11, %xmm4
	vsubss	%xmm11, %xmm4, %xmm14
	vsubss	%xmm14, %xmm4, %xmm15
	vsubss	%xmm15, %xmm11, %xmm11
	vfmsub213ss	%xmm6, %xmm2, %xmm2     # xmm2 = (xmm2 * xmm2) - xmm6
	vsubss	%xmm14, %xmm6, %xmm6
	vaddss	%xmm6, %xmm11, %xmm6
	vmovss	%xmm6, 160(%rsp)                # 4-byte Spill
	vaddss	%xmm4, %xmm12, %xmm11
	vsubss	%xmm4, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm4, %xmm12, %xmm4
	vaddss	%xmm9, %xmm11, %xmm12
	vsubss	%xmm11, %xmm12, %xmm14
	vsubss	%xmm14, %xmm12, %xmm15
	vsubss	%xmm15, %xmm11, %xmm11
	vsubss	%xmm14, %xmm9, %xmm9
	vfmadd231ss	%xmm0, %xmm7, %xmm8     # xmm8 = (xmm7 * xmm0) + xmm8
	vaddss	%xmm1, %xmm12, %xmm7
	vaddss	%xmm9, %xmm11, %xmm9
	vsubss	%xmm12, %xmm7, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vsubss	%xmm11, %xmm7, %xmm11
	vsubss	%xmm11, %xmm12, %xmm11
	vmulss	%xmm0, %xmm0, %xmm12
	vfmsub213ss	%xmm12, %xmm0, %xmm0    # xmm0 = (xmm0 * xmm0) - xmm12
	vaddss	%xmm0, %xmm10, %xmm14
	vsubss	%xmm10, %xmm14, %xmm15
	vsubss	%xmm15, %xmm0, %xmm0
	vsubss	%xmm15, %xmm14, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vaddss	%xmm1, %xmm11, %xmm1
	vaddss	%xmm3, %xmm7, %xmm11
	vsubss	%xmm7, %xmm11, %xmm15
	vsubss	%xmm15, %xmm11, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vsubss	%xmm15, %xmm3, %xmm3
	vaddss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm2, %xmm5, %xmm2
	vaddss	%xmm2, %xmm13, %xmm2
	vaddss	%xmm0, %xmm11, %xmm5
	vaddss	%xmm3, %xmm6, %xmm3
	vsubss	%xmm11, %xmm5, %xmm6
	vsubss	%xmm6, %xmm0, %xmm0
	vsubss	%xmm6, %xmm5, %xmm6
	vsubss	%xmm6, %xmm11, %xmm6
	vaddss	160(%rsp), %xmm2, %xmm2         # 4-byte Folded Reload
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm9, %xmm2
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm2
	vaddss	%xmm2, %xmm14, %xmm3
	vaddss	%xmm3, %xmm12, %xmm4
	vsubss	%xmm4, %xmm12, %xmm0
	vaddss	%xmm3, %xmm0, %xmm6
	vmovups	240(%rsp), %xmm9                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm4, %xmm9, %xmm0
	vsubss	%xmm9, %xmm0, %xmm7
	vsubss	%xmm7, %xmm0, %xmm8
	vsubss	%xmm8, %xmm9, %xmm8
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm8, %xmm4
	vmovshdup	%xmm9, %xmm7            # xmm7 = xmm9[1,1,3,3]
	vsubss	%xmm3, %xmm14, %xmm3
	vaddss	%xmm6, %xmm7, %xmm8
	vsubss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm7, %xmm7
	vsubss	%xmm9, %xmm6, %xmm6
	vaddss	%xmm4, %xmm8, %xmm9
	vsubss	%xmm8, %xmm9, %xmm10
	vaddss	%xmm6, %xmm7, %xmm6
	vsubss	%xmm10, %xmm9, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm2, %xmm3, %xmm3
	vsubss	%xmm2, %xmm5, %xmm2
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vmovups	96(%rsp), %xmm8                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm3, %xmm8, %xmm5
	vaddss	%xmm1, %xmm2, %xmm1
	vsubss	%xmm8, %xmm5, %xmm2
	vsubss	%xmm2, %xmm5, %xmm7
	vsubss	%xmm2, %xmm3, %xmm2
	vsubss	%xmm7, %xmm8, %xmm3
	vmovaps	%xmm8, %xmm10
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm6, %xmm5, %xmm3
	vsubss	%xmm5, %xmm3, %xmm7
	vsubss	%xmm7, %xmm3, %xmm8
	vsubss	%xmm8, %xmm5, %xmm5
	vsubss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm4, %xmm3, %xmm7
	vaddss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm3, %xmm7, %xmm6
	vsubss	%xmm6, %xmm4, %xmm4
	vsubss	%xmm6, %xmm7, %xmm6
	vsubss	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm10, %xmm6           # xmm6 = xmm10[1,1,3,3]
	vaddss	%xmm1, %xmm6, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm4, %xmm3, %xmm2
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm7, %xmm2
	vsubss	%xmm2, %xmm7, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm9, %xmm3
	vsubss	%xmm3, %xmm9, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vaddss	%xmm3, %xmm0, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vinsertps	$16, %xmm1, %xmm2, %xmm2 # xmm2 = xmm2[0],xmm1[0],xmm2[2,3]
	vinsertps	$16, %xmm0, %xmm4, %xmm0 # xmm0 = xmm4[0],xmm0[0],xmm4[2,3]
	incq	%rdx
	addq	$16, %rcx
	cmpq	%rax, %rdx
	jl	.LBB41_49
.LBB41_50:                              #   in Loop: Header=BB41_47 Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm0
	setp	%al
	setne	%cl
	orb	%al, %cl
	je	.LBB41_45
# %bb.51:                               #   in Loop: Header=BB41_47 Depth=1
	vcomiss	%xmm0, %xmm1
	vbroadcastss	.LCPI41_0(%rip), %xmm1  # xmm1 = [NaN,NaN,NaN,NaN]
	vmovaps	%xmm1, %xmm3
	ja	.LBB41_46
# %bb.52:                               #   in Loop: Header=BB41_47 Depth=1
	vmovups	%xmm2, 96(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm0, 240(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vxorps	%xmm2, %xmm2, %xmm2
	vucomiss	%xmm2, %xmm0
	setnp	%cl
	sete	%dl
	vmovd	%xmm0, %eax
	vmovd	.LCPI41_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovdqa	%xmm1, %xmm7
	testb	%cl, %dl
	jne	.LBB41_55
# %bb.53:                               #   in Loop: Header=BB41_47 Depth=1
	movl	%eax, %ecx
	andl	$2139095040, %ecx               # imm = 0x7F800000
	vmovdqa	%xmm1, %xmm7
	cmpl	$2139095040, %ecx               # imm = 0x7F800000
	je	.LBB41_55
# %bb.54:                               #   in Loop: Header=BB41_47 Depth=1
	vmovd	%ecx, %xmm7
.LBB41_55:                              #   in Loop: Header=BB41_47 Depth=1
	vmovups	240(%rsp), %xmm3                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vucomiss	%xmm2, %xmm0
	setnp	%cl
	sete	%dl
	vmovdqa	%xmm1, %xmm5
	testb	%cl, %dl
	jne	.LBB41_58
# %bb.56:                               #   in Loop: Header=BB41_47 Depth=1
	andl	$2139095040, %eax               # imm = 0x7F800000
	vmovdqa	%xmm1, %xmm5
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB41_58
# %bb.57:                               #   in Loop: Header=BB41_47 Depth=1
	movl	$2130706432, %ecx               # imm = 0x7F000000
	subl	%eax, %ecx
	vmovd	%ecx, %xmm5
.LBB41_58:                              #   in Loop: Header=BB41_47 Depth=1
	vmovss	.LCPI41_2(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm5, %xmm6
	vmulss	%xmm5, %xmm3, %xmm3
	vmulss	%xmm6, %xmm3, %xmm3
	vmovups	%xmm4, 320(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmulss	%xmm5, %xmm4, %xmm4
	vmulss	%xmm6, %xmm4, %xmm4
	vmovss	%xmm4, 160(%rsp)                # 4-byte Spill
	vmulss	96(%rsp), %xmm5, %xmm5          # 16-byte Folded Reload
	vmulss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, 56(%rsp)                 # 4-byte Spill
	movl	$3, %eax
	vcvtsi2ss	%eax, %xmm13, %xmm6
	vdivss	%xmm0, %xmm7, %xmm8
	vmulss	%xmm1, %xmm6, %xmm9
	vmovaps	%xmm1, %xmm0
	vfmsub213ss	%xmm9, %xmm6, %xmm0     # xmm0 = (xmm6 * xmm0) - xmm9
	vmovss	%xmm7, 116(%rsp)                # 4-byte Spill
	vmulss	%xmm2, %xmm6, %xmm7
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm7, %xmm6, %xmm10    # xmm10 = (xmm6 * xmm10) - xmm7
	vaddss	%xmm0, %xmm7, %xmm11
	vxorps	%xmm4, %xmm4, %xmm4
	vsubss	%xmm7, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm7, %xmm7
	vsubss	%xmm12, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vfmadd231ss	%xmm6, %xmm4, %xmm0     # xmm0 = (xmm4 * xmm6) + xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm0, %xmm11, %xmm6
	vsubss	%xmm6, %xmm11, %xmm7
	vaddss	%xmm0, %xmm7, %xmm0
	vmovss	%xmm0, 152(%rsp)                # 4-byte Spill
	vaddss	%xmm6, %xmm9, %xmm7
	vsubss	%xmm7, %xmm9, %xmm9
	vaddss	%xmm6, %xmm9, %xmm6
	vmulss	%xmm4, %xmm8, %xmm9
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm9, %xmm8, %xmm10    # xmm10 = (xmm8 * xmm10) - xmm9
	vmulss	%xmm4, %xmm8, %xmm11
	vaddss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm9, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm9, %xmm9
	vmovaps	%xmm8, %xmm14
	vfmsub213ss	%xmm11, %xmm4, %xmm14   # xmm14 = (xmm4 * xmm14) - xmm11
	vsubss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm11, %xmm9, %xmm9
	vmulss	%xmm8, %xmm8, %xmm11
	vmovaps	%xmm8, %xmm13
	vfmsub213ss	%xmm11, %xmm8, %xmm13   # xmm13 = (xmm8 * xmm13) - xmm11
	vaddss	%xmm13, %xmm12, %xmm15
	vsubss	%xmm12, %xmm15, %xmm2
	vsubss	%xmm2, %xmm15, %xmm1
	vsubss	%xmm1, %xmm12, %xmm1
	vsubss	%xmm2, %xmm13, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vfmadd231ss	%xmm8, %xmm4, %xmm10    # xmm10 = (xmm4 * xmm8) + xmm10
	vfmadd231ss	%xmm4, %xmm4, %xmm14    # xmm14 = (xmm4 * xmm4) + xmm14
	vaddss	%xmm14, %xmm10, %xmm2
	vfmadd231ss	%xmm4, %xmm8, %xmm9     # xmm9 = (xmm8 * xmm4) + xmm9
	vaddss	%xmm2, %xmm9, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm15, %xmm2
	vsubss	%xmm2, %xmm15, %xmm9
	vaddss	%xmm1, %xmm9, %xmm9
	vaddss	%xmm2, %xmm11, %xmm10
	vsubss	%xmm10, %xmm11, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vmovss	160(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm10, %xmm2
	vmovaps	%xmm10, %xmm11
	vfmsub213ss	%xmm2, %xmm0, %xmm11    # xmm11 = (xmm0 * xmm11) - xmm2
	vfmadd231ss	%xmm1, %xmm0, %xmm11    # xmm11 = (xmm0 * xmm1) + xmm11
	vmulss	%xmm1, %xmm3, %xmm12
	vfmsub213ss	%xmm12, %xmm3, %xmm1    # xmm1 = (xmm3 * xmm1) - xmm12
	vaddss	%xmm2, %xmm12, %xmm13
	vsubss	%xmm12, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm2, %xmm12, %xmm2
	vfmadd231ss	%xmm10, %xmm5, %xmm1    # xmm1 = (xmm5 * xmm10) + xmm1
	vmulss	%xmm3, %xmm10, %xmm12
	vfmsub213ss	%xmm12, %xmm3, %xmm10   # xmm10 = (xmm3 * xmm10) - xmm12
	vaddss	%xmm10, %xmm13, %xmm14
	vsubss	%xmm13, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm5
	vsubss	%xmm5, %xmm13, %xmm5
	vsubss	%xmm15, %xmm10, %xmm10
	vaddss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm1, %xmm11, %xmm1
	vfmadd231ss	%xmm9, %xmm3, %xmm2     # xmm2 = (xmm3 * xmm9) + xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm14, %xmm2
	vsubss	%xmm2, %xmm14, %xmm5
	vaddss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm2, %xmm12, %xmm5
	vsubss	%xmm5, %xmm12, %xmm9
	vaddss	%xmm2, %xmm9, %xmm2
	vbroadcastss	.LCPI41_3(%rip), %xmm0  # xmm0 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm0, %xmm5, %xmm9
	vsubss	%xmm5, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm10
	vsubss	%xmm10, %xmm9, %xmm9
	vsubss	%xmm10, %xmm5, %xmm10
	vsubss	%xmm10, %xmm7, %xmm10
	vaddss	%xmm9, %xmm10, %xmm9
	vxorps	%xmm0, %xmm2, %xmm10
	vsubss	%xmm2, %xmm6, %xmm2
	vsubss	%xmm6, %xmm2, %xmm11
	vsubss	%xmm11, %xmm10, %xmm10
	vsubss	%xmm11, %xmm2, %xmm11
	vsubss	%xmm11, %xmm6, %xmm11
	vaddss	%xmm10, %xmm11, %xmm10
	vaddss	%xmm2, %xmm9, %xmm11
	vsubss	%xmm2, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm2, %xmm2
	vsubss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm2, %xmm9, %xmm2
	vmovss	152(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm0, %xmm1
	vaddss	%xmm1, %xmm10, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm11, %xmm2
	vsubss	%xmm2, %xmm11, %xmm9
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm2, %xmm5, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vaddss	%xmm2, %xmm5, %xmm2
	vmulss	%xmm4, %xmm9, %xmm5
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm5, %xmm9, %xmm10    # xmm10 = (xmm9 * xmm10) - xmm5
	vmulss	%xmm2, %xmm8, %xmm11
	vaddss	%xmm5, %xmm11, %xmm12
	vsubss	%xmm5, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm5, %xmm5
	vmovaps	%xmm8, %xmm14
	vfmsub213ss	%xmm11, %xmm2, %xmm14   # xmm14 = (xmm2 * xmm14) - xmm11
	vsubss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm5, %xmm11, %xmm5
	vfmadd231ss	%xmm1, %xmm8, %xmm10    # xmm10 = (xmm8 * xmm1) + xmm10
	vmulss	%xmm8, %xmm9, %xmm1
	vfmsub213ss	%xmm1, %xmm9, %xmm8     # xmm8 = (xmm9 * xmm8) - xmm1
	vaddss	%xmm8, %xmm12, %xmm11
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm11, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm13, %xmm8, %xmm8
	vaddss	%xmm8, %xmm12, %xmm8
	vfmadd231ss	%xmm2, %xmm4, %xmm14    # xmm14 = (xmm4 * xmm2) + xmm14
	vaddss	%xmm14, %xmm10, %xmm2
	vfmadd231ss	%xmm9, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm9) + xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm2, %xmm11, %xmm5
	vsubss	%xmm5, %xmm11, %xmm8
	vaddss	%xmm2, %xmm8, %xmm4
	vaddss	%xmm5, %xmm1, %xmm9
	vsubss	%xmm9, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm10
	vmulss	%xmm10, %xmm9, %xmm1
	vmovaps	%xmm10, %xmm2
	vfmsub213ss	%xmm1, %xmm9, %xmm2     # xmm2 = (xmm9 * xmm2) - xmm1
	vmulss	%xmm9, %xmm10, %xmm5
	vaddss	%xmm5, %xmm1, %xmm11
	vsubss	%xmm1, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm1, %xmm1
	vmovaps	%xmm9, %xmm13
	vfmsub213ss	%xmm5, %xmm10, %xmm13   # xmm13 = (xmm10 * xmm13) - xmm5
	vsubss	%xmm12, %xmm5, %xmm5
	vaddss	%xmm5, %xmm1, %xmm1
	vmulss	%xmm9, %xmm9, %xmm5
	vmovaps	%xmm9, %xmm12
	vfmsub213ss	%xmm5, %xmm9, %xmm12    # xmm12 = (xmm9 * xmm12) - xmm5
	vaddss	%xmm12, %xmm11, %xmm14
	vsubss	%xmm11, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm0
	vsubss	%xmm0, %xmm11, %xmm0
	vsubss	%xmm15, %xmm12, %xmm11
	vaddss	%xmm0, %xmm11, %xmm0
	vmovss	%xmm4, 112(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm9, %xmm4, %xmm2     # xmm2 = (xmm4 * xmm9) + xmm2
	vfmadd231ss	%xmm10, %xmm10, %xmm13  # xmm13 = (xmm10 * xmm10) + xmm13
	vaddss	%xmm2, %xmm13, %xmm2
	vfmadd231ss	%xmm4, %xmm9, %xmm1     # xmm1 = (xmm9 * xmm4) + xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm2
	vaddss	%xmm0, %xmm2, %xmm11
	vaddss	%xmm1, %xmm5, %xmm12
	vsubss	%xmm12, %xmm5, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	160(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm12, %xmm1
	vmovaps	%xmm12, %xmm2
	vfmsub213ss	%xmm1, %xmm4, %xmm2     # xmm2 = (xmm4 * xmm2) - xmm1
	vfmadd231ss	%xmm0, %xmm4, %xmm2     # xmm2 = (xmm4 * xmm0) + xmm2
	vmulss	%xmm0, %xmm3, %xmm5
	vfmsub213ss	%xmm5, %xmm3, %xmm0     # xmm0 = (xmm3 * xmm0) - xmm5
	vaddss	%xmm1, %xmm5, %xmm13
	vsubss	%xmm5, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm14, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm1
	vfmadd231ss	56(%rsp), %xmm12, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm12 * mem) + xmm0
	vmulss	%xmm3, %xmm12, %xmm5
	vfmsub213ss	%xmm5, %xmm3, %xmm12    # xmm12 = (xmm3 * xmm12) - xmm5
	vaddss	%xmm12, %xmm13, %xmm14
	vsubss	%xmm13, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm8
	vsubss	%xmm8, %xmm13, %xmm8
	vsubss	%xmm15, %xmm12, %xmm12
	vaddss	%xmm12, %xmm8, %xmm8
	vaddss	%xmm2, %xmm0, %xmm0
	vfmadd231ss	%xmm11, %xmm3, %xmm1    # xmm1 = (xmm3 * xmm11) + xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm5, %xmm2
	vsubss	%xmm2, %xmm5, %xmm5
	vaddss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm2, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm5, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vbroadcastss	.LCPI41_3(%rip), %xmm4  # xmm4 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm4, %xmm2, %xmm2
	vsubss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm7, %xmm2
	vsubss	%xmm1, %xmm6, %xmm7
	vsubss	%xmm6, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm11
	vsubss	%xmm11, %xmm6, %xmm6
	vxorps	%xmm4, %xmm1, %xmm1
	vsubss	%xmm8, %xmm1, %xmm1
	vaddss	%xmm1, %xmm6, %xmm1
	vaddss	%xmm2, %xmm7, %xmm6
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm7, %xmm2
	vmovss	152(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm2
	vaddss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm5, %xmm0
	vsubss	%xmm0, %xmm5, %xmm5
	vaddss	%xmm1, %xmm5, %xmm1
	vmulss	%xmm0, %xmm10, %xmm5
	vmulss	%xmm1, %xmm9, %xmm6
	vmovaps	%xmm9, %xmm7
	vfmsub213ss	%xmm6, %xmm1, %xmm7     # xmm7 = (xmm1 * xmm7) - xmm6
	vfmadd231ss	%xmm10, %xmm1, %xmm7    # xmm7 = (xmm1 * xmm10) + xmm7
	vfmsub213ss	%xmm5, %xmm0, %xmm10    # xmm10 = (xmm0 * xmm10) - xmm5
	vaddss	%xmm6, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm8
	vsubss	%xmm8, %xmm1, %xmm11
	vsubss	%xmm11, %xmm5, %xmm5
	vsubss	%xmm8, %xmm6, %xmm6
	vfmadd231ss	%xmm2, %xmm9, %xmm10    # xmm10 = (xmm9 * xmm2) + xmm10
	vmulss	%xmm0, %xmm9, %xmm2
	vfmsub213ss	%xmm2, %xmm0, %xmm9     # xmm9 = (xmm0 * xmm9) - xmm2
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm1, %xmm9, %xmm6
	vsubss	%xmm1, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vsubss	%xmm8, %xmm9, %xmm8
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm7, %xmm10, %xmm7
	vfmadd231ss	112(%rsp), %xmm0, %xmm5 # 4-byte Folded Reload
                                        # xmm5 = (xmm0 * mem) + xmm5
	vaddss	%xmm5, %xmm7, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm5
	vaddss	%xmm0, %xmm5, %xmm6
	vaddss	%xmm1, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm0
	vaddss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm5, %xmm3, %xmm0
	vmulss	%xmm1, %xmm3, %xmm2
	vmovss	160(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm4, %xmm7
	vmovaps	%xmm5, %xmm8
	vfmsub213ss	%xmm7, %xmm4, %xmm8     # xmm8 = (xmm4 * xmm8) - xmm7
	vfmadd231ss	%xmm1, %xmm4, %xmm8     # xmm8 = (xmm4 * xmm1) + xmm8
	vfmsub213ss	%xmm2, %xmm3, %xmm1     # xmm1 = (xmm3 * xmm1) - xmm2
	vfmadd231ss	56(%rsp), %xmm5, %xmm1  # 4-byte Folded Reload
                                        # xmm1 = (xmm5 * mem) + xmm1
	vfmsub213ss	%xmm0, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm5) - xmm0
	vaddss	%xmm7, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm9
	vsubss	%xmm9, %xmm4, %xmm10
	vsubss	%xmm10, %xmm2, %xmm2
	vsubss	%xmm9, %xmm7, %xmm7
	vaddss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm5, %xmm4, %xmm7
	vsubss	%xmm4, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vsubss	%xmm9, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm1, %xmm8, %xmm1
	vfmadd231ss	%xmm6, %xmm3, %xmm2     # xmm2 = (xmm3 * xmm6) + xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm1, %xmm7, %xmm2
	vsubss	%xmm2, %xmm7, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	movl	$2, %eax
	vxorps	%xmm13, %xmm13, %xmm13
	vcvtsi2ss	%eax, %xmm13, %xmm4
	vmovss	%xmm4, 160(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm0, %xmm2
	vmulss	116(%rsp), %xmm4, %xmm5         # 4-byte Folded Reload
	vmulss	%xmm5, %xmm3, %xmm8
	vmulss	%xmm5, %xmm2, %xmm4
	vmulss	%xmm5, %xmm1, %xmm5
	vmulss	%xmm8, %xmm8, %xmm2
	vmovss	%xmm2, 56(%rsp)                 # 4-byte Spill
	vmovaps	%xmm8, %xmm1
	vfmsub213ss	%xmm2, %xmm8, %xmm1     # xmm1 = (xmm8 * xmm1) - xmm2
	vmulss	%xmm4, %xmm8, %xmm6
	vmovaps	%xmm4, %xmm11
	vfmsub213ss	%xmm6, %xmm8, %xmm11    # xmm11 = (xmm8 * xmm11) - xmm6
	vmulss	%xmm4, %xmm8, %xmm7
	vmovaps	%xmm8, %xmm3
	vfmsub213ss	%xmm7, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm3) - xmm7
	vaddss	%xmm7, %xmm6, %xmm10
	vsubss	%xmm6, %xmm10, %xmm9
	vsubss	%xmm9, %xmm10, %xmm12
	vsubss	%xmm12, %xmm6, %xmm6
	vsubss	%xmm9, %xmm7, %xmm7
	vaddss	%xmm7, %xmm6, %xmm9
	vaddss	%xmm1, %xmm10, %xmm6
	vsubss	%xmm10, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm12
	vsubss	%xmm12, %xmm10, %xmm10
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm10, %xmm10
	vmulss	%xmm5, %xmm8, %xmm1
	vmovaps	%xmm5, %xmm7
	vfmsub213ss	%xmm1, %xmm8, %xmm7     # xmm7 = (xmm8 * xmm7) - xmm1
	vmulss	%xmm4, %xmm4, %xmm12
	vaddss	%xmm1, %xmm12, %xmm14
	vsubss	%xmm1, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm13
	vsubss	%xmm13, %xmm1, %xmm1
	vmovaps	%xmm4, %xmm13
	vfmsub213ss	%xmm12, %xmm4, %xmm13   # xmm13 = (xmm4 * xmm13) - xmm12
	vsubss	%xmm15, %xmm12, %xmm12
	vaddss	%xmm1, %xmm12, %xmm0
	vmovss	%xmm0, 152(%rsp)                # 4-byte Spill
	vmulss	%xmm5, %xmm8, %xmm1
	vaddss	%xmm1, %xmm14, %xmm0
	vsubss	%xmm14, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm15
	vsubss	%xmm15, %xmm14, %xmm14
	vmovaps	%xmm8, %xmm15
	vfmsub213ss	%xmm1, %xmm5, %xmm15    # xmm15 = (xmm5 * xmm15) - xmm1
	vsubss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm14, %xmm1
	vmovss	%xmm1, 116(%rsp)                # 4-byte Spill
	vaddss	%xmm0, %xmm11, %xmm1
	vsubss	%xmm0, %xmm1, %xmm2
	vsubss	%xmm2, %xmm1, %xmm12
	vsubss	%xmm12, %xmm0, %xmm0
	vsubss	%xmm2, %xmm11, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm3, %xmm1, %xmm2
	vsubss	%xmm1, %xmm2, %xmm11
	vsubss	%xmm11, %xmm2, %xmm12
	vsubss	%xmm12, %xmm1, %xmm1
	vsubss	%xmm11, %xmm3, %xmm3
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm2, %xmm9, %xmm3
	vsubss	%xmm2, %xmm3, %xmm11
	vsubss	%xmm11, %xmm3, %xmm12
	vsubss	%xmm12, %xmm2, %xmm2
	vsubss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm2, %xmm9, %xmm2
	vaddss	%xmm3, %xmm10, %xmm9
	vsubss	%xmm3, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm12, %xmm3, %xmm3
	vxorps	%xmm12, %xmm12, %xmm12
	vsubss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm3, %xmm10, %xmm3
	vmulss	%xmm12, %xmm8, %xmm10
	vmovaps	%xmm5, %xmm14
	vfmadd231ss	%xmm5, %xmm4, %xmm10    # xmm10 = (xmm4 * xmm5) + xmm10
	vfmadd231ss	%xmm4, %xmm5, %xmm10    # xmm10 = (xmm5 * xmm4) + xmm10
	vfmadd231ss	%xmm8, %xmm12, %xmm10   # xmm10 = (xmm12 * xmm8) + xmm10
	vaddss	%xmm7, %xmm10, %xmm7
	vaddss	%xmm7, %xmm13, %xmm7
	vaddss	%xmm7, %xmm15, %xmm7
	vaddss	152(%rsp), %xmm7, %xmm7         # 4-byte Folded Reload
	vaddss	116(%rsp), %xmm7, %xmm7         # 4-byte Folded Reload
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm1
	vsubss	%xmm1, %xmm9, %xmm2
	vaddss	%xmm0, %xmm2, %xmm7
	vaddss	%xmm1, %xmm6, %xmm0
	vsubss	%xmm0, %xmm6, %xmm2
	vaddss	%xmm1, %xmm2, %xmm2
	vmovss	56(%rsp), %xmm3                 # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm3, %xmm1
	vsubss	%xmm1, %xmm3, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vbroadcastss	.LCPI41_3(%rip), %xmm5  # xmm5 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm5, %xmm1, %xmm3
	vxorps	%xmm5, %xmm0, %xmm6
	vmovaps	%xmm5, %xmm11
	vmovups	240(%rsp), %xmm9                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm1, %xmm9, %xmm5
	vsubss	%xmm9, %xmm5, %xmm1
	vmovaps	%xmm9, %xmm10
	vsubss	%xmm1, %xmm5, %xmm9
	vsubss	%xmm9, %xmm10, %xmm9
	vxorps	%xmm2, %xmm11, %xmm10
	vsubss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm1, %xmm9, %xmm3
	vmovups	320(%rsp), %xmm9                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm0, %xmm9, %xmm0
	vsubss	%xmm9, %xmm0, %xmm1
	vmovaps	%xmm9, %xmm11
	vsubss	%xmm1, %xmm0, %xmm9
	vsubss	%xmm9, %xmm11, %xmm9
	vsubss	%xmm1, %xmm6, %xmm1
	vaddss	%xmm1, %xmm9, %xmm6
	vaddss	%xmm3, %xmm0, %xmm1
	vsubss	%xmm0, %xmm1, %xmm9
	vsubss	%xmm9, %xmm1, %xmm11
	vsubss	%xmm11, %xmm0, %xmm0
	vsubss	%xmm9, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm0
	vmovups	96(%rsp), %xmm11                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm2, %xmm11, %xmm2
	vsubss	%xmm11, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm9
	vsubss	%xmm9, %xmm11, %xmm9
	vmovaps	%xmm11, %xmm12
	vsubss	%xmm3, %xmm10, %xmm3
	vaddss	%xmm3, %xmm9, %xmm3
	vaddss	%xmm6, %xmm2, %xmm9
	vsubss	%xmm2, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm2, %xmm2
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm0, %xmm9, %xmm6
	vsubss	%xmm9, %xmm6, %xmm10
	vsubss	%xmm10, %xmm6, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vsubss	%xmm10, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vmovshdup	%xmm12, %xmm9           # xmm9 = xmm12[1,1,3,3]
	vsubss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm3, %xmm7, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm0, %xmm6, %xmm2
	vsubss	%xmm2, %xmm6, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm5, %xmm2
	vsubss	%xmm2, %xmm5, %xmm5
	vaddss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm4, %xmm8, %xmm1
	vaddss	%xmm1, %xmm14, %xmm1
	vmulss	160(%rsp), %xmm1, %xmm1         # 4-byte Folded Reload
	vdivss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vinsertps	$16, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm0[0],xmm1[2,3]
	vaddss	%xmm2, %xmm8, %xmm0
	vsubss	%xmm0, %xmm8, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vinsertps	$16, %xmm2, %xmm0, %xmm3 # xmm3 = xmm0[0],xmm2[0],xmm0[2,3]
	jmp	.LBB41_46
.LBB41_59:
	callq	omp_get_wtime
	vsubsd	296(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm13, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	264(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp5920:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp5921:
# %bb.60:
	movq	%rax, %r13
	movq	8(%rsp), %rax                   # 8-byte Reload
	leaq	32(%rax), %r14
.Ltmp5922:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp5923:
# %bb.61:
.Ltmp5924:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp5925:
# %bb.62:
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp5926:
	leaq	120(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp5927:
# %bb.63:
.Ltmp5928:
	leaq	120(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp5929:
# %bb.64:
.Ltmp5931:
	callq	mpfr_get_default_rounding_mode
.Ltmp5932:
# %bb.65:
.Ltmp5933:
	leaq	120(%rsp), %rdi
	leaq	264(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp5934:
# %bb.66:
.Ltmp5936:
	leaq	120(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5937:
# %bb.67:
.Ltmp5938:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp5939:
# %bb.68:
.Ltmp5940:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp5941:
# %bb.69:
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp5942:
	leaq	24(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp5943:
# %bb.70:
.Ltmp5944:
	leaq	24(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp5945:
# %bb.71:
.Ltmp5947:
	callq	mpfr_get_default_rounding_mode
.Ltmp5948:
# %bb.72:
.Ltmp5949:
	leaq	24(%rsp), %rdi
	leaq	120(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp5950:
# %bb.73:
.Ltmp5952:
	callq	mpfr_get_default_rounding_mode
.Ltmp5953:
# %bb.74:
.Ltmp5954:
	movl	%eax, %ebp
	leaq	24(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp5955:
# %bb.75:
.Ltmp5956:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp5957:
# %bb.76:
.Ltmp5958:
	movl	%eax, %r12d
	leaq	64(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp5959:
# %bb.77:
.Ltmp5960:
	leaq	64(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp5961:
# %bb.78:
.Ltmp5963:
	leaq	64(%rsp), %rdi
	leaq	24(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_abs
.Ltmp5964:
# %bb.79:
.Ltmp5966:
	leaq	64(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp5967:
# %bb.80:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 336(%rsp)
	cmpq	$0, 88(%rsp)
	je	.LBB41_82
# %bb.81:
.Ltmp5969:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5970:
.LBB41_82:
	cmpq	$0, 48(%rsp)
	je	.LBB41_84
# %bb.83:
.Ltmp5972:
	leaq	24(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5973:
.LBB41_84:
	cmpq	$0, 144(%rsp)
	je	.LBB41_86
# %bb.85:
.Ltmp5975:
	leaq	120(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5976:
.LBB41_86:
	cmpq	$0, 288(%rsp)
	je	.LBB41_88
# %bb.87:
.Ltmp5978:
	leaq	264(%rsp), %rdi
	callq	mpfr_clear
.Ltmp5979:
.LBB41_88:
	leaq	496(%rsp), %r12
	movq	%r12, 480(%rsp)
	movl	$846033518, 496(%rsp)           # imm = 0x326D726E
	movw	$32, 500(%rsp)
	movq	$5, 488(%rsp)
.Ltmp5981:
	leaq	480(%rsp), %rdi
	leaq	336(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp5982:
# %bb.89:
	movq	480(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB41_91
# %bb.90:
	callq	_ZdlPv
.LBB41_91:
	movq	%r15, %rdi
	callq	_ZdaPv
	movl	$160, %edi
	callq	_Znam
	movq	%rax, %r15
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rax)
	vmovupd	%xmm0, 16(%rax)
	vmovupd	%xmm0, 32(%rax)
	vmovupd	%xmm0, 48(%rax)
	vmovupd	%xmm0, 64(%rax)
	vmovupd	%xmm0, 80(%rax)
	vmovupd	%xmm0, 96(%rax)
	vmovupd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 128(%rax)
	vmovupd	%xmm0, 144(%rax)
	callq	omp_get_wtime
	vxorps	%xmm13, %xmm13, %xmm13
	vmovsd	%xmm0, 96(%rsp)                 # 8-byte Spill
	movq	192(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	movq	%rax, %rcx
	shlq	$4, %rcx
	xorl	%edx, %edx
	vxorpd	%xmm0, %xmm0, %xmm0
	vbroadcastss	.LCPI41_3(%rip), %xmm12 # xmm12 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	xorl	%esi, %esi
	jmp	.LBB41_93
	.p2align	4, 0x90
.LBB41_92:                              #   in Loop: Header=BB41_93 Depth=1
	vmovlhps	%xmm1, %xmm2, %xmm1             # xmm1 = xmm2[0],xmm1[0]
	movq	%rsi, %rdi
	shlq	$4, %rdi
	vmovups	%xmm1, (%r15,%rdi)
	incq	%rsi
	cmpq	$10, %rsi
	je	.LBB41_99
.LBB41_93:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB41_97 Depth 2
	vcvtsi2ss	%edx, %xmm14, %xmm1
	vblendps	$1, %xmm1, %xmm13, %xmm2        # xmm2 = xmm1[0],xmm13[1,2,3]
	vxorps	%xmm1, %xmm1, %xmm1
	testl	%eax, %eax
	jle	.LBB41_92
# %bb.94:                               #   in Loop: Header=BB41_93 Depth=1
	xorl	%edi, %edi
	jmp	.LBB41_97
	.p2align	4, 0x90
.LBB41_95:                              #   in Loop: Header=BB41_97 Depth=2
	vmovups	(%rbx,%rdi), %xmm5
	vmovddup	8(%rbx,%rdi), %xmm4             # xmm4 = mem[0,0]
.LBB41_96:                              #   in Loop: Header=BB41_97 Depth=2
	vaddss	%xmm5, %xmm2, %xmm3
	vsubss	%xmm2, %xmm3, %xmm6
	vsubss	%xmm6, %xmm3, %xmm7
	vsubss	%xmm7, %xmm2, %xmm7
	vsubss	%xmm6, %xmm5, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vmovshdup	%xmm5, %xmm5            # xmm5 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm2, %xmm7
	vsubss	%xmm2, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm2, %xmm2
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm2, %xmm5
	vaddss	%xmm6, %xmm7, %xmm2
	vsubss	%xmm7, %xmm2, %xmm8
	vsubss	%xmm8, %xmm2, %xmm9
	vsubss	%xmm9, %xmm7, %xmm7
	vsubss	%xmm8, %xmm6, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm4, %xmm1, %xmm7
	vsubss	%xmm1, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm1, %xmm9
	vsubss	%xmm8, %xmm4, %xmm8
	vaddss	%xmm8, %xmm9, %xmm8
	vaddss	%xmm5, %xmm7, %xmm9
	vsubss	%xmm7, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm7, %xmm5
	vaddss	%xmm6, %xmm9, %xmm7
	vsubss	%xmm9, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm9, %xmm6
	vmovshdup	%xmm1, %xmm1            # xmm1 = xmm1[1,1,3,3]
	vmovshdup	%xmm4, %xmm4            # xmm4 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm1, %xmm7, %xmm4
	vsubss	%xmm4, %xmm7, %xmm5
	vaddss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm2
	vinsertps	$16, %xmm1, %xmm2, %xmm1 # xmm1 = xmm2[0],xmm1[0],xmm2[2,3]
	vaddss	%xmm5, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm5, %xmm3, %xmm3
	vinsertps	$16, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[0],xmm3[0],xmm2[2,3]
	addq	$16, %rdi
	cmpq	%rdi, %rcx
	je	.LBB41_92
.LBB41_97:                              #   Parent Loop BB41_93 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rdi), %xmm3              # xmm3 = mem[0],zero,zero,zero
	vcomiss	%xmm3, %xmm0
	jbe	.LBB41_95
# %bb.98:                               #   in Loop: Header=BB41_97 Depth=2
	vinsertps	$16, 4(%rbx,%rdi), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vxorps	%xmm3, %xmm12, %xmm5
	vmovsd	8(%rbx,%rdi), %xmm3             # xmm3 = mem[0],zero
	vxorps	%xmm3, %xmm12, %xmm4
	jmp	.LBB41_96
.LBB41_99:
	vmovups	%xmm12, 544(%rsp)               # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	omp_get_wtime
	vsubsd	96(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm14, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	264(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp5984:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp5985:
# %bb.100:
	movq	%rax, %r13
	movq	8(%rsp), %rax                   # 8-byte Reload
	leaq	64(%rax), %r14
.Ltmp5986:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp5987:
# %bb.101:
.Ltmp5988:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp5989:
# %bb.102:
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp5990:
	leaq	120(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp5991:
# %bb.103:
.Ltmp5992:
	leaq	120(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp5993:
# %bb.104:
.Ltmp5995:
	callq	mpfr_get_default_rounding_mode
.Ltmp5996:
# %bb.105:
.Ltmp5997:
	leaq	120(%rsp), %rdi
	leaq	264(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp5998:
# %bb.106:
.Ltmp6000:
	leaq	120(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6001:
# %bb.107:
.Ltmp6002:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp6003:
# %bb.108:
.Ltmp6004:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp6005:
# %bb.109:
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp6006:
	leaq	24(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp6007:
# %bb.110:
.Ltmp6008:
	leaq	24(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp6009:
# %bb.111:
.Ltmp6011:
	callq	mpfr_get_default_rounding_mode
.Ltmp6012:
# %bb.112:
.Ltmp6013:
	leaq	24(%rsp), %rdi
	leaq	120(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp6014:
# %bb.113:
.Ltmp6016:
	callq	mpfr_get_default_rounding_mode
.Ltmp6017:
# %bb.114:
.Ltmp6018:
	movl	%eax, %ebp
	leaq	24(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6019:
# %bb.115:
.Ltmp6020:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp6021:
# %bb.116:
.Ltmp6022:
	movl	%eax, %r12d
	leaq	64(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp6023:
# %bb.117:
.Ltmp6024:
	leaq	64(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6025:
# %bb.118:
.Ltmp6027:
	leaq	64(%rsp), %rdi
	leaq	24(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_abs
.Ltmp6028:
# %bb.119:
.Ltmp6030:
	leaq	64(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp6031:
# %bb.120:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 336(%rsp)
	cmpq	$0, 88(%rsp)
	je	.LBB41_122
# %bb.121:
.Ltmp6033:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6034:
.LBB41_122:
	cmpq	$0, 48(%rsp)
	je	.LBB41_124
# %bb.123:
.Ltmp6036:
	leaq	24(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6037:
.LBB41_124:
	cmpq	$0, 144(%rsp)
	je	.LBB41_126
# %bb.125:
.Ltmp6039:
	leaq	120(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6040:
.LBB41_126:
	cmpq	$0, 288(%rsp)
	je	.LBB41_128
# %bb.127:
.Ltmp6042:
	leaq	264(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6043:
.LBB41_128:
	leaq	464(%rsp), %r12
	movq	%r12, 448(%rsp)
	movl	$1836413793, 464(%rsp)          # imm = 0x6D757361
	movw	$32, 468(%rsp)
	movq	$5, 456(%rsp)
.Ltmp6045:
	leaq	448(%rsp), %rdi
	leaq	336(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp6046:
# %bb.129:
	movq	448(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB41_131
# %bb.130:
	callq	_ZdlPv
.LBB41_131:
	movq	%r15, %rdi
	callq	_ZdaPv
	leaq	216(%rsp), %rsi
	movq	192(%rsp), %r13                 # 8-byte Reload
	movq	%r13, %rdi
	movq	%rbx, %rdx
	movq	16(%rsp), %rcx                  # 8-byte Reload
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	64(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	cmpl	$0, (%r13)
	jle	.LBB41_172
# %bb.132:
	movq	8(%rsp), %rax                   # 8-byte Reload
	leaq	256(%rax), %r15
	xorl	%r14d, %r14d
	movq	16(%rsp), %rsi                  # 8-byte Reload
	jmp	.LBB41_134
	.p2align	4, 0x90
.LBB41_133:                             #   in Loop: Header=BB41_134 Depth=1
	incq	%r14
	movq	192(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r15
	movq	96(%rsp), %rsi                  # 8-byte Reload
	addq	$16, %rsi
	cmpq	%rax, %r14
	jge	.LBB41_172
.LBB41_134:                             # =>This Inner Loop Header: Depth=1
.Ltmp6048:
	leaq	120(%rsp), %rdi
	movq	%rsi, 96(%rsp)                  # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp6049:
# %bb.135:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6051:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp6052:
# %bb.136:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6053:
	movq	%rax, %rbp
	leaq	120(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6054:
# %bb.137:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6055:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp6056:
# %bb.138:                              #   in Loop: Header=BB41_134 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp6057:
	leaq	24(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp6058:
# %bb.139:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6059:
	leaq	24(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6060:
# %bb.140:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6062:
	callq	mpfr_get_default_rounding_mode
.Ltmp6063:
# %bb.141:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6064:
	leaq	24(%rsp), %rdi
	movq	%r15, %rsi
	leaq	120(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp6065:
# %bb.142:                              #   in Loop: Header=BB41_134 Depth=1
	cmpq	$0, 144(%rsp)
	je	.LBB41_144
# %bb.143:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6067:
	leaq	120(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6068:
.LBB41_144:                             #   in Loop: Header=BB41_134 Depth=1
.Ltmp6070:
	leaq	24(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6071:
# %bb.145:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6072:
	movq	%rax, %rbp
	leaq	24(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6073:
# %bb.146:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6074:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp6075:
# %bb.147:                              #   in Loop: Header=BB41_134 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp6076:
	leaq	264(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp6077:
# %bb.148:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6078:
	leaq	264(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6079:
# %bb.149:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6081:
	callq	mpfr_get_default_rounding_mode
.Ltmp6082:
# %bb.150:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6083:
	leaq	264(%rsp), %rdi
	leaq	24(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp6084:
# %bb.151:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6086:
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6087:
# %bb.152:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6088:
	movq	%rax, %rbp
	leaq	264(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6089:
# %bb.153:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6090:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp6091:
# %bb.154:                              #   in Loop: Header=BB41_134 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp6092:
	leaq	120(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp6093:
# %bb.155:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6094:
	leaq	120(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6095:
# %bb.156:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6097:
	callq	mpfr_get_default_rounding_mode
.Ltmp6098:
	leaq	64(%rsp), %r12
# %bb.157:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6099:
	leaq	120(%rsp), %rdi
	movq	%r12, %rsi
	leaq	264(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp6100:
# %bb.158:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6102:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp6103:
# %bb.159:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6104:
	movq	%rax, %r12
	leaq	120(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6105:
# %bb.160:                              #   in Loop: Header=BB41_134 Depth=1
	movq	%rax, %rbp
	cmpq	%rax, %r12
	je	.LBB41_164
# %bb.161:                              #   in Loop: Header=BB41_134 Depth=1
	cmpq	$0, 88(%rsp)
	je	.LBB41_163
# %bb.162:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6106:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6107:
.LBB41_163:                             #   in Loop: Header=BB41_134 Depth=1
.Ltmp6108:
	leaq	64(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp6109:
.LBB41_164:                             #   in Loop: Header=BB41_134 Depth=1
.Ltmp6110:
	callq	mpfr_get_default_rounding_mode
.Ltmp6111:
# %bb.165:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6112:
	leaq	64(%rsp), %rdi
	leaq	120(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp6113:
# %bb.166:                              #   in Loop: Header=BB41_134 Depth=1
	cmpq	$0, 144(%rsp)
	je	.LBB41_168
# %bb.167:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6115:
	leaq	120(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6116:
.LBB41_168:                             #   in Loop: Header=BB41_134 Depth=1
	cmpq	$0, 288(%rsp)
	je	.LBB41_170
# %bb.169:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6118:
	leaq	264(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6119:
.LBB41_170:                             #   in Loop: Header=BB41_134 Depth=1
	cmpq	$0, 48(%rsp)
	je	.LBB41_133
# %bb.171:                              #   in Loop: Header=BB41_134 Depth=1
.Ltmp6121:
	leaq	24(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6122:
	jmp	.LBB41_133
.LBB41_172:
.Ltmp6124:
	callq	mpfr_get_default_rounding_mode
.Ltmp6125:
# %bb.173:
.Ltmp6126:
	movl	%eax, %ebp
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6127:
# %bb.174:
.Ltmp6128:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp6129:
# %bb.175:
.Ltmp6130:
	movl	%eax, %r15d
	leaq	24(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp6131:
# %bb.176:
.Ltmp6132:
	leaq	24(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp6133:
# %bb.177:
.Ltmp6135:
	leaq	24(%rsp), %rdi
	leaq	64(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp6136:
# %bb.178:
.Ltmp6138:
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6139:
# %bb.179:
.Ltmp6140:
	movq	%rax, %r12
	leaq	24(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6141:
# %bb.180:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB41_184
# %bb.181:
	cmpq	$0, 88(%rsp)
	je	.LBB41_183
# %bb.182:
.Ltmp6142:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6143:
.LBB41_183:
.Ltmp6144:
	leaq	64(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp6145:
.LBB41_184:
.Ltmp6146:
	callq	mpfr_get_default_rounding_mode
.Ltmp6147:
# %bb.185:
.Ltmp6148:
	leaq	64(%rsp), %rdi
	leaq	24(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp6149:
# %bb.186:
	cmpq	$0, 48(%rsp)
	je	.LBB41_188
# %bb.187:
.Ltmp6151:
	leaq	24(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6152:
.LBB41_188:
	callq	omp_get_wtime
	vmovsd	%xmm0, 96(%rsp)                 # 8-byte Spill
.Ltmp6154:
	leaq	216(%rsp), %rsi
	movq	192(%rsp), %r15                 # 8-byte Reload
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	16(%rsp), %r14                  # 8-byte Reload
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp6155:
# %bb.189:
.Ltmp6156:
	leaq	216(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp6157:
# %bb.190:
.Ltmp6158:
	leaq	216(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp6159:
# %bb.191:
.Ltmp6160:
	leaq	216(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp6161:
# %bb.192:
.Ltmp6162:
	leaq	216(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp6163:
# %bb.193:
.Ltmp6164:
	leaq	216(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp6165:
# %bb.194:
.Ltmp6166:
	leaq	216(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp6167:
# %bb.195:
.Ltmp6168:
	leaq	216(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp6169:
# %bb.196:
.Ltmp6170:
	leaq	216(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp6171:
# %bb.197:
.Ltmp6172:
	leaq	216(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKT_PS8_PS7_
.Ltmp6173:
# %bb.198:
	callq	omp_get_wtime
	vsubsd	96(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp6175:
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6176:
# %bb.199:
	movq	%rax, %r12
	movq	8(%rsp), %rax                   # 8-byte Reload
	leaq	96(%rax), %r15
.Ltmp6177:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp6178:
# %bb.200:
.Ltmp6179:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp6180:
# %bb.201:
	movl	%eax, %ebp
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp6181:
	leaq	24(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp6182:
# %bb.202:
.Ltmp6183:
	leaq	24(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp6184:
# %bb.203:
.Ltmp6186:
	callq	mpfr_get_default_rounding_mode
.Ltmp6187:
# %bb.204:
.Ltmp6188:
	leaq	24(%rsp), %rdi
	leaq	64(%rsp), %rsi
	movq	%r15, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp6189:
# %bb.205:
.Ltmp6191:
	leaq	24(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp6192:
# %bb.206:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 120(%rsp)
	cmpq	$0, 48(%rsp)
	je	.LBB41_208
# %bb.207:
.Ltmp6194:
	leaq	24(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6195:
.LBB41_208:
	leaq	432(%rsp), %r15
	movq	%r15, 416(%rsp)
	movl	$2037413985, 432(%rsp)          # imm = 0x79707861
	movw	$32, 436(%rsp)
	movq	$5, 424(%rsp)
.Ltmp6197:
	leaq	416(%rsp), %rdi
	leaq	120(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp6198:
# %bb.209:
	movq	416(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB41_211
# %bb.210:
	callq	_ZdlPv
.LBB41_211:
	cmpq	$0, 88(%rsp)
	je	.LBB41_213
# %bb.212:
.Ltmp6200:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6201:
.LBB41_213:
	movslq	4(%rsp), %r12
	movq	%r12, %r15
	shlq	$4, %r15
	testq	%r12, %r12
	movq	$-1, %rdi
	cmovnsq	%r15, %rdi
	callq	_Znam
	movq	%rax, %rbp
	testq	%r12, %r12
	movq	16(%rsp), %r14                  # 8-byte Reload
	je	.LBB41_217
# %bb.214:
	movq	%rbp, %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_intel_fast_memset@PLT
	movq	232(%rsp), %r13                 # 8-byte Reload
	testl	%r12d, %r12d
	jle	.LBB41_217
# %bb.215:
	xorl	%r15d, %r15d
	xorl	%r12d, %r12d
	.p2align	4, 0x90
.LBB41_216:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, (%r14,%r15)
	vmovupd	(%r14,%r15), %xmm0
	vmovupd	%xmm0, (%rbp,%r15)
	incq	%r12
	movslq	4(%rsp), %rax
	addq	$16, %r15
	addq	$32, %r13
	cmpq	%rax, %r12
	jl	.LBB41_216
.LBB41_217:
	leaq	4(%rsp), %rdi
	leaq	216(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%rbp, 160(%rsp)                 # 8-byte Spill
	movq	%rbp, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	64(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	cmpl	$0, 4(%rsp)
	jle	.LBB41_258
# %bb.218:
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	leaq	24(%rsp), %r13
	movq	160(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB41_220
	.p2align	4, 0x90
.LBB41_219:                             #   in Loop: Header=BB41_220 Depth=1
	movq	96(%rsp), %rdx                  # 8-byte Reload
	incq	%rdx
	movslq	4(%rsp), %rax
	movq	240(%rsp), %rsi                 # 8-byte Reload
	addq	$16, %rsi
	movq	%rdx, %rcx
	movq	%rdx, 96(%rsp)                  # 8-byte Spill
	cmpq	%rax, %rdx
	jge	.LBB41_258
.LBB41_220:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %r14
	movq	192(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %r15d
.Ltmp6203:
	leaq	120(%rsp), %rdi
	movq	%rsi, 240(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp6204:
# %bb.221:                              #   in Loop: Header=BB41_220 Depth=1
	movq	96(%rsp), %rax                  # 8-byte Reload
	addl	%r15d, %eax
	addl	$8, %eax
	movslq	%eax, %r15
	shlq	$5, %r15
	addq	8(%rsp), %r15                   # 8-byte Folded Reload
.Ltmp6206:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp6207:
# %bb.222:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6208:
	movq	%rax, %rbp
	leaq	120(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6209:
# %bb.223:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6210:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp6211:
# %bb.224:                              #   in Loop: Header=BB41_220 Depth=1
	movl	%eax, %r13d
	cmpq	%r12, %rbp
	cmovgq	%rbp, %r12
.Ltmp6212:
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp6213:
# %bb.225:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6214:
	movq	%r14, %rdi
	xorl	%esi, %esi
	movl	%r13d, %edx
	callq	mpfr_set_si
.Ltmp6215:
# %bb.226:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6217:
	callq	mpfr_get_default_rounding_mode
.Ltmp6218:
# %bb.227:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6219:
	movq	%r14, %r13
	movq	%r14, %rdi
	movq	%r15, %rsi
	leaq	120(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp6220:
# %bb.228:                              #   in Loop: Header=BB41_220 Depth=1
	cmpq	$0, 144(%rsp)
	je	.LBB41_230
# %bb.229:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6222:
	leaq	120(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6223:
.LBB41_230:                             #   in Loop: Header=BB41_220 Depth=1
.Ltmp6225:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp6226:
# %bb.231:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6227:
	movq	%rax, %r15
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp6228:
# %bb.232:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6229:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp6230:
# %bb.233:                              #   in Loop: Header=BB41_220 Depth=1
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp6231:
	leaq	264(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp6232:
# %bb.234:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6233:
	leaq	264(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp6234:
# %bb.235:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6236:
	callq	mpfr_get_default_rounding_mode
.Ltmp6237:
# %bb.236:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6238:
	leaq	264(%rsp), %rdi
	movq	%r13, %rsi
	movq	%r13, %rdx
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp6239:
# %bb.237:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6241:
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6242:
# %bb.238:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6243:
	movq	%rax, %r15
	leaq	264(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6244:
# %bb.239:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6245:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp6246:
# %bb.240:                              #   in Loop: Header=BB41_220 Depth=1
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp6247:
	leaq	120(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp6248:
# %bb.241:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6249:
	leaq	120(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp6250:
# %bb.242:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6252:
	callq	mpfr_get_default_rounding_mode
.Ltmp6253:
	leaq	64(%rsp), %r14
# %bb.243:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6254:
	leaq	120(%rsp), %rdi
	movq	%r14, %rsi
	leaq	264(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp6255:
# %bb.244:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6257:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp6258:
# %bb.245:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6259:
	movq	%rax, %r12
	leaq	120(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6260:
# %bb.246:                              #   in Loop: Header=BB41_220 Depth=1
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB41_250
# %bb.247:                              #   in Loop: Header=BB41_220 Depth=1
	cmpq	$0, 88(%rsp)
	je	.LBB41_249
# %bb.248:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6261:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6262:
.LBB41_249:                             #   in Loop: Header=BB41_220 Depth=1
.Ltmp6263:
	leaq	64(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp6264:
.LBB41_250:                             #   in Loop: Header=BB41_220 Depth=1
.Ltmp6265:
	callq	mpfr_get_default_rounding_mode
.Ltmp6266:
# %bb.251:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6267:
	leaq	64(%rsp), %rdi
	leaq	120(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp6268:
# %bb.252:                              #   in Loop: Header=BB41_220 Depth=1
	cmpq	$0, 144(%rsp)
	je	.LBB41_254
# %bb.253:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6270:
	leaq	120(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6271:
.LBB41_254:                             #   in Loop: Header=BB41_220 Depth=1
	cmpq	$0, 288(%rsp)
	je	.LBB41_256
# %bb.255:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6273:
	leaq	264(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6274:
.LBB41_256:                             #   in Loop: Header=BB41_220 Depth=1
	cmpq	$0, 48(%rsp)
	je	.LBB41_219
# %bb.257:                              #   in Loop: Header=BB41_220 Depth=1
.Ltmp6276:
	movq	%r13, %rdi
	callq	mpfr_clear
.Ltmp6277:
	jmp	.LBB41_219
.LBB41_258:
.Ltmp6279:
	callq	mpfr_get_default_rounding_mode
.Ltmp6280:
# %bb.259:
.Ltmp6281:
	movl	%eax, %ebp
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6282:
# %bb.260:
.Ltmp6283:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp6284:
# %bb.261:
.Ltmp6285:
	movl	%eax, %r15d
	leaq	24(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp6286:
# %bb.262:
.Ltmp6287:
	leaq	24(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp6288:
# %bb.263:
.Ltmp6290:
	leaq	24(%rsp), %rdi
	leaq	64(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp6291:
# %bb.264:
.Ltmp6293:
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6294:
# %bb.265:
.Ltmp6295:
	movq	%rax, %r12
	leaq	24(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6296:
# %bb.266:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB41_270
# %bb.267:
	cmpq	$0, 88(%rsp)
	je	.LBB41_269
# %bb.268:
.Ltmp6297:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6298:
.LBB41_269:
.Ltmp6299:
	leaq	64(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp6300:
.LBB41_270:
.Ltmp6301:
	callq	mpfr_get_default_rounding_mode
.Ltmp6302:
# %bb.271:
.Ltmp6303:
	leaq	64(%rsp), %rdi
	leaq	24(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp6304:
# %bb.272:
	cmpq	$0, 48(%rsp)
	je	.LBB41_274
# %bb.273:
.Ltmp6306:
	leaq	24(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6307:
.LBB41_274:
	callq	omp_get_wtime
	vmovsd	%xmm0, 96(%rsp)                 # 8-byte Spill
.Ltmp6309:
	leaq	4(%rsp), %rdi
	leaq	216(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	16(%rsp), %r14                  # 8-byte Reload
	movq	%r14, %r8
	movq	160(%rsp), %r15                 # 8-byte Reload
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6310:
# %bb.275:
.Ltmp6311:
	leaq	4(%rsp), %rdi
	leaq	216(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6312:
# %bb.276:
.Ltmp6313:
	leaq	4(%rsp), %rdi
	leaq	216(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6314:
# %bb.277:
.Ltmp6315:
	leaq	4(%rsp), %rdi
	leaq	216(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6316:
# %bb.278:
.Ltmp6317:
	leaq	4(%rsp), %rdi
	leaq	216(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6318:
# %bb.279:
.Ltmp6319:
	leaq	4(%rsp), %rdi
	leaq	216(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6320:
# %bb.280:
.Ltmp6321:
	leaq	4(%rsp), %rdi
	leaq	216(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6322:
# %bb.281:
.Ltmp6323:
	leaq	4(%rsp), %rdi
	leaq	216(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6324:
# %bb.282:
.Ltmp6325:
	leaq	4(%rsp), %rdi
	leaq	216(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6326:
# %bb.283:
.Ltmp6327:
	leaq	4(%rsp), %rdi
	leaq	216(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6328:
# %bb.284:
	callq	omp_get_wtime
	vsubsd	96(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp6330:
	leaq	64(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6331:
# %bb.285:
	movq	%rax, %r15
	movq	8(%rsp), %rdi                   # 8-byte Reload
	subq	$-128, %rdi
.Ltmp6332:
	movq	%rdi, %r13
	callq	mpfr_get_prec
.Ltmp6333:
# %bb.286:
.Ltmp6334:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp6335:
# %bb.287:
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp6336:
	leaq	24(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp6337:
# %bb.288:
.Ltmp6338:
	leaq	24(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp6339:
# %bb.289:
.Ltmp6341:
	callq	mpfr_get_default_rounding_mode
.Ltmp6342:
# %bb.290:
.Ltmp6343:
	leaq	24(%rsp), %rdi
	leaq	64(%rsp), %rsi
	movq	%r13, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp6344:
# %bb.291:
.Ltmp6346:
	leaq	24(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE1EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp6347:
# %bb.292:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 120(%rsp)
	cmpq	$0, 48(%rsp)
	je	.LBB41_294
# %bb.293:
.Ltmp6349:
	leaq	24(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6350:
.LBB41_294:
	leaq	400(%rsp), %r15
	movq	%r15, 384(%rsp)
	movl	$1986880871, 400(%rsp)          # imm = 0x766D6567
	movw	$32, 404(%rsp)
	movq	$5, 392(%rsp)
.Ltmp6352:
	leaq	384(%rsp), %rdi
	leaq	120(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp6353:
# %bb.295:
	movq	384(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB41_297
# %bb.296:
	callq	_ZdlPv
.LBB41_297:
	movq	160(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	cmpq	$0, 88(%rsp)
	je	.LBB41_299
# %bb.298:
.Ltmp6355:
	leaq	64(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6356:
.LBB41_299:
	movq	%rbx, %rdi
	callq	_ZdaPv
	movq	16(%rsp), %rdi                  # 8-byte Reload
	callq	_ZdaPv
	xorl	%eax, %eax
	vcvtsi2ss	%eax, %xmm2, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vblendps	$1, %xmm0, %xmm1, %xmm0         # xmm0 = xmm0[0],xmm1[1,2,3]
	movl	$2097152, %r14d                 # imm = 0x200000
	leaq	68(%rsp), %rbx
	vxorps	%xmm15, %xmm15, %xmm15
	movl	$1, %ebp
	jmp	.LBB41_302
.LBB41_300:                             #   in Loop: Header=BB41_302 Depth=1
	vmovups	64(%rsp), %xmm2                 # AlignMOV convert to UnAlignMOV 
	vmovddup	72(%rsp), %xmm4                 # xmm4 = mem[0,0]
	.p2align	4, 0x90
.LBB41_301:                             #   in Loop: Header=BB41_302 Depth=1
	vmovups	240(%rsp), %xmm5                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm2, %xmm5, %xmm0
	vsubss	%xmm5, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm3
	vsubss	%xmm3, %xmm5, %xmm3
	vsubss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm3, %xmm3
	vmovshdup	%xmm5, %xmm1            # xmm1 = xmm5[1,1,3,3]
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm5
	vsubss	%xmm1, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm2
	vaddss	%xmm3, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm6
	vsubss	%xmm6, %xmm1, %xmm7
	vsubss	%xmm7, %xmm5, %xmm5
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm5, %xmm3
	vmovups	96(%rsp), %xmm10                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm4, %xmm10, %xmm5
	vsubss	%xmm10, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm10, %xmm7
	vsubss	%xmm6, %xmm4, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm2, %xmm5, %xmm7
	vsubss	%xmm5, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vsubss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vaddss	%xmm3, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm5, %xmm9
	vsubss	%xmm9, %xmm7, %xmm7
	vsubss	%xmm8, %xmm3, %xmm3
	vaddss	%xmm3, %xmm7, %xmm3
	vmovshdup	%xmm10, %xmm7           # xmm7 = xmm10[1,1,3,3]
	vmovshdup	%xmm4, %xmm4            # xmm4 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm7, %xmm4
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm4
	vsubss	%xmm4, %xmm5, %xmm3
	vaddss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm4, %xmm1, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm4, %xmm1, %xmm4
	vinsertps	$16, %xmm3, %xmm4, %xmm1 # xmm1 = xmm4[0],xmm3[0],xmm4[2,3]
	vaddss	%xmm5, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm5
	vinsertps	$16, %xmm5, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm5[0],xmm2[2,3]
	leal	-1(%r14), %eax
	cmpl	$1, %r14d
	movl	%eax, %r14d
	jbe	.LBB41_314
.LBB41_302:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB41_309 Depth 2
	vmovups	%xmm0, 240(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm1, 96(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vxorps	%xmm11, %xmm11, %xmm11
	vcvtsi2ss	%r14d, %xmm11, %xmm5
	vmulss	%xmm5, %xmm5, %xmm0
	vmulss	%xmm5, %xmm15, %xmm1
	vxorps	%xmm2, %xmm2, %xmm2
	vfmsub213ss	%xmm1, %xmm5, %xmm2     # xmm2 = (xmm5 * xmm2) - xmm1
	vmulss	%xmm5, %xmm15, %xmm4
	vmovaps	%xmm5, %xmm3
	vaddss	%xmm4, %xmm1, %xmm6
	vsubss	%xmm1, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm1, %xmm8
	vaddss	%xmm1, %xmm15, %xmm9
	vsubss	%xmm1, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm1, %xmm11
	vfmadd231ss	%xmm15, %xmm15, %xmm1   # xmm1 = (xmm15 * xmm15) + xmm1
	vfmadd231ss	%xmm15, %xmm15, %xmm1   # xmm1 = (xmm15 * xmm15) + xmm1
	vfmadd231ss	%xmm5, %xmm15, %xmm1    # xmm1 = (xmm15 * xmm5) + xmm1
	vfmsub213ss	%xmm0, %xmm5, %xmm5     # xmm5 = (xmm5 * xmm5) - xmm0
	vfmsub213ss	%xmm4, %xmm15, %xmm3    # xmm3 = (xmm15 * xmm3) - xmm4
	vsubss	%xmm7, %xmm4, %xmm7
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm5, %xmm6, %xmm8
	vsubss	%xmm6, %xmm8, %xmm12
	vsubss	%xmm12, %xmm8, %xmm13
	vsubss	%xmm13, %xmm6, %xmm6
	vsubss	%xmm12, %xmm5, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vsubss	%xmm10, %xmm15, %xmm6
	vaddss	%xmm6, %xmm11, %xmm6
	vaddss	%xmm4, %xmm9, %xmm10
	vsubss	%xmm9, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm9, %xmm9
	vsubss	%xmm11, %xmm4, %xmm4
	vaddss	%xmm4, %xmm9, %xmm4
	vaddss	%xmm2, %xmm10, %xmm9
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm12, %xmm10, %xmm10
	vsubss	%xmm11, %xmm2, %xmm11
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm3, %xmm9, %xmm11
	vsubss	%xmm9, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm9, %xmm9
	vsubss	%xmm12, %xmm3, %xmm12
	vaddss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm7, %xmm11, %xmm12
	vsubss	%xmm11, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm11, %xmm11
	vsubss	%xmm13, %xmm7, %xmm7
	vaddss	%xmm7, %xmm11, %xmm7
	vaddss	%xmm5, %xmm12, %xmm11
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm11, %xmm14
	vsubss	%xmm14, %xmm12, %xmm12
	vsubss	%xmm13, %xmm5, %xmm5
	vaddss	%xmm5, %xmm12, %xmm5
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm15, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm1, %xmm10, %xmm1
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm11, %xmm2
	vsubss	%xmm2, %xmm11, %xmm3
	vaddss	%xmm1, %xmm3, %xmm3
	vaddss	%xmm2, %xmm8, %xmm1
	vsubss	%xmm1, %xmm8, %xmm4
	vaddss	%xmm2, %xmm4, %xmm6
	vaddss	%xmm1, %xmm0, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm9
	vmulss	%xmm4, %xmm4, %xmm0
	vmulss	%xmm4, %xmm9, %xmm5
	vmulss	%xmm4, %xmm9, %xmm7
	vmovaps	%xmm4, %xmm1
	vfmsub213ss	%xmm7, %xmm9, %xmm1     # xmm1 = (xmm9 * xmm1) - xmm7
	vmulss	%xmm9, %xmm9, %xmm8
	vmovaps	%xmm9, %xmm2
	vmulss	%xmm3, %xmm4, %xmm10
	vfmadd231ss	%xmm6, %xmm9, %xmm10    # xmm10 = (xmm9 * xmm6) + xmm10
	vfmadd231ss	%xmm9, %xmm6, %xmm10    # xmm10 = (xmm6 * xmm9) + xmm10
	vfmsub213ss	%xmm5, %xmm4, %xmm9     # xmm9 = (xmm4 * xmm9) - xmm5
	vmulss	%xmm6, %xmm4, %xmm11
	vmulss	%xmm4, %xmm6, %xmm12
	vmovaps	%xmm4, %xmm13
	vfmsub213ss	%xmm12, %xmm6, %xmm13   # xmm13 = (xmm6 * xmm13) - xmm12
	vfmsub213ss	%xmm11, %xmm4, %xmm6    # xmm6 = (xmm4 * xmm6) - xmm11
	vfmadd231ss	%xmm4, %xmm3, %xmm10    # xmm10 = (xmm3 * xmm4) + xmm10
	vfmsub213ss	%xmm0, %xmm4, %xmm4     # xmm4 = (xmm4 * xmm4) - xmm0
	vaddss	%xmm7, %xmm5, %xmm3
	vsubss	%xmm5, %xmm3, %xmm14
	vsubss	%xmm14, %xmm3, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm14, %xmm7, %xmm7
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm4, %xmm3, %xmm7
	vsubss	%xmm3, %xmm7, %xmm14
	vsubss	%xmm14, %xmm7, %xmm15
	vsubss	%xmm15, %xmm3, %xmm3
	vsubss	%xmm14, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vfmsub213ss	%xmm8, %xmm2, %xmm2     # xmm2 = (xmm2 * xmm2) - xmm8
	vaddss	%xmm8, %xmm11, %xmm4
	vsubss	%xmm11, %xmm4, %xmm14
	vsubss	%xmm14, %xmm4, %xmm15
	vsubss	%xmm15, %xmm11, %xmm11
	vsubss	%xmm14, %xmm8, %xmm8
	vaddss	%xmm8, %xmm11, %xmm8
	vaddss	%xmm4, %xmm12, %xmm11
	vsubss	%xmm4, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm4, %xmm12, %xmm4
	vaddss	%xmm9, %xmm11, %xmm12
	vsubss	%xmm11, %xmm12, %xmm14
	vsubss	%xmm14, %xmm12, %xmm15
	vsubss	%xmm15, %xmm11, %xmm11
	vsubss	%xmm14, %xmm9, %xmm9
	vaddss	%xmm9, %xmm11, %xmm9
	vaddss	%xmm1, %xmm12, %xmm11
	vsubss	%xmm12, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm1, %xmm1
	vaddss	%xmm1, %xmm12, %xmm1
	vaddss	%xmm5, %xmm11, %xmm12
	vsubss	%xmm11, %xmm12, %xmm14
	vsubss	%xmm14, %xmm12, %xmm15
	vsubss	%xmm15, %xmm11, %xmm11
	vsubss	%xmm14, %xmm5, %xmm5
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm3, %xmm12, %xmm11
	vsubss	%xmm12, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vxorps	%xmm15, %xmm15, %xmm15
	vsubss	%xmm14, %xmm3, %xmm3
	vaddss	%xmm3, %xmm12, %xmm3
	vaddss	%xmm6, %xmm10, %xmm6
	vaddss	%xmm2, %xmm6, %xmm2
	vaddss	%xmm2, %xmm13, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm4, %xmm2, %xmm2
	vcvtsi2ss	%ebp, %xmm0, %xmm8
	vaddss	%xmm2, %xmm9, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm11, %xmm2
	vsubss	%xmm2, %xmm11, %xmm3
	vaddss	%xmm1, %xmm3, %xmm6
	vaddss	%xmm2, %xmm7, %xmm3
	vsubss	%xmm3, %xmm7, %xmm1
	vaddss	%xmm2, %xmm1, %xmm13
	vaddss	%xmm3, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm5
	vucomiss	.LCPI41_4(%rip), %xmm1
	setnp	%al
	sete	%cl
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm4, %xmm4, %xmm4
	testb	%al, %cl
	jne	.LBB41_301
# %bb.303:                              #   in Loop: Header=BB41_302 Depth=1
	vucomiss	.LCPI41_5(%rip), %xmm1
	setnp	%al
	sete	%cl
	vbroadcastss	.LCPI41_3(%rip), %xmm4  # xmm4 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vmovaps	%xmm4, %xmm2
	testb	%al, %cl
	jne	.LBB41_301
# %bb.304:                              #   in Loop: Header=BB41_302 Depth=1
	vucomiss	%xmm15, %xmm1
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB41_306
# %bb.305:                              #   in Loop: Header=BB41_302 Depth=1
	vbroadcastss	%xmm1, %xmm0
	vandps	544(%rsp), %xmm0, %xmm0         # 16-byte Folded Reload
	vbroadcastss	.LCPI41_4(%rip), %xmm1  # xmm1 = [+Inf,+Inf,+Inf,+Inf]
	vorps	%xmm1, %xmm0, %xmm2
	vmovaps	%xmm2, %xmm4
	jmp	.LBB41_301
.LBB41_306:                             #   in Loop: Header=BB41_302 Depth=1
	vdivss	%xmm1, %xmm8, %xmm0
	vmulss	%xmm1, %xmm0, %xmm3
	vmovss	%xmm3, 116(%rsp)                # 4-byte Spill
	vmovaps	%xmm1, %xmm2
	vfmsub213ss	%xmm3, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm2) - xmm3
	vmulss	%xmm5, %xmm0, %xmm3
	vmovss	%xmm6, 192(%rsp)                # 4-byte Spill
	vmovaps	%xmm5, %xmm6
	vmovss	%xmm5, 8(%rsp)                  # 4-byte Spill
	vfmsub213ss	%xmm3, %xmm0, %xmm6     # xmm6 = (xmm0 * xmm6) - xmm3
	vmulss	%xmm1, %xmm15, %xmm14
	vaddss	%xmm3, %xmm14, %xmm7
	vsubss	%xmm3, %xmm7, %xmm4
	vsubss	%xmm4, %xmm7, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vsubss	%xmm4, %xmm14, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vmovss	%xmm3, 16(%rsp)                 # 4-byte Spill
	vaddss	%xmm2, %xmm7, %xmm4
	vsubss	%xmm7, %xmm4, %xmm9
	vsubss	%xmm9, %xmm4, %xmm10
	vsubss	%xmm10, %xmm7, %xmm7
	vsubss	%xmm9, %xmm2, %xmm2
	vaddss	%xmm2, %xmm7, %xmm11
	vmulss	%xmm0, %xmm13, %xmm2
	vmovaps	%xmm13, %xmm9
	vfmsub213ss	%xmm2, %xmm0, %xmm9     # xmm9 = (xmm0 * xmm9) - xmm2
	vmulss	%xmm5, %xmm15, %xmm12
	vaddss	%xmm2, %xmm12, %xmm10
	vsubss	%xmm2, %xmm10, %xmm7
	vmovaps	%xmm13, %xmm5
	vmovss	%xmm13, 160(%rsp)               # 4-byte Spill
	vsubss	%xmm7, %xmm10, %xmm13
	vsubss	%xmm13, %xmm2, %xmm2
	vsubss	%xmm7, %xmm12, %xmm7
	vmovss	%xmm12, 56(%rsp)                # 4-byte Spill
	vaddss	%xmm7, %xmm2, %xmm2
	vmovss	%xmm2, 112(%rsp)                # 4-byte Spill
	vmovaps	%xmm14, %xmm3
	vaddss	%xmm14, %xmm10, %xmm2
	vsubss	%xmm10, %xmm2, %xmm13
	vsubss	%xmm13, %xmm2, %xmm14
	vsubss	%xmm14, %xmm10, %xmm10
	vsubss	%xmm13, %xmm3, %xmm13
	vmovss	%xmm3, 152(%rsp)                # 4-byte Spill
	vaddss	%xmm13, %xmm10, %xmm7
	vmovss	%xmm7, 232(%rsp)                # 4-byte Spill
	vaddss	%xmm6, %xmm2, %xmm13
	vsubss	%xmm2, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm2, %xmm2
	vsubss	%xmm14, %xmm6, %xmm6
	vaddss	%xmm6, %xmm2, %xmm10
	vmovaps	%xmm1, %xmm2
	vfmsub132ss	.LCPI41_6(%rip), %xmm3, %xmm2 # xmm2 = (xmm2 * mem) - xmm3
	vaddss	%xmm2, %xmm13, %xmm6
	vsubss	%xmm13, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm13, %xmm13
	vsubss	%xmm14, %xmm2, %xmm14
	vmovups	%xmm2, 320(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm14, %xmm13, %xmm13
	vmovss	16(%rsp), %xmm3                 # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vaddss	%xmm3, %xmm6, %xmm14
	vsubss	%xmm6, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm7
	vsubss	%xmm7, %xmm6, %xmm6
	vsubss	%xmm15, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm11, %xmm14, %xmm6
	vsubss	%xmm14, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm15
	vsubss	%xmm15, %xmm14, %xmm14
	vsubss	%xmm7, %xmm11, %xmm7
	vaddss	%xmm7, %xmm14, %xmm7
	vmulss	192(%rsp), %xmm0, %xmm11        # 4-byte Folded Reload
	vfmadd231ss	.LCPI41_6(%rip), %xmm5, %xmm11 # xmm11 = (xmm5 * mem) + xmm11
	vmovss	8(%rsp), %xmm14                 # 4-byte Reload
                                        # xmm14 = mem[0],zero,zero,zero
	vfmadd231ss	.LCPI41_6(%rip), %xmm14, %xmm11 # xmm11 = (xmm14 * mem) + xmm11
	vfmadd231ss	.LCPI41_6(%rip), %xmm1, %xmm11 # xmm11 = (xmm1 * mem) + xmm11
	vaddss	%xmm9, %xmm11, %xmm9
	vmovaps	%xmm14, %xmm5
	vfmsub132ss	.LCPI41_6(%rip), %xmm12, %xmm5 # xmm5 = (xmm5 * mem) - xmm12
	vmovss	%xmm5, 16(%rsp)                 # 4-byte Spill
	vaddss	%xmm5, %xmm9, %xmm9
	vaddss	%xmm2, %xmm9, %xmm9
	vaddss	112(%rsp), %xmm9, %xmm9         # 4-byte Folded Reload
	vaddss	232(%rsp), %xmm9, %xmm9         # 4-byte Folded Reload
	vaddss	%xmm10, %xmm9, %xmm2
	vaddss	%xmm2, %xmm13, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm3
	vsubss	%xmm3, %xmm6, %xmm6
	vaddss	%xmm2, %xmm6, %xmm9
	vaddss	%xmm3, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm4
	vaddss	%xmm3, %xmm4, %xmm3
	vmovss	116(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vsubss	%xmm6, %xmm8, %xmm4
	vsubss	%xmm8, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm10
	vsubss	%xmm10, %xmm8, %xmm8
	vbroadcastss	.LCPI41_3(%rip), %xmm5  # xmm5 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm5, %xmm6, %xmm6
	vsubss	%xmm7, %xmm6, %xmm6
	vxorps	%xmm5, %xmm2, %xmm7
	vmovaps	%xmm5, %xmm12
	vaddss	%xmm6, %xmm8, %xmm6
	vxorps	%xmm5, %xmm5, %xmm5
	vsubss	%xmm2, %xmm5, %xmm2
	vsubss	.LCPI41_6(%rip), %xmm2, %xmm8
	vsubss	%xmm8, %xmm7, %xmm7
	vsubss	%xmm8, %xmm2, %xmm8
	vsubss	%xmm8, %xmm5, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm6, %xmm2, %xmm8
	vsubss	%xmm2, %xmm8, %xmm10
	vsubss	%xmm10, %xmm8, %xmm11
	vsubss	%xmm11, %xmm2, %xmm2
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm2, %xmm2
	vxorps	%xmm3, %xmm12, %xmm6
	vsubss	%xmm3, %xmm5, %xmm3
	vsubss	.LCPI41_6(%rip), %xmm3, %xmm10
	vsubss	%xmm10, %xmm6, %xmm6
	vsubss	%xmm10, %xmm3, %xmm10
	vsubss	%xmm10, %xmm5, %xmm10
	vaddss	%xmm6, %xmm10, %xmm6
	vaddss	%xmm7, %xmm3, %xmm10
	vsubss	%xmm3, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm3, %xmm3
	vsubss	%xmm11, %xmm7, %xmm7
	vaddss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm2, %xmm10, %xmm7
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm7, %xmm12
	vsubss	%xmm12, %xmm10, %xmm10
	vsubss	%xmm11, %xmm2, %xmm2
	vaddss	%xmm2, %xmm10, %xmm2
	vsubss	%xmm9, %xmm5, %xmm9
	vaddss	%xmm6, %xmm9, %xmm6
	vaddss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm2, %xmm7, %xmm3
	vsubss	%xmm3, %xmm7, %xmm6
	vaddss	%xmm2, %xmm6, %xmm2
	vmovss	%xmm2, 112(%rsp)                # 4-byte Spill
	vaddss	%xmm3, %xmm8, %xmm2
	vsubss	%xmm2, %xmm8, %xmm6
	vaddss	%xmm3, %xmm6, %xmm3
	vmovss	%xmm3, 232(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm4, %xmm13
	vsubss	%xmm13, %xmm4, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vmovss	%xmm2, 296(%rsp)                # 4-byte Spill
	vdivss	%xmm1, %xmm13, %xmm5
	vmovaps	%xmm14, %xmm6
	vmulss	%xmm5, %xmm14, %xmm2
	vfmsub213ss	%xmm2, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm6) - xmm2
	vmovss	152(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vaddss	%xmm7, %xmm2, %xmm3
	vsubss	%xmm2, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm7, %xmm11
	vaddss	%xmm4, %xmm2, %xmm8
	vmulss	%xmm1, %xmm5, %xmm4
	vmovss	%xmm4, 188(%rsp)                # 4-byte Spill
	vmovaps	%xmm1, %xmm2
	vfmsub213ss	%xmm4, %xmm5, %xmm2     # xmm2 = (xmm5 * xmm2) - xmm4
	vaddss	%xmm2, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm9
	vsubss	%xmm9, %xmm4, %xmm10
	vsubss	%xmm10, %xmm3, %xmm3
	vsubss	%xmm9, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm15
	vmovss	160(%rsp), %xmm14               # 4-byte Reload
                                        # xmm14 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm14, %xmm2
	vmovaps	%xmm14, %xmm9
	vfmsub213ss	%xmm2, %xmm5, %xmm9     # xmm9 = (xmm5 * xmm9) - xmm2
	vmovss	%xmm5, 116(%rsp)                # 4-byte Spill
	vmovss	56(%rsp), %xmm7                 # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vaddss	%xmm7, %xmm2, %xmm10
	vsubss	%xmm2, %xmm10, %xmm3
	vsubss	%xmm3, %xmm10, %xmm12
	vsubss	%xmm12, %xmm2, %xmm2
	vsubss	%xmm3, %xmm7, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 312(%rsp)                # 4-byte Spill
	vmovaps	%xmm11, %xmm3
	vaddss	%xmm11, %xmm10, %xmm2
	vsubss	%xmm10, %xmm2, %xmm12
	vsubss	%xmm12, %xmm2, %xmm11
	vsubss	%xmm11, %xmm10, %xmm10
	vsubss	%xmm12, %xmm3, %xmm11
	vaddss	%xmm11, %xmm10, %xmm3
	vmovss	%xmm3, 308(%rsp)                # 4-byte Spill
	vaddss	%xmm6, %xmm2, %xmm11
	vsubss	%xmm2, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm3
	vsubss	%xmm3, %xmm2, %xmm2
	vsubss	%xmm12, %xmm6, %xmm3
	vaddss	%xmm3, %xmm2, %xmm10
	vmovups	320(%rsp), %xmm2                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm2, %xmm11, %xmm3
	vsubss	%xmm11, %xmm3, %xmm6
	vsubss	%xmm6, %xmm3, %xmm12
	vsubss	%xmm12, %xmm11, %xmm11
	vsubss	%xmm6, %xmm2, %xmm6
	vaddss	%xmm6, %xmm11, %xmm6
	vaddss	%xmm3, %xmm8, %xmm11
	vsubss	%xmm3, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm12, %xmm8, %xmm7
	vaddss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm15, %xmm11, %xmm7
	vsubss	%xmm11, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm12
	vsubss	%xmm12, %xmm11, %xmm11
	vsubss	%xmm8, %xmm15, %xmm8
	vaddss	%xmm8, %xmm11, %xmm8
	vmulss	192(%rsp), %xmm5, %xmm11        # 4-byte Folded Reload
	vfmadd231ss	.LCPI41_6(%rip), %xmm14, %xmm11 # xmm11 = (xmm14 * mem) + xmm11
	vmovss	8(%rsp), %xmm15                 # 4-byte Reload
                                        # xmm15 = mem[0],zero,zero,zero
	vfmadd231ss	.LCPI41_6(%rip), %xmm15, %xmm11 # xmm11 = (xmm15 * mem) + xmm11
	vfmadd231ss	.LCPI41_6(%rip), %xmm1, %xmm11 # xmm11 = (xmm1 * mem) + xmm11
	vaddss	%xmm9, %xmm11, %xmm9
	vaddss	16(%rsp), %xmm9, %xmm9          # 4-byte Folded Reload
	vaddss	%xmm2, %xmm9, %xmm9
	vaddss	312(%rsp), %xmm9, %xmm9         # 4-byte Folded Reload
	vaddss	308(%rsp), %xmm9, %xmm9         # 4-byte Folded Reload
	vaddss	%xmm10, %xmm9, %xmm2
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm2, %xmm7, %xmm3
	vsubss	%xmm3, %xmm7, %xmm6
	vaddss	%xmm2, %xmm6, %xmm9
	vaddss	%xmm3, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm4
	vaddss	%xmm3, %xmm4, %xmm3
	vmovss	188(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vsubss	%xmm6, %xmm13, %xmm4
	vsubss	%xmm13, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm13, %xmm8
	vbroadcastss	.LCPI41_3(%rip), %xmm5  # xmm5 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm5, %xmm6, %xmm6
	vsubss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm6, %xmm8, %xmm6
	vmovss	296(%rsp), %xmm10               # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm10, %xmm7
	vsubss	%xmm10, %xmm7, %xmm8
	vmovaps	%xmm10, %xmm11
	vsubss	%xmm8, %xmm7, %xmm10
	vsubss	%xmm10, %xmm11, %xmm10
	vxorps	%xmm5, %xmm2, %xmm2
	vsubss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm10, %xmm2
	vaddss	%xmm6, %xmm7, %xmm8
	vsubss	%xmm7, %xmm8, %xmm10
	vsubss	%xmm10, %xmm8, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vmovss	232(%rsp), %xmm11               # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm11, %xmm7
	vsubss	%xmm11, %xmm7, %xmm10
	vmovaps	%xmm11, %xmm12
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm12, %xmm11
	vxorps	%xmm5, %xmm3, %xmm3
	vsubss	%xmm10, %xmm3, %xmm3
	vaddss	%xmm3, %xmm11, %xmm3
	vaddss	%xmm2, %xmm7, %xmm10
	vsubss	%xmm7, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm7, %xmm7
	vsubss	%xmm11, %xmm2, %xmm2
	vaddss	%xmm2, %xmm7, %xmm2
	vaddss	%xmm6, %xmm10, %xmm7
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm7, %xmm12
	vsubss	%xmm12, %xmm10, %xmm10
	vsubss	%xmm11, %xmm6, %xmm6
	vaddss	%xmm6, %xmm10, %xmm6
	vmovss	112(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm9, %xmm5, %xmm9
	vaddss	%xmm3, %xmm9, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm7, %xmm3
	vsubss	%xmm3, %xmm7, %xmm6
	vaddss	%xmm2, %xmm6, %xmm2
	vmovss	%xmm2, 232(%rsp)                # 4-byte Spill
	vaddss	%xmm3, %xmm8, %xmm2
	vsubss	%xmm2, %xmm8, %xmm6
	vaddss	%xmm3, %xmm6, %xmm3
	vmovss	%xmm3, 296(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm4, %xmm14
	vsubss	%xmm14, %xmm4, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vmovss	%xmm2, 188(%rsp)                # 4-byte Spill
	vdivss	%xmm1, %xmm14, %xmm10
	vmovaps	%xmm15, %xmm6
	vmulss	%xmm15, %xmm10, %xmm2
	vfmsub213ss	%xmm2, %xmm10, %xmm6    # xmm6 = (xmm10 * xmm6) - xmm2
	vmovss	152(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vaddss	%xmm5, %xmm2, %xmm3
	vsubss	%xmm2, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vsubss	%xmm4, %xmm5, %xmm4
	vmovaps	%xmm5, %xmm11
	vaddss	%xmm4, %xmm2, %xmm8
	vmulss	%xmm1, %xmm10, %xmm4
	vmovss	%xmm4, 312(%rsp)                # 4-byte Spill
	vmovaps	%xmm1, %xmm2
	vfmsub213ss	%xmm4, %xmm10, %xmm2    # xmm2 = (xmm10 * xmm2) - xmm4
	vaddss	%xmm2, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vsubss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm7
	vmovss	160(%rsp), %xmm9                # 4-byte Reload
                                        # xmm9 = mem[0],zero,zero,zero
	vmulss	%xmm9, %xmm10, %xmm2
	vfmsub213ss	%xmm2, %xmm10, %xmm9    # xmm9 = (xmm10 * xmm9) - xmm2
	vmovaps	%xmm10, %xmm15
	vmovss	%xmm10, 112(%rsp)               # 4-byte Spill
	vmovss	56(%rsp), %xmm5                 # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vaddss	%xmm5, %xmm2, %xmm10
	vsubss	%xmm2, %xmm10, %xmm3
	vsubss	%xmm3, %xmm10, %xmm12
	vsubss	%xmm12, %xmm2, %xmm2
	vsubss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 308(%rsp)                # 4-byte Spill
	vmovaps	%xmm11, %xmm3
	vaddss	%xmm11, %xmm10, %xmm2
	vsubss	%xmm10, %xmm2, %xmm12
	vsubss	%xmm12, %xmm2, %xmm11
	vsubss	%xmm11, %xmm10, %xmm10
	vsubss	%xmm12, %xmm3, %xmm11
	vaddss	%xmm11, %xmm10, %xmm3
	vmovss	%xmm3, 316(%rsp)                # 4-byte Spill
	vaddss	%xmm6, %xmm2, %xmm11
	vsubss	%xmm2, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm3
	vsubss	%xmm3, %xmm2, %xmm2
	vsubss	%xmm12, %xmm6, %xmm3
	vaddss	%xmm3, %xmm2, %xmm10
	vmovups	320(%rsp), %xmm2                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm2, %xmm11, %xmm3
	vsubss	%xmm11, %xmm3, %xmm6
	vsubss	%xmm6, %xmm3, %xmm12
	vsubss	%xmm12, %xmm11, %xmm11
	vsubss	%xmm6, %xmm2, %xmm6
	vaddss	%xmm6, %xmm11, %xmm6
	vaddss	%xmm3, %xmm8, %xmm11
	vsubss	%xmm3, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm3, %xmm3
	vsubss	%xmm12, %xmm8, %xmm8
	vaddss	%xmm3, %xmm8, %xmm3
	vaddss	%xmm7, %xmm11, %xmm8
	vsubss	%xmm11, %xmm8, %xmm12
	vsubss	%xmm12, %xmm8, %xmm13
	vsubss	%xmm13, %xmm11, %xmm11
	vsubss	%xmm12, %xmm7, %xmm7
	vaddss	%xmm7, %xmm11, %xmm7
	vmulss	192(%rsp), %xmm15, %xmm11       # 4-byte Folded Reload
	vmovss	160(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vfmadd231ss	.LCPI41_6(%rip), %xmm5, %xmm11 # xmm11 = (xmm5 * mem) + xmm11
	vmovss	8(%rsp), %xmm13                 # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vfmadd231ss	.LCPI41_6(%rip), %xmm13, %xmm11 # xmm11 = (xmm13 * mem) + xmm11
	vfmadd231ss	.LCPI41_6(%rip), %xmm1, %xmm11 # xmm11 = (xmm1 * mem) + xmm11
	vaddss	%xmm9, %xmm11, %xmm9
	vaddss	16(%rsp), %xmm9, %xmm9          # 4-byte Folded Reload
	vaddss	%xmm2, %xmm9, %xmm9
	vaddss	308(%rsp), %xmm9, %xmm9         # 4-byte Folded Reload
	vaddss	316(%rsp), %xmm9, %xmm9         # 4-byte Folded Reload
	vaddss	%xmm10, %xmm9, %xmm2
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm3
	vsubss	%xmm3, %xmm8, %xmm6
	vaddss	%xmm2, %xmm6, %xmm9
	vaddss	%xmm3, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm4
	vaddss	%xmm3, %xmm4, %xmm3
	vmovss	312(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vsubss	%xmm6, %xmm14, %xmm4
	vsubss	%xmm14, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm14, %xmm8
	vbroadcastss	.LCPI41_3(%rip), %xmm5  # xmm5 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm5, %xmm6, %xmm6
	vsubss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm6, %xmm8, %xmm6
	vmovss	188(%rsp), %xmm10               # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm10, %xmm7
	vsubss	%xmm10, %xmm7, %xmm8
	vmovaps	%xmm10, %xmm11
	vsubss	%xmm8, %xmm7, %xmm10
	vsubss	%xmm10, %xmm11, %xmm10
	vxorps	%xmm5, %xmm2, %xmm2
	vsubss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm10, %xmm2
	vaddss	%xmm6, %xmm7, %xmm8
	vsubss	%xmm7, %xmm8, %xmm10
	vsubss	%xmm10, %xmm8, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vmovss	296(%rsp), %xmm11               # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm11, %xmm7
	vsubss	%xmm11, %xmm7, %xmm10
	vmovaps	%xmm11, %xmm12
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm12, %xmm11
	vxorps	%xmm5, %xmm3, %xmm3
	vsubss	%xmm10, %xmm3, %xmm3
	vaddss	%xmm3, %xmm11, %xmm3
	vaddss	%xmm2, %xmm7, %xmm10
	vsubss	%xmm7, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm7, %xmm7
	vsubss	%xmm11, %xmm2, %xmm2
	vaddss	%xmm2, %xmm7, %xmm2
	vaddss	%xmm6, %xmm10, %xmm7
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm7, %xmm12
	vsubss	%xmm12, %xmm10, %xmm10
	vsubss	%xmm11, %xmm6, %xmm6
	vaddss	%xmm6, %xmm10, %xmm6
	vmovss	232(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm9, %xmm5, %xmm9
	vaddss	%xmm3, %xmm9, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm7, %xmm3
	vsubss	%xmm3, %xmm7, %xmm6
	vaddss	%xmm2, %xmm6, %xmm2
	vmovss	%xmm2, 232(%rsp)                # 4-byte Spill
	vaddss	%xmm3, %xmm8, %xmm2
	vsubss	%xmm2, %xmm8, %xmm6
	vaddss	%xmm3, %xmm6, %xmm3
	vmovss	%xmm3, 296(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm4, %xmm15
	vsubss	%xmm15, %xmm4, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vmovss	%xmm2, 188(%rsp)                # 4-byte Spill
	vdivss	%xmm1, %xmm15, %xmm11
	vmovaps	%xmm13, %xmm2
	vmulss	%xmm13, %xmm11, %xmm3
	vfmsub213ss	%xmm3, %xmm11, %xmm2    # xmm2 = (xmm11 * xmm2) - xmm3
	vmovss	152(%rsp), %xmm5                # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vaddss	%xmm5, %xmm3, %xmm6
	vsubss	%xmm3, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm4, %xmm5, %xmm4
	vmovaps	%xmm5, %xmm14
	vaddss	%xmm4, %xmm3, %xmm12
	vmulss	%xmm1, %xmm11, %xmm4
	vmovaps	%xmm1, %xmm3
	vfmsub213ss	%xmm4, %xmm11, %xmm3    # xmm3 = (xmm11 * xmm3) - xmm4
	vaddss	%xmm3, %xmm6, %xmm9
	vsubss	%xmm6, %xmm9, %xmm8
	vsubss	%xmm8, %xmm9, %xmm10
	vsubss	%xmm10, %xmm6, %xmm6
	vsubss	%xmm8, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm10
	vmovss	160(%rsp), %xmm3                # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm11, %xmm6
	vfmsub213ss	%xmm6, %xmm11, %xmm3    # xmm3 = (xmm11 * xmm3) - xmm6
	vmovss	56(%rsp), %xmm13                # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vaddss	%xmm6, %xmm13, %xmm5
	vsubss	%xmm6, %xmm5, %xmm8
	vsubss	%xmm8, %xmm5, %xmm7
	vsubss	%xmm7, %xmm6, %xmm6
	vsubss	%xmm8, %xmm13, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vmovss	%xmm6, 56(%rsp)                 # 4-byte Spill
	vaddss	%xmm5, %xmm14, %xmm7
	vsubss	%xmm5, %xmm7, %xmm6
	vsubss	%xmm6, %xmm7, %xmm13
	vsubss	%xmm13, %xmm5, %xmm5
	vsubss	%xmm6, %xmm14, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, 152(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm13
	vsubss	%xmm13, %xmm5, %xmm8
	vsubss	%xmm8, %xmm7, %xmm7
	vsubss	%xmm13, %xmm2, %xmm2
	vaddss	%xmm2, %xmm7, %xmm6
	vmovups	320(%rsp), %xmm2                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm2, %xmm5, %xmm7
	vsubss	%xmm5, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm13
	vsubss	%xmm13, %xmm5, %xmm5
	vsubss	%xmm8, %xmm2, %xmm8
	vaddss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm7, %xmm12, %xmm8
	vsubss	%xmm7, %xmm8, %xmm13
	vsubss	%xmm13, %xmm8, %xmm14
	vsubss	%xmm14, %xmm7, %xmm7
	vsubss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm7, %xmm12, %xmm7
	vaddss	%xmm10, %xmm8, %xmm12
	vsubss	%xmm8, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm8, %xmm8
	vsubss	%xmm13, %xmm10, %xmm10
	vaddss	%xmm10, %xmm8, %xmm8
	vmulss	192(%rsp), %xmm11, %xmm10       # 4-byte Folded Reload
	vmovss	160(%rsp), %xmm13               # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vfmadd231ss	.LCPI41_6(%rip), %xmm13, %xmm10 # xmm10 = (xmm13 * mem) + xmm10
	vmovss	8(%rsp), %xmm13                 # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vfmadd231ss	.LCPI41_6(%rip), %xmm13, %xmm10 # xmm10 = (xmm13 * mem) + xmm10
	vfmadd231ss	.LCPI41_6(%rip), %xmm1, %xmm10 # xmm10 = (xmm1 * mem) + xmm10
	vaddss	%xmm3, %xmm10, %xmm1
	vaddss	16(%rsp), %xmm1, %xmm1          # 4-byte Folded Reload
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	56(%rsp), %xmm1, %xmm1          # 4-byte Folded Reload
	vaddss	152(%rsp), %xmm1, %xmm1         # 4-byte Folded Reload
	vaddss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm1, %xmm12, %xmm2
	vsubss	%xmm2, %xmm12, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm9, %xmm3
	vsubss	%xmm3, %xmm9, %xmm5
	vaddss	%xmm2, %xmm5, %xmm5
	vaddss	%xmm3, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm4
	vaddss	%xmm3, %xmm4, %xmm3
	vbroadcastss	.LCPI41_3(%rip), %xmm9  # xmm9 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm9, %xmm4
	vsubss	%xmm2, %xmm15, %xmm2
	vsubss	%xmm15, %xmm2, %xmm6
	vsubss	%xmm6, %xmm2, %xmm7
	vsubss	%xmm7, %xmm15, %xmm7
	vxorps	%xmm15, %xmm15, %xmm15
	vxorps	%xmm3, %xmm9, %xmm8
	vmovaps	%xmm9, %xmm10
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vmovss	188(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm7, %xmm3
	vsubss	%xmm7, %xmm3, %xmm6
	vmovaps	%xmm7, %xmm9
	vsubss	%xmm6, %xmm3, %xmm7
	vsubss	%xmm7, %xmm9, %xmm7
	vsubss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm4, %xmm3, %xmm7
	vsubss	%xmm3, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vxorps	%xmm5, %xmm10, %xmm9
	vsubss	%xmm8, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vmovss	296(%rsp), %xmm8                # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm8, %xmm4
	vsubss	%xmm8, %xmm4, %xmm5
	vmovaps	%xmm8, %xmm10
	vsubss	%xmm5, %xmm4, %xmm8
	vsubss	%xmm8, %xmm10, %xmm8
	vsubss	%xmm5, %xmm9, %xmm5
	vaddss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm6, %xmm4, %xmm8
	vsubss	%xmm4, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vsubss	%xmm9, %xmm6, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm3, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm10
	vsubss	%xmm10, %xmm8, %xmm8
	vsubss	%xmm9, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vmovss	232(%rsp), %xmm8                # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm6, %xmm3
	vsubss	%xmm3, %xmm6, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm3, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm4
	vaddss	%xmm3, %xmm4, %xmm3
	vaddss	%xmm1, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmovss	%xmm0, 64(%rsp)
	vmovss	116(%rsp), %xmm1                # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovss	%xmm1, 68(%rsp)
	vmovss	112(%rsp), %xmm1                # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovss	%xmm1, 72(%rsp)
	vmovss	%xmm11, 76(%rsp)
	xorl	%eax, %eax
	movb	$1, %dl
	movq	%rbx, %rcx
	jmp	.LBB41_309
	.p2align	4, 0x90
.LBB41_307:                             #   in Loop: Header=BB41_309 Depth=2
	movslq	%eax, %rsi
	incl	%eax
	vmovss	%xmm1, 64(%rsp,%rsi,4)
.LBB41_308:                             #   in Loop: Header=BB41_309 Depth=2
	cmpl	$4, %eax
	setl	%sil
	addq	$4, %rcx
	testb	%sil, %dl
	movl	$0, %edx
	je	.LBB41_311
.LBB41_309:                             #   Parent Loop BB41_302 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rcx), %xmm2                   # xmm2 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vucomiss	%xmm15, %xmm0
	setnp	%sil
	sete	%dil
	testb	%sil, %dil
	je	.LBB41_307
# %bb.310:                              #   in Loop: Header=BB41_309 Depth=2
	vmovaps	%xmm1, %xmm0
	jmp	.LBB41_308
.LBB41_311:                             #   in Loop: Header=BB41_302 Depth=1
	cmpl	$3, %eax
	jg	.LBB41_300
# %bb.312:                              #   in Loop: Header=BB41_302 Depth=1
	vmovss	(%rcx), %xmm1                   # xmm1 = mem[0],zero,zero,zero
	movslq	%eax, %rcx
	vaddss	%xmm1, %xmm0, %xmm2
	vmovss	%xmm2, 64(%rsp,%rcx,4)
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 68(%rsp,%rcx,4)
	cmpl	$1, %eax
	jg	.LBB41_300
# %bb.313:                              #   in Loop: Header=BB41_302 Depth=1
	leaq	(%rsp,%rcx,4), %rdi
	addq	$72, %rdi
	movl	$1, %ecx
	subl	%eax, %ecx
	leaq	4(,%rcx,4), %rdx
	xorl	%esi, %esi
	callq	_intel_fast_memset@PLT
	vxorps	%xmm15, %xmm15, %xmm15
	jmp	.LBB41_300
.LBB41_314:
	movl	$90, %eax
	vxorps	%xmm11, %xmm11, %xmm11
	vcvtsi2ss	%eax, %xmm11, %xmm7
	vmulss	%xmm2, %xmm7, %xmm0
	vmovss	%xmm0, 96(%rsp)                 # 4-byte Spill
	vmovaps	%xmm2, %xmm8
	vfmsub213ss	%xmm0, %xmm7, %xmm8     # xmm8 = (xmm7 * xmm8) - xmm0
	vmulss	%xmm5, %xmm7, %xmm6
	vmovaps	%xmm5, %xmm13
	vfmsub213ss	%xmm6, %xmm7, %xmm13    # xmm13 = (xmm7 * xmm13) - xmm6
	vxorps	%xmm0, %xmm0, %xmm0
	vmulss	%xmm2, %xmm0, %xmm14
	vaddss	%xmm6, %xmm14, %xmm10
	vsubss	%xmm6, %xmm10, %xmm9
	vsubss	%xmm9, %xmm10, %xmm11
	vsubss	%xmm11, %xmm6, %xmm6
	vsubss	%xmm9, %xmm14, %xmm9
	vaddss	%xmm6, %xmm9, %xmm0
	vmovss	%xmm0, 160(%rsp)                # 4-byte Spill
	vaddss	%xmm8, %xmm10, %xmm9
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm12, %xmm10, %xmm10
	vsubss	%xmm11, %xmm8, %xmm8
	vaddss	%xmm8, %xmm10, %xmm10
	vmulss	%xmm4, %xmm7, %xmm11
	vmovaps	%xmm4, %xmm8
	vfmsub213ss	%xmm11, %xmm7, %xmm8    # xmm8 = (xmm7 * xmm8) - xmm11
	vxorps	%xmm6, %xmm6, %xmm6
	vmulss	%xmm5, %xmm6, %xmm15
	vaddss	%xmm15, %xmm11, %xmm0
	vsubss	%xmm11, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm12
	vsubss	%xmm12, %xmm11, %xmm11
	vmovaps	%xmm5, %xmm12
	vfmsub213ss	%xmm15, %xmm6, %xmm12   # xmm12 = (xmm6 * xmm12) - xmm15
	vsubss	%xmm1, %xmm15, %xmm1
	vaddss	%xmm1, %xmm11, %xmm1
	vmovss	%xmm1, 240(%rsp)                # 4-byte Spill
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm0, %xmm1, %xmm6
	vsubss	%xmm6, %xmm1, %xmm15
	vsubss	%xmm15, %xmm0, %xmm0
	vmovaps	%xmm2, %xmm15
	vfmsub132ss	.LCPI41_6(%rip), %xmm14, %xmm15 # xmm15 = (xmm15 * mem) - xmm14
	vsubss	%xmm6, %xmm14, %xmm6
	vaddss	%xmm6, %xmm0, %xmm0
	vmovss	%xmm0, 192(%rsp)                # 4-byte Spill
	vaddss	%xmm1, %xmm13, %xmm0
	vsubss	%xmm1, %xmm0, %xmm6
	vsubss	%xmm6, %xmm0, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vsubss	%xmm6, %xmm13, %xmm6
	vaddss	%xmm6, %xmm1, %xmm1
	vmovss	%xmm1, 8(%rsp)                  # 4-byte Spill
	vaddss	%xmm0, %xmm15, %xmm6
	vsubss	%xmm0, %xmm6, %xmm11
	vsubss	%xmm11, %xmm6, %xmm13
	vsubss	%xmm13, %xmm0, %xmm0
	vsubss	%xmm11, %xmm15, %xmm11
	vaddss	%xmm0, %xmm11, %xmm0
	vmovss	160(%rsp), %xmm1                # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm1, %xmm6, %xmm11
	vsubss	%xmm6, %xmm11, %xmm13
	vsubss	%xmm13, %xmm11, %xmm14
	vsubss	%xmm14, %xmm6, %xmm6
	vsubss	%xmm13, %xmm1, %xmm13
	vaddss	%xmm6, %xmm13, %xmm6
	vaddss	%xmm10, %xmm11, %xmm13
	vsubss	%xmm11, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm1
	vsubss	%xmm1, %xmm11, %xmm1
	vsubss	%xmm14, %xmm10, %xmm10
	vaddss	%xmm1, %xmm10, %xmm1
	vmulss	%xmm3, %xmm7, %xmm3
	vxorps	%xmm7, %xmm7, %xmm7
	vfmadd231ss	%xmm4, %xmm7, %xmm3     # xmm3 = (xmm7 * xmm4) + xmm3
	vfmadd231ss	%xmm5, %xmm7, %xmm3     # xmm3 = (xmm7 * xmm5) + xmm3
	vxorps	%xmm5, %xmm5, %xmm5
	vfmadd231ss	%xmm2, %xmm5, %xmm3     # xmm3 = (xmm5 * xmm2) + xmm3
	vaddss	%xmm3, %xmm8, %xmm2
	vaddss	%xmm2, %xmm12, %xmm2
	vaddss	%xmm2, %xmm15, %xmm2
	vaddss	240(%rsp), %xmm2, %xmm2         # 4-byte Folded Reload
	vaddss	192(%rsp), %xmm2, %xmm2         # 4-byte Folded Reload
	vaddss	8(%rsp), %xmm2, %xmm2           # 4-byte Folded Reload
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm13, %xmm1
	vsubss	%xmm1, %xmm13, %xmm2
	vaddss	%xmm0, %xmm2, %xmm3
	vaddss	%xmm1, %xmm9, %xmm4
	vsubss	%xmm4, %xmm9, %xmm2
	vaddss	%xmm1, %xmm2, %xmm2
	vmovss	96(%rsp), %xmm1                 # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm4, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm1
	vucomiss	%xmm5, %xmm0
	vaddss	%xmm4, %xmm1, %xmm1
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB41_316
# %bb.315:
	vxorps	%xmm3, %xmm3, %xmm3
	vblendps	$1, %xmm0, %xmm3, %xmm0         # xmm0 = xmm0[0],xmm3[1,2,3]
	jmp	.LBB41_327
.LBB41_316:
	vcomiss	%xmm0, %xmm5
	jbe	.LBB41_318
# %bb.317:
	vbroadcastss	.LCPI41_0(%rip), %xmm0  # xmm0 = [NaN,NaN,NaN,NaN]
	vmovaps	%xmm0, %xmm3
	jmp	.LBB41_327
.LBB41_318:
	vmovss	%xmm1, 160(%rsp)                # 4-byte Spill
	vmovss	%xmm2, 240(%rsp)                # 4-byte Spill
	vmovss	%xmm3, 16(%rsp)                 # 4-byte Spill
	vmovups	%xmm0, 192(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vxorps	%xmm3, %xmm3, %xmm3
	vucomiss	%xmm3, %xmm0
	setnp	%cl
	sete	%dl
	vmovd	.LCPI41_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	vmovdqa	%xmm1, %xmm2
	testb	%cl, %dl
	jne	.LBB41_322
# %bb.319:
	movl	%eax, %ecx
	andl	$2139095040, %ecx               # imm = 0x7F800000
	cmpl	$2139095040, %ecx               # imm = 0x7F800000
	je	.LBB41_321
# %bb.320:
	vmovd	%ecx, %xmm2
	jmp	.LBB41_322
.LBB41_321:
	vmovd	.LCPI41_1(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
.LBB41_322:
	vucomiss	%xmm3, %xmm0
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB41_326
# %bb.323:
	andl	$2139095040, %eax               # imm = 0x7F800000
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB41_325
# %bb.324:
	movl	$2130706432, %ecx               # imm = 0x7F000000
	subl	%eax, %ecx
	vmovd	%ecx, %xmm1
	jmp	.LBB41_326
.LBB41_325:
	vmovd	.LCPI41_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
.LBB41_326:
	vmovss	.LCPI41_2(%rip), %xmm8          # xmm8 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm8, %xmm5
	vmulss	192(%rsp), %xmm1, %xmm3         # 16-byte Folded Reload
	vmulss	%xmm5, %xmm3, %xmm3
	vmulss	160(%rsp), %xmm1, %xmm4         # 4-byte Folded Reload
	vmulss	%xmm5, %xmm4, %xmm4
	movl	$3, %eax
	vxorps	%xmm9, %xmm9, %xmm9
	vcvtsi2ss	%eax, %xmm9, %xmm6
	vmulss	240(%rsp), %xmm1, %xmm1         # 4-byte Folded Reload
	vmulss	%xmm5, %xmm1, %xmm1
	vmovss	%xmm1, 96(%rsp)                 # 4-byte Spill
	vmulss	%xmm6, %xmm8, %xmm7
	vfmsub213ss	%xmm7, %xmm6, %xmm8     # xmm8 = (xmm6 * xmm8) - xmm7
	vxorps	%xmm1, %xmm1, %xmm1
	vmulss	%xmm1, %xmm6, %xmm9
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm9, %xmm6, %xmm10    # xmm10 = (xmm6 * xmm10) - xmm9
	vaddss	%xmm8, %xmm9, %xmm11
	vsubss	%xmm9, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm9, %xmm13
	vsubss	%xmm12, %xmm8, %xmm8
	vdivss	%xmm0, %xmm2, %xmm9
	vaddss	%xmm8, %xmm13, %xmm0
	vfmadd231ss	%xmm6, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm6) + xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm0, %xmm11, %xmm10
	vsubss	%xmm10, %xmm11, %xmm6
	vaddss	%xmm0, %xmm6, %xmm0
	vmovss	%xmm0, 8(%rsp)                  # 4-byte Spill
	vaddss	%xmm7, %xmm10, %xmm8
	vsubss	%xmm8, %xmm7, %xmm0
	vaddss	%xmm0, %xmm10, %xmm7
	vmulss	%xmm1, %xmm9, %xmm0
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm0, %xmm9, %xmm10    # xmm10 = (xmm9 * xmm10) - xmm0
	vmulss	%xmm1, %xmm9, %xmm11
	vaddss	%xmm0, %xmm11, %xmm12
	vsubss	%xmm0, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm0, %xmm0
	vmovaps	%xmm9, %xmm14
	vfmsub213ss	%xmm11, %xmm1, %xmm14   # xmm14 = (xmm1 * xmm14) - xmm11
	vsubss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm0, %xmm11, %xmm0
	vmulss	%xmm9, %xmm9, %xmm11
	vmovaps	%xmm9, %xmm13
	vfmsub213ss	%xmm11, %xmm9, %xmm13   # xmm13 = (xmm9 * xmm13) - xmm11
	vaddss	%xmm13, %xmm12, %xmm15
	vmovss	%xmm2, 56(%rsp)                 # 4-byte Spill
	vsubss	%xmm12, %xmm15, %xmm2
	vsubss	%xmm2, %xmm15, %xmm5
	vsubss	%xmm5, %xmm12, %xmm5
	vsubss	%xmm2, %xmm13, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vfmadd231ss	%xmm9, %xmm1, %xmm10    # xmm10 = (xmm1 * xmm9) + xmm10
	vfmadd231ss	%xmm1, %xmm1, %xmm14    # xmm14 = (xmm1 * xmm1) + xmm14
	vaddss	%xmm14, %xmm10, %xmm5
	vfmadd231ss	%xmm1, %xmm9, %xmm0     # xmm0 = (xmm9 * xmm1) + xmm0
	vaddss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm15, %xmm2
	vsubss	%xmm2, %xmm15, %xmm5
	vaddss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm2, %xmm11, %xmm10
	vsubss	%xmm10, %xmm11, %xmm5
	vaddss	%xmm2, %xmm5, %xmm2
	vmulss	%xmm4, %xmm10, %xmm5
	vmovaps	%xmm10, %xmm11
	vfmsub213ss	%xmm5, %xmm4, %xmm11    # xmm11 = (xmm4 * xmm11) - xmm5
	vfmadd231ss	%xmm2, %xmm4, %xmm11    # xmm11 = (xmm4 * xmm2) + xmm11
	vmulss	%xmm2, %xmm3, %xmm12
	vfmsub213ss	%xmm12, %xmm3, %xmm2    # xmm2 = (xmm3 * xmm2) - xmm12
	vaddss	%xmm5, %xmm12, %xmm13
	vsubss	%xmm12, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm5, %xmm5
	vaddss	%xmm5, %xmm12, %xmm5
	vfmadd231ss	96(%rsp), %xmm10, %xmm2 # 4-byte Folded Reload
                                        # xmm2 = (xmm10 * mem) + xmm2
	vmulss	%xmm3, %xmm10, %xmm12
	vfmsub213ss	%xmm12, %xmm3, %xmm10   # xmm10 = (xmm3 * xmm10) - xmm12
	vaddss	%xmm10, %xmm13, %xmm14
	vsubss	%xmm13, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm6
	vsubss	%xmm6, %xmm13, %xmm6
	vsubss	%xmm15, %xmm10, %xmm10
	vaddss	%xmm6, %xmm10, %xmm6
	vaddss	%xmm2, %xmm11, %xmm2
	vfmadd231ss	%xmm0, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm0) + xmm5
	vaddss	%xmm5, %xmm2, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm0, %xmm14, %xmm2
	vsubss	%xmm2, %xmm14, %xmm5
	vaddss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm2, %xmm12, %xmm5
	vsubss	%xmm5, %xmm12, %xmm6
	vaddss	%xmm2, %xmm6, %xmm2
	vbroadcastss	.LCPI41_3(%rip), %xmm11 # xmm11 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm5, %xmm11, %xmm6
	vsubss	%xmm5, %xmm8, %xmm5
	vsubss	%xmm8, %xmm5, %xmm10
	vsubss	%xmm10, %xmm6, %xmm6
	vsubss	%xmm10, %xmm5, %xmm10
	vsubss	%xmm10, %xmm8, %xmm10
	vaddss	%xmm6, %xmm10, %xmm6
	vxorps	%xmm2, %xmm11, %xmm10
	vsubss	%xmm2, %xmm7, %xmm2
	vsubss	%xmm7, %xmm2, %xmm11
	vsubss	%xmm11, %xmm10, %xmm10
	vsubss	%xmm11, %xmm2, %xmm11
	vsubss	%xmm11, %xmm7, %xmm11
	vaddss	%xmm10, %xmm11, %xmm10
	vaddss	%xmm6, %xmm2, %xmm11
	vsubss	%xmm2, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm2, %xmm2
	vsubss	%xmm12, %xmm6, %xmm6
	vaddss	%xmm6, %xmm2, %xmm2
	vmovss	8(%rsp), %xmm6                  # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm11, %xmm2
	vsubss	%xmm2, %xmm11, %xmm6
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm2, %xmm5, %xmm2
	vmulss	%xmm1, %xmm6, %xmm5
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm5, %xmm6, %xmm10    # xmm10 = (xmm6 * xmm10) - xmm5
	vmulss	%xmm2, %xmm9, %xmm11
	vaddss	%xmm5, %xmm11, %xmm12
	vsubss	%xmm5, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm5, %xmm5
	vmovaps	%xmm9, %xmm14
	vfmsub213ss	%xmm11, %xmm2, %xmm14   # xmm14 = (xmm2 * xmm14) - xmm11
	vsubss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm5, %xmm11, %xmm5
	vfmadd231ss	%xmm0, %xmm9, %xmm10    # xmm10 = (xmm9 * xmm0) + xmm10
	vmulss	%xmm6, %xmm9, %xmm0
	vfmsub213ss	%xmm0, %xmm6, %xmm9     # xmm9 = (xmm6 * xmm9) - xmm0
	vaddss	%xmm9, %xmm12, %xmm11
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm11, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm13, %xmm9, %xmm9
	vaddss	%xmm9, %xmm12, %xmm9
	vfmadd231ss	%xmm2, %xmm1, %xmm14    # xmm14 = (xmm1 * xmm2) + xmm14
	vaddss	%xmm14, %xmm10, %xmm2
	vfmadd231ss	%xmm6, %xmm1, %xmm5     # xmm5 = (xmm1 * xmm6) + xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm9, %xmm2
	vaddss	%xmm2, %xmm11, %xmm5
	vsubss	%xmm5, %xmm11, %xmm6
	vaddss	%xmm2, %xmm6, %xmm9
	vaddss	%xmm5, %xmm0, %xmm10
	vsubss	%xmm10, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm11
	vmulss	%xmm11, %xmm10, %xmm0
	vmovaps	%xmm11, %xmm2
	vfmsub213ss	%xmm0, %xmm10, %xmm2    # xmm2 = (xmm10 * xmm2) - xmm0
	vmulss	%xmm10, %xmm11, %xmm5
	vaddss	%xmm5, %xmm0, %xmm6
	vsubss	%xmm0, %xmm6, %xmm12
	vsubss	%xmm12, %xmm6, %xmm13
	vsubss	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm10, %xmm13
	vfmsub213ss	%xmm5, %xmm11, %xmm13   # xmm13 = (xmm11 * xmm13) - xmm5
	vsubss	%xmm12, %xmm5, %xmm5
	vaddss	%xmm5, %xmm0, %xmm0
	vmulss	%xmm10, %xmm10, %xmm5
	vmovaps	%xmm10, %xmm12
	vfmsub213ss	%xmm5, %xmm10, %xmm12   # xmm12 = (xmm10 * xmm12) - xmm5
	vaddss	%xmm6, %xmm12, %xmm14
	vsubss	%xmm6, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm1
	vsubss	%xmm1, %xmm6, %xmm1
	vsubss	%xmm15, %xmm12, %xmm6
	vaddss	%xmm6, %xmm1, %xmm1
	vmovss	%xmm9, 152(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm10, %xmm9, %xmm2    # xmm2 = (xmm9 * xmm10) + xmm2
	vfmadd231ss	%xmm11, %xmm11, %xmm13  # xmm13 = (xmm11 * xmm11) + xmm13
	vaddss	%xmm2, %xmm13, %xmm2
	vfmadd231ss	%xmm9, %xmm10, %xmm0    # xmm0 = (xmm10 * xmm9) + xmm0
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm2
	vaddss	%xmm0, %xmm2, %xmm12
	vaddss	%xmm1, %xmm5, %xmm0
	vsubss	%xmm0, %xmm5, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmulss	%xmm0, %xmm4, %xmm2
	vmovaps	%xmm0, %xmm5
	vfmsub213ss	%xmm2, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm5) - xmm2
	vfmadd231ss	%xmm1, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm1) + xmm5
	vmulss	%xmm1, %xmm3, %xmm6
	vfmsub213ss	%xmm6, %xmm3, %xmm1     # xmm1 = (xmm3 * xmm1) - xmm6
	vaddss	%xmm2, %xmm6, %xmm13
	vsubss	%xmm6, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vfmadd231ss	96(%rsp), %xmm0, %xmm1  # 4-byte Folded Reload
                                        # xmm1 = (xmm0 * mem) + xmm1
	vmulss	%xmm0, %xmm3, %xmm6
	vfmsub213ss	%xmm6, %xmm3, %xmm0     # xmm0 = (xmm3 * xmm0) - xmm6
	vaddss	%xmm0, %xmm13, %xmm14
	vsubss	%xmm13, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm9
	vsubss	%xmm9, %xmm13, %xmm9
	vsubss	%xmm15, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm5, %xmm1, %xmm1
	vfmadd231ss	%xmm12, %xmm3, %xmm2    # xmm2 = (xmm3 * xmm12) + xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm6, %xmm2
	vsubss	%xmm2, %xmm6, %xmm5
	vaddss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm2, %xmm8, %xmm5
	vsubss	%xmm8, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm9
	vsubss	%xmm9, %xmm8, %xmm8
	vbroadcastss	.LCPI41_3(%rip), %xmm12 # xmm12 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm12, %xmm2
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vsubss	%xmm1, %xmm7, %xmm6
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm9
	vsubss	%xmm9, %xmm7, %xmm7
	vxorps	%xmm1, %xmm12, %xmm1
	vsubss	%xmm8, %xmm1, %xmm1
	vaddss	%xmm1, %xmm7, %xmm1
	vaddss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm6, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm6, %xmm6
	vsubss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vmovss	8(%rsp), %xmm6                  # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm2
	vaddss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm5, %xmm0
	vsubss	%xmm0, %xmm5, %xmm5
	vaddss	%xmm1, %xmm5, %xmm1
	vmulss	%xmm1, %xmm10, %xmm5
	vmovaps	%xmm10, %xmm6
	vfmsub213ss	%xmm5, %xmm1, %xmm6     # xmm6 = (xmm1 * xmm6) - xmm5
	vfmadd231ss	%xmm11, %xmm1, %xmm6    # xmm6 = (xmm1 * xmm11) + xmm6
	vmulss	%xmm0, %xmm11, %xmm1
	vfmsub213ss	%xmm1, %xmm0, %xmm11    # xmm11 = (xmm0 * xmm11) - xmm1
	vaddss	%xmm5, %xmm1, %xmm7
	vsubss	%xmm1, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm1, %xmm1
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm1, %xmm1
	vfmadd231ss	%xmm2, %xmm10, %xmm11   # xmm11 = (xmm10 * xmm2) + xmm11
	vfmadd231ss	152(%rsp), %xmm0, %xmm1 # 4-byte Folded Reload
                                        # xmm1 = (xmm0 * mem) + xmm1
	vmulss	%xmm0, %xmm10, %xmm2
	vfmsub213ss	%xmm2, %xmm0, %xmm10    # xmm10 = (xmm0 * xmm10) - xmm2
	vaddss	%xmm7, %xmm10, %xmm0
	vsubss	%xmm7, %xmm0, %xmm5
	vsubss	%xmm5, %xmm0, %xmm8
	vsubss	%xmm8, %xmm7, %xmm7
	vsubss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm5, %xmm7, %xmm5
	vaddss	%xmm6, %xmm11, %xmm6
	vaddss	%xmm1, %xmm6, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm5
	vsubss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm5, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm5, %xmm2, %xmm2
	vmulss	%xmm2, %xmm3, %xmm5
	vmulss	%xmm1, %xmm4, %xmm6
	vmovaps	%xmm1, %xmm7
	vfmsub213ss	%xmm6, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm7) - xmm6
	vfmadd231ss	%xmm2, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm2) + xmm7
	vfmsub213ss	%xmm5, %xmm3, %xmm2     # xmm2 = (xmm3 * xmm2) - xmm5
	vaddss	%xmm6, %xmm5, %xmm4
	vsubss	%xmm5, %xmm4, %xmm8
	vsubss	%xmm8, %xmm4, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vsubss	%xmm8, %xmm6, %xmm6
	vfmadd231ss	96(%rsp), %xmm1, %xmm2  # 4-byte Folded Reload
                                        # xmm2 = (xmm1 * mem) + xmm2
	vmulss	%xmm1, %xmm3, %xmm8
	vfmsub213ss	%xmm8, %xmm3, %xmm1     # xmm1 = (xmm3 * xmm1) - xmm8
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm1, %xmm4, %xmm6
	vsubss	%xmm4, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vsubss	%xmm9, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm7, %xmm2, %xmm2
	vfmadd231ss	%xmm0, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm0) + xmm5
	vaddss	%xmm5, %xmm2, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm8, %xmm2
	movl	$2, %eax
	vcvtsi2ss	%eax, %xmm3, %xmm4
	vmovss	%xmm4, 96(%rsp)                 # 4-byte Spill
	vsubss	%xmm2, %xmm8, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmulss	56(%rsp), %xmm4, %xmm5          # 4-byte Folded Reload
	vmulss	%xmm5, %xmm2, %xmm14
	vmulss	%xmm5, %xmm1, %xmm4
	vmulss	%xmm5, %xmm0, %xmm5
	vmulss	%xmm4, %xmm14, %xmm0
	vmovaps	%xmm4, %xmm11
	vfmsub213ss	%xmm0, %xmm14, %xmm11   # xmm11 = (xmm14 * xmm11) - xmm0
	vmulss	%xmm4, %xmm14, %xmm1
	vaddss	%xmm1, %xmm0, %xmm8
	vsubss	%xmm0, %xmm8, %xmm6
	vsubss	%xmm6, %xmm8, %xmm7
	vsubss	%xmm7, %xmm0, %xmm0
	vmovaps	%xmm14, %xmm3
	vfmsub213ss	%xmm1, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm3) - xmm1
	vsubss	%xmm6, %xmm1, %xmm1
	vmulss	%xmm14, %xmm14, %xmm6
	vaddss	%xmm1, %xmm0, %xmm9
	vmovaps	%xmm14, %xmm0
	vfmsub213ss	%xmm6, %xmm14, %xmm0    # xmm0 = (xmm14 * xmm0) - xmm6
	vaddss	%xmm0, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm10
	vsubss	%xmm10, %xmm8, %xmm8
	vsubss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm10
	vmulss	%xmm5, %xmm14, %xmm0
	vmovaps	%xmm5, %xmm8
	vfmsub213ss	%xmm0, %xmm14, %xmm8    # xmm8 = (xmm14 * xmm8) - xmm0
	vmulss	%xmm4, %xmm4, %xmm1
	vaddss	%xmm1, %xmm0, %xmm15
	vsubss	%xmm0, %xmm15, %xmm12
	vsubss	%xmm12, %xmm15, %xmm13
	vsubss	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm4, %xmm13
	vfmsub213ss	%xmm1, %xmm4, %xmm13    # xmm13 = (xmm4 * xmm13) - xmm1
	vsubss	%xmm12, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 8(%rsp)                  # 4-byte Spill
	vmulss	%xmm5, %xmm14, %xmm1
	vaddss	%xmm1, %xmm15, %xmm0
	vsubss	%xmm15, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm12
	vsubss	%xmm12, %xmm15, %xmm12
	vmovaps	%xmm14, %xmm15
	vfmsub213ss	%xmm1, %xmm5, %xmm15    # xmm15 = (xmm5 * xmm15) - xmm1
	vsubss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm12, %xmm1
	vmovss	%xmm1, 56(%rsp)                 # 4-byte Spill
	vaddss	%xmm0, %xmm11, %xmm2
	vsubss	%xmm0, %xmm2, %xmm12
	vsubss	%xmm12, %xmm2, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm12, %xmm11, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm3, %xmm2, %xmm1
	vsubss	%xmm2, %xmm1, %xmm11
	vsubss	%xmm11, %xmm1, %xmm12
	vsubss	%xmm12, %xmm2, %xmm2
	vsubss	%xmm11, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm1, %xmm9, %xmm3
	vsubss	%xmm1, %xmm3, %xmm11
	vsubss	%xmm11, %xmm3, %xmm12
	vsubss	%xmm12, %xmm1, %xmm1
	vsubss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm3, %xmm10, %xmm9
	vsubss	%xmm3, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm12, %xmm3, %xmm3
	vsubss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm3, %xmm10, %xmm3
	vxorps	%xmm11, %xmm11, %xmm11
	vmulss	%xmm11, %xmm14, %xmm10
	vfmadd231ss	%xmm5, %xmm4, %xmm10    # xmm10 = (xmm4 * xmm5) + xmm10
	vfmadd231ss	%xmm4, %xmm5, %xmm10    # xmm10 = (xmm5 * xmm4) + xmm10
	vfmadd231ss	%xmm11, %xmm14, %xmm10  # xmm10 = (xmm14 * xmm11) + xmm10
	vaddss	%xmm8, %xmm10, %xmm8
	vaddss	%xmm13, %xmm8, %xmm8
	vaddss	%xmm15, %xmm8, %xmm8
	vaddss	8(%rsp), %xmm8, %xmm8           # 4-byte Folded Reload
	vaddss	56(%rsp), %xmm8, %xmm8          # 4-byte Folded Reload
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm2
	vsubss	%xmm2, %xmm9, %xmm1
	vaddss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm2, %xmm7, %xmm0
	vsubss	%xmm0, %xmm7, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm0, %xmm6, %xmm3
	vsubss	%xmm3, %xmm6, %xmm6
	vaddss	%xmm0, %xmm6, %xmm0
	vbroadcastss	.LCPI41_3(%rip), %xmm10 # xmm10 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm3, %xmm10, %xmm7
	vmovups	192(%rsp), %xmm8                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm3, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm3
	vmovaps	%xmm8, %xmm9
	vsubss	%xmm3, %xmm6, %xmm8
	vsubss	%xmm8, %xmm9, %xmm8
	vxorps	%xmm0, %xmm10, %xmm9
	vmovaps	%xmm10, %xmm11
	vsubss	%xmm3, %xmm7, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vmovss	160(%rsp), %xmm8                # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm8, %xmm0
	vsubss	%xmm8, %xmm0, %xmm7
	vmovaps	%xmm8, %xmm10
	vsubss	%xmm7, %xmm0, %xmm8
	vsubss	%xmm8, %xmm10, %xmm8
	vsubss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm3, %xmm0, %xmm8
	vsubss	%xmm0, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm0, %xmm0
	vxorps	%xmm2, %xmm11, %xmm10
	vsubss	%xmm9, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm0
	vmovss	240(%rsp), %xmm9                # 4-byte Reload
                                        # xmm9 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm9, %xmm2
	vsubss	%xmm9, %xmm2, %xmm3
	vmovaps	%xmm9, %xmm11
	vsubss	%xmm3, %xmm2, %xmm9
	vsubss	%xmm9, %xmm11, %xmm9
	vsubss	%xmm3, %xmm10, %xmm3
	vaddss	%xmm3, %xmm9, %xmm3
	vaddss	%xmm7, %xmm2, %xmm9
	vsubss	%xmm2, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm2, %xmm2
	vsubss	%xmm10, %xmm7, %xmm7
	vaddss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm0, %xmm9, %xmm7
	vsubss	%xmm9, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vsubss	%xmm10, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vmovss	16(%rsp), %xmm9                 # 4-byte Reload
                                        # xmm9 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm8, %xmm2
	vsubss	%xmm2, %xmm8, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm6, %xmm3
	vsubss	%xmm3, %xmm6, %xmm6
	vaddss	%xmm2, %xmm6, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm4, %xmm14, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vmulss	96(%rsp), %xmm1, %xmm1          # 4-byte Folded Reload
	vdivss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vinsertps	$16, %xmm0, %xmm1, %xmm3 # xmm3 = xmm1[0],xmm0[0],xmm1[2,3]
	vaddss	%xmm2, %xmm14, %xmm0
	vsubss	%xmm0, %xmm14, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
.LBB41_327:
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm0
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB41_329
# %bb.328:
	vxorps	%xmm4, %xmm4, %xmm4
	vblendps	$2, %xmm4, %xmm0, %xmm1         # xmm1 = xmm0[0],xmm4[1],xmm0[2,3]
	jmp	.LBB41_340
.LBB41_329:
	vcomiss	%xmm0, %xmm1
	jbe	.LBB41_331
# %bb.330:
	vbroadcastss	.LCPI41_0(%rip), %xmm1  # xmm1 = [NaN,NaN,NaN,NaN]
	vmovaps	%xmm1, %xmm4
	jmp	.LBB41_340
.LBB41_331:
	vmovups	%xmm3, 240(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vmovups	%xmm1, 192(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm0, 160(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vxorps	%xmm3, %xmm3, %xmm3
	vucomiss	%xmm3, %xmm0
	setnp	%cl
	sete	%dl
	vmovd	.LCPI41_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	vmovdqa	%xmm1, %xmm13
	testb	%cl, %dl
	jne	.LBB41_335
# %bb.332:
	movl	%eax, %ecx
	andl	$2139095040, %ecx               # imm = 0x7F800000
	cmpl	$2139095040, %ecx               # imm = 0x7F800000
	je	.LBB41_334
# %bb.333:
	vmovd	%ecx, %xmm13
	jmp	.LBB41_335
.LBB41_334:
	vmovd	.LCPI41_1(%rip), %xmm13         # xmm13 = mem[0],zero,zero,zero
.LBB41_335:
	vucomiss	%xmm3, %xmm0
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB41_339
# %bb.336:
	andl	$2139095040, %eax               # imm = 0x7F800000
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB41_338
# %bb.337:
	movl	$2130706432, %ecx               # imm = 0x7F000000
	subl	%eax, %ecx
	vmovd	%ecx, %xmm1
	jmp	.LBB41_339
.LBB41_338:
	vmovd	.LCPI41_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
.LBB41_339:
	vmovss	.LCPI41_2(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm1, %xmm5
	vmulss	160(%rsp), %xmm1, %xmm3         # 16-byte Folded Reload
	vmulss	%xmm5, %xmm3, %xmm3
	vmulss	192(%rsp), %xmm1, %xmm4         # 16-byte Folded Reload
	vmulss	%xmm5, %xmm4, %xmm4
	movl	$3, %eax
	vxorps	%xmm12, %xmm12, %xmm12
	vcvtsi2ss	%eax, %xmm12, %xmm6
	vmulss	240(%rsp), %xmm1, %xmm1         # 16-byte Folded Reload
	vmulss	%xmm5, %xmm1, %xmm1
	vmovss	%xmm1, 96(%rsp)                 # 4-byte Spill
	vmulss	%xmm2, %xmm6, %xmm7
	vfmsub213ss	%xmm7, %xmm6, %xmm2     # xmm2 = (xmm6 * xmm2) - xmm7
	vxorps	%xmm1, %xmm1, %xmm1
	vmulss	%xmm1, %xmm6, %xmm8
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm8, %xmm6, %xmm10    # xmm10 = (xmm6 * xmm10) - xmm8
	vaddss	%xmm2, %xmm8, %xmm11
	vsubss	%xmm8, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm9
	vsubss	%xmm9, %xmm8, %xmm8
	vdivss	%xmm0, %xmm13, %xmm9
	vsubss	%xmm12, %xmm2, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vfmadd231ss	%xmm6, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm6) + xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm0, %xmm11, %xmm10
	vsubss	%xmm10, %xmm11, %xmm6
	vaddss	%xmm0, %xmm6, %xmm0
	vmovss	%xmm0, 8(%rsp)                  # 4-byte Spill
	vaddss	%xmm7, %xmm10, %xmm8
	vsubss	%xmm8, %xmm7, %xmm0
	vaddss	%xmm0, %xmm10, %xmm7
	vmulss	%xmm1, %xmm9, %xmm0
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm0, %xmm9, %xmm10    # xmm10 = (xmm9 * xmm10) - xmm0
	vmulss	%xmm1, %xmm9, %xmm11
	vaddss	%xmm0, %xmm11, %xmm12
	vmovss	%xmm13, 16(%rsp)                # 4-byte Spill
	vsubss	%xmm0, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm0, %xmm0
	vmovaps	%xmm9, %xmm14
	vfmsub213ss	%xmm11, %xmm1, %xmm14   # xmm14 = (xmm1 * xmm14) - xmm11
	vsubss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm0, %xmm11, %xmm0
	vmulss	%xmm9, %xmm9, %xmm11
	vmovaps	%xmm9, %xmm13
	vfmsub213ss	%xmm11, %xmm9, %xmm13   # xmm13 = (xmm9 * xmm13) - xmm11
	vaddss	%xmm13, %xmm12, %xmm15
	vsubss	%xmm12, %xmm15, %xmm2
	vsubss	%xmm2, %xmm15, %xmm5
	vsubss	%xmm5, %xmm12, %xmm5
	vsubss	%xmm2, %xmm13, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vfmadd231ss	%xmm9, %xmm1, %xmm10    # xmm10 = (xmm1 * xmm9) + xmm10
	vfmadd231ss	%xmm1, %xmm1, %xmm14    # xmm14 = (xmm1 * xmm1) + xmm14
	vaddss	%xmm14, %xmm10, %xmm5
	vfmadd231ss	%xmm1, %xmm9, %xmm0     # xmm0 = (xmm9 * xmm1) + xmm0
	vaddss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm15, %xmm2
	vsubss	%xmm2, %xmm15, %xmm5
	vaddss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm2, %xmm11, %xmm10
	vsubss	%xmm10, %xmm11, %xmm5
	vaddss	%xmm2, %xmm5, %xmm2
	vmulss	%xmm4, %xmm10, %xmm5
	vmovaps	%xmm10, %xmm11
	vfmsub213ss	%xmm5, %xmm4, %xmm11    # xmm11 = (xmm4 * xmm11) - xmm5
	vfmadd231ss	%xmm2, %xmm4, %xmm11    # xmm11 = (xmm4 * xmm2) + xmm11
	vmulss	%xmm2, %xmm3, %xmm12
	vfmsub213ss	%xmm12, %xmm3, %xmm2    # xmm2 = (xmm3 * xmm2) - xmm12
	vaddss	%xmm5, %xmm12, %xmm13
	vsubss	%xmm12, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm5, %xmm5
	vaddss	%xmm5, %xmm12, %xmm5
	vfmadd231ss	96(%rsp), %xmm10, %xmm2 # 4-byte Folded Reload
                                        # xmm2 = (xmm10 * mem) + xmm2
	vmulss	%xmm3, %xmm10, %xmm12
	vfmsub213ss	%xmm12, %xmm3, %xmm10   # xmm10 = (xmm3 * xmm10) - xmm12
	vaddss	%xmm10, %xmm13, %xmm14
	vsubss	%xmm13, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm6
	vsubss	%xmm6, %xmm13, %xmm6
	vsubss	%xmm15, %xmm10, %xmm10
	vaddss	%xmm6, %xmm10, %xmm6
	vaddss	%xmm2, %xmm11, %xmm2
	vfmadd231ss	%xmm0, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm0) + xmm5
	vaddss	%xmm5, %xmm2, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm0, %xmm14, %xmm2
	vsubss	%xmm2, %xmm14, %xmm5
	vaddss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm2, %xmm12, %xmm5
	vsubss	%xmm5, %xmm12, %xmm6
	vaddss	%xmm2, %xmm6, %xmm2
	vbroadcastss	.LCPI41_3(%rip), %xmm11 # xmm11 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm5, %xmm11, %xmm6
	vsubss	%xmm5, %xmm8, %xmm5
	vsubss	%xmm8, %xmm5, %xmm10
	vsubss	%xmm10, %xmm6, %xmm6
	vsubss	%xmm10, %xmm5, %xmm10
	vsubss	%xmm10, %xmm8, %xmm10
	vaddss	%xmm6, %xmm10, %xmm6
	vxorps	%xmm2, %xmm11, %xmm10
	vsubss	%xmm2, %xmm7, %xmm2
	vsubss	%xmm7, %xmm2, %xmm11
	vsubss	%xmm11, %xmm10, %xmm10
	vsubss	%xmm11, %xmm2, %xmm11
	vsubss	%xmm11, %xmm7, %xmm11
	vaddss	%xmm10, %xmm11, %xmm10
	vaddss	%xmm6, %xmm2, %xmm11
	vsubss	%xmm2, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm2, %xmm2
	vsubss	%xmm12, %xmm6, %xmm6
	vaddss	%xmm6, %xmm2, %xmm2
	vmovss	8(%rsp), %xmm6                  # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm11, %xmm2
	vsubss	%xmm2, %xmm11, %xmm6
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm2, %xmm5, %xmm2
	vmulss	%xmm1, %xmm6, %xmm5
	vxorps	%xmm10, %xmm10, %xmm10
	vfmsub213ss	%xmm5, %xmm6, %xmm10    # xmm10 = (xmm6 * xmm10) - xmm5
	vmulss	%xmm2, %xmm9, %xmm11
	vaddss	%xmm5, %xmm11, %xmm12
	vsubss	%xmm5, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm5, %xmm5
	vmovaps	%xmm9, %xmm14
	vfmsub213ss	%xmm11, %xmm2, %xmm14   # xmm14 = (xmm2 * xmm14) - xmm11
	vsubss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm5, %xmm11, %xmm5
	vfmadd231ss	%xmm0, %xmm9, %xmm10    # xmm10 = (xmm9 * xmm0) + xmm10
	vmulss	%xmm6, %xmm9, %xmm0
	vfmsub213ss	%xmm0, %xmm6, %xmm9     # xmm9 = (xmm6 * xmm9) - xmm0
	vaddss	%xmm9, %xmm12, %xmm11
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm11, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm13, %xmm9, %xmm9
	vaddss	%xmm9, %xmm12, %xmm9
	vfmadd231ss	%xmm2, %xmm1, %xmm14    # xmm14 = (xmm1 * xmm2) + xmm14
	vaddss	%xmm14, %xmm10, %xmm2
	vfmadd231ss	%xmm6, %xmm1, %xmm5     # xmm5 = (xmm1 * xmm6) + xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm9, %xmm2
	vaddss	%xmm2, %xmm11, %xmm5
	vsubss	%xmm5, %xmm11, %xmm6
	vaddss	%xmm2, %xmm6, %xmm9
	vaddss	%xmm5, %xmm0, %xmm10
	vsubss	%xmm10, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm11
	vmulss	%xmm11, %xmm10, %xmm0
	vmovaps	%xmm11, %xmm2
	vfmsub213ss	%xmm0, %xmm10, %xmm2    # xmm2 = (xmm10 * xmm2) - xmm0
	vmulss	%xmm10, %xmm11, %xmm5
	vaddss	%xmm5, %xmm0, %xmm6
	vsubss	%xmm0, %xmm6, %xmm12
	vsubss	%xmm12, %xmm6, %xmm13
	vsubss	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm10, %xmm13
	vfmsub213ss	%xmm5, %xmm11, %xmm13   # xmm13 = (xmm11 * xmm13) - xmm5
	vsubss	%xmm12, %xmm5, %xmm5
	vaddss	%xmm5, %xmm0, %xmm0
	vmulss	%xmm10, %xmm10, %xmm5
	vmovaps	%xmm10, %xmm12
	vfmsub213ss	%xmm5, %xmm10, %xmm12   # xmm12 = (xmm10 * xmm12) - xmm5
	vaddss	%xmm6, %xmm12, %xmm14
	vsubss	%xmm6, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm1
	vsubss	%xmm1, %xmm6, %xmm1
	vsubss	%xmm15, %xmm12, %xmm6
	vaddss	%xmm6, %xmm1, %xmm1
	vmovss	%xmm9, 56(%rsp)                 # 4-byte Spill
	vfmadd231ss	%xmm10, %xmm9, %xmm2    # xmm2 = (xmm9 * xmm10) + xmm2
	vfmadd231ss	%xmm11, %xmm11, %xmm13  # xmm13 = (xmm11 * xmm11) + xmm13
	vaddss	%xmm2, %xmm13, %xmm2
	vfmadd231ss	%xmm9, %xmm10, %xmm0    # xmm0 = (xmm10 * xmm9) + xmm0
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm2
	vaddss	%xmm0, %xmm2, %xmm12
	vaddss	%xmm1, %xmm5, %xmm0
	vsubss	%xmm0, %xmm5, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmulss	%xmm0, %xmm4, %xmm2
	vmovaps	%xmm0, %xmm5
	vfmsub213ss	%xmm2, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm5) - xmm2
	vfmadd231ss	%xmm1, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm1) + xmm5
	vmulss	%xmm1, %xmm3, %xmm6
	vfmsub213ss	%xmm6, %xmm3, %xmm1     # xmm1 = (xmm3 * xmm1) - xmm6
	vaddss	%xmm2, %xmm6, %xmm13
	vsubss	%xmm6, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vfmadd231ss	96(%rsp), %xmm0, %xmm1  # 4-byte Folded Reload
                                        # xmm1 = (xmm0 * mem) + xmm1
	vmulss	%xmm0, %xmm3, %xmm6
	vfmsub213ss	%xmm6, %xmm3, %xmm0     # xmm0 = (xmm3 * xmm0) - xmm6
	vaddss	%xmm0, %xmm13, %xmm14
	vsubss	%xmm13, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm9
	vsubss	%xmm9, %xmm13, %xmm9
	vsubss	%xmm15, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm5, %xmm1, %xmm1
	vfmadd231ss	%xmm12, %xmm3, %xmm2    # xmm2 = (xmm3 * xmm12) + xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm6, %xmm2
	vsubss	%xmm2, %xmm6, %xmm5
	vaddss	%xmm1, %xmm5, %xmm1
	vsubss	%xmm2, %xmm8, %xmm5
	vsubss	%xmm8, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm9
	vsubss	%xmm9, %xmm8, %xmm8
	vbroadcastss	.LCPI41_3(%rip), %xmm12 # xmm12 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm12, %xmm2
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vsubss	%xmm1, %xmm7, %xmm6
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm9
	vsubss	%xmm9, %xmm7, %xmm7
	vxorps	%xmm1, %xmm12, %xmm1
	vsubss	%xmm8, %xmm1, %xmm1
	vaddss	%xmm1, %xmm7, %xmm1
	vaddss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm6, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm6, %xmm6
	vsubss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vmovss	8(%rsp), %xmm6                  # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmulss	%xmm1, %xmm10, %xmm2
	vmovaps	%xmm10, %xmm5
	vfmsub213ss	%xmm2, %xmm1, %xmm5     # xmm5 = (xmm1 * xmm5) - xmm2
	vfmadd231ss	%xmm11, %xmm1, %xmm5    # xmm5 = (xmm1 * xmm11) + xmm5
	vmulss	%xmm6, %xmm11, %xmm1
	vfmsub213ss	%xmm1, %xmm6, %xmm11    # xmm11 = (xmm6 * xmm11) - xmm1
	vaddss	%xmm2, %xmm1, %xmm7
	vsubss	%xmm1, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm1, %xmm1
	vsubss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vfmadd231ss	%xmm0, %xmm10, %xmm11   # xmm11 = (xmm10 * xmm0) + xmm11
	vmulss	%xmm6, %xmm10, %xmm0
	vfmsub213ss	%xmm0, %xmm6, %xmm10    # xmm10 = (xmm6 * xmm10) - xmm0
	vaddss	%xmm7, %xmm10, %xmm2
	vsubss	%xmm7, %xmm2, %xmm8
	vsubss	%xmm8, %xmm2, %xmm9
	vsubss	%xmm9, %xmm7, %xmm7
	vsubss	%xmm8, %xmm10, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm5, %xmm11, %xmm5
	vfmadd231ss	56(%rsp), %xmm6, %xmm1  # 4-byte Folded Reload
                                        # xmm1 = (xmm6 * mem) + xmm1
	vaddss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm2, %xmm5
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm5, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vmulss	%xmm0, %xmm3, %xmm5
	vmulss	%xmm2, %xmm4, %xmm6
	vmovaps	%xmm2, %xmm7
	vfmsub213ss	%xmm6, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm7) - xmm6
	vfmadd231ss	%xmm0, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm0) + xmm7
	vfmsub213ss	%xmm5, %xmm3, %xmm0     # xmm0 = (xmm3 * xmm0) - xmm5
	vaddss	%xmm6, %xmm5, %xmm4
	vsubss	%xmm5, %xmm4, %xmm8
	vsubss	%xmm8, %xmm4, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vfmadd231ss	96(%rsp), %xmm2, %xmm0  # 4-byte Folded Reload
                                        # xmm0 = (xmm2 * mem) + xmm0
	vmulss	%xmm2, %xmm3, %xmm9
	vfmsub213ss	%xmm9, %xmm3, %xmm2     # xmm2 = (xmm3 * xmm2) - xmm9
	vsubss	%xmm8, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm2, %xmm4, %xmm6
	vsubss	%xmm4, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vsubss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm2
	vaddss	%xmm7, %xmm0, %xmm0
	vfmadd231ss	%xmm1, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm1) + xmm5
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm9, %xmm2
	vsubss	%xmm2, %xmm9, %xmm3
	movl	$2, %eax
	vxorps	%xmm13, %xmm13, %xmm13
	vcvtsi2ss	%eax, %xmm13, %xmm4
	vmovss	%xmm4, 96(%rsp)                 # 4-byte Spill
	vaddss	%xmm1, %xmm3, %xmm1
	vmulss	16(%rsp), %xmm4, %xmm4          # 4-byte Folded Reload
	vmulss	%xmm4, %xmm2, %xmm14
	vmulss	%xmm4, %xmm1, %xmm5
	vmulss	%xmm4, %xmm0, %xmm4
	vmulss	%xmm14, %xmm14, %xmm6
	vmulss	%xmm5, %xmm14, %xmm0
	vmovaps	%xmm5, %xmm11
	vfmsub213ss	%xmm0, %xmm14, %xmm11   # xmm11 = (xmm14 * xmm11) - xmm0
	vmulss	%xmm5, %xmm14, %xmm1
	vaddss	%xmm1, %xmm0, %xmm8
	vsubss	%xmm0, %xmm8, %xmm7
	vsubss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm9, %xmm0, %xmm0
	vmovaps	%xmm14, %xmm3
	vfmsub213ss	%xmm1, %xmm5, %xmm3     # xmm3 = (xmm5 * xmm3) - xmm1
	vsubss	%xmm7, %xmm1, %xmm1
	vmovaps	%xmm14, %xmm10
	vfmsub213ss	%xmm6, %xmm14, %xmm10   # xmm10 = (xmm14 * xmm10) - xmm6
	vaddss	%xmm1, %xmm0, %xmm9
	vaddss	%xmm10, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm0
	vsubss	%xmm0, %xmm7, %xmm1
	vsubss	%xmm1, %xmm8, %xmm1
	vsubss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm0, %xmm1, %xmm10
	vmulss	%xmm4, %xmm14, %xmm0
	vmovaps	%xmm4, %xmm8
	vfmsub213ss	%xmm0, %xmm14, %xmm8    # xmm8 = (xmm14 * xmm8) - xmm0
	vmulss	%xmm5, %xmm5, %xmm1
	vaddss	%xmm1, %xmm0, %xmm15
	vsubss	%xmm0, %xmm15, %xmm12
	vsubss	%xmm12, %xmm15, %xmm13
	vsubss	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm5, %xmm13
	vfmsub213ss	%xmm1, %xmm5, %xmm13    # xmm13 = (xmm5 * xmm13) - xmm1
	vsubss	%xmm12, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 8(%rsp)                  # 4-byte Spill
	vmulss	%xmm4, %xmm14, %xmm1
	vaddss	%xmm1, %xmm15, %xmm0
	vsubss	%xmm15, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm12
	vsubss	%xmm12, %xmm15, %xmm12
	vmovaps	%xmm14, %xmm15
	vfmsub213ss	%xmm1, %xmm4, %xmm15    # xmm15 = (xmm4 * xmm15) - xmm1
	vsubss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm12, %xmm1
	vmovss	%xmm1, 16(%rsp)                 # 4-byte Spill
	vaddss	%xmm0, %xmm11, %xmm2
	vsubss	%xmm0, %xmm2, %xmm12
	vsubss	%xmm12, %xmm2, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm12, %xmm11, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm3, %xmm2, %xmm1
	vsubss	%xmm2, %xmm1, %xmm11
	vsubss	%xmm11, %xmm1, %xmm12
	vsubss	%xmm12, %xmm2, %xmm2
	vsubss	%xmm11, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm1, %xmm9, %xmm3
	vsubss	%xmm1, %xmm3, %xmm11
	vsubss	%xmm11, %xmm3, %xmm12
	vsubss	%xmm12, %xmm1, %xmm1
	vsubss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm3, %xmm10, %xmm9
	vsubss	%xmm3, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm12, %xmm3, %xmm3
	vsubss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm3, %xmm10, %xmm3
	vxorps	%xmm11, %xmm11, %xmm11
	vmulss	%xmm11, %xmm14, %xmm10
	vfmadd231ss	%xmm4, %xmm5, %xmm10    # xmm10 = (xmm5 * xmm4) + xmm10
	vfmadd231ss	%xmm5, %xmm4, %xmm10    # xmm10 = (xmm4 * xmm5) + xmm10
	vfmadd231ss	%xmm11, %xmm14, %xmm10  # xmm10 = (xmm14 * xmm11) + xmm10
	vaddss	%xmm8, %xmm10, %xmm8
	vaddss	%xmm13, %xmm8, %xmm8
	vaddss	%xmm15, %xmm8, %xmm8
	vaddss	8(%rsp), %xmm8, %xmm8           # 4-byte Folded Reload
	vaddss	16(%rsp), %xmm8, %xmm8          # 4-byte Folded Reload
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm2
	vsubss	%xmm2, %xmm9, %xmm1
	vaddss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm2, %xmm7, %xmm0
	vsubss	%xmm0, %xmm7, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm0, %xmm6, %xmm3
	vsubss	%xmm3, %xmm6, %xmm6
	vaddss	%xmm0, %xmm6, %xmm0
	vbroadcastss	.LCPI41_3(%rip), %xmm10 # xmm10 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm3, %xmm10, %xmm7
	vmovups	160(%rsp), %xmm8                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm3, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm3
	vmovaps	%xmm8, %xmm9
	vsubss	%xmm3, %xmm6, %xmm8
	vsubss	%xmm8, %xmm9, %xmm8
	vxorps	%xmm0, %xmm10, %xmm9
	vxorps	%xmm2, %xmm10, %xmm10
	vsubss	%xmm3, %xmm7, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vmovups	192(%rsp), %xmm8                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm0, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm0
	vmovaps	%xmm8, %xmm11
	vsubss	%xmm0, %xmm7, %xmm8
	vsubss	%xmm8, %xmm11, %xmm8
	vsubss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm8, %xmm8
	vaddss	%xmm3, %xmm7, %xmm0
	vsubss	%xmm7, %xmm0, %xmm9
	vsubss	%xmm9, %xmm0, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm9, %xmm3, %xmm3
	vaddss	%xmm3, %xmm7, %xmm3
	vmovups	240(%rsp), %xmm11               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vsubss	%xmm2, %xmm11, %xmm2
	vsubss	%xmm11, %xmm2, %xmm7
	vsubss	%xmm7, %xmm2, %xmm9
	vsubss	%xmm9, %xmm11, %xmm9
	vmovaps	%xmm11, %xmm12
	vsubss	%xmm7, %xmm10, %xmm7
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm2, %xmm8, %xmm9
	vsubss	%xmm2, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm2, %xmm2
	vsubss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm3, %xmm9, %xmm8
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm8, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vsubss	%xmm10, %xmm3, %xmm3
	vaddss	%xmm3, %xmm9, %xmm3
	vmovshdup	%xmm12, %xmm9           # xmm9 = xmm12[1,1,3,3]
	vsubss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm8, %xmm2
	vsubss	%xmm2, %xmm8, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm3, %xmm6, %xmm2
	vsubss	%xmm2, %xmm6, %xmm6
	vaddss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm5, %xmm14, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vmulss	96(%rsp), %xmm1, %xmm1          # 4-byte Folded Reload
	vdivss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm5, %xmm2
	vsubss	%xmm2, %xmm5, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vinsertps	$16, %xmm0, %xmm1, %xmm4 # xmm4 = xmm1[0],xmm0[0],xmm1[2,3]
	vaddss	%xmm2, %xmm14, %xmm1
	vsubss	%xmm1, %xmm14, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vinsertps	$16, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[2,3]
.LBB41_340:
	vmovlhps	%xmm4, %xmm1, %xmm0             # xmm0 = xmm1[0],xmm4[0]
	vmovups	%xmm0, 64(%rsp)
	leaq	368(%rsp), %r14
	movq	%r14, 352(%rsp)
	movq	$32, 24(%rsp)
.Ltmp6358:
	leaq	352(%rsp), %rdi
	leaq	24(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp6359:
# %bb.341:
	movq	%rax, 352(%rsp)
	movq	24(%rsp), %rcx
	movq	%rcx, 368(%rsp)
	vmovupd	.L.str.63(%rip), %ymm0
	vmovupd	%ymm0, (%rax)
	movq	%rcx, 360(%rsp)
	movq	352(%rsp), %rax
	movb	$0, (%rax,%rcx)
.Ltmp6361:
	leaq	352(%rsp), %rdi
	leaq	64(%rsp), %rsi
	xorl	%edx, %edx
	vzeroupper
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp6362:
# %bb.342:
	movq	352(%rsp), %rdi
	cmpq	%r14, %rdi
	je	.LBB41_344
# %bb.343:
	callq	_ZdlPv
.LBB41_344:
	addq	$568, %rsp                      # imm = 0x238
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB41_345:
	.cfi_def_cfa_offset 624
.Ltmp6357:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_346:
.Ltmp6351:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_347:
.Ltmp6308:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_348:
.Ltmp6202:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_349:
.Ltmp6196:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_350:
.Ltmp6153:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_351:
.Ltmp6044:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_352:
.Ltmp6041:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_353:
.Ltmp6038:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_354:
.Ltmp6035:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_355:
.Ltmp5980:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_356:
.Ltmp5977:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_357:
.Ltmp5974:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_358:
.Ltmp5971:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_359:
.Ltmp5916:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_360:
.Ltmp5913:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_361:
.Ltmp5910:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_362:
.Ltmp5907:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_363:
.Ltmp6363:
	movq	%rax, %rbx
	movq	352(%rsp), %rdi
	cmpq	%r14, %rdi
	jne	.LBB41_381
	jmp	.LBB41_382
.LBB41_364:
.Ltmp6360:
	movq	%rax, %rdi
	callq	_Unwind_Resume@PLT
.LBB41_365:
.Ltmp6354:
	movq	%rax, %rbx
	movq	384(%rsp), %rdi
	jmp	.LBB41_369
.LBB41_366:
.Ltmp6348:
	movq	%rax, %rbx
	jmp	.LBB41_441
.LBB41_367:
.Ltmp6292:
	movq	%rax, %rbx
	jmp	.LBB41_441
.LBB41_368:
.Ltmp6199:
	movq	%rax, %rbx
	movq	416(%rsp), %rdi
.LBB41_369:
	cmpq	%r15, %rdi
	je	.LBB41_447
# %bb.370:
	callq	_ZdlPv
	jmp	.LBB41_447
.LBB41_371:
.Ltmp6193:
	movq	%rax, %rbx
	jmp	.LBB41_441
.LBB41_372:
.Ltmp6137:
	movq	%rax, %rbx
	jmp	.LBB41_441
.LBB41_373:
.Ltmp6047:
	movq	%rax, %rbx
	movq	448(%rsp), %rdi
	jmp	.LBB41_380
.LBB41_374:
.Ltmp6032:
	jmp	.LBB41_385
.LBB41_375:
.Ltmp6029:
	jmp	.LBB41_385
.LBB41_376:
.Ltmp5983:
	movq	%rax, %rbx
	movq	480(%rsp), %rdi
	jmp	.LBB41_380
.LBB41_377:
.Ltmp5968:
	jmp	.LBB41_385
.LBB41_378:
.Ltmp5965:
	jmp	.LBB41_385
.LBB41_379:
.Ltmp5919:
	movq	%rax, %rbx
	movq	512(%rsp), %rdi
.LBB41_380:
	cmpq	%r12, %rdi
	je	.LBB41_382
.LBB41_381:
	callq	_ZdlPv
.LBB41_382:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB41_383:
.Ltmp5904:
	jmp	.LBB41_385
.LBB41_384:
.Ltmp5901:
.LBB41_385:
	movq	%rax, %rbx
	leaq	64(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB41_408
.LBB41_386:
.Ltmp6345:
	movq	%rax, %rbx
	jmp	.LBB41_441
.LBB41_387:
.Ltmp6190:
	movq	%rax, %rbx
	jmp	.LBB41_441
.LBB41_388:
.Ltmp6015:
	jmp	.LBB41_407
.LBB41_389:
.Ltmp5999:
	movq	%rax, %rbx
	jmp	.LBB41_409
.LBB41_390:
.Ltmp5951:
	jmp	.LBB41_407
.LBB41_391:
.Ltmp5935:
	movq	%rax, %rbx
	jmp	.LBB41_409
.LBB41_392:
.Ltmp5887:
	jmp	.LBB41_407
.LBB41_393:
.Ltmp5871:
	movq	%rax, %rbx
	jmp	.LBB41_409
.LBB41_394:
.Ltmp6305:
	movq	%rax, %rbx
	jmp	.LBB41_441
.LBB41_395:
.Ltmp6150:
	movq	%rax, %rbx
	jmp	.LBB41_441
.LBB41_396:
.Ltmp6340:
	movq	%rax, %rbx
	jmp	.LBB41_447
.LBB41_397:
.Ltmp6289:
	movq	%rax, %rbx
	jmp	.LBB41_447
.LBB41_398:
.Ltmp6185:
	movq	%rax, %rbx
	jmp	.LBB41_447
.LBB41_399:
.Ltmp6134:
	movq	%rax, %rbx
	jmp	.LBB41_447
.LBB41_400:
.Ltmp6026:
	jmp	.LBB41_407
.LBB41_401:
.Ltmp6010:
	movq	%rax, %rbx
	jmp	.LBB41_409
.LBB41_402:
.Ltmp5994:
	jmp	.LBB41_412
.LBB41_403:
.Ltmp5962:
	jmp	.LBB41_407
.LBB41_404:
.Ltmp5946:
	movq	%rax, %rbx
	jmp	.LBB41_409
.LBB41_405:
.Ltmp5930:
	jmp	.LBB41_412
.LBB41_406:
.Ltmp5898:
.LBB41_407:
	movq	%rax, %rbx
.LBB41_408:
	leaq	24(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB41_409:
	leaq	120(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	leaq	264(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB41_410:
.Ltmp5882:
	movq	%rax, %rbx
	jmp	.LBB41_409
.LBB41_411:
.Ltmp5866:
.LBB41_412:
	movq	%rax, %rbx
	leaq	264(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB41_413:
.Ltmp6329:
	movq	%rax, %rbx
	jmp	.LBB41_447
.LBB41_414:
.Ltmp6174:
	movq	%rax, %rbx
	jmp	.LBB41_447
.LBB41_415:
.Ltmp6278:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_416:
.Ltmp6275:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_417:
.Ltmp6272:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_418:
.Ltmp6224:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_419:
.Ltmp6123:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_420:
.Ltmp6120:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_421:
.Ltmp6117:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_422:
.Ltmp6069:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB41_423:
.Ltmp6205:
	movq	%rax, %rbx
	jmp	.LBB41_447
.LBB41_424:
.Ltmp6050:
	movq	%rax, %rbx
	jmp	.LBB41_447
.LBB41_425:
.Ltmp6256:
	jmp	.LBB41_434
.LBB41_426:
.Ltmp6240:
	jmp	.LBB41_439
.LBB41_427:
.Ltmp6221:
	jmp	.LBB41_431
.LBB41_428:
.Ltmp6101:
	jmp	.LBB41_434
.LBB41_429:
.Ltmp6085:
	jmp	.LBB41_439
.LBB41_430:
.Ltmp6066:
.LBB41_431:
	movq	%rax, %rbx
	leaq	24(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB41_445
.LBB41_432:
.Ltmp6269:
	jmp	.LBB41_434
.LBB41_433:
.Ltmp6114:
.LBB41_434:
	movq	%rax, %rbx
	leaq	120(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB41_440
.LBB41_435:
.Ltmp6251:
	jmp	.LBB41_439
.LBB41_436:
.Ltmp6235:
	movq	%rax, %rbx
	jmp	.LBB41_441
.LBB41_437:
.Ltmp6216:
	jmp	.LBB41_444
.LBB41_438:
.Ltmp6096:
.LBB41_439:
	movq	%rax, %rbx
.LBB41_440:
	leaq	264(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB41_441:
	leaq	24(%rsp), %rdi
	jmp	.LBB41_446
.LBB41_442:
.Ltmp6080:
	movq	%rax, %rbx
	jmp	.LBB41_441
.LBB41_443:
.Ltmp6061:
.LBB41_444:
	movq	%rax, %rbx
.LBB41_445:
	leaq	120(%rsp), %rdi
.LBB41_446:
	callq	_ZN4mpfr6mprealD2Ev
.LBB41_447:
	leaq	64(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end41:
	.size	_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_, .Lfunc_end41-_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.cfi_endproc
	.section	.gcc_except_table._Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"aG",@progbits,_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE1EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.p2align	2, 0x0
GCC_except_table41:
.Lexception34:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase23-.Lttbaseref23
.Lttbaseref23:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end34-.Lcst_begin34
.Lcst_begin34:
	.uleb128 .Lfunc_begin34-.Lfunc_begin34  # >> Call Site 1 <<
	.uleb128 .Ltmp5856-.Lfunc_begin34       #   Call between .Lfunc_begin34 and .Ltmp5856
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5856-.Lfunc_begin34       # >> Call Site 2 <<
	.uleb128 .Ltmp5865-.Ltmp5856            #   Call between .Ltmp5856 and .Ltmp5865
	.uleb128 .Ltmp5866-.Lfunc_begin34       #     jumps to .Ltmp5866
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5867-.Lfunc_begin34       # >> Call Site 3 <<
	.uleb128 .Ltmp5870-.Ltmp5867            #   Call between .Ltmp5867 and .Ltmp5870
	.uleb128 .Ltmp5871-.Lfunc_begin34       #     jumps to .Ltmp5871
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5872-.Lfunc_begin34       # >> Call Site 4 <<
	.uleb128 .Ltmp5881-.Ltmp5872            #   Call between .Ltmp5872 and .Ltmp5881
	.uleb128 .Ltmp5882-.Lfunc_begin34       #     jumps to .Ltmp5882
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5883-.Lfunc_begin34       # >> Call Site 5 <<
	.uleb128 .Ltmp5886-.Ltmp5883            #   Call between .Ltmp5883 and .Ltmp5886
	.uleb128 .Ltmp5887-.Lfunc_begin34       #     jumps to .Ltmp5887
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5888-.Lfunc_begin34       # >> Call Site 6 <<
	.uleb128 .Ltmp5897-.Ltmp5888            #   Call between .Ltmp5888 and .Ltmp5897
	.uleb128 .Ltmp5898-.Lfunc_begin34       #     jumps to .Ltmp5898
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5899-.Lfunc_begin34       # >> Call Site 7 <<
	.uleb128 .Ltmp5900-.Ltmp5899            #   Call between .Ltmp5899 and .Ltmp5900
	.uleb128 .Ltmp5901-.Lfunc_begin34       #     jumps to .Ltmp5901
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5902-.Lfunc_begin34       # >> Call Site 8 <<
	.uleb128 .Ltmp5903-.Ltmp5902            #   Call between .Ltmp5902 and .Ltmp5903
	.uleb128 .Ltmp5904-.Lfunc_begin34       #     jumps to .Ltmp5904
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5905-.Lfunc_begin34       # >> Call Site 9 <<
	.uleb128 .Ltmp5906-.Ltmp5905            #   Call between .Ltmp5905 and .Ltmp5906
	.uleb128 .Ltmp5907-.Lfunc_begin34       #     jumps to .Ltmp5907
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5908-.Lfunc_begin34       # >> Call Site 10 <<
	.uleb128 .Ltmp5909-.Ltmp5908            #   Call between .Ltmp5908 and .Ltmp5909
	.uleb128 .Ltmp5910-.Lfunc_begin34       #     jumps to .Ltmp5910
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5911-.Lfunc_begin34       # >> Call Site 11 <<
	.uleb128 .Ltmp5912-.Ltmp5911            #   Call between .Ltmp5911 and .Ltmp5912
	.uleb128 .Ltmp5913-.Lfunc_begin34       #     jumps to .Ltmp5913
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5914-.Lfunc_begin34       # >> Call Site 12 <<
	.uleb128 .Ltmp5915-.Ltmp5914            #   Call between .Ltmp5914 and .Ltmp5915
	.uleb128 .Ltmp5916-.Lfunc_begin34       #     jumps to .Ltmp5916
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5917-.Lfunc_begin34       # >> Call Site 13 <<
	.uleb128 .Ltmp5918-.Ltmp5917            #   Call between .Ltmp5917 and .Ltmp5918
	.uleb128 .Ltmp5919-.Lfunc_begin34       #     jumps to .Ltmp5919
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5918-.Lfunc_begin34       # >> Call Site 14 <<
	.uleb128 .Ltmp5920-.Ltmp5918            #   Call between .Ltmp5918 and .Ltmp5920
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5920-.Lfunc_begin34       # >> Call Site 15 <<
	.uleb128 .Ltmp5929-.Ltmp5920            #   Call between .Ltmp5920 and .Ltmp5929
	.uleb128 .Ltmp5930-.Lfunc_begin34       #     jumps to .Ltmp5930
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5931-.Lfunc_begin34       # >> Call Site 16 <<
	.uleb128 .Ltmp5934-.Ltmp5931            #   Call between .Ltmp5931 and .Ltmp5934
	.uleb128 .Ltmp5935-.Lfunc_begin34       #     jumps to .Ltmp5935
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5936-.Lfunc_begin34       # >> Call Site 17 <<
	.uleb128 .Ltmp5945-.Ltmp5936            #   Call between .Ltmp5936 and .Ltmp5945
	.uleb128 .Ltmp5946-.Lfunc_begin34       #     jumps to .Ltmp5946
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5947-.Lfunc_begin34       # >> Call Site 18 <<
	.uleb128 .Ltmp5950-.Ltmp5947            #   Call between .Ltmp5947 and .Ltmp5950
	.uleb128 .Ltmp5951-.Lfunc_begin34       #     jumps to .Ltmp5951
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5952-.Lfunc_begin34       # >> Call Site 19 <<
	.uleb128 .Ltmp5961-.Ltmp5952            #   Call between .Ltmp5952 and .Ltmp5961
	.uleb128 .Ltmp5962-.Lfunc_begin34       #     jumps to .Ltmp5962
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5963-.Lfunc_begin34       # >> Call Site 20 <<
	.uleb128 .Ltmp5964-.Ltmp5963            #   Call between .Ltmp5963 and .Ltmp5964
	.uleb128 .Ltmp5965-.Lfunc_begin34       #     jumps to .Ltmp5965
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5966-.Lfunc_begin34       # >> Call Site 21 <<
	.uleb128 .Ltmp5967-.Ltmp5966            #   Call between .Ltmp5966 and .Ltmp5967
	.uleb128 .Ltmp5968-.Lfunc_begin34       #     jumps to .Ltmp5968
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5969-.Lfunc_begin34       # >> Call Site 22 <<
	.uleb128 .Ltmp5970-.Ltmp5969            #   Call between .Ltmp5969 and .Ltmp5970
	.uleb128 .Ltmp5971-.Lfunc_begin34       #     jumps to .Ltmp5971
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5972-.Lfunc_begin34       # >> Call Site 23 <<
	.uleb128 .Ltmp5973-.Ltmp5972            #   Call between .Ltmp5972 and .Ltmp5973
	.uleb128 .Ltmp5974-.Lfunc_begin34       #     jumps to .Ltmp5974
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5975-.Lfunc_begin34       # >> Call Site 24 <<
	.uleb128 .Ltmp5976-.Ltmp5975            #   Call between .Ltmp5975 and .Ltmp5976
	.uleb128 .Ltmp5977-.Lfunc_begin34       #     jumps to .Ltmp5977
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5978-.Lfunc_begin34       # >> Call Site 25 <<
	.uleb128 .Ltmp5979-.Ltmp5978            #   Call between .Ltmp5978 and .Ltmp5979
	.uleb128 .Ltmp5980-.Lfunc_begin34       #     jumps to .Ltmp5980
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp5981-.Lfunc_begin34       # >> Call Site 26 <<
	.uleb128 .Ltmp5982-.Ltmp5981            #   Call between .Ltmp5981 and .Ltmp5982
	.uleb128 .Ltmp5983-.Lfunc_begin34       #     jumps to .Ltmp5983
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5982-.Lfunc_begin34       # >> Call Site 27 <<
	.uleb128 .Ltmp5984-.Ltmp5982            #   Call between .Ltmp5982 and .Ltmp5984
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5984-.Lfunc_begin34       # >> Call Site 28 <<
	.uleb128 .Ltmp5993-.Ltmp5984            #   Call between .Ltmp5984 and .Ltmp5993
	.uleb128 .Ltmp5994-.Lfunc_begin34       #     jumps to .Ltmp5994
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp5995-.Lfunc_begin34       # >> Call Site 29 <<
	.uleb128 .Ltmp5998-.Ltmp5995            #   Call between .Ltmp5995 and .Ltmp5998
	.uleb128 .Ltmp5999-.Lfunc_begin34       #     jumps to .Ltmp5999
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6000-.Lfunc_begin34       # >> Call Site 30 <<
	.uleb128 .Ltmp6009-.Ltmp6000            #   Call between .Ltmp6000 and .Ltmp6009
	.uleb128 .Ltmp6010-.Lfunc_begin34       #     jumps to .Ltmp6010
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6011-.Lfunc_begin34       # >> Call Site 31 <<
	.uleb128 .Ltmp6014-.Ltmp6011            #   Call between .Ltmp6011 and .Ltmp6014
	.uleb128 .Ltmp6015-.Lfunc_begin34       #     jumps to .Ltmp6015
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6016-.Lfunc_begin34       # >> Call Site 32 <<
	.uleb128 .Ltmp6025-.Ltmp6016            #   Call between .Ltmp6016 and .Ltmp6025
	.uleb128 .Ltmp6026-.Lfunc_begin34       #     jumps to .Ltmp6026
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6027-.Lfunc_begin34       # >> Call Site 33 <<
	.uleb128 .Ltmp6028-.Ltmp6027            #   Call between .Ltmp6027 and .Ltmp6028
	.uleb128 .Ltmp6029-.Lfunc_begin34       #     jumps to .Ltmp6029
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6030-.Lfunc_begin34       # >> Call Site 34 <<
	.uleb128 .Ltmp6031-.Ltmp6030            #   Call between .Ltmp6030 and .Ltmp6031
	.uleb128 .Ltmp6032-.Lfunc_begin34       #     jumps to .Ltmp6032
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6033-.Lfunc_begin34       # >> Call Site 35 <<
	.uleb128 .Ltmp6034-.Ltmp6033            #   Call between .Ltmp6033 and .Ltmp6034
	.uleb128 .Ltmp6035-.Lfunc_begin34       #     jumps to .Ltmp6035
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6036-.Lfunc_begin34       # >> Call Site 36 <<
	.uleb128 .Ltmp6037-.Ltmp6036            #   Call between .Ltmp6036 and .Ltmp6037
	.uleb128 .Ltmp6038-.Lfunc_begin34       #     jumps to .Ltmp6038
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6039-.Lfunc_begin34       # >> Call Site 37 <<
	.uleb128 .Ltmp6040-.Ltmp6039            #   Call between .Ltmp6039 and .Ltmp6040
	.uleb128 .Ltmp6041-.Lfunc_begin34       #     jumps to .Ltmp6041
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6042-.Lfunc_begin34       # >> Call Site 38 <<
	.uleb128 .Ltmp6043-.Ltmp6042            #   Call between .Ltmp6042 and .Ltmp6043
	.uleb128 .Ltmp6044-.Lfunc_begin34       #     jumps to .Ltmp6044
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6045-.Lfunc_begin34       # >> Call Site 39 <<
	.uleb128 .Ltmp6046-.Ltmp6045            #   Call between .Ltmp6045 and .Ltmp6046
	.uleb128 .Ltmp6047-.Lfunc_begin34       #     jumps to .Ltmp6047
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6046-.Lfunc_begin34       # >> Call Site 40 <<
	.uleb128 .Ltmp6048-.Ltmp6046            #   Call between .Ltmp6046 and .Ltmp6048
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6048-.Lfunc_begin34       # >> Call Site 41 <<
	.uleb128 .Ltmp6049-.Ltmp6048            #   Call between .Ltmp6048 and .Ltmp6049
	.uleb128 .Ltmp6050-.Lfunc_begin34       #     jumps to .Ltmp6050
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6051-.Lfunc_begin34       # >> Call Site 42 <<
	.uleb128 .Ltmp6060-.Ltmp6051            #   Call between .Ltmp6051 and .Ltmp6060
	.uleb128 .Ltmp6061-.Lfunc_begin34       #     jumps to .Ltmp6061
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6062-.Lfunc_begin34       # >> Call Site 43 <<
	.uleb128 .Ltmp6065-.Ltmp6062            #   Call between .Ltmp6062 and .Ltmp6065
	.uleb128 .Ltmp6066-.Lfunc_begin34       #     jumps to .Ltmp6066
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6067-.Lfunc_begin34       # >> Call Site 44 <<
	.uleb128 .Ltmp6068-.Ltmp6067            #   Call between .Ltmp6067 and .Ltmp6068
	.uleb128 .Ltmp6069-.Lfunc_begin34       #     jumps to .Ltmp6069
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6070-.Lfunc_begin34       # >> Call Site 45 <<
	.uleb128 .Ltmp6079-.Ltmp6070            #   Call between .Ltmp6070 and .Ltmp6079
	.uleb128 .Ltmp6080-.Lfunc_begin34       #     jumps to .Ltmp6080
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6081-.Lfunc_begin34       # >> Call Site 46 <<
	.uleb128 .Ltmp6084-.Ltmp6081            #   Call between .Ltmp6081 and .Ltmp6084
	.uleb128 .Ltmp6085-.Lfunc_begin34       #     jumps to .Ltmp6085
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6086-.Lfunc_begin34       # >> Call Site 47 <<
	.uleb128 .Ltmp6095-.Ltmp6086            #   Call between .Ltmp6086 and .Ltmp6095
	.uleb128 .Ltmp6096-.Lfunc_begin34       #     jumps to .Ltmp6096
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6097-.Lfunc_begin34       # >> Call Site 48 <<
	.uleb128 .Ltmp6100-.Ltmp6097            #   Call between .Ltmp6097 and .Ltmp6100
	.uleb128 .Ltmp6101-.Lfunc_begin34       #     jumps to .Ltmp6101
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6102-.Lfunc_begin34       # >> Call Site 49 <<
	.uleb128 .Ltmp6113-.Ltmp6102            #   Call between .Ltmp6102 and .Ltmp6113
	.uleb128 .Ltmp6114-.Lfunc_begin34       #     jumps to .Ltmp6114
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6115-.Lfunc_begin34       # >> Call Site 50 <<
	.uleb128 .Ltmp6116-.Ltmp6115            #   Call between .Ltmp6115 and .Ltmp6116
	.uleb128 .Ltmp6117-.Lfunc_begin34       #     jumps to .Ltmp6117
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6118-.Lfunc_begin34       # >> Call Site 51 <<
	.uleb128 .Ltmp6119-.Ltmp6118            #   Call between .Ltmp6118 and .Ltmp6119
	.uleb128 .Ltmp6120-.Lfunc_begin34       #     jumps to .Ltmp6120
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6121-.Lfunc_begin34       # >> Call Site 52 <<
	.uleb128 .Ltmp6122-.Ltmp6121            #   Call between .Ltmp6121 and .Ltmp6122
	.uleb128 .Ltmp6123-.Lfunc_begin34       #     jumps to .Ltmp6123
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6124-.Lfunc_begin34       # >> Call Site 53 <<
	.uleb128 .Ltmp6133-.Ltmp6124            #   Call between .Ltmp6124 and .Ltmp6133
	.uleb128 .Ltmp6134-.Lfunc_begin34       #     jumps to .Ltmp6134
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6135-.Lfunc_begin34       # >> Call Site 54 <<
	.uleb128 .Ltmp6136-.Ltmp6135            #   Call between .Ltmp6135 and .Ltmp6136
	.uleb128 .Ltmp6137-.Lfunc_begin34       #     jumps to .Ltmp6137
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6138-.Lfunc_begin34       # >> Call Site 55 <<
	.uleb128 .Ltmp6149-.Ltmp6138            #   Call between .Ltmp6138 and .Ltmp6149
	.uleb128 .Ltmp6150-.Lfunc_begin34       #     jumps to .Ltmp6150
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6151-.Lfunc_begin34       # >> Call Site 56 <<
	.uleb128 .Ltmp6152-.Ltmp6151            #   Call between .Ltmp6151 and .Ltmp6152
	.uleb128 .Ltmp6153-.Lfunc_begin34       #     jumps to .Ltmp6153
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6154-.Lfunc_begin34       # >> Call Site 57 <<
	.uleb128 .Ltmp6173-.Ltmp6154            #   Call between .Ltmp6154 and .Ltmp6173
	.uleb128 .Ltmp6174-.Lfunc_begin34       #     jumps to .Ltmp6174
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6175-.Lfunc_begin34       # >> Call Site 58 <<
	.uleb128 .Ltmp6184-.Ltmp6175            #   Call between .Ltmp6175 and .Ltmp6184
	.uleb128 .Ltmp6185-.Lfunc_begin34       #     jumps to .Ltmp6185
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6186-.Lfunc_begin34       # >> Call Site 59 <<
	.uleb128 .Ltmp6189-.Ltmp6186            #   Call between .Ltmp6186 and .Ltmp6189
	.uleb128 .Ltmp6190-.Lfunc_begin34       #     jumps to .Ltmp6190
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6191-.Lfunc_begin34       # >> Call Site 60 <<
	.uleb128 .Ltmp6192-.Ltmp6191            #   Call between .Ltmp6191 and .Ltmp6192
	.uleb128 .Ltmp6193-.Lfunc_begin34       #     jumps to .Ltmp6193
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6194-.Lfunc_begin34       # >> Call Site 61 <<
	.uleb128 .Ltmp6195-.Ltmp6194            #   Call between .Ltmp6194 and .Ltmp6195
	.uleb128 .Ltmp6196-.Lfunc_begin34       #     jumps to .Ltmp6196
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6197-.Lfunc_begin34       # >> Call Site 62 <<
	.uleb128 .Ltmp6198-.Ltmp6197            #   Call between .Ltmp6197 and .Ltmp6198
	.uleb128 .Ltmp6199-.Lfunc_begin34       #     jumps to .Ltmp6199
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6200-.Lfunc_begin34       # >> Call Site 63 <<
	.uleb128 .Ltmp6201-.Ltmp6200            #   Call between .Ltmp6200 and .Ltmp6201
	.uleb128 .Ltmp6202-.Lfunc_begin34       #     jumps to .Ltmp6202
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6201-.Lfunc_begin34       # >> Call Site 64 <<
	.uleb128 .Ltmp6203-.Ltmp6201            #   Call between .Ltmp6201 and .Ltmp6203
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6203-.Lfunc_begin34       # >> Call Site 65 <<
	.uleb128 .Ltmp6204-.Ltmp6203            #   Call between .Ltmp6203 and .Ltmp6204
	.uleb128 .Ltmp6205-.Lfunc_begin34       #     jumps to .Ltmp6205
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6206-.Lfunc_begin34       # >> Call Site 66 <<
	.uleb128 .Ltmp6215-.Ltmp6206            #   Call between .Ltmp6206 and .Ltmp6215
	.uleb128 .Ltmp6216-.Lfunc_begin34       #     jumps to .Ltmp6216
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6217-.Lfunc_begin34       # >> Call Site 67 <<
	.uleb128 .Ltmp6220-.Ltmp6217            #   Call between .Ltmp6217 and .Ltmp6220
	.uleb128 .Ltmp6221-.Lfunc_begin34       #     jumps to .Ltmp6221
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6222-.Lfunc_begin34       # >> Call Site 68 <<
	.uleb128 .Ltmp6223-.Ltmp6222            #   Call between .Ltmp6222 and .Ltmp6223
	.uleb128 .Ltmp6224-.Lfunc_begin34       #     jumps to .Ltmp6224
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6225-.Lfunc_begin34       # >> Call Site 69 <<
	.uleb128 .Ltmp6234-.Ltmp6225            #   Call between .Ltmp6225 and .Ltmp6234
	.uleb128 .Ltmp6235-.Lfunc_begin34       #     jumps to .Ltmp6235
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6236-.Lfunc_begin34       # >> Call Site 70 <<
	.uleb128 .Ltmp6239-.Ltmp6236            #   Call between .Ltmp6236 and .Ltmp6239
	.uleb128 .Ltmp6240-.Lfunc_begin34       #     jumps to .Ltmp6240
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6241-.Lfunc_begin34       # >> Call Site 71 <<
	.uleb128 .Ltmp6250-.Ltmp6241            #   Call between .Ltmp6241 and .Ltmp6250
	.uleb128 .Ltmp6251-.Lfunc_begin34       #     jumps to .Ltmp6251
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6252-.Lfunc_begin34       # >> Call Site 72 <<
	.uleb128 .Ltmp6255-.Ltmp6252            #   Call between .Ltmp6252 and .Ltmp6255
	.uleb128 .Ltmp6256-.Lfunc_begin34       #     jumps to .Ltmp6256
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6257-.Lfunc_begin34       # >> Call Site 73 <<
	.uleb128 .Ltmp6268-.Ltmp6257            #   Call between .Ltmp6257 and .Ltmp6268
	.uleb128 .Ltmp6269-.Lfunc_begin34       #     jumps to .Ltmp6269
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6270-.Lfunc_begin34       # >> Call Site 74 <<
	.uleb128 .Ltmp6271-.Ltmp6270            #   Call between .Ltmp6270 and .Ltmp6271
	.uleb128 .Ltmp6272-.Lfunc_begin34       #     jumps to .Ltmp6272
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6273-.Lfunc_begin34       # >> Call Site 75 <<
	.uleb128 .Ltmp6274-.Ltmp6273            #   Call between .Ltmp6273 and .Ltmp6274
	.uleb128 .Ltmp6275-.Lfunc_begin34       #     jumps to .Ltmp6275
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6276-.Lfunc_begin34       # >> Call Site 76 <<
	.uleb128 .Ltmp6277-.Ltmp6276            #   Call between .Ltmp6276 and .Ltmp6277
	.uleb128 .Ltmp6278-.Lfunc_begin34       #     jumps to .Ltmp6278
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6279-.Lfunc_begin34       # >> Call Site 77 <<
	.uleb128 .Ltmp6288-.Ltmp6279            #   Call between .Ltmp6279 and .Ltmp6288
	.uleb128 .Ltmp6289-.Lfunc_begin34       #     jumps to .Ltmp6289
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6290-.Lfunc_begin34       # >> Call Site 78 <<
	.uleb128 .Ltmp6291-.Ltmp6290            #   Call between .Ltmp6290 and .Ltmp6291
	.uleb128 .Ltmp6292-.Lfunc_begin34       #     jumps to .Ltmp6292
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6293-.Lfunc_begin34       # >> Call Site 79 <<
	.uleb128 .Ltmp6304-.Ltmp6293            #   Call between .Ltmp6293 and .Ltmp6304
	.uleb128 .Ltmp6305-.Lfunc_begin34       #     jumps to .Ltmp6305
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6306-.Lfunc_begin34       # >> Call Site 80 <<
	.uleb128 .Ltmp6307-.Ltmp6306            #   Call between .Ltmp6306 and .Ltmp6307
	.uleb128 .Ltmp6308-.Lfunc_begin34       #     jumps to .Ltmp6308
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6309-.Lfunc_begin34       # >> Call Site 81 <<
	.uleb128 .Ltmp6328-.Ltmp6309            #   Call between .Ltmp6309 and .Ltmp6328
	.uleb128 .Ltmp6329-.Lfunc_begin34       #     jumps to .Ltmp6329
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6330-.Lfunc_begin34       # >> Call Site 82 <<
	.uleb128 .Ltmp6339-.Ltmp6330            #   Call between .Ltmp6330 and .Ltmp6339
	.uleb128 .Ltmp6340-.Lfunc_begin34       #     jumps to .Ltmp6340
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6341-.Lfunc_begin34       # >> Call Site 83 <<
	.uleb128 .Ltmp6344-.Ltmp6341            #   Call between .Ltmp6341 and .Ltmp6344
	.uleb128 .Ltmp6345-.Lfunc_begin34       #     jumps to .Ltmp6345
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6346-.Lfunc_begin34       # >> Call Site 84 <<
	.uleb128 .Ltmp6347-.Ltmp6346            #   Call between .Ltmp6346 and .Ltmp6347
	.uleb128 .Ltmp6348-.Lfunc_begin34       #     jumps to .Ltmp6348
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6349-.Lfunc_begin34       # >> Call Site 85 <<
	.uleb128 .Ltmp6350-.Ltmp6349            #   Call between .Ltmp6349 and .Ltmp6350
	.uleb128 .Ltmp6351-.Lfunc_begin34       #     jumps to .Ltmp6351
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6352-.Lfunc_begin34       # >> Call Site 86 <<
	.uleb128 .Ltmp6353-.Ltmp6352            #   Call between .Ltmp6352 and .Ltmp6353
	.uleb128 .Ltmp6354-.Lfunc_begin34       #     jumps to .Ltmp6354
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6355-.Lfunc_begin34       # >> Call Site 87 <<
	.uleb128 .Ltmp6356-.Ltmp6355            #   Call between .Ltmp6355 and .Ltmp6356
	.uleb128 .Ltmp6357-.Lfunc_begin34       #     jumps to .Ltmp6357
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6356-.Lfunc_begin34       # >> Call Site 88 <<
	.uleb128 .Ltmp6358-.Ltmp6356            #   Call between .Ltmp6356 and .Ltmp6358
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6358-.Lfunc_begin34       # >> Call Site 89 <<
	.uleb128 .Ltmp6359-.Ltmp6358            #   Call between .Ltmp6358 and .Ltmp6359
	.uleb128 .Ltmp6360-.Lfunc_begin34       #     jumps to .Ltmp6360
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6361-.Lfunc_begin34       # >> Call Site 90 <<
	.uleb128 .Ltmp6362-.Ltmp6361            #   Call between .Ltmp6361 and .Ltmp6362
	.uleb128 .Ltmp6363-.Lfunc_begin34       #     jumps to .Ltmp6363
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6362-.Lfunc_begin34       # >> Call Site 91 <<
	.uleb128 .Lfunc_end41-.Ltmp6362         #   Call between .Ltmp6362 and .Lfunc_end41
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end34:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase23:
	.p2align	2, 0x0
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2, 0x0                          # -- Begin function _Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.LCPI42_0:
	.long	0x7fc00000                      #  NaN
.LCPI42_1:
	.long	0x3f800000                      #  1
.LCPI42_2:
	.long	0x3f000000                      #  0.5
.LCPI42_3:
	.long	0x80000000                      #  -0
.LCPI42_4:
	.long	0x7f800000                      #  +Inf
.LCPI42_5:
	.long	0xff800000                      #  -Inf
.LCPI42_7:
	.long	0x00000000                      #  0
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4, 0x0
.LCPI42_6:
	.zero	16
	.section	.text._Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"axG",@progbits,_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.weak	_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.p2align	4, 0x90
	.type	_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,@function
_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_: # 
.Lfunc_begin35:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception35
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$568, %rsp                      # imm = 0x238
	.cfi_def_cfa_offset 624
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, 152(%rsp)                  # 8-byte Spill
	movq	%rcx, 336(%rsp)                 # 8-byte Spill
	movq	%rdx, %r12
	movq	%rsi, %rbp
	movq	%rdi, %rbx
	vcvtsi2sdl	(%rdi), %xmm0, %xmm0
	callq	sqrt
	vcvttsd2si	%xmm0, %eax
	movl	%eax, 28(%rsp)
	movq	%rbx, 208(%rsp)                 # 8-byte Spill
	movslq	(%rbx), %r15
	movq	%r15, %r13
	shlq	$4, %r13
	testq	%r15, %r15
	movq	$-1, %r14
	cmovnsq	%r13, %r14
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %rbx
	movq	%rax, 40(%rsp)                  # 8-byte Spill
	testq	%r15, %r15
	je	.LBB42_5
# %bb.1:
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movq	%r13, %rdx
	callq	_intel_fast_memset@PLT
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%r13, %rdx
	callq	_intel_fast_memset@PLT
	testl	%r15d, %r15d
	jle	.LBB42_6
# %bb.2:
	movq	%rbp, 48(%rsp)                  # 8-byte Spill
	movl	$8, %ebp
	xorl	%r15d, %r15d
	movq	336(%rsp), %r13                 # 8-byte Reload
	movq	32(%rsp), %r14                  # 8-byte Reload
	.p2align	4, 0x90
.LBB42_3:                               # =>This Inner Loop Header: Depth=1
	movq	%r12, %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, -8(%rbx,%rbp)
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, -8(%r14,%rbp)
	incq	%r15
	movq	208(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$16, %rbp
	addq	$32, %r13
	addq	$32, %r12
	cmpq	%rax, %r15
	jl	.LBB42_3
# %bb.4:
	movq	48(%rsp), %rbp                  # 8-byte Reload
	jmp	.LBB42_6
.LBB42_5:
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, 32(%rsp)                  # 8-byte Spill
.LBB42_6:
	movq	%rbp, %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 240(%rsp)
	movl	$160, %edi
	callq	_Znam
	movq	%rax, %r15
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rax)
	vmovupd	%xmm0, 16(%rax)
	vmovupd	%xmm0, 32(%rax)
	vmovupd	%xmm0, 48(%rax)
	vmovupd	%xmm0, 64(%rax)
	vmovupd	%xmm0, 80(%rax)
	vmovupd	%xmm0, 96(%rax)
	vmovupd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 128(%rax)
	vmovupd	%xmm0, 144(%rax)
	callq	omp_get_wtime
	vmovsd	%xmm0, 72(%rsp)                 # 8-byte Spill
	movq	208(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %ecx
	xorl	%eax, %eax
	xorl	%edx, %edx
	movq	32(%rsp), %r9                   # 8-byte Reload
	jmp	.LBB42_9
	.p2align	4, 0x90
.LBB42_7:                               #   in Loop: Header=BB42_9 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
.LBB42_8:                               #   in Loop: Header=BB42_9 Depth=1
	vmovups	48(%rsp), %xmm1                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmovlhps	%xmm0, %xmm1, %xmm0             # xmm0 = xmm1[0],xmm0[0]
	movq	%rdx, %rsi
	shlq	$4, %rsi
	vmovups	%xmm0, (%r15,%rsi)
	incq	%rdx
	cmpq	$10, %rdx
	je	.LBB42_12
.LBB42_9:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB42_11 Depth 2
	vcvtsi2ss	%eax, %xmm14, %xmm0
	vblendps	$14, .LCPI42_6(%rip), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[1,2,3]
	vmovups	%xmm0, 48(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	testl	%ecx, %ecx
	jle	.LBB42_7
# %bb.10:                               #   in Loop: Header=BB42_9 Depth=1
	movq	208(%rsp), %rcx                 # 8-byte Reload
	movl	(%rcx), %ecx
	movslq	%ecx, %rsi
	vxorps	%xmm0, %xmm0, %xmm0
	movl	$12, %edi
	xorl	%r8d, %r8d
	.p2align	4, 0x90
.LBB42_11:                              #   Parent Loop BB42_9 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	%xmm0, 256(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovss	-12(%r9,%rdi), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vmovss	-12(%rbx,%rdi), %xmm3           # xmm3 = mem[0],zero,zero,zero
	vmovss	-8(%r9,%rdi), %xmm7             # xmm7 = mem[0],zero,zero,zero
	vmovss	-8(%rbx,%rdi), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm6, %xmm3, %xmm2
	vmovss	%xmm2, 224(%rsp)                # 4-byte Spill
	vmovaps	%xmm6, %xmm4
	vfmsub213ss	%xmm2, %xmm3, %xmm4     # xmm4 = (xmm3 * xmm4) - xmm2
	vmulss	%xmm7, %xmm3, %xmm8
	vmovaps	%xmm7, %xmm9
	vmulss	%xmm6, %xmm1, %xmm11
	vmovaps	%xmm6, %xmm10
	vaddss	%xmm4, %xmm8, %xmm12
	vsubss	%xmm4, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm4, %xmm14
	vsubss	%xmm13, %xmm8, %xmm13
	vaddss	%xmm11, %xmm12, %xmm2
	vmovss	%xmm2, 160(%rsp)                # 4-byte Spill
	vsubss	%xmm12, %xmm2, %xmm15
	vaddss	%xmm13, %xmm14, %xmm13
	vsubss	%xmm15, %xmm2, %xmm14
	vsubss	%xmm14, %xmm12, %xmm12
	vsubss	%xmm15, %xmm11, %xmm14
	vfmsub213ss	%xmm8, %xmm3, %xmm9     # xmm9 = (xmm3 * xmm9) - xmm8
	vaddss	%xmm14, %xmm12, %xmm12
	vmovss	-4(%r9,%rdi), %xmm0             # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 8(%rsp)                  # 4-byte Spill
	vaddss	%xmm12, %xmm13, %xmm14
	vfmsub213ss	%xmm11, %xmm1, %xmm10   # xmm10 = (xmm1 * xmm10) - xmm11
	vsubss	%xmm13, %xmm14, %xmm11
	vsubss	%xmm11, %xmm14, %xmm15
	vsubss	%xmm15, %xmm13, %xmm13
	vmulss	%xmm0, %xmm3, %xmm15
	vsubss	%xmm11, %xmm12, %xmm11
	vmovaps	%xmm0, %xmm12
	vfmsub213ss	%xmm15, %xmm3, %xmm12   # xmm12 = (xmm3 * xmm12) - xmm15
	vaddss	%xmm11, %xmm13, %xmm11
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm10, %xmm9, %xmm12
	vsubss	%xmm9, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm2
	vsubss	%xmm2, %xmm9, %xmm5
	vsubss	%xmm13, %xmm10, %xmm9
	vmovss	%xmm1, 4(%rsp)                  # 4-byte Spill
	vmulss	%xmm7, %xmm1, %xmm10
	vmovaps	%xmm7, %xmm13
	vfmsub213ss	%xmm10, %xmm1, %xmm13   # xmm13 = (xmm1 * xmm13) - xmm10
	vmovss	-4(%rbx,%rdi), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm6, %xmm4, %xmm1
	vaddss	%xmm1, %xmm10, %xmm0
	vsubss	%xmm10, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm8
	vsubss	%xmm8, %xmm10, %xmm8
	vaddss	%xmm5, %xmm9, %xmm5
	vmovaps	%xmm6, %xmm9
	vfmsub213ss	%xmm1, %xmm4, %xmm9     # xmm9 = (xmm4 * xmm9) - xmm1
	vsubss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm13, %xmm11, %xmm2
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm15, %xmm12, %xmm8
	vsubss	%xmm12, %xmm8, %xmm10
	vsubss	%xmm10, %xmm8, %xmm11
	vsubss	%xmm11, %xmm12, %xmm11
	vsubss	%xmm10, %xmm15, %xmm10
	vaddss	%xmm10, %xmm11, %xmm10
	vaddss	%xmm2, %xmm9, %xmm2
	vaddss	%xmm0, %xmm14, %xmm9
	vsubss	%xmm0, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm12, %xmm0, %xmm0
	vsubss	%xmm11, %xmm14, %xmm11
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm9, %xmm8, %xmm11
	vsubss	%xmm8, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm8, %xmm8
	vsubss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm10, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vfmadd231ss	(%rbx,%rdi), %xmm6, %xmm0 # xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ss	%xmm7, %xmm4, %xmm0     # xmm0 = (xmm4 * xmm7) + xmm0
	vmovss	4(%rsp), %xmm1                  # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vfmadd231ss	8(%rsp), %xmm1, %xmm0   # 4-byte Folded Reload
                                        # xmm0 = (xmm1 * mem) + xmm0
	vfmadd231ss	(%r9,%rdi), %xmm3, %xmm0 # xmm0 = (xmm3 * mem) + xmm0
	vaddss	%xmm0, %xmm11, %xmm1
	vsubss	%xmm1, %xmm11, %xmm2
	vmovss	160(%rsp), %xmm8                # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vaddss	%xmm1, %xmm8, %xmm4
	vmovss	224(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vaddss	%xmm4, %xmm6, %xmm5
	vaddss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm5, %xmm6, %xmm0
	vmovups	48(%rsp), %xmm9                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm5, %xmm9, %xmm2
	vsubss	%xmm9, %xmm2, %xmm6
	vaddss	%xmm4, %xmm0, %xmm0
	vsubss	%xmm6, %xmm2, %xmm7
	vsubss	%xmm7, %xmm9, %xmm7
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm4, %xmm8, %xmm4
	vaddss	%xmm5, %xmm7, %xmm5
	vmovshdup	%xmm9, %xmm6            # xmm6 = xmm9[1,1,3,3]
	vaddss	%xmm0, %xmm6, %xmm7
	vaddss	%xmm1, %xmm4, %xmm1
	vsubss	%xmm6, %xmm7, %xmm4
	vsubss	%xmm4, %xmm7, %xmm8
	vsubss	%xmm4, %xmm0, %xmm0
	vsubss	%xmm8, %xmm6, %xmm4
	vmovups	256(%rsp), %xmm10               # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm1, %xmm10, %xmm6
	vsubss	%xmm10, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm9
	vsubss	%xmm9, %xmm10, %xmm9
	vsubss	%xmm8, %xmm1, %xmm1
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm7, %xmm5, %xmm8
	vaddss	%xmm0, %xmm4, %xmm0
	vsubss	%xmm5, %xmm8, %xmm4
	vsubss	%xmm4, %xmm8, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vmovshdup	%xmm10, %xmm9           # xmm9 = xmm10[1,1,3,3]
	vsubss	%xmm4, %xmm7, %xmm4
	vaddss	%xmm4, %xmm5, %xmm4
	vaddss	%xmm0, %xmm4, %xmm5
	vaddss	%xmm1, %xmm9, %xmm1
	vsubss	%xmm4, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm9
	vsubss	%xmm7, %xmm0, %xmm0
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm6, %xmm5, %xmm4
	vsubss	%xmm5, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vsubss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm1, %xmm8, %xmm3
	vsubss	%xmm3, %xmm8, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	vinsertps	$16, %xmm2, %xmm4, %xmm1 # xmm1 = xmm4[0],xmm2[0],xmm4[2,3]
	vmovups	%xmm1, 48(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	incq	%r8
	addq	$16, %rdi
	cmpq	%rsi, %r8
	jl	.LBB42_11
	jmp	.LBB42_8
.LBB42_12:
	callq	omp_get_wtime
	vsubsd	72(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm14, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	280(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp6364:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp6365:
# %bb.13:
.Ltmp6366:
	movq	%rax, %r13
	movq	152(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp6367:
# %bb.14:
.Ltmp6368:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp6369:
# %bb.15:
	movl	%eax, %r12d
	cmpq	%rbp, %r13
	cmovgq	%r13, %rbp
.Ltmp6370:
	leaq	176(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp6371:
# %bb.16:
.Ltmp6372:
	leaq	176(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6373:
# %bb.17:
.Ltmp6375:
	callq	mpfr_get_default_rounding_mode
.Ltmp6376:
# %bb.18:
.Ltmp6377:
	leaq	176(%rsp), %rdi
	leaq	280(%rsp), %rsi
	movq	152(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp6378:
# %bb.19:
.Ltmp6380:
	leaq	176(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6381:
# %bb.20:
.Ltmp6382:
	movq	%rax, %r13
	movq	152(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp6383:
# %bb.21:
.Ltmp6384:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp6385:
# %bb.22:
	movl	%eax, %r12d
	cmpq	%rbp, %r13
	cmovgq	%r13, %rbp
.Ltmp6386:
	leaq	80(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp6387:
# %bb.23:
.Ltmp6388:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6389:
# %bb.24:
.Ltmp6391:
	callq	mpfr_get_default_rounding_mode
.Ltmp6392:
# %bb.25:
.Ltmp6393:
	leaq	80(%rsp), %rdi
	leaq	176(%rsp), %rsi
	movq	152(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp6394:
# %bb.26:
.Ltmp6396:
	callq	mpfr_get_default_rounding_mode
.Ltmp6397:
# %bb.27:
.Ltmp6398:
	movl	%eax, %r12d
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6399:
# %bb.28:
.Ltmp6400:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp6401:
# %bb.29:
.Ltmp6402:
	movl	%eax, %r13d
	leaq	112(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp6403:
# %bb.30:
.Ltmp6404:
	leaq	112(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r13d, %edx
	callq	mpfr_set_si
.Ltmp6405:
# %bb.31:
.Ltmp6407:
	leaq	112(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movl	%r12d, %edx
	callq	mpfr_abs
.Ltmp6408:
# %bb.32:
.Ltmp6410:
	leaq	112(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp6411:
# %bb.33:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 352(%rsp)
	cmpq	$0, 136(%rsp)
	je	.LBB42_35
# %bb.34:
.Ltmp6413:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6414:
.LBB42_35:
	cmpq	$0, 104(%rsp)
	je	.LBB42_37
# %bb.36:
.Ltmp6416:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6417:
.LBB42_37:
	cmpq	$0, 200(%rsp)
	je	.LBB42_39
# %bb.38:
.Ltmp6419:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6420:
.LBB42_39:
	cmpq	$0, 304(%rsp)
	je	.LBB42_41
# %bb.40:
.Ltmp6422:
	leaq	280(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6423:
.LBB42_41:
	leaq	552(%rsp), %r12
	movq	%r12, 536(%rsp)
	movl	$544501604, 552(%rsp)           # imm = 0x20746F64
	movw	$32, 556(%rsp)
	movq	$5, 544(%rsp)
.Ltmp6425:
	leaq	536(%rsp), %rdi
	leaq	352(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp6426:
# %bb.42:
	movq	536(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB42_44
# %bb.43:
	callq	_ZdlPv
.LBB42_44:
	movq	%r15, %rdi
	callq	_ZdaPv
	movl	$160, %edi
	callq	_Znam
	movq	%rax, %r15
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rax)
	vmovupd	%xmm0, 16(%rax)
	vmovupd	%xmm0, 32(%rax)
	vmovupd	%xmm0, 48(%rax)
	vmovupd	%xmm0, 64(%rax)
	vmovupd	%xmm0, 80(%rax)
	vmovupd	%xmm0, 96(%rax)
	vmovupd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 128(%rax)
	vmovupd	%xmm0, 144(%rax)
	callq	omp_get_wtime
	movq	%r15, %rdi
	vmovsd	%xmm0, 400(%rsp)                # 8-byte Spill
	movq	40(%rsp), %rbp                  # 8-byte Reload
	addq	$12, %rbp
	xorl	%r15d, %r15d
	movl	$2, %r12d
	xorl	%r13d, %r13d
	movq	%rdi, 344(%rsp)                 # 8-byte Spill
	jmp	.LBB42_47
	.p2align	4, 0x90
.LBB42_45:                              #   in Loop: Header=BB42_47 Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
	vblendps	$2, %xmm1, %xmm0, %xmm5         # xmm5 = xmm0[0],xmm1[1],xmm0[2,3]
.LBB42_46:                              #   in Loop: Header=BB42_47 Depth=1
	vmovlhps	%xmm1, %xmm5, %xmm0             # xmm0 = xmm5[0],xmm1[0]
	movq	%r13, %rax
	shlq	$4, %rax
	vmovups	%xmm0, (%rdi,%rax)
	incq	%r13
	cmpq	$10, %r13
	je	.LBB42_59
.LBB42_47:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB42_49 Depth 2
	vxorps	%xmm11, %xmm11, %xmm11
	vcvtsi2ss	%r15d, %xmm11, %xmm0
	vblendps	$14, .LCPI42_6(%rip), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[1,2,3]
	movq	208(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	vxorps	%xmm2, %xmm2, %xmm2
	testq	%rax, %rax
	jle	.LBB42_50
# %bb.48:                               #   in Loop: Header=BB42_47 Depth=1
	movq	%rbp, %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB42_49:                              #   Parent Loop BB42_47 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	%xmm2, 48(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovss	-12(%rcx), %xmm1                # xmm1 = mem[0],zero,zero,zero
	vmovss	-8(%rcx), %xmm3                 # xmm3 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm1, %xmm4
	vmovss	%xmm4, 224(%rsp)                # 4-byte Spill
	vmovaps	%xmm1, %xmm2
	vfmsub213ss	%xmm4, %xmm1, %xmm2     # xmm2 = (xmm1 * xmm2) - xmm4
	vmulss	%xmm3, %xmm1, %xmm4
	vmovaps	%xmm3, %xmm5
	vmulss	%xmm1, %xmm3, %xmm7
	vmovaps	%xmm1, %xmm6
	vaddss	%xmm4, %xmm2, %xmm8
	vfmsub213ss	%xmm4, %xmm1, %xmm5     # xmm5 = (xmm1 * xmm5) - xmm4
	vsubss	%xmm2, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm9, %xmm4, %xmm4
	vsubss	%xmm10, %xmm2, %xmm9
	vaddss	%xmm7, %xmm8, %xmm2
	vsubss	%xmm8, %xmm2, %xmm10
	vsubss	%xmm10, %xmm2, %xmm11
	vaddss	%xmm4, %xmm9, %xmm9
	vsubss	%xmm11, %xmm8, %xmm4
	vsubss	%xmm10, %xmm7, %xmm8
	vaddss	%xmm4, %xmm8, %xmm8
	vfmsub213ss	%xmm7, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm6) - xmm7
	vmovss	-4(%rcx), %xmm4                 # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm1, %xmm7
	vmovaps	%xmm4, %xmm10
	vfmsub213ss	%xmm7, %xmm1, %xmm10    # xmm10 = (xmm1 * xmm10) - xmm7
	vaddss	%xmm8, %xmm9, %xmm11
	vsubss	%xmm9, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm9, %xmm9
	vmulss	%xmm3, %xmm3, %xmm13
	vsubss	%xmm12, %xmm8, %xmm8
	vmovaps	%xmm3, %xmm12
	vfmsub213ss	%xmm13, %xmm3, %xmm12   # xmm12 = (xmm3 * xmm12) - xmm13
	vaddss	%xmm8, %xmm9, %xmm8
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm6, %xmm5, %xmm9
	vaddss	%xmm12, %xmm8, %xmm8
	vsubss	%xmm5, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm12
	vsubss	%xmm12, %xmm5, %xmm5
	vmulss	%xmm1, %xmm4, %xmm12
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm12, %xmm13, %xmm6
	vsubss	%xmm13, %xmm6, %xmm10
	vsubss	%xmm10, %xmm6, %xmm14
	vsubss	%xmm14, %xmm13, %xmm13
	vmovaps	%xmm1, %xmm14
	vfmsub213ss	%xmm12, %xmm4, %xmm14   # xmm14 = (xmm4 * xmm14) - xmm12
	vsubss	%xmm10, %xmm12, %xmm10
	vaddss	%xmm10, %xmm13, %xmm10
	vaddss	%xmm7, %xmm9, %xmm12
	vsubss	%xmm9, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm15
	vsubss	%xmm15, %xmm9, %xmm9
	vsubss	%xmm13, %xmm7, %xmm7
	vaddss	%xmm14, %xmm8, %xmm8
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm6, %xmm11, %xmm9
	vsubss	%xmm6, %xmm9, %xmm13
	vsubss	%xmm13, %xmm9, %xmm14
	vsubss	%xmm14, %xmm6, %xmm6
	vsubss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm9, %xmm12, %xmm13
	vaddss	%xmm6, %xmm11, %xmm6
	vsubss	%xmm12, %xmm13, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vsubss	%xmm11, %xmm13, %xmm11
	vsubss	%xmm11, %xmm12, %xmm11
	vaddss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm9, %xmm11, %xmm7
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm7, %xmm5, %xmm5
	vmovss	(%rcx), %xmm6                   # xmm6 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm1, %xmm6, %xmm5     # xmm5 = (xmm6 * xmm1) + xmm5
	vfmadd231ss	%xmm3, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm3) + xmm5
	vfmadd231ss	%xmm4, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm4) + xmm5
	vfmadd231ss	%xmm6, %xmm1, %xmm5     # xmm5 = (xmm1 * xmm6) + xmm5
	vaddss	%xmm5, %xmm13, %xmm3
	vsubss	%xmm3, %xmm13, %xmm1
	vaddss	%xmm3, %xmm2, %xmm4
	vmovss	224(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vaddss	%xmm4, %xmm7, %xmm6
	vaddss	%xmm5, %xmm1, %xmm1
	vsubss	%xmm6, %xmm7, %xmm5
	vmovaps	%xmm0, %xmm9
	vaddss	%xmm6, %xmm0, %xmm0
	vsubss	%xmm9, %xmm0, %xmm7
	vaddss	%xmm4, %xmm5, %xmm5
	vsubss	%xmm7, %xmm0, %xmm8
	vsubss	%xmm8, %xmm9, %xmm8
	vsubss	%xmm7, %xmm6, %xmm6
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm6, %xmm8, %xmm4
	vmovshdup	%xmm9, %xmm6            # xmm6 = xmm9[1,1,3,3]
	vaddss	%xmm5, %xmm6, %xmm7
	vaddss	%xmm3, %xmm2, %xmm2
	vsubss	%xmm6, %xmm7, %xmm3
	vsubss	%xmm3, %xmm7, %xmm8
	vsubss	%xmm3, %xmm5, %xmm3
	vsubss	%xmm8, %xmm6, %xmm5
	vmovups	48(%rsp), %xmm10                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm2, %xmm10, %xmm6
	vsubss	%xmm10, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm9
	vsubss	%xmm9, %xmm10, %xmm9
	vsubss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm9, %xmm2
	vaddss	%xmm7, %xmm4, %xmm8
	vaddss	%xmm3, %xmm5, %xmm3
	vsubss	%xmm4, %xmm8, %xmm5
	vsubss	%xmm5, %xmm8, %xmm9
	vsubss	%xmm9, %xmm4, %xmm4
	vmovshdup	%xmm10, %xmm9           # xmm9 = xmm10[1,1,3,3]
	vsubss	%xmm5, %xmm7, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm3, %xmm4, %xmm5
	vaddss	%xmm2, %xmm9, %xmm2
	vsubss	%xmm4, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm9
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm3, %xmm4, %xmm3
	vaddss	%xmm6, %xmm5, %xmm4
	vsubss	%xmm5, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vsubss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm5, %xmm3, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm2, %xmm8, %xmm3
	vsubss	%xmm3, %xmm8, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vaddss	%xmm3, %xmm0, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vinsertps	$16, %xmm1, %xmm2, %xmm2 # xmm2 = xmm2[0],xmm1[0],xmm2[2,3]
	vinsertps	$16, %xmm0, %xmm4, %xmm0 # xmm0 = xmm4[0],xmm0[0],xmm4[2,3]
	incq	%rdx
	addq	$16, %rcx
	cmpq	%rax, %rdx
	jl	.LBB42_49
.LBB42_50:                              #   in Loop: Header=BB42_47 Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm0
	setp	%al
	setne	%cl
	orb	%al, %cl
	je	.LBB42_45
# %bb.51:                               #   in Loop: Header=BB42_47 Depth=1
	vcomiss	%xmm0, %xmm1
	vbroadcastss	.LCPI42_0(%rip), %xmm1  # xmm1 = [NaN,NaN,NaN,NaN]
	vmovaps	%xmm1, %xmm5
	ja	.LBB42_46
# %bb.52:                               #   in Loop: Header=BB42_47 Depth=1
	vmovups	%xmm2, 48(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm0, 224(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vmovd	.LCPI42_1(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm0
	setnp	%cl
	sete	%dl
	vmovd	%xmm0, %eax
	vmovdqa	%xmm2, %xmm3
	testb	%cl, %dl
	jne	.LBB42_55
# %bb.53:                               #   in Loop: Header=BB42_47 Depth=1
	movl	%eax, %ecx
	andl	$2139095040, %ecx               # imm = 0x7F800000
	vmovdqa	%xmm2, %xmm3
	cmpl	$2139095040, %ecx               # imm = 0x7F800000
	je	.LBB42_55
# %bb.54:                               #   in Loop: Header=BB42_47 Depth=1
	vmovd	%ecx, %xmm3
.LBB42_55:                              #   in Loop: Header=BB42_47 Depth=1
	vmovd	%xmm3, 16(%rsp)                 # 4-byte Folded Spill
	vucomiss	%xmm1, %xmm0
	setnp	%cl
	sete	%dl
	vmovdqa	%xmm2, %xmm1
	testb	%cl, %dl
	movq	344(%rsp), %rdi                 # 8-byte Reload
	vmovups	224(%rsp), %xmm4                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	jne	.LBB42_58
# %bb.56:                               #   in Loop: Header=BB42_47 Depth=1
	andl	$2139095040, %eax               # imm = 0x7F800000
	vmovdqa	%xmm2, %xmm1
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB42_58
# %bb.57:                               #   in Loop: Header=BB42_47 Depth=1
	movl	$2130706432, %ecx               # imm = 0x7F000000
	subl	%eax, %ecx
	vmovd	%ecx, %xmm1
.LBB42_58:                              #   in Loop: Header=BB42_47 Depth=1
	vmovss	.LCPI42_2(%rip), %xmm15         # xmm15 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm15, %xmm2
	vmulss	%xmm1, %xmm4, %xmm3
	vmulss	%xmm2, %xmm3, %xmm3
	vmovss	%xmm3, 256(%rsp)                # 4-byte Spill
	vmovshdup	%xmm4, %xmm4            # xmm4 = xmm4[1,1,3,3]
	vmulss	%xmm1, %xmm4, %xmm4
	vmulss	%xmm2, %xmm4, %xmm3
	vmovss	%xmm3, 160(%rsp)                # 4-byte Spill
	vmovups	48(%rsp), %xmm3                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmulss	%xmm1, %xmm3, %xmm5
	vmulss	%xmm2, %xmm5, %xmm4
	vmovss	%xmm4, 224(%rsp)                # 4-byte Spill
	vmovshdup	%xmm3, %xmm5            # xmm5 = xmm3[1,1,3,3]
	vmulss	%xmm1, %xmm5, %xmm1
	vmulss	%xmm2, %xmm1, %xmm1
	vmovss	%xmm1, 48(%rsp)                 # 4-byte Spill
	movl	$3, %eax
	vxorps	%xmm11, %xmm11, %xmm11
	vcvtsi2ss	%eax, %xmm11, %xmm2
	vmulss	%xmm2, %xmm15, %xmm1
	vmovaps	%xmm15, %xmm6
	vfmsub213ss	%xmm1, %xmm2, %xmm6     # xmm6 = (xmm2 * xmm6) - xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vmulss	%xmm3, %xmm2, %xmm5
	vxorps	%xmm7, %xmm7, %xmm7
	vfmsub213ss	%xmm5, %xmm2, %xmm7     # xmm7 = (xmm2 * xmm7) - xmm5
	vaddss	%xmm5, %xmm6, %xmm8
	vsubss	%xmm6, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm6, %xmm6
	vsubss	%xmm9, %xmm5, %xmm9
	vaddss	%xmm6, %xmm9, %xmm9
	vaddss	%xmm3, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm10
	vsubss	%xmm10, %xmm6, %xmm11
	vsubss	%xmm11, %xmm8, %xmm8
	vsubss	%xmm10, %xmm3, %xmm10
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm8, %xmm9, %xmm10
	vsubss	%xmm9, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm9, %xmm9
	vsubss	%xmm11, %xmm8, %xmm8
	vaddss	%xmm8, %xmm9, %xmm8
	vaddss	%xmm7, %xmm8, %xmm8
	vaddss	%xmm3, %xmm7, %xmm9
	vsubss	%xmm7, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm12, %xmm7, %xmm7
	vsubss	%xmm11, %xmm3, %xmm11
	vaddss	%xmm7, %xmm11, %xmm7
	vaddss	%xmm5, %xmm9, %xmm11
	vsubss	%xmm9, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm9, %xmm9
	vsubss	%xmm12, %xmm5, %xmm5
	vaddss	%xmm5, %xmm9, %xmm5
	vaddss	%xmm3, %xmm10, %xmm12
	vsubss	%xmm3, %xmm12, %xmm9
	vsubss	%xmm9, %xmm12, %xmm13
	vsubss	%xmm13, %xmm3, %xmm13
	vsubss	%xmm9, %xmm10, %xmm9
	vaddss	%xmm9, %xmm13, %xmm10
	vaddss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm11, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm9
	vsubss	%xmm9, %xmm11, %xmm11
	vmovss	16(%rsp), %xmm3                 # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm3, %xmm9
	vxorps	%xmm3, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm14, %xmm12, %xmm8
	vaddss	%xmm8, %xmm11, %xmm8
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vfmadd231ss	%xmm15, %xmm3, %xmm0    # xmm0 = (xmm3 * xmm15) + xmm0
	vfmadd231ss	%xmm3, %xmm3, %xmm0     # xmm0 = (xmm3 * xmm3) + xmm0
	vfmadd231ss	%xmm3, %xmm3, %xmm0     # xmm0 = (xmm3 * xmm3) + xmm0
	vfmadd231ss	%xmm2, %xmm3, %xmm0     # xmm0 = (xmm3 * xmm2) + xmm0
	vxorps	%xmm4, %xmm4, %xmm4
	vaddss	%xmm0, %xmm13, %xmm2
	vsubss	%xmm2, %xmm13, %xmm5
	vaddss	%xmm0, %xmm5, %xmm0
	vmovss	%xmm0, 8(%rsp)                  # 4-byte Spill
	vaddss	%xmm2, %xmm6, %xmm0
	vsubss	%xmm0, %xmm6, %xmm6
	vaddss	%xmm2, %xmm6, %xmm2
	vmovss	%xmm2, 4(%rsp)                  # 4-byte Spill
	vaddss	%xmm0, %xmm1, %xmm2
	vmovss	%xmm2, 40(%rsp)                 # 4-byte Spill
	vsubss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, 72(%rsp)                 # 4-byte Spill
	vmulss	%xmm9, %xmm9, %xmm3
	vmovaps	%xmm9, %xmm0
	vfmsub213ss	%xmm3, %xmm9, %xmm0     # xmm0 = (xmm9 * xmm0) - xmm3
	vmulss	%xmm4, %xmm9, %xmm2
	vmulss	%xmm4, %xmm9, %xmm10
	vaddss	%xmm2, %xmm0, %xmm11
	vsubss	%xmm0, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm0, %xmm0
	vsubss	%xmm12, %xmm2, %xmm12
	vaddss	%xmm0, %xmm12, %xmm12
	vaddss	%xmm10, %xmm11, %xmm1
	vsubss	%xmm11, %xmm1, %xmm13
	vsubss	%xmm13, %xmm1, %xmm14
	vsubss	%xmm14, %xmm11, %xmm11
	vsubss	%xmm13, %xmm10, %xmm13
	vaddss	%xmm13, %xmm11, %xmm11
	vxorps	%xmm13, %xmm13, %xmm13
	vfmsub213ss	%xmm2, %xmm9, %xmm13    # xmm13 = (xmm9 * xmm13) - xmm2
	vmovaps	%xmm9, %xmm14
	vfmsub213ss	%xmm10, %xmm4, %xmm14   # xmm14 = (xmm4 * xmm14) - xmm10
	vaddss	%xmm11, %xmm12, %xmm15
	vsubss	%xmm12, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm5
	vsubss	%xmm5, %xmm12, %xmm5
	vsubss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm14, %xmm13, %xmm5
	vsubss	%xmm13, %xmm5, %xmm11
	vsubss	%xmm11, %xmm5, %xmm12
	vsubss	%xmm12, %xmm13, %xmm12
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm14, %xmm0
	vsubss	%xmm11, %xmm14, %xmm11
	vaddss	%xmm11, %xmm12, %xmm11
	vaddss	%xmm4, %xmm10, %xmm12
	vsubss	%xmm4, %xmm12, %xmm13
	vsubss	%xmm13, %xmm10, %xmm10
	vsubss	%xmm13, %xmm12, %xmm13
	vsubss	%xmm13, %xmm4, %xmm13
	vaddss	%xmm10, %xmm13, %xmm10
	vaddss	%xmm2, %xmm5, %xmm13
	vsubss	%xmm5, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vaddss	%xmm15, %xmm12, %xmm5
	vsubss	%xmm12, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm14
	vsubss	%xmm14, %xmm12, %xmm12
	vsubss	%xmm6, %xmm15, %xmm6
	vaddss	%xmm6, %xmm12, %xmm6
	vaddss	%xmm5, %xmm13, %xmm12
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm12, %xmm15
	vsubss	%xmm15, %xmm13, %xmm13
	vsubss	%xmm14, %xmm5, %xmm5
	vaddss	%xmm5, %xmm13, %xmm5
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vfmadd231ss	%xmm9, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm9) + xmm0
	vfmadd231ss	%xmm2, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm2) + xmm0
	vfmadd231ss	%xmm2, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm2) + xmm0
	vfmadd231ss	%xmm2, %xmm9, %xmm0     # xmm0 = (xmm9 * xmm2) + xmm0
	vaddss	%xmm0, %xmm12, %xmm2
	vsubss	%xmm2, %xmm12, %xmm5
	vaddss	%xmm0, %xmm5, %xmm0
	vmovss	%xmm0, (%rsp)                   # 4-byte Spill
	vaddss	%xmm2, %xmm1, %xmm5
	vsubss	%xmm5, %xmm1, %xmm0
	vaddss	%xmm2, %xmm0, %xmm10
	vaddss	%xmm5, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vmovss	256(%rsp), %xmm12               # 4-byte Reload
                                        # xmm12 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm12, %xmm3
	vmovss	%xmm3, 12(%rsp)                 # 4-byte Spill
	vmovaps	%xmm4, %xmm2
	vfmsub213ss	%xmm3, %xmm12, %xmm2    # xmm2 = (xmm12 * xmm2) - xmm3
	vmulss	%xmm1, %xmm12, %xmm3
	vaddss	%xmm3, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm13
	vsubss	%xmm13, %xmm2, %xmm2
	vmovaps	%xmm1, %xmm14
	vfmsub213ss	%xmm3, %xmm12, %xmm14   # xmm14 = (xmm12 * xmm14) - xmm3
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	160(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm0, %xmm3
	vaddss	%xmm3, %xmm5, %xmm13
	vsubss	%xmm5, %xmm13, %xmm6
	vsubss	%xmm6, %xmm13, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vmovaps	%xmm4, %xmm15
	vfmsub213ss	%xmm3, %xmm0, %xmm15    # xmm15 = (xmm0 * xmm15) - xmm3
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm3, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmovaps	%xmm10, %xmm6
	vmovss	%xmm10, 20(%rsp)                # 4-byte Spill
	vmulss	%xmm10, %xmm12, %xmm3
	vfmsub213ss	%xmm3, %xmm12, %xmm6    # xmm6 = (xmm12 * xmm6) - xmm3
	vaddss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm0, %xmm10
	vmulss	%xmm1, %xmm0, %xmm6
	vmovaps	%xmm1, %xmm7
	vfmsub213ss	%xmm6, %xmm0, %xmm7     # xmm7 = (xmm0 * xmm7) - xmm6
	vaddss	%xmm7, %xmm2, %xmm2
	vmovss	224(%rsp), %xmm8                # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm8, %xmm7
	vmovaps	%xmm4, %xmm0
	vfmsub213ss	%xmm7, %xmm8, %xmm0     # xmm0 = (xmm8 * xmm0) - xmm7
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	%xmm0, 24(%rsp)                 # 4-byte Spill
	vaddss	%xmm15, %xmm14, %xmm2
	vsubss	%xmm14, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm11
	vsubss	%xmm11, %xmm14, %xmm11
	vsubss	%xmm0, %xmm15, %xmm0
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm7, %xmm6, %xmm11
	vsubss	%xmm6, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm14, %xmm7, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm3, %xmm2, %xmm7
	vsubss	%xmm2, %xmm7, %xmm14
	vsubss	%xmm14, %xmm7, %xmm15
	vsubss	%xmm15, %xmm2, %xmm2
	vsubss	%xmm14, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm5, %xmm11, %xmm3
	vsubss	%xmm11, %xmm3, %xmm14
	vsubss	%xmm14, %xmm3, %xmm15
	vsubss	%xmm15, %xmm11, %xmm11
	vsubss	%xmm14, %xmm5, %xmm5
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm3, %xmm7, %xmm11
	vsubss	%xmm7, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm14, %xmm3, %xmm3
	vaddss	%xmm3, %xmm7, %xmm3
	vaddss	24(%rsp), %xmm0, %xmm0          # 4-byte Folded Reload
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vfmadd231ss	48(%rsp), %xmm4, %xmm0  # 4-byte Folded Reload
                                        # xmm0 = (xmm4 * mem) + xmm0
	vfmadd231ss	%xmm1, %xmm8, %xmm0     # xmm0 = (xmm8 * xmm1) + xmm0
	vfmadd231ss	20(%rsp), %xmm10, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm10 * mem) + xmm0
	vfmadd231ss	(%rsp), %xmm12, %xmm0   # 4-byte Folded Reload
                                        # xmm0 = (xmm12 * mem) + xmm0
	vaddss	%xmm0, %xmm11, %xmm1
	vsubss	%xmm1, %xmm11, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm13, %xmm2
	vsubss	%xmm2, %xmm13, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	12(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm4
	vbroadcastss	.LCPI42_3(%rip), %xmm8  # xmm8 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm3, %xmm8, %xmm5
	vmovss	40(%rsp), %xmm6                 # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm6, %xmm2
	vsubss	%xmm6, %xmm2, %xmm3
	vsubss	%xmm3, %xmm5, %xmm5
	vsubss	%xmm3, %xmm2, %xmm3
	vsubss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm5, %xmm3, %xmm3
	vxorps	%xmm4, %xmm8, %xmm5
	vmovss	72(%rsp), %xmm7                 # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm7, %xmm4
	vsubss	%xmm7, %xmm4, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm6, %xmm4, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm5, %xmm6, %xmm5
	vxorps	%xmm1, %xmm8, %xmm6
	vmovss	4(%rsp), %xmm8                  # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm8, %xmm1
	vsubss	%xmm8, %xmm1, %xmm7
	vsubss	%xmm7, %xmm6, %xmm6
	vsubss	%xmm7, %xmm1, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	8(%rsp), %xmm6, %xmm6           # 4-byte Folded Reload
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm4, %xmm3, %xmm6
	vsubss	%xmm3, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm5, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm0, (%rsp)                   # 4-byte Spill
	vaddss	%xmm1, %xmm6, %xmm3
	vsubss	%xmm3, %xmm6, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm3, %xmm2, %xmm10
	vsubss	%xmm10, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm11
	vmulss	%xmm9, %xmm10, %xmm12
	vmovaps	%xmm9, %xmm2
	vfmsub213ss	%xmm12, %xmm10, %xmm2   # xmm2 = (xmm10 * xmm2) - xmm12
	vxorps	%xmm0, %xmm0, %xmm0
	vmulss	%xmm0, %xmm10, %xmm13
	vaddss	%xmm2, %xmm13, %xmm3
	vsubss	%xmm2, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm5, %xmm2, %xmm2
	vsubss	%xmm4, %xmm13, %xmm4
	vaddss	%xmm4, %xmm2, %xmm4
	vmulss	%xmm9, %xmm11, %xmm5
	vaddss	%xmm5, %xmm3, %xmm2
	vsubss	%xmm3, %xmm2, %xmm6
	vsubss	%xmm6, %xmm2, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vmovaps	%xmm9, %xmm7
	vfmsub213ss	%xmm5, %xmm11, %xmm7    # xmm7 = (xmm11 * xmm7) - xmm5
	vsubss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vsubss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm4, %xmm3
	vxorps	%xmm4, %xmm4, %xmm4
	vfmsub213ss	%xmm13, %xmm10, %xmm4   # xmm4 = (xmm10 * xmm4) - xmm13
	vmulss	%xmm0, %xmm11, %xmm6
	vxorps	%xmm8, %xmm8, %xmm8
	vfmsub213ss	%xmm6, %xmm11, %xmm8    # xmm8 = (xmm11 * xmm8) - xmm6
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vmulss	%xmm1, %xmm9, %xmm8
	vmovaps	%xmm9, %xmm14
	vfmsub213ss	%xmm8, %xmm1, %xmm14    # xmm14 = (xmm1 * xmm14) - xmm8
	vaddss	%xmm3, %xmm14, %xmm3
	vaddss	%xmm7, %xmm4, %xmm14
	vsubss	%xmm4, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm0
	vsubss	%xmm0, %xmm4, %xmm0
	vsubss	%xmm15, %xmm7, %xmm4
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm6, %xmm8, %xmm4
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm13, %xmm14, %xmm7
	vsubss	%xmm14, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm15
	vsubss	%xmm15, %xmm14, %xmm14
	vsubss	%xmm8, %xmm13, %xmm8
	vaddss	%xmm8, %xmm14, %xmm8
	vaddss	%xmm5, %xmm4, %xmm13
	vsubss	%xmm4, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm14, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm7, %xmm13, %xmm5
	vsubss	%xmm7, %xmm5, %xmm14
	vsubss	%xmm14, %xmm5, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm7, %xmm13, %xmm7
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vfmadd231ss	(%rsp), %xmm9, %xmm0    # 4-byte Folded Reload
                                        # xmm0 = (xmm9 * mem) + xmm0
	vxorps	%xmm3, %xmm3, %xmm3
	vfmadd231ss	%xmm1, %xmm3, %xmm0     # xmm0 = (xmm3 * xmm1) + xmm0
	vfmadd231ss	%xmm11, %xmm3, %xmm0    # xmm0 = (xmm3 * xmm11) + xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vfmadd231ss	%xmm10, %xmm1, %xmm0    # xmm0 = (xmm1 * xmm10) + xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm3
	vaddss	%xmm0, %xmm3, %xmm9
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm8
	vaddss	%xmm0, %xmm12, %xmm11
	vsubss	%xmm11, %xmm12, %xmm1
	vaddss	%xmm0, %xmm1, %xmm10
	vmulss	%xmm11, %xmm11, %xmm13
	vmovaps	%xmm11, %xmm0
	vfmsub213ss	%xmm13, %xmm11, %xmm0   # xmm0 = (xmm11 * xmm0) - xmm13
	vmulss	%xmm10, %xmm11, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm10, %xmm4
	vfmsub213ss	%xmm1, %xmm11, %xmm4    # xmm4 = (xmm11 * xmm4) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm11, %xmm10, %xmm3
	vaddss	%xmm3, %xmm2, %xmm0
	vsubss	%xmm2, %xmm0, %xmm5
	vsubss	%xmm5, %xmm0, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm11, %xmm6
	vfmsub213ss	%xmm3, %xmm10, %xmm6    # xmm6 = (xmm10 * xmm6) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vmovaps	%xmm8, %xmm7
	vmulss	%xmm8, %xmm11, %xmm2
	vmovaps	%xmm8, %xmm5
	vmovaps	%xmm8, %xmm12
	vfmsub213ss	%xmm2, %xmm11, %xmm5    # xmm5 = (xmm11 * xmm5) - xmm2
	vaddss	%xmm5, %xmm1, %xmm1
	vmulss	%xmm10, %xmm10, %xmm5
	vmovaps	%xmm10, %xmm7
	vfmsub213ss	%xmm5, %xmm10, %xmm7    # xmm7 = (xmm10 * xmm7) - xmm5
	vaddss	%xmm7, %xmm1, %xmm1
	vmulss	%xmm11, %xmm8, %xmm7
	vmovaps	%xmm11, %xmm8
	vfmsub213ss	%xmm7, %xmm12, %xmm8    # xmm8 = (xmm12 * xmm8) - xmm7
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm6, %xmm4, %xmm8
	vsubss	%xmm4, %xmm8, %xmm14
	vsubss	%xmm14, %xmm8, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm14, %xmm6, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm7, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm14, %xmm7, %xmm7
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm2, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm14
	vsubss	%xmm14, %xmm7, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm3, %xmm6, %xmm8
	vsubss	%xmm6, %xmm8, %xmm14
	vsubss	%xmm14, %xmm8, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm14, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm7, %xmm8, %xmm6
	vsubss	%xmm7, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm14, %xmm8, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vmovss	%xmm9, 12(%rsp)                 # 4-byte Spill
	vfmadd231ss	%xmm11, %xmm9, %xmm1    # xmm1 = (xmm9 * xmm11) + xmm1
	vmovss	%xmm10, 20(%rsp)                # 4-byte Spill
	vmovss	%xmm12, (%rsp)                  # 4-byte Spill
	vfmadd231ss	%xmm10, %xmm12, %xmm1   # xmm1 = (xmm12 * xmm10) + xmm1
	vfmadd231ss	%xmm12, %xmm10, %xmm1   # xmm1 = (xmm10 * xmm12) + xmm1
	vfmadd231ss	%xmm9, %xmm11, %xmm1    # xmm1 = (xmm11 * xmm9) + xmm1
	vaddss	%xmm1, %xmm6, %xmm2
	vsubss	%xmm2, %xmm6, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm1, 24(%rsp)                 # 4-byte Spill
	vaddss	%xmm2, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm12
	vaddss	%xmm1, %xmm13, %xmm10
	vsubss	%xmm10, %xmm13, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmovss	256(%rsp), %xmm15               # 4-byte Reload
                                        # xmm15 = mem[0],zero,zero,zero
	vmulss	%xmm10, %xmm15, %xmm3
	vmovss	%xmm3, 320(%rsp)                # 4-byte Spill
	vmovaps	%xmm10, %xmm2
	vfmsub213ss	%xmm3, %xmm15, %xmm2    # xmm2 = (xmm15 * xmm2) - xmm3
	vmulss	%xmm1, %xmm15, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm1, %xmm6
	vfmsub213ss	%xmm3, %xmm15, %xmm6    # xmm6 = (xmm15 * xmm6) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm3
	vmovss	160(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm10, %xmm5
	vaddss	%xmm5, %xmm4, %xmm2
	vsubss	%xmm4, %xmm2, %xmm7
	vsubss	%xmm7, %xmm2, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vmovaps	%xmm10, %xmm8
	vfmsub213ss	%xmm5, %xmm0, %xmm8     # xmm8 = (xmm0 * xmm8) - xmm5
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm12, %xmm7
	vmovss	%xmm12, 316(%rsp)               # 4-byte Spill
	vmulss	%xmm12, %xmm15, %xmm4
	vfmsub213ss	%xmm4, %xmm15, %xmm7    # xmm7 = (xmm15 * xmm7) - xmm4
	vaddss	%xmm7, %xmm3, %xmm3
	vmovaps	%xmm0, %xmm12
	vmulss	%xmm1, %xmm0, %xmm7
	vmovaps	%xmm1, %xmm9
	vfmsub213ss	%xmm7, %xmm0, %xmm9     # xmm9 = (xmm0 * xmm9) - xmm7
	vaddss	%xmm3, %xmm9, %xmm3
	vmovss	224(%rsp), %xmm13               # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vmulss	%xmm10, %xmm13, %xmm9
	vmovaps	%xmm10, %xmm0
	vfmsub213ss	%xmm9, %xmm13, %xmm0    # xmm0 = (xmm13 * xmm0) - xmm9
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm0, 312(%rsp)                # 4-byte Spill
	vaddss	%xmm6, %xmm8, %xmm3
	vsubss	%xmm6, %xmm3, %xmm0
	vsubss	%xmm0, %xmm3, %xmm14
	vsubss	%xmm14, %xmm6, %xmm6
	vsubss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm7, %xmm9, %xmm6
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm14
	vsubss	%xmm14, %xmm7, %xmm7
	vsubss	%xmm8, %xmm9, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm3, %xmm8
	vsubss	%xmm3, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm14
	vsubss	%xmm14, %xmm3, %xmm3
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm5, %xmm6, %xmm4
	vsubss	%xmm6, %xmm4, %xmm9
	vsubss	%xmm9, %xmm4, %xmm14
	vsubss	%xmm14, %xmm6, %xmm6
	vsubss	%xmm9, %xmm5, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vaddss	%xmm4, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm14
	vsubss	%xmm14, %xmm8, %xmm8
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm4, %xmm8, %xmm4
	vaddss	312(%rsp), %xmm0, %xmm0         # 4-byte Folded Reload
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vfmadd231ss	48(%rsp), %xmm10, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm10 * mem) + xmm0
	vfmadd231ss	%xmm1, %xmm13, %xmm0    # xmm0 = (xmm13 * xmm1) + xmm0
	vfmadd231ss	316(%rsp), %xmm12, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm12 * mem) + xmm0
	vfmadd231ss	24(%rsp), %xmm15, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm15 * mem) + xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm3
	vaddss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmovss	320(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm4
	vaddss	%xmm0, %xmm4, %xmm4
	vbroadcastss	.LCPI42_3(%rip), %xmm8  # xmm8 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm8, %xmm5
	vmovss	40(%rsp), %xmm6                 # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm6, %xmm0
	vsubss	%xmm6, %xmm0, %xmm2
	vsubss	%xmm2, %xmm5, %xmm5
	vsubss	%xmm2, %xmm0, %xmm2
	vsubss	%xmm2, %xmm6, %xmm2
	vaddss	%xmm5, %xmm2, %xmm2
	vxorps	%xmm4, %xmm8, %xmm5
	vmovss	72(%rsp), %xmm7                 # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm7, %xmm4
	vsubss	%xmm7, %xmm4, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm6, %xmm4, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm5, %xmm6, %xmm5
	vxorps	%xmm1, %xmm8, %xmm6
	vmovss	4(%rsp), %xmm8                  # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm8, %xmm1
	vsubss	%xmm8, %xmm1, %xmm7
	vsubss	%xmm7, %xmm6, %xmm6
	vsubss	%xmm7, %xmm1, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	8(%rsp), %xmm6, %xmm6           # 4-byte Folded Reload
	vsubss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm4, %xmm2, %xmm6
	vsubss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm5, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm2
	vsubss	%xmm2, %xmm5, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm1, 320(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm6, %xmm3
	vsubss	%xmm3, %xmm6, %xmm1
	vaddss	%xmm2, %xmm1, %xmm13
	vaddss	%xmm3, %xmm0, %xmm14
	vsubss	%xmm14, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm1
	vmulss	%xmm11, %xmm14, %xmm0
	vmovss	%xmm0, 24(%rsp)                 # 4-byte Spill
	vmovaps	%xmm11, %xmm2
	vfmsub213ss	%xmm0, %xmm14, %xmm2    # xmm2 = (xmm14 * xmm2) - xmm0
	vmovss	20(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm14, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm0, %xmm6
	vmovaps	%xmm0, %xmm10
	vfmsub213ss	%xmm3, %xmm14, %xmm6    # xmm6 = (xmm14 * xmm6) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm3
	vmulss	%xmm1, %xmm11, %xmm5
	vaddss	%xmm5, %xmm4, %xmm2
	vsubss	%xmm4, %xmm2, %xmm7
	vsubss	%xmm7, %xmm2, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vmovaps	%xmm11, %xmm8
	vfmsub213ss	%xmm5, %xmm1, %xmm8     # xmm8 = (xmm1 * xmm8) - xmm5
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vmovss	(%rsp), %xmm7                   # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm7, %xmm14, %xmm4
	vfmsub213ss	%xmm4, %xmm14, %xmm7    # xmm7 = (xmm14 * xmm7) - xmm4
	vaddss	%xmm7, %xmm3, %xmm3
	vmulss	%xmm0, %xmm1, %xmm7
	vmovaps	%xmm0, %xmm9
	vmovaps	%xmm0, %xmm12
	vfmsub213ss	%xmm7, %xmm1, %xmm9     # xmm9 = (xmm1 * xmm9) - xmm7
	vaddss	%xmm3, %xmm9, %xmm3
	vmulss	%xmm11, %xmm13, %xmm9
	vmovaps	%xmm11, %xmm10
	vfmsub213ss	%xmm9, %xmm13, %xmm10   # xmm10 = (xmm13 * xmm10) - xmm9
	vaddss	%xmm3, %xmm10, %xmm3
	vaddss	%xmm6, %xmm8, %xmm10
	vsubss	%xmm6, %xmm10, %xmm0
	vsubss	%xmm0, %xmm10, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm7, %xmm9, %xmm6
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm8, %xmm9, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm10, %xmm8
	vsubss	%xmm10, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm4, %xmm10, %xmm4
	vaddss	%xmm5, %xmm6, %xmm9
	vsubss	%xmm6, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vaddss	%xmm9, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm10
	vsubss	%xmm10, %xmm6, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vfmadd231ss	320(%rsp), %xmm11, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm11 * mem) + xmm0
	vfmadd231ss	%xmm12, %xmm13, %xmm0   # xmm0 = (xmm13 * xmm12) + xmm0
	vfmadd231ss	(%rsp), %xmm1, %xmm0    # 4-byte Folded Reload
                                        # xmm0 = (xmm1 * mem) + xmm0
	vfmadd231ss	12(%rsp), %xmm14, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm14 * mem) + xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm3
	vaddss	%xmm0, %xmm3, %xmm9
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm8
	vmovss	24(%rsp), %xmm1                 # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm11
	vsubss	%xmm11, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm12
	vmulss	%xmm11, %xmm11, %xmm13
	vmovaps	%xmm11, %xmm0
	vfmsub213ss	%xmm13, %xmm11, %xmm0   # xmm0 = (xmm11 * xmm0) - xmm13
	vmulss	%xmm12, %xmm11, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm12, %xmm4
	vfmsub213ss	%xmm1, %xmm11, %xmm4    # xmm4 = (xmm11 * xmm4) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm11, %xmm12, %xmm3
	vaddss	%xmm3, %xmm2, %xmm0
	vsubss	%xmm2, %xmm0, %xmm5
	vsubss	%xmm5, %xmm0, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm11, %xmm6
	vfmsub213ss	%xmm3, %xmm12, %xmm6    # xmm6 = (xmm12 * xmm6) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vmovaps	%xmm8, %xmm7
	vmulss	%xmm8, %xmm11, %xmm2
	vmovaps	%xmm8, %xmm5
	vmovaps	%xmm8, %xmm10
	vfmsub213ss	%xmm2, %xmm11, %xmm5    # xmm5 = (xmm11 * xmm5) - xmm2
	vaddss	%xmm5, %xmm1, %xmm1
	vmulss	%xmm12, %xmm12, %xmm5
	vmovaps	%xmm12, %xmm7
	vfmsub213ss	%xmm5, %xmm12, %xmm7    # xmm7 = (xmm12 * xmm7) - xmm5
	vaddss	%xmm7, %xmm1, %xmm1
	vmulss	%xmm11, %xmm8, %xmm7
	vmovaps	%xmm11, %xmm8
	vfmsub213ss	%xmm7, %xmm10, %xmm8    # xmm8 = (xmm10 * xmm8) - xmm7
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm6, %xmm4, %xmm8
	vsubss	%xmm4, %xmm8, %xmm14
	vsubss	%xmm14, %xmm8, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm14, %xmm6, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm7, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm14, %xmm7, %xmm7
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm2, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm14
	vsubss	%xmm14, %xmm7, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm3, %xmm6, %xmm8
	vsubss	%xmm6, %xmm8, %xmm14
	vsubss	%xmm14, %xmm8, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm14, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm7, %xmm8, %xmm6
	vsubss	%xmm7, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm14, %xmm8, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vmovss	%xmm9, 12(%rsp)                 # 4-byte Spill
	vfmadd231ss	%xmm11, %xmm9, %xmm1    # xmm1 = (xmm9 * xmm11) + xmm1
	vmovss	%xmm10, (%rsp)                  # 4-byte Spill
	vmovss	%xmm12, 20(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm12, %xmm10, %xmm1   # xmm1 = (xmm10 * xmm12) + xmm1
	vfmadd231ss	%xmm10, %xmm12, %xmm1   # xmm1 = (xmm12 * xmm10) + xmm1
	vfmadd231ss	%xmm9, %xmm11, %xmm1    # xmm1 = (xmm11 * xmm9) + xmm1
	vaddss	%xmm1, %xmm6, %xmm2
	vsubss	%xmm2, %xmm6, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm1, 24(%rsp)                 # 4-byte Spill
	vaddss	%xmm2, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm12
	vaddss	%xmm1, %xmm13, %xmm10
	vsubss	%xmm10, %xmm13, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmovss	256(%rsp), %xmm15               # 4-byte Reload
                                        # xmm15 = mem[0],zero,zero,zero
	vmulss	%xmm10, %xmm15, %xmm3
	vmovss	%xmm3, 320(%rsp)                # 4-byte Spill
	vmovaps	%xmm10, %xmm2
	vfmsub213ss	%xmm3, %xmm15, %xmm2    # xmm2 = (xmm15 * xmm2) - xmm3
	vmulss	%xmm1, %xmm15, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm1, %xmm6
	vfmsub213ss	%xmm3, %xmm15, %xmm6    # xmm6 = (xmm15 * xmm6) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm3
	vmovss	160(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm10, %xmm5
	vaddss	%xmm5, %xmm4, %xmm2
	vsubss	%xmm4, %xmm2, %xmm7
	vsubss	%xmm7, %xmm2, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vmovaps	%xmm10, %xmm8
	vfmsub213ss	%xmm5, %xmm0, %xmm8     # xmm8 = (xmm0 * xmm8) - xmm5
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm12, %xmm7
	vmovss	%xmm12, 316(%rsp)               # 4-byte Spill
	vmulss	%xmm12, %xmm15, %xmm4
	vfmsub213ss	%xmm4, %xmm15, %xmm7    # xmm7 = (xmm15 * xmm7) - xmm4
	vaddss	%xmm7, %xmm3, %xmm3
	vmovaps	%xmm0, %xmm12
	vmulss	%xmm1, %xmm0, %xmm7
	vmovaps	%xmm1, %xmm9
	vfmsub213ss	%xmm7, %xmm0, %xmm9     # xmm9 = (xmm0 * xmm9) - xmm7
	vaddss	%xmm3, %xmm9, %xmm3
	vmovss	224(%rsp), %xmm13               # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vmulss	%xmm10, %xmm13, %xmm9
	vmovaps	%xmm10, %xmm0
	vfmsub213ss	%xmm9, %xmm13, %xmm0    # xmm0 = (xmm13 * xmm0) - xmm9
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm0, 312(%rsp)                # 4-byte Spill
	vaddss	%xmm6, %xmm8, %xmm3
	vsubss	%xmm6, %xmm3, %xmm0
	vsubss	%xmm0, %xmm3, %xmm14
	vsubss	%xmm14, %xmm6, %xmm6
	vsubss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm7, %xmm9, %xmm6
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm14
	vsubss	%xmm14, %xmm7, %xmm7
	vsubss	%xmm8, %xmm9, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm3, %xmm8
	vsubss	%xmm3, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm14
	vsubss	%xmm14, %xmm3, %xmm3
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm5, %xmm6, %xmm4
	vsubss	%xmm6, %xmm4, %xmm9
	vsubss	%xmm9, %xmm4, %xmm14
	vsubss	%xmm14, %xmm6, %xmm6
	vsubss	%xmm9, %xmm5, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vaddss	%xmm4, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm14
	vsubss	%xmm14, %xmm8, %xmm8
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm4, %xmm8, %xmm4
	vaddss	312(%rsp), %xmm0, %xmm0         # 4-byte Folded Reload
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vfmadd231ss	48(%rsp), %xmm10, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm10 * mem) + xmm0
	vfmadd231ss	%xmm1, %xmm13, %xmm0    # xmm0 = (xmm13 * xmm1) + xmm0
	vfmadd231ss	316(%rsp), %xmm12, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm12 * mem) + xmm0
	vfmadd231ss	24(%rsp), %xmm15, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm15 * mem) + xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm3
	vaddss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmovss	320(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm4
	vaddss	%xmm0, %xmm4, %xmm4
	vmovss	40(%rsp), %xmm6                 # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm6, %xmm0
	vsubss	%xmm6, %xmm0, %xmm5
	vmovaps	%xmm6, %xmm7
	vsubss	%xmm5, %xmm0, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vbroadcastss	.LCPI42_3(%rip), %xmm8  # xmm8 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm8, %xmm2
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vmovss	72(%rsp), %xmm7                 # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm6
	vmovaps	%xmm7, %xmm9
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm9, %xmm7
	vxorps	%xmm4, %xmm8, %xmm4
	vmovaps	%xmm8, %xmm9
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vmovss	4(%rsp), %xmm8                  # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm7
	vmovaps	%xmm8, %xmm10
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm10, %xmm8
	vxorps	%xmm1, %xmm9, %xmm1
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	8(%rsp), %xmm1, %xmm1           # 4-byte Folded Reload
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm5, %xmm2, %xmm3
	vsubss	%xmm2, %xmm3, %xmm7
	vsubss	%xmm7, %xmm3, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm6, %xmm5, %xmm4
	vsubss	%xmm5, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm5, %xmm5
	vsubss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm4
	vaddss	%xmm1, %xmm4, %xmm1
	vmovss	%xmm1, 72(%rsp)                 # 4-byte Spill
	vaddss	%xmm2, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm1
	vaddss	%xmm2, %xmm1, %xmm7
	vaddss	%xmm4, %xmm0, %xmm8
	vsubss	%xmm8, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm5
	vmulss	%xmm11, %xmm8, %xmm1
	vmovss	%xmm1, 4(%rsp)                  # 4-byte Spill
	vmovaps	%xmm11, %xmm0
	vfmsub213ss	%xmm1, %xmm8, %xmm0     # xmm0 = (xmm8 * xmm0) - xmm1
	vmovss	20(%rsp), %xmm1                 # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm8, %xmm2
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm0, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm6
	vsubss	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm1, %xmm6
	vmovaps	%xmm1, %xmm12
	vfmsub213ss	%xmm2, %xmm8, %xmm6     # xmm6 = (xmm8 * xmm6) - xmm2
	vsubss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vmulss	%xmm5, %xmm11, %xmm4
	vaddss	%xmm4, %xmm3, %xmm2
	vsubss	%xmm3, %xmm2, %xmm9
	vsubss	%xmm9, %xmm2, %xmm10
	vsubss	%xmm10, %xmm3, %xmm3
	vmovaps	%xmm11, %xmm10
	vfmsub213ss	%xmm4, %xmm5, %xmm10    # xmm10 = (xmm5 * xmm10) - xmm4
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm4
	vsubss	%xmm0, %xmm4, %xmm9
	vsubss	%xmm9, %xmm4, %xmm14
	vsubss	%xmm14, %xmm0, %xmm0
	vsubss	%xmm9, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm0
	vmovss	(%rsp), %xmm9                   # 4-byte Reload
                                        # xmm9 = mem[0],zero,zero,zero
	vmulss	%xmm9, %xmm8, %xmm3
	vfmsub213ss	%xmm3, %xmm8, %xmm9     # xmm9 = (xmm8 * xmm9) - xmm3
	vaddss	%xmm0, %xmm9, %xmm0
	vmulss	%xmm1, %xmm5, %xmm9
	vmovaps	%xmm1, %xmm14
	vfmsub213ss	%xmm9, %xmm5, %xmm14    # xmm14 = (xmm5 * xmm14) - xmm9
	vaddss	%xmm0, %xmm14, %xmm0
	vmulss	%xmm7, %xmm11, %xmm14
	vmovaps	%xmm11, %xmm15
	vfmsub213ss	%xmm14, %xmm7, %xmm15   # xmm15 = (xmm7 * xmm15) - xmm14
	vaddss	%xmm0, %xmm15, %xmm15
	vaddss	%xmm6, %xmm10, %xmm0
	vsubss	%xmm6, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm13
	vsubss	%xmm13, %xmm6, %xmm6
	vsubss	%xmm1, %xmm10, %xmm1
	vaddss	%xmm1, %xmm6, %xmm1
	vaddss	%xmm14, %xmm9, %xmm6
	vsubss	%xmm9, %xmm6, %xmm10
	vsubss	%xmm10, %xmm6, %xmm13
	vsubss	%xmm13, %xmm9, %xmm9
	vsubss	%xmm10, %xmm14, %xmm10
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm3, %xmm0, %xmm10
	vsubss	%xmm0, %xmm10, %xmm13
	vsubss	%xmm13, %xmm10, %xmm14
	vsubss	%xmm14, %xmm0, %xmm0
	vsubss	%xmm13, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm3
	vaddss	%xmm4, %xmm6, %xmm13
	vsubss	%xmm6, %xmm13, %xmm0
	vsubss	%xmm0, %xmm13, %xmm14
	vsubss	%xmm14, %xmm6, %xmm6
	vsubss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm0, %xmm6, %xmm4
	vaddss	%xmm13, %xmm10, %xmm0
	vsubss	%xmm10, %xmm0, %xmm6
	vsubss	%xmm6, %xmm0, %xmm14
	vmovss	%xmm0, 8(%rsp)                  # 4-byte Spill
	vsubss	%xmm14, %xmm10, %xmm10
	vsubss	%xmm6, %xmm13, %xmm6
	vaddss	%xmm6, %xmm10, %xmm6
	vaddss	%xmm1, %xmm15, %xmm1
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm6, %xmm1, %xmm6
	vfmadd231ss	72(%rsp), %xmm11, %xmm6 # 4-byte Folded Reload
                                        # xmm6 = (xmm11 * mem) + xmm6
	vfmadd231ss	%xmm12, %xmm7, %xmm6    # xmm6 = (xmm7 * xmm12) + xmm6
	vfmadd231ss	(%rsp), %xmm5, %xmm6    # 4-byte Folded Reload
                                        # xmm6 = (xmm5 * mem) + xmm6
	vfmadd231ss	12(%rsp), %xmm8, %xmm6  # 4-byte Folded Reload
                                        # xmm6 = (xmm8 * mem) + xmm6
	vaddss	%xmm6, %xmm0, %xmm1
	vaddss	%xmm1, %xmm2, %xmm3
	vmovaps	%xmm1, %xmm8
	vmovss	%xmm1, 72(%rsp)                 # 4-byte Spill
	vsubss	%xmm3, %xmm2, %xmm9
	vmovss	4(%rsp), %xmm0                  # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vaddss	%xmm3, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm2
	vaddss	%xmm3, %xmm2, %xmm5
	vmovss	256(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm7, %xmm0
	vmovss	%xmm0, 4(%rsp)                  # 4-byte Spill
	vmovaps	%xmm1, %xmm2
	vfmsub213ss	%xmm0, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm0
	vmulss	%xmm5, %xmm7, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm10
	vsubss	%xmm10, %xmm4, %xmm11
	vsubss	%xmm11, %xmm2, %xmm2
	vmovaps	%xmm5, %xmm11
	vfmsub213ss	%xmm3, %xmm7, %xmm11    # xmm11 = (xmm7 * xmm11) - xmm3
	vsubss	%xmm10, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm3
	vmovss	160(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm0, %xmm10
	vaddss	%xmm4, %xmm10, %xmm2
	vsubss	%xmm4, %xmm2, %xmm12
	vsubss	%xmm12, %xmm2, %xmm13
	vsubss	%xmm13, %xmm4, %xmm4
	vmovaps	%xmm1, %xmm13
	vfmsub213ss	%xmm10, %xmm0, %xmm13   # xmm13 = (xmm0 * xmm13) - xmm10
	vsubss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm4, %xmm10, %xmm4
	vaddss	%xmm4, %xmm3, %xmm10
	vsubss	%xmm3, %xmm10, %xmm12
	vsubss	%xmm12, %xmm10, %xmm14
	vsubss	%xmm14, %xmm3, %xmm3
	vsubss	%xmm12, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm8, %xmm9, %xmm9
	vmulss	%xmm7, %xmm9, %xmm4
	vmovaps	%xmm9, %xmm12
	vfmsub213ss	%xmm4, %xmm7, %xmm12    # xmm12 = (xmm7 * xmm12) - xmm4
	vaddss	%xmm3, %xmm12, %xmm3
	vmulss	%xmm5, %xmm0, %xmm12
	vmovaps	%xmm5, %xmm14
	vfmsub213ss	%xmm12, %xmm0, %xmm14   # xmm14 = (xmm0 * xmm14) - xmm12
	vaddss	%xmm3, %xmm14, %xmm3
	vmovss	224(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm0, %xmm14
	vmovaps	%xmm1, %xmm15
	vfmsub213ss	%xmm14, %xmm0, %xmm15   # xmm15 = (xmm0 * xmm15) - xmm14
	vaddss	%xmm3, %xmm15, %xmm3
	vaddss	%xmm13, %xmm11, %xmm15
	vsubss	%xmm11, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm8
	vsubss	%xmm8, %xmm11, %xmm8
	vsubss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm14, %xmm12, %xmm8
	vsubss	%xmm12, %xmm8, %xmm11
	vsubss	%xmm11, %xmm8, %xmm13
	vsubss	%xmm13, %xmm12, %xmm12
	vsubss	%xmm11, %xmm14, %xmm11
	vaddss	%xmm11, %xmm12, %xmm11
	vaddss	%xmm4, %xmm15, %xmm12
	vsubss	%xmm15, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm15, %xmm14
	vsubss	%xmm13, %xmm4, %xmm4
	vaddss	%xmm4, %xmm14, %xmm4
	vaddss	%xmm10, %xmm8, %xmm13
	vsubss	%xmm8, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm14, %xmm10, %xmm10
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm13, %xmm12, %xmm10
	vsubss	%xmm12, %xmm10, %xmm14
	vsubss	%xmm14, %xmm10, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm0, %xmm12, %xmm0
	vfmadd231ss	48(%rsp), %xmm1, %xmm0  # 4-byte Folded Reload
                                        # xmm0 = (xmm1 * mem) + xmm0
	vfmadd231ss	224(%rsp), %xmm5, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm5 * mem) + xmm0
	vfmadd231ss	160(%rsp), %xmm9, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm9 * mem) + xmm0
	vmovss	8(%rsp), %xmm1                  # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	72(%rsp), %xmm1, %xmm1          # 4-byte Folded Reload
	vaddss	%xmm6, %xmm1, %xmm1
	vfmadd231ss	%xmm1, %xmm7, %xmm0     # xmm0 = (xmm7 * xmm1) + xmm0
	vaddss	%xmm0, %xmm10, %xmm1
	vsubss	%xmm1, %xmm10, %xmm3
	vaddss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmovss	4(%rsp), %xmm4                  # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm4
	vxorps	%xmm6, %xmm6, %xmm6
	vcvtsi2ss	%r12d, %xmm6, %xmm5
	vaddss	%xmm0, %xmm4, %xmm0
	vmulss	16(%rsp), %xmm5, %xmm4          # 4-byte Folded Reload
	vmulss	%xmm4, %xmm2, %xmm2
	vmulss	%xmm4, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm5 # xmm5 = xmm2[0],xmm0[0],xmm2[2,3]
	vmulss	%xmm4, %xmm1, %xmm1
	vmulss	%xmm4, %xmm3, %xmm2
	vinsertps	$16, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[2,3]
	jmp	.LBB42_46
.LBB42_59:
	movq	%rdi, %r12
	callq	omp_get_wtime
	vsubsd	400(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vxorps	%xmm11, %xmm11, %xmm11
	vcvtsi2sd	%eax, %xmm11, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	280(%rsp), %r15
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp6428:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp6429:
# %bb.60:
	movq	%rax, %r13
	movq	152(%rsp), %rax                 # 8-byte Reload
	leaq	32(%rax), %r12
.Ltmp6430:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp6431:
# %bb.61:
.Ltmp6432:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp6433:
# %bb.62:
	movl	%eax, %ebp
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp6434:
	leaq	176(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp6435:
# %bb.63:
.Ltmp6436:
	leaq	176(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp6437:
# %bb.64:
.Ltmp6439:
	callq	mpfr_get_default_rounding_mode
.Ltmp6440:
# %bb.65:
.Ltmp6441:
	leaq	176(%rsp), %rdi
	leaq	280(%rsp), %rsi
	movq	%r12, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp6442:
# %bb.66:
.Ltmp6444:
	leaq	176(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6445:
# %bb.67:
.Ltmp6446:
	movq	%rax, %r13
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp6447:
# %bb.68:
.Ltmp6448:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp6449:
# %bb.69:
	movl	%eax, %ebp
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp6450:
	leaq	80(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp6451:
# %bb.70:
.Ltmp6452:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp6453:
# %bb.71:
.Ltmp6455:
	callq	mpfr_get_default_rounding_mode
.Ltmp6456:
# %bb.72:
.Ltmp6457:
	leaq	80(%rsp), %rdi
	leaq	176(%rsp), %rsi
	movq	%r12, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp6458:
# %bb.73:
.Ltmp6460:
	callq	mpfr_get_default_rounding_mode
.Ltmp6461:
# %bb.74:
.Ltmp6462:
	movl	%eax, %ebp
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6463:
# %bb.75:
.Ltmp6464:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp6465:
# %bb.76:
.Ltmp6466:
	movl	%eax, %r12d
	leaq	112(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp6467:
# %bb.77:
.Ltmp6468:
	leaq	112(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6469:
# %bb.78:
.Ltmp6471:
	leaq	112(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_abs
.Ltmp6472:
# %bb.79:
.Ltmp6474:
	leaq	112(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp6475:
# %bb.80:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 352(%rsp)
	cmpq	$0, 136(%rsp)
	je	.LBB42_82
# %bb.81:
.Ltmp6477:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6478:
.LBB42_82:
	cmpq	$0, 104(%rsp)
	je	.LBB42_84
# %bb.83:
.Ltmp6480:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6481:
.LBB42_84:
	cmpq	$0, 200(%rsp)
	je	.LBB42_86
# %bb.85:
.Ltmp6483:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6484:
.LBB42_86:
	cmpq	$0, 304(%rsp)
	je	.LBB42_88
# %bb.87:
.Ltmp6486:
	leaq	280(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6487:
.LBB42_88:
	leaq	520(%rsp), %r15
	movq	%r15, 504(%rsp)
	movl	$846033518, 520(%rsp)           # imm = 0x326D726E
	movw	$32, 524(%rsp)
	movq	$5, 512(%rsp)
.Ltmp6489:
	leaq	504(%rsp), %rdi
	leaq	352(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp6490:
# %bb.89:
	movq	504(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB42_91
# %bb.90:
	callq	_ZdlPv
.LBB42_91:
	movq	344(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	movl	$160, %edi
	callq	_Znam
	movq	%rax, %r15
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rax)
	vmovupd	%xmm0, 16(%rax)
	vmovupd	%xmm0, 32(%rax)
	vmovupd	%xmm0, 48(%rax)
	vmovupd	%xmm0, 64(%rax)
	vmovupd	%xmm0, 80(%rax)
	vmovupd	%xmm0, 96(%rax)
	vmovupd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 128(%rax)
	vmovupd	%xmm0, 144(%rax)
	callq	omp_get_wtime
	vxorps	%xmm11, %xmm11, %xmm11
	vmovsd	%xmm0, 48(%rsp)                 # 8-byte Spill
	movq	208(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	movq	%rax, %rcx
	shlq	$4, %rcx
	xorl	%edx, %edx
	vxorpd	%xmm0, %xmm0, %xmm0
	vbroadcastss	.LCPI42_3(%rip), %xmm10 # xmm10 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	xorl	%esi, %esi
	jmp	.LBB42_93
	.p2align	4, 0x90
.LBB42_92:                              #   in Loop: Header=BB42_93 Depth=1
	vmovlhps	%xmm1, %xmm2, %xmm1             # xmm1 = xmm2[0],xmm1[0]
	movq	%rsi, %rdi
	shlq	$4, %rdi
	vmovups	%xmm1, (%r15,%rdi)
	incq	%rsi
	cmpq	$10, %rsi
	je	.LBB42_99
.LBB42_93:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB42_97 Depth 2
	vcvtsi2ss	%edx, %xmm12, %xmm1
	vblendps	$1, %xmm1, %xmm11, %xmm2        # xmm2 = xmm1[0],xmm11[1,2,3]
	vxorps	%xmm1, %xmm1, %xmm1
	testl	%eax, %eax
	jle	.LBB42_92
# %bb.94:                               #   in Loop: Header=BB42_93 Depth=1
	xorl	%edi, %edi
	jmp	.LBB42_97
	.p2align	4, 0x90
.LBB42_95:                              #   in Loop: Header=BB42_97 Depth=2
	vmovups	(%rbx,%rdi), %xmm5
	vmovddup	8(%rbx,%rdi), %xmm4             # xmm4 = mem[0,0]
.LBB42_96:                              #   in Loop: Header=BB42_97 Depth=2
	vaddss	%xmm5, %xmm2, %xmm3
	vsubss	%xmm2, %xmm3, %xmm6
	vsubss	%xmm6, %xmm3, %xmm7
	vsubss	%xmm7, %xmm2, %xmm7
	vsubss	%xmm6, %xmm5, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vmovshdup	%xmm2, %xmm2            # xmm2 = xmm2[1,1,3,3]
	vmovshdup	%xmm5, %xmm5            # xmm5 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm2, %xmm7
	vsubss	%xmm2, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm2, %xmm2
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm4, %xmm1, %xmm5
	vsubss	%xmm1, %xmm5, %xmm8
	vsubss	%xmm8, %xmm5, %xmm9
	vsubss	%xmm9, %xmm1, %xmm9
	vsubss	%xmm8, %xmm4, %xmm8
	vaddss	%xmm8, %xmm9, %xmm8
	vmovshdup	%xmm1, %xmm1            # xmm1 = xmm1[1,1,3,3]
	vaddss	%xmm1, %xmm8, %xmm1
	vmovshdup	%xmm4, %xmm4            # xmm4 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm7, %xmm6, %xmm4
	vsubss	%xmm6, %xmm4, %xmm8
	vsubss	%xmm8, %xmm4, %xmm9
	vsubss	%xmm9, %xmm6, %xmm6
	vsubss	%xmm8, %xmm7, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm6, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm6, %xmm6
	vsubss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vaddss	%xmm5, %xmm7, %xmm6
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm9
	vsubss	%xmm9, %xmm7, %xmm7
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm7, %xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm1, %xmm6, %xmm2
	vsubss	%xmm2, %xmm6, %xmm5
	vaddss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vinsertps	$16, %xmm1, %xmm2, %xmm1 # xmm1 = xmm2[0],xmm1[0],xmm2[2,3]
	vaddss	%xmm5, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm5, %xmm3, %xmm3
	vinsertps	$16, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[0],xmm3[0],xmm2[2,3]
	addq	$16, %rdi
	cmpq	%rdi, %rcx
	je	.LBB42_92
.LBB42_97:                              #   Parent Loop BB42_93 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rdi), %xmm3              # xmm3 = mem[0],zero,zero,zero
	vcomiss	%xmm3, %xmm0
	jbe	.LBB42_95
# %bb.98:                               #   in Loop: Header=BB42_97 Depth=2
	vinsertps	$16, 4(%rbx,%rdi), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vxorps	%xmm3, %xmm10, %xmm5
	vmovsd	8(%rbx,%rdi), %xmm3             # xmm3 = mem[0],zero
	vxorps	%xmm3, %xmm10, %xmm4
	jmp	.LBB42_96
.LBB42_99:
	vmovups	%xmm10, 320(%rsp)               # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	omp_get_wtime
	vsubsd	48(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm12, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	280(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp6492:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp6493:
# %bb.100:
	movq	%rax, %r13
	movq	152(%rsp), %rax                 # 8-byte Reload
	leaq	64(%rax), %r14
.Ltmp6494:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp6495:
# %bb.101:
.Ltmp6496:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp6497:
# %bb.102:
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp6498:
	leaq	176(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp6499:
# %bb.103:
.Ltmp6500:
	leaq	176(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp6501:
# %bb.104:
.Ltmp6503:
	callq	mpfr_get_default_rounding_mode
.Ltmp6504:
# %bb.105:
.Ltmp6505:
	leaq	176(%rsp), %rdi
	leaq	280(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp6506:
# %bb.106:
.Ltmp6508:
	leaq	176(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6509:
# %bb.107:
.Ltmp6510:
	movq	%rax, %r13
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp6511:
# %bb.108:
.Ltmp6512:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp6513:
# %bb.109:
	movl	%eax, %ebp
	cmpq	%r12, %r13
	cmovgq	%r13, %r12
.Ltmp6514:
	leaq	80(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp6515:
# %bb.110:
.Ltmp6516:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp6517:
# %bb.111:
.Ltmp6519:
	callq	mpfr_get_default_rounding_mode
.Ltmp6520:
# %bb.112:
.Ltmp6521:
	leaq	80(%rsp), %rdi
	leaq	176(%rsp), %rsi
	movq	%r14, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp6522:
# %bb.113:
.Ltmp6524:
	callq	mpfr_get_default_rounding_mode
.Ltmp6525:
# %bb.114:
.Ltmp6526:
	movl	%eax, %ebp
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6527:
# %bb.115:
.Ltmp6528:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp6529:
# %bb.116:
.Ltmp6530:
	movl	%eax, %r12d
	leaq	112(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp6531:
# %bb.117:
.Ltmp6532:
	leaq	112(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6533:
# %bb.118:
.Ltmp6535:
	leaq	112(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_abs
.Ltmp6536:
# %bb.119:
.Ltmp6538:
	leaq	112(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp6539:
# %bb.120:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 352(%rsp)
	cmpq	$0, 136(%rsp)
	je	.LBB42_122
# %bb.121:
.Ltmp6541:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6542:
.LBB42_122:
	cmpq	$0, 104(%rsp)
	je	.LBB42_124
# %bb.123:
.Ltmp6544:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6545:
.LBB42_124:
	cmpq	$0, 200(%rsp)
	je	.LBB42_126
# %bb.125:
.Ltmp6547:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6548:
.LBB42_126:
	cmpq	$0, 304(%rsp)
	je	.LBB42_128
# %bb.127:
.Ltmp6550:
	leaq	280(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6551:
.LBB42_128:
	leaq	488(%rsp), %r12
	movq	%r12, 472(%rsp)
	movl	$1836413793, 488(%rsp)          # imm = 0x6D757361
	movw	$32, 492(%rsp)
	movq	$5, 480(%rsp)
.Ltmp6553:
	leaq	472(%rsp), %rdi
	leaq	352(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp6554:
# %bb.129:
	movq	472(%rsp), %rdi
	cmpq	%r12, %rdi
	je	.LBB42_131
# %bb.130:
	callq	_ZdlPv
.LBB42_131:
	movq	%r15, %rdi
	callq	_ZdaPv
	leaq	240(%rsp), %rsi
	movq	208(%rsp), %r13                 # 8-byte Reload
	movq	%r13, %rdi
	movq	%rbx, %rdx
	movq	32(%rsp), %rcx                  # 8-byte Reload
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	112(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	cmpl	$0, (%r13)
	jle	.LBB42_172
# %bb.132:
	movq	152(%rsp), %rax                 # 8-byte Reload
	leaq	256(%rax), %r15
	xorl	%r14d, %r14d
	movq	32(%rsp), %rsi                  # 8-byte Reload
	jmp	.LBB42_134
	.p2align	4, 0x90
.LBB42_133:                             #   in Loop: Header=BB42_134 Depth=1
	incq	%r14
	movq	208(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r15
	movq	48(%rsp), %rsi                  # 8-byte Reload
	addq	$16, %rsi
	cmpq	%rax, %r14
	jge	.LBB42_172
.LBB42_134:                             # =>This Inner Loop Header: Depth=1
.Ltmp6556:
	leaq	176(%rsp), %rdi
	movq	%rsi, 48(%rsp)                  # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp6557:
# %bb.135:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6559:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp6560:
# %bb.136:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6561:
	movq	%rax, %rbp
	leaq	176(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6562:
# %bb.137:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6563:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp6564:
# %bb.138:                              #   in Loop: Header=BB42_134 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp6565:
	leaq	80(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp6566:
# %bb.139:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6567:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6568:
# %bb.140:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6570:
	callq	mpfr_get_default_rounding_mode
.Ltmp6571:
# %bb.141:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6572:
	leaq	80(%rsp), %rdi
	movq	%r15, %rsi
	leaq	176(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp6573:
# %bb.142:                              #   in Loop: Header=BB42_134 Depth=1
	cmpq	$0, 200(%rsp)
	je	.LBB42_144
# %bb.143:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6575:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6576:
.LBB42_144:                             #   in Loop: Header=BB42_134 Depth=1
.Ltmp6578:
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6579:
# %bb.145:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6580:
	movq	%rax, %rbp
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6581:
# %bb.146:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6582:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp6583:
# %bb.147:                              #   in Loop: Header=BB42_134 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp6584:
	leaq	280(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp6585:
# %bb.148:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6586:
	leaq	280(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6587:
# %bb.149:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6589:
	callq	mpfr_get_default_rounding_mode
.Ltmp6590:
# %bb.150:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6591:
	leaq	280(%rsp), %rdi
	leaq	80(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp6592:
# %bb.151:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6594:
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6595:
# %bb.152:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6596:
	movq	%rax, %rbp
	leaq	280(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6597:
# %bb.153:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6598:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp6599:
# %bb.154:                              #   in Loop: Header=BB42_134 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp6600:
	leaq	176(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp6601:
# %bb.155:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6602:
	leaq	176(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6603:
# %bb.156:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6605:
	callq	mpfr_get_default_rounding_mode
.Ltmp6606:
	leaq	112(%rsp), %r12
# %bb.157:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6607:
	leaq	176(%rsp), %rdi
	movq	%r12, %rsi
	leaq	280(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp6608:
# %bb.158:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6610:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp6611:
# %bb.159:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6612:
	movq	%rax, %r12
	leaq	176(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6613:
# %bb.160:                              #   in Loop: Header=BB42_134 Depth=1
	movq	%rax, %rbp
	cmpq	%rax, %r12
	je	.LBB42_164
# %bb.161:                              #   in Loop: Header=BB42_134 Depth=1
	cmpq	$0, 136(%rsp)
	je	.LBB42_163
# %bb.162:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6614:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6615:
.LBB42_163:                             #   in Loop: Header=BB42_134 Depth=1
.Ltmp6616:
	leaq	112(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp6617:
.LBB42_164:                             #   in Loop: Header=BB42_134 Depth=1
.Ltmp6618:
	callq	mpfr_get_default_rounding_mode
.Ltmp6619:
# %bb.165:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6620:
	leaq	112(%rsp), %rdi
	leaq	176(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp6621:
# %bb.166:                              #   in Loop: Header=BB42_134 Depth=1
	cmpq	$0, 200(%rsp)
	je	.LBB42_168
# %bb.167:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6623:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6624:
.LBB42_168:                             #   in Loop: Header=BB42_134 Depth=1
	cmpq	$0, 304(%rsp)
	je	.LBB42_170
# %bb.169:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6626:
	leaq	280(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6627:
.LBB42_170:                             #   in Loop: Header=BB42_134 Depth=1
	cmpq	$0, 104(%rsp)
	je	.LBB42_133
# %bb.171:                              #   in Loop: Header=BB42_134 Depth=1
.Ltmp6629:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6630:
	jmp	.LBB42_133
.LBB42_172:
.Ltmp6632:
	callq	mpfr_get_default_rounding_mode
.Ltmp6633:
# %bb.173:
.Ltmp6634:
	movl	%eax, %ebp
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6635:
# %bb.174:
.Ltmp6636:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp6637:
# %bb.175:
.Ltmp6638:
	movl	%eax, %r15d
	leaq	80(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp6639:
# %bb.176:
.Ltmp6640:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp6641:
# %bb.177:
.Ltmp6643:
	leaq	80(%rsp), %rdi
	leaq	112(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp6644:
# %bb.178:
.Ltmp6646:
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6647:
# %bb.179:
.Ltmp6648:
	movq	%rax, %r12
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6649:
# %bb.180:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB42_184
# %bb.181:
	cmpq	$0, 136(%rsp)
	je	.LBB42_183
# %bb.182:
.Ltmp6650:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6651:
.LBB42_183:
.Ltmp6652:
	leaq	112(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp6653:
.LBB42_184:
.Ltmp6654:
	callq	mpfr_get_default_rounding_mode
.Ltmp6655:
# %bb.185:
.Ltmp6656:
	leaq	112(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp6657:
# %bb.186:
	cmpq	$0, 104(%rsp)
	je	.LBB42_188
# %bb.187:
.Ltmp6659:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6660:
.LBB42_188:
	callq	omp_get_wtime
	vmovsd	%xmm0, 48(%rsp)                 # 8-byte Spill
.Ltmp6662:
	leaq	240(%rsp), %rsi
	movq	208(%rsp), %r15                 # 8-byte Reload
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	32(%rsp), %r14                  # 8-byte Reload
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp6663:
# %bb.189:
.Ltmp6664:
	leaq	240(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp6665:
# %bb.190:
.Ltmp6666:
	leaq	240(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp6667:
# %bb.191:
.Ltmp6668:
	leaq	240(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp6669:
# %bb.192:
.Ltmp6670:
	leaq	240(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp6671:
# %bb.193:
.Ltmp6672:
	leaq	240(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp6673:
# %bb.194:
.Ltmp6674:
	leaq	240(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp6675:
# %bb.195:
.Ltmp6676:
	leaq	240(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp6677:
# %bb.196:
.Ltmp6678:
	leaq	240(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp6679:
# %bb.197:
.Ltmp6680:
	leaq	240(%rsp), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movq	%r14, %rcx
	callq	_Z4axpyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKT_PS8_PS7_
.Ltmp6681:
# %bb.198:
	callq	omp_get_wtime
	vsubsd	48(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp6683:
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6684:
# %bb.199:
	movq	%rax, %r12
	movq	152(%rsp), %rax                 # 8-byte Reload
	leaq	96(%rax), %r15
.Ltmp6685:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp6686:
# %bb.200:
.Ltmp6687:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp6688:
# %bb.201:
	movl	%eax, %ebp
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp6689:
	leaq	80(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp6690:
# %bb.202:
.Ltmp6691:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp6692:
# %bb.203:
.Ltmp6694:
	callq	mpfr_get_default_rounding_mode
.Ltmp6695:
# %bb.204:
.Ltmp6696:
	leaq	80(%rsp), %rdi
	leaq	112(%rsp), %rsi
	movq	%r15, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp6697:
# %bb.205:
.Ltmp6699:
	leaq	80(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp6700:
# %bb.206:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 176(%rsp)
	cmpq	$0, 104(%rsp)
	je	.LBB42_208
# %bb.207:
.Ltmp6702:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6703:
.LBB42_208:
	leaq	456(%rsp), %r15
	movq	%r15, 440(%rsp)
	movl	$2037413985, 456(%rsp)          # imm = 0x79707861
	movw	$32, 460(%rsp)
	movq	$5, 448(%rsp)
.Ltmp6705:
	leaq	440(%rsp), %rdi
	leaq	176(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp6706:
# %bb.209:
	movq	440(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB42_211
# %bb.210:
	callq	_ZdlPv
.LBB42_211:
	cmpq	$0, 136(%rsp)
	je	.LBB42_213
# %bb.212:
.Ltmp6708:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6709:
.LBB42_213:
	movslq	28(%rsp), %r12
	movq	%r12, %r15
	shlq	$4, %r15
	testq	%r12, %r12
	movq	$-1, %rdi
	cmovnsq	%r15, %rdi
	callq	_Znam
	movq	%rax, %rbp
	testq	%r12, %r12
	movq	32(%rsp), %r14                  # 8-byte Reload
	je	.LBB42_217
# %bb.214:
	movq	%rbp, %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_intel_fast_memset@PLT
	movq	336(%rsp), %r13                 # 8-byte Reload
	testl	%r12d, %r12d
	jle	.LBB42_217
# %bb.215:
	xorl	%r15d, %r15d
	xorl	%r12d, %r12d
	.p2align	4, 0x90
.LBB42_216:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, (%r14,%r15)
	vmovupd	(%r14,%r15), %xmm0
	vmovupd	%xmm0, (%rbp,%r15)
	incq	%r12
	movslq	28(%rsp), %rax
	addq	$16, %r15
	addq	$32, %r13
	cmpq	%rax, %r12
	jl	.LBB42_216
.LBB42_217:
	leaq	28(%rsp), %rdi
	leaq	240(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%rbp, 160(%rsp)                 # 8-byte Spill
	movq	%rbp, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	112(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	cmpl	$0, 28(%rsp)
	jle	.LBB42_258
# %bb.218:
	xorl	%eax, %eax
	movq	%rax, 48(%rsp)                  # 8-byte Spill
	leaq	80(%rsp), %r13
	movq	160(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB42_220
	.p2align	4, 0x90
.LBB42_219:                             #   in Loop: Header=BB42_220 Depth=1
	movq	48(%rsp), %rdx                  # 8-byte Reload
	incq	%rdx
	movslq	28(%rsp), %rax
	movq	224(%rsp), %rsi                 # 8-byte Reload
	addq	$16, %rsi
	movq	%rdx, %rcx
	movq	%rdx, 48(%rsp)                  # 8-byte Spill
	cmpq	%rax, %rdx
	jge	.LBB42_258
.LBB42_220:                             # =>This Inner Loop Header: Depth=1
	movq	%r13, %r14
	movq	208(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %r15d
.Ltmp6711:
	leaq	176(%rsp), %rdi
	movq	%rsi, 224(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp6712:
# %bb.221:                              #   in Loop: Header=BB42_220 Depth=1
	movq	48(%rsp), %rax                  # 8-byte Reload
	addl	%r15d, %eax
	addl	$8, %eax
	movslq	%eax, %r12
	shlq	$5, %r12
	addq	152(%rsp), %r12                 # 8-byte Folded Reload
.Ltmp6714:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp6715:
# %bb.222:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6716:
	movq	%rax, %rbp
	leaq	176(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6717:
# %bb.223:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6718:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp6719:
# %bb.224:                              #   in Loop: Header=BB42_220 Depth=1
	movl	%eax, %r13d
	cmpq	%r15, %rbp
	cmovgq	%rbp, %r15
.Ltmp6720:
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp6721:
# %bb.225:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6722:
	movq	%r14, %rdi
	xorl	%esi, %esi
	movl	%r13d, %edx
	callq	mpfr_set_si
.Ltmp6723:
# %bb.226:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6725:
	callq	mpfr_get_default_rounding_mode
.Ltmp6726:
# %bb.227:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6727:
	movq	%r14, %r13
	movq	%r14, %rdi
	movq	%r12, %rsi
	leaq	176(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp6728:
# %bb.228:                              #   in Loop: Header=BB42_220 Depth=1
	cmpq	$0, 200(%rsp)
	je	.LBB42_230
# %bb.229:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6730:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6731:
.LBB42_230:                             #   in Loop: Header=BB42_220 Depth=1
.Ltmp6733:
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp6734:
# %bb.231:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6735:
	movq	%rax, %r12
	movq	%r13, %rdi
	callq	mpfr_get_prec
.Ltmp6736:
# %bb.232:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6737:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp6738:
# %bb.233:                              #   in Loop: Header=BB42_220 Depth=1
	movl	%eax, %ebp
	cmpq	%r15, %r12
	cmovgq	%r12, %r15
.Ltmp6739:
	leaq	280(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp6740:
# %bb.234:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6741:
	leaq	280(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp6742:
# %bb.235:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6744:
	callq	mpfr_get_default_rounding_mode
.Ltmp6745:
# %bb.236:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6746:
	leaq	280(%rsp), %rdi
	movq	%r13, %rsi
	movq	%r13, %rdx
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp6747:
# %bb.237:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6749:
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6750:
# %bb.238:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6751:
	movq	%rax, %r12
	leaq	280(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6752:
# %bb.239:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6753:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp6754:
# %bb.240:                              #   in Loop: Header=BB42_220 Depth=1
	movl	%eax, %ebp
	cmpq	%r15, %r12
	cmovgq	%r12, %r15
.Ltmp6755:
	leaq	176(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp6756:
# %bb.241:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6757:
	leaq	176(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp6758:
# %bb.242:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6760:
	callq	mpfr_get_default_rounding_mode
.Ltmp6761:
	leaq	112(%rsp), %r14
# %bb.243:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6762:
	leaq	176(%rsp), %rdi
	movq	%r14, %rsi
	leaq	280(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp6763:
# %bb.244:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6765:
	movq	%r14, %rdi
	callq	mpfr_get_prec
.Ltmp6766:
# %bb.245:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6767:
	movq	%rax, %r15
	leaq	176(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6768:
# %bb.246:                              #   in Loop: Header=BB42_220 Depth=1
	movq	%rax, %r12
	cmpq	%rax, %r15
	je	.LBB42_250
# %bb.247:                              #   in Loop: Header=BB42_220 Depth=1
	cmpq	$0, 136(%rsp)
	je	.LBB42_249
# %bb.248:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6769:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6770:
.LBB42_249:                             #   in Loop: Header=BB42_220 Depth=1
.Ltmp6771:
	leaq	112(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp6772:
.LBB42_250:                             #   in Loop: Header=BB42_220 Depth=1
.Ltmp6773:
	callq	mpfr_get_default_rounding_mode
.Ltmp6774:
# %bb.251:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6775:
	leaq	112(%rsp), %rdi
	leaq	176(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp6776:
# %bb.252:                              #   in Loop: Header=BB42_220 Depth=1
	cmpq	$0, 200(%rsp)
	je	.LBB42_254
# %bb.253:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6778:
	leaq	176(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6779:
.LBB42_254:                             #   in Loop: Header=BB42_220 Depth=1
	cmpq	$0, 304(%rsp)
	je	.LBB42_256
# %bb.255:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6781:
	leaq	280(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6782:
.LBB42_256:                             #   in Loop: Header=BB42_220 Depth=1
	cmpq	$0, 104(%rsp)
	je	.LBB42_219
# %bb.257:                              #   in Loop: Header=BB42_220 Depth=1
.Ltmp6784:
	movq	%r13, %rdi
	callq	mpfr_clear
.Ltmp6785:
	jmp	.LBB42_219
.LBB42_258:
.Ltmp6787:
	callq	mpfr_get_default_rounding_mode
.Ltmp6788:
# %bb.259:
.Ltmp6789:
	movl	%eax, %ebp
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6790:
# %bb.260:
.Ltmp6791:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp6792:
# %bb.261:
.Ltmp6793:
	movl	%eax, %r15d
	leaq	80(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp6794:
# %bb.262:
.Ltmp6795:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp6796:
# %bb.263:
.Ltmp6798:
	leaq	80(%rsp), %rdi
	leaq	112(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp6799:
# %bb.264:
.Ltmp6801:
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6802:
# %bb.265:
.Ltmp6803:
	movq	%rax, %r12
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6804:
# %bb.266:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB42_270
# %bb.267:
	cmpq	$0, 136(%rsp)
	je	.LBB42_269
# %bb.268:
.Ltmp6805:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6806:
.LBB42_269:
.Ltmp6807:
	leaq	112(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp6808:
.LBB42_270:
.Ltmp6809:
	callq	mpfr_get_default_rounding_mode
.Ltmp6810:
# %bb.271:
.Ltmp6811:
	leaq	112(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp6812:
# %bb.272:
	cmpq	$0, 104(%rsp)
	je	.LBB42_274
# %bb.273:
.Ltmp6814:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6815:
.LBB42_274:
	callq	omp_get_wtime
	vmovsd	%xmm0, 48(%rsp)                 # 8-byte Spill
.Ltmp6817:
	leaq	28(%rsp), %rdi
	leaq	240(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	32(%rsp), %r14                  # 8-byte Reload
	movq	%r14, %r8
	movq	160(%rsp), %r15                 # 8-byte Reload
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6818:
# %bb.275:
.Ltmp6819:
	leaq	28(%rsp), %rdi
	leaq	240(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6820:
# %bb.276:
.Ltmp6821:
	leaq	28(%rsp), %rdi
	leaq	240(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6822:
# %bb.277:
.Ltmp6823:
	leaq	28(%rsp), %rdi
	leaq	240(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6824:
# %bb.278:
.Ltmp6825:
	leaq	28(%rsp), %rdi
	leaq	240(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6826:
# %bb.279:
.Ltmp6827:
	leaq	28(%rsp), %rdi
	leaq	240(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6828:
# %bb.280:
.Ltmp6829:
	leaq	28(%rsp), %rdi
	leaq	240(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6830:
# %bb.281:
.Ltmp6831:
	leaq	28(%rsp), %rdi
	leaq	240(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6832:
# %bb.282:
.Ltmp6833:
	leaq	28(%rsp), %rdi
	leaq	240(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6834:
# %bb.283:
.Ltmp6835:
	leaq	28(%rsp), %rdi
	leaq	240(%rsp), %rdx
	movq	%rdi, %rsi
	movq	%rbx, %rcx
	movq	%r14, %r8
	movq	%r15, %r9
	callq	_Z4gemvIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiS6_RKT_PS8_SA_PS7_
.Ltmp6836:
# %bb.284:
	callq	omp_get_wtime
	vsubsd	48(%rsp), %xmm0, %xmm0          # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp6838:
	leaq	112(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6839:
# %bb.285:
	movq	%rax, %r15
	movq	152(%rsp), %rdi                 # 8-byte Reload
	subq	$-128, %rdi
.Ltmp6840:
	movq	%rdi, %r13
	callq	mpfr_get_prec
.Ltmp6841:
# %bb.286:
.Ltmp6842:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp6843:
# %bb.287:
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp6844:
	leaq	80(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp6845:
# %bb.288:
.Ltmp6846:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp6847:
# %bb.289:
.Ltmp6849:
	callq	mpfr_get_default_rounding_mode
.Ltmp6850:
# %bb.290:
.Ltmp6851:
	leaq	80(%rsp), %rdi
	leaq	112(%rsp), %rsi
	movq	%r13, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp6852:
# %bb.291:
.Ltmp6854:
	leaq	80(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7qX_real7qx_realIfLNS_9AlgorithmE0EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp6855:
# %bb.292:
	vunpcklpd	%xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
	vmovupd	%xmm0, 176(%rsp)
	cmpq	$0, 104(%rsp)
	je	.LBB42_294
# %bb.293:
.Ltmp6857:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6858:
.LBB42_294:
	leaq	424(%rsp), %r15
	movq	%r15, 408(%rsp)
	movl	$1986880871, 424(%rsp)          # imm = 0x766D6567
	movw	$32, 428(%rsp)
	movq	$5, 416(%rsp)
.Ltmp6860:
	leaq	408(%rsp), %rdi
	leaq	176(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp6861:
# %bb.295:
	movq	408(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB42_297
# %bb.296:
	callq	_ZdlPv
.LBB42_297:
	movq	160(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	cmpq	$0, 136(%rsp)
	je	.LBB42_299
# %bb.298:
.Ltmp6863:
	leaq	112(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6864:
.LBB42_299:
	movq	%rbx, %rdi
	callq	_ZdaPv
	movq	32(%rsp), %rdi                  # 8-byte Reload
	callq	_ZdaPv
	xorl	%eax, %eax
	vcvtsi2ss	%eax, %xmm2, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vblendps	$1, %xmm0, %xmm5, %xmm2         # xmm2 = xmm0[0],xmm5[1,2,3]
	movl	$2097152, %ebx                  # imm = 0x200000
	vxorps	%xmm13, %xmm13, %xmm13
	movl	$1, %ebp
	jmp	.LBB42_302
.LBB42_300:                             #   in Loop: Header=BB42_302 Depth=1
	vmovups	112(%rsp), %xmm5                # AlignMOV convert to UnAlignMOV 
	vmovddup	120(%rsp), %xmm0                # xmm0 = mem[0,0]
	.p2align	4, 0x90
.LBB42_301:                             #   in Loop: Header=BB42_302 Depth=1
	vmovups	224(%rsp), %xmm4                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm5, %xmm4, %xmm2
	vsubss	%xmm4, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm3
	vsubss	%xmm3, %xmm4, %xmm3
	vsubss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vmovshdup	%xmm4, %xmm3            # xmm3 = xmm4[1,1,3,3]
	vmovshdup	%xmm5, %xmm4            # xmm4 = xmm5[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm5
	vsubss	%xmm3, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm3, %xmm3
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vmovups	48(%rsp), %xmm8                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm0, %xmm8, %xmm4
	vsubss	%xmm8, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vsubss	%xmm6, %xmm0, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vmovshdup	%xmm8, %xmm7            # xmm7 = xmm8[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm0, %xmm0            # xmm0 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm5, %xmm1, %xmm6
	vsubss	%xmm1, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm3, %xmm1, %xmm5
	vsubss	%xmm1, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vsubss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm4, %xmm5, %xmm3
	vsubss	%xmm5, %xmm3, %xmm7
	vsubss	%xmm7, %xmm3, %xmm8
	vsubss	%xmm8, %xmm5, %xmm5
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm5, %xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm3, %xmm1
	vsubss	%xmm1, %xmm3, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm1, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vinsertps	$16, %xmm0, %xmm1, %xmm5 # xmm5 = xmm1[0],xmm0[0],xmm1[2,3]
	vaddss	%xmm4, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm4
	vinsertps	$16, %xmm4, %xmm3, %xmm2 # xmm2 = xmm3[0],xmm4[0],xmm3[2,3]
	leal	-1(%rbx), %eax
	cmpl	$1, %ebx
	movl	%eax, %ebx
	jbe	.LBB42_314
.LBB42_302:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB42_307 Depth 2
	vmovups	%xmm2, 224(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm5, 48(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vxorps	%xmm11, %xmm11, %xmm11
	vcvtsi2ss	%ebx, %xmm11, %xmm0
	vmulss	%xmm0, %xmm0, %xmm3
	vmovaps	%xmm0, %xmm1
	vfmsub213ss	%xmm3, %xmm0, %xmm1     # xmm1 = (xmm0 * xmm1) - xmm3
	vmulss	%xmm0, %xmm13, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vfmsub213ss	%xmm2, %xmm0, %xmm5     # xmm5 = (xmm0 * xmm5) - xmm2
	vmulss	%xmm0, %xmm13, %xmm4
	vmovaps	%xmm0, %xmm6
	vfmsub213ss	%xmm4, %xmm13, %xmm6    # xmm6 = (xmm13 * xmm6) - xmm4
	vaddss	%xmm2, %xmm1, %xmm7
	vsubss	%xmm1, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm1, %xmm1
	vsubss	%xmm8, %xmm2, %xmm8
	vaddss	%xmm1, %xmm8, %xmm8
	vaddss	%xmm4, %xmm7, %xmm1
	vsubss	%xmm7, %xmm1, %xmm9
	vsubss	%xmm9, %xmm1, %xmm10
	vsubss	%xmm10, %xmm7, %xmm7
	vsubss	%xmm9, %xmm4, %xmm9
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm8, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm8, %xmm8
	vsubss	%xmm10, %xmm7, %xmm7
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm5, %xmm7, %xmm7
	vaddss	%xmm7, %xmm13, %xmm7
	vaddss	%xmm6, %xmm7, %xmm7
	vaddss	%xmm6, %xmm5, %xmm8
	vsubss	%xmm5, %xmm8, %xmm10
	vsubss	%xmm10, %xmm8, %xmm11
	vsubss	%xmm11, %xmm5, %xmm5
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm4, %xmm13, %xmm6
	vsubss	%xmm13, %xmm6, %xmm10
	vsubss	%xmm10, %xmm6, %xmm11
	vsubss	%xmm11, %xmm13, %xmm11
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm11, %xmm4
	vaddss	%xmm2, %xmm8, %xmm10
	vsubss	%xmm8, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vsubss	%xmm11, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm6, %xmm9, %xmm8
	vsubss	%xmm6, %xmm8, %xmm11
	vsubss	%xmm11, %xmm8, %xmm12
	vsubss	%xmm12, %xmm6, %xmm6
	vsubss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm6, %xmm9, %xmm6
	vaddss	%xmm8, %xmm10, %xmm9
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm12, %xmm10, %xmm10
	vsubss	%xmm11, %xmm8, %xmm8
	vaddss	%xmm8, %xmm10, %xmm8
	vaddss	%xmm5, %xmm7, %xmm5
	vaddss	%xmm4, %xmm5, %xmm4
	vaddss	%xmm2, %xmm4, %xmm2
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vfmadd231ss	%xmm0, %xmm13, %xmm2    # xmm2 = (xmm13 * xmm0) + xmm2
	vfmadd231ss	%xmm13, %xmm13, %xmm2   # xmm2 = (xmm13 * xmm13) + xmm2
	vfmadd231ss	%xmm13, %xmm13, %xmm2   # xmm2 = (xmm13 * xmm13) + xmm2
	vfmadd231ss	%xmm0, %xmm13, %xmm2    # xmm2 = (xmm13 * xmm0) + xmm2
	vaddss	%xmm2, %xmm9, %xmm4
	vsubss	%xmm4, %xmm9, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm4, %xmm1, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm4, %xmm1, %xmm2
	vaddss	%xmm5, %xmm3, %xmm1
	vsubss	%xmm1, %xmm3, %xmm3
	vaddss	%xmm5, %xmm3, %xmm3
	vmulss	%xmm1, %xmm1, %xmm4
	vmovaps	%xmm1, %xmm5
	vfmsub213ss	%xmm4, %xmm1, %xmm5     # xmm5 = (xmm1 * xmm5) - xmm4
	vmulss	%xmm3, %xmm1, %xmm8
	vmovaps	%xmm3, %xmm6
	vfmsub213ss	%xmm8, %xmm1, %xmm6     # xmm6 = (xmm1 * xmm6) - xmm8
	vmulss	%xmm1, %xmm3, %xmm9
	vmovaps	%xmm1, %xmm7
	vfmsub213ss	%xmm9, %xmm3, %xmm7     # xmm7 = (xmm3 * xmm7) - xmm9
	vaddss	%xmm5, %xmm8, %xmm10
	vsubss	%xmm5, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm5, %xmm5
	vsubss	%xmm11, %xmm8, %xmm8
	vaddss	%xmm5, %xmm8, %xmm11
	vaddss	%xmm9, %xmm10, %xmm5
	vsubss	%xmm10, %xmm5, %xmm8
	vsubss	%xmm8, %xmm5, %xmm12
	vsubss	%xmm12, %xmm10, %xmm10
	vsubss	%xmm8, %xmm9, %xmm8
	vaddss	%xmm8, %xmm10, %xmm9
	vmulss	%xmm2, %xmm1, %xmm8
	vmovaps	%xmm2, %xmm10
	vfmsub213ss	%xmm8, %xmm1, %xmm10    # xmm10 = (xmm1 * xmm10) - xmm8
	vmulss	%xmm3, %xmm3, %xmm12
	vaddss	%xmm9, %xmm11, %xmm13
	vsubss	%xmm11, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm11, %xmm11
	vmovaps	%xmm3, %xmm15
	vfmsub213ss	%xmm12, %xmm3, %xmm15   # xmm15 = (xmm3 * xmm15) - xmm12
	vsubss	%xmm14, %xmm9, %xmm9
	vmulss	%xmm1, %xmm2, %xmm14
	vaddss	%xmm9, %xmm11, %xmm9
	vmovaps	%xmm1, %xmm11
	vfmsub213ss	%xmm14, %xmm2, %xmm11   # xmm11 = (xmm2 * xmm11) - xmm14
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm15, %xmm9, %xmm9
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm7, %xmm6, %xmm10
	vsubss	%xmm6, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm11, %xmm7, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm14, %xmm12, %xmm7
	vsubss	%xmm12, %xmm7, %xmm11
	vsubss	%xmm11, %xmm7, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm11, %xmm14, %xmm11
	vaddss	%xmm11, %xmm12, %xmm11
	vaddss	%xmm8, %xmm10, %xmm12
	vsubss	%xmm10, %xmm12, %xmm14
	vsubss	%xmm14, %xmm12, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vsubss	%xmm14, %xmm8, %xmm8
	vaddss	%xmm8, %xmm10, %xmm8
	vaddss	%xmm7, %xmm13, %xmm10
	vsubss	%xmm7, %xmm10, %xmm14
	vsubss	%xmm14, %xmm10, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm7, %xmm13, %xmm7
	vaddss	%xmm10, %xmm12, %xmm13
	vsubss	%xmm12, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm10, %xmm10
	vaddss	%xmm10, %xmm12, %xmm10
	vaddss	%xmm6, %xmm9, %xmm6
	vaddss	%xmm6, %xmm11, %xmm6
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm6, %xmm10, %xmm6
	vfmadd231ss	%xmm1, %xmm0, %xmm6     # xmm6 = (xmm0 * xmm1) + xmm6
	vfmadd231ss	%xmm3, %xmm2, %xmm6     # xmm6 = (xmm2 * xmm3) + xmm6
	vfmadd231ss	%xmm2, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm2) + xmm6
	vcvtsi2ss	%ebp, %xmm0, %xmm9
	vfmadd231ss	%xmm0, %xmm1, %xmm6     # xmm6 = (xmm1 * xmm0) + xmm6
	vaddss	%xmm6, %xmm13, %xmm0
	vsubss	%xmm0, %xmm13, %xmm1
	vxorps	%xmm13, %xmm13, %xmm13
	vaddss	%xmm6, %xmm1, %xmm6
	vaddss	%xmm0, %xmm5, %xmm2
	vsubss	%xmm2, %xmm5, %xmm1
	vaddss	%xmm0, %xmm1, %xmm3
	vaddss	%xmm2, %xmm4, %xmm1
	vsubss	%xmm1, %xmm4, %xmm0
	vaddss	%xmm2, %xmm0, %xmm2
	vucomiss	.LCPI42_4(%rip), %xmm1
	setnp	%al
	sete	%cl
	vxorps	%xmm5, %xmm5, %xmm5
	vxorps	%xmm0, %xmm0, %xmm0
	testb	%al, %cl
	jne	.LBB42_301
# %bb.303:                              #   in Loop: Header=BB42_302 Depth=1
	vucomiss	.LCPI42_5(%rip), %xmm1
	setnp	%al
	sete	%cl
	vbroadcastss	.LCPI42_3(%rip), %xmm0  # xmm0 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vmovaps	%xmm0, %xmm5
	testb	%al, %cl
	jne	.LBB42_301
# %bb.304:                              #   in Loop: Header=BB42_302 Depth=1
	vucomiss	%xmm13, %xmm1
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB42_306
# %bb.305:                              #   in Loop: Header=BB42_302 Depth=1
	vbroadcastss	%xmm1, %xmm0
	vandps	320(%rsp), %xmm0, %xmm0         # 16-byte Folded Reload
	vbroadcastss	.LCPI42_4(%rip), %xmm1  # xmm1 = [+Inf,+Inf,+Inf,+Inf]
	vorps	%xmm1, %xmm0, %xmm5
	vmovaps	%xmm5, %xmm0
	jmp	.LBB42_301
.LBB42_306:                             #   in Loop: Header=BB42_302 Depth=1
	vdivss	%xmm1, %xmm9, %xmm0
	vmulss	%xmm1, %xmm0, %xmm4
	vmovss	%xmm4, 8(%rsp)                  # 4-byte Spill
	vmovaps	%xmm1, %xmm7
	vfmsub213ss	%xmm4, %xmm0, %xmm7     # xmm7 = (xmm0 * xmm7) - xmm4
	vmulss	%xmm2, %xmm0, %xmm10
	vmovaps	%xmm2, %xmm8
	vfmsub213ss	%xmm10, %xmm0, %xmm8    # xmm8 = (xmm0 * xmm8) - xmm10
	vmulss	%xmm1, %xmm13, %xmm5
	vmovaps	%xmm1, %xmm4
	vfmsub213ss	%xmm5, %xmm13, %xmm4    # xmm4 = (xmm13 * xmm4) - xmm5
	vaddss	%xmm7, %xmm10, %xmm11
	vsubss	%xmm7, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm7, %xmm7
	vsubss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm7, %xmm10, %xmm10
	vaddss	%xmm5, %xmm11, %xmm7
	vmovss	%xmm7, 32(%rsp)                 # 4-byte Spill
	vsubss	%xmm11, %xmm7, %xmm12
	vsubss	%xmm12, %xmm7, %xmm13
	vsubss	%xmm13, %xmm11, %xmm11
	vsubss	%xmm12, %xmm5, %xmm12
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm10, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm10, %xmm10
	vmulss	%xmm3, %xmm0, %xmm14
	vsubss	%xmm13, %xmm11, %xmm11
	vmovaps	%xmm3, %xmm13
	vfmsub213ss	%xmm14, %xmm0, %xmm13   # xmm13 = (xmm0 * xmm13) - xmm14
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm13, %xmm10, %xmm7
	vmovss	%xmm7, 16(%rsp)                 # 4-byte Spill
	vmovaps	%xmm4, %xmm7
	vaddss	%xmm4, %xmm8, %xmm11
	vsubss	%xmm8, %xmm11, %xmm13
	vsubss	%xmm13, %xmm11, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm13, %xmm4, %xmm13
	vmovups	%xmm4, 208(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm13, %xmm8, %xmm10
	vmovaps	%xmm2, %xmm4
	vmovss	%xmm2, 72(%rsp)                 # 4-byte Spill
	vmulss	.LCPI42_7(%rip), %xmm2, %xmm15
	vfmsub132ss	.LCPI42_7(%rip), %xmm15, %xmm4 # xmm4 = (xmm4 * mem) - xmm15
	vmovss	%xmm4, 40(%rsp)                 # 4-byte Spill
	vmovaps	%xmm5, %xmm2
	vmovss	%xmm5, 4(%rsp)                  # 4-byte Spill
	vaddss	%xmm5, %xmm15, %xmm13
	vmovss	%xmm3, 160(%rsp)                # 4-byte Spill
	vsubss	%xmm15, %xmm13, %xmm3
	vsubss	%xmm3, %xmm13, %xmm5
	vsubss	%xmm5, %xmm15, %xmm5
	vsubss	%xmm3, %xmm2, %xmm3
	vaddss	%xmm3, %xmm5, %xmm2
	vmovss	%xmm2, 256(%rsp)                # 4-byte Spill
	vaddss	%xmm14, %xmm11, %xmm3
	vsubss	%xmm11, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm15
	vsubss	%xmm15, %xmm11, %xmm11
	vsubss	%xmm5, %xmm14, %xmm5
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm12, %xmm13, %xmm11
	vsubss	%xmm13, %xmm11, %xmm14
	vsubss	%xmm14, %xmm12, %xmm12
	vsubss	%xmm14, %xmm11, %xmm14
	vsubss	%xmm14, %xmm13, %xmm14
	vaddss	%xmm12, %xmm14, %xmm12
	vaddss	%xmm3, %xmm11, %xmm14
	vsubss	%xmm3, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm15, %xmm11, %xmm8
	vaddss	%xmm3, %xmm8, %xmm3
	vaddss	16(%rsp), %xmm4, %xmm8          # 4-byte Folded Reload
	vaddss	%xmm7, %xmm8, %xmm8
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm2, %xmm8, %xmm8
	vaddss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm5, %xmm12, %xmm5
	vaddss	%xmm3, %xmm5, %xmm3
	vfmadd231ss	.LCPI42_7(%rip), %xmm1, %xmm3 # xmm3 = (xmm1 * mem) + xmm3
	vmovss	72(%rsp), %xmm15                # 4-byte Reload
                                        # xmm15 = mem[0],zero,zero,zero
	vfmadd231ss	.LCPI42_7(%rip), %xmm15, %xmm3 # xmm3 = (xmm15 * mem) + xmm3
	vmovss	160(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vfmadd231ss	.LCPI42_7(%rip), %xmm4, %xmm3 # xmm3 = (xmm4 * mem) + xmm3
	vfmadd231ss	%xmm6, %xmm0, %xmm3     # xmm3 = (xmm0 * xmm6) + xmm3
	vaddss	%xmm3, %xmm14, %xmm5
	vsubss	%xmm5, %xmm14, %xmm8
	vaddss	%xmm3, %xmm8, %xmm3
	vmovss	32(%rsp), %xmm2                 # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vaddss	%xmm5, %xmm2, %xmm8
	vsubss	%xmm8, %xmm2, %xmm7
	vaddss	%xmm5, %xmm7, %xmm5
	vmovss	8(%rsp), %xmm2                  # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm8, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm8
	vsubss	%xmm7, %xmm9, %xmm2
	vsubss	%xmm9, %xmm2, %xmm10
	vsubss	%xmm10, %xmm2, %xmm11
	vsubss	%xmm11, %xmm9, %xmm9
	vmovss	%xmm6, 8(%rsp)                  # 4-byte Spill
	vbroadcastss	.LCPI42_3(%rip), %xmm11 # xmm11 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm7, %xmm11, %xmm7
	vsubss	%xmm10, %xmm7, %xmm7
	vaddss	%xmm7, %xmm9, %xmm7
	vxorps	%xmm11, %xmm8, %xmm9
	vxorps	%xmm6, %xmm6, %xmm6
	vsubss	%xmm8, %xmm6, %xmm8
	vsubss	.LCPI42_7(%rip), %xmm8, %xmm10
	vsubss	%xmm10, %xmm9, %xmm9
	vsubss	%xmm10, %xmm8, %xmm10
	vsubss	%xmm10, %xmm6, %xmm10
	vaddss	%xmm9, %xmm10, %xmm9
	vxorps	%xmm5, %xmm11, %xmm10
	vsubss	%xmm5, %xmm6, %xmm5
	vsubss	.LCPI42_7(%rip), %xmm5, %xmm11
	vsubss	%xmm11, %xmm10, %xmm10
	vsubss	%xmm11, %xmm5, %xmm11
	vsubss	%xmm11, %xmm6, %xmm11
	vaddss	%xmm10, %xmm11, %xmm10
	vaddss	.LCPI42_7(%rip), %xmm10, %xmm10
	vsubss	%xmm3, %xmm10, %xmm3
	vaddss	%xmm7, %xmm8, %xmm10
	vsubss	%xmm7, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm7, %xmm7
	vsubss	%xmm11, %xmm8, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm7, %xmm9, %xmm8
	vsubss	%xmm7, %xmm8, %xmm11
	vsubss	%xmm11, %xmm8, %xmm12
	vsubss	%xmm12, %xmm7, %xmm7
	vsubss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm5, %xmm8, %xmm9
	vsubss	%xmm8, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vsubss	%xmm11, %xmm5, %xmm5
	vaddss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm5, %xmm7, %xmm5
	vaddss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm3, %xmm9, %xmm5
	vsubss	%xmm5, %xmm9, %xmm7
	vaddss	%xmm3, %xmm7, %xmm3
	vmovss	%xmm3, 16(%rsp)                 # 4-byte Spill
	vaddss	%xmm5, %xmm10, %xmm3
	vsubss	%xmm3, %xmm10, %xmm7
	vaddss	%xmm5, %xmm7, %xmm5
	vmovss	%xmm5, (%rsp)                   # 4-byte Spill
	vaddss	%xmm3, %xmm2, %xmm14
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 12(%rsp)                 # 4-byte Spill
	vdivss	%xmm1, %xmm14, %xmm6
	vmulss	%xmm1, %xmm6, %xmm2
	vmovss	%xmm2, 20(%rsp)                 # 4-byte Spill
	vmovaps	%xmm1, %xmm3
	vfmsub213ss	%xmm2, %xmm6, %xmm3     # xmm3 = (xmm6 * xmm3) - xmm2
	vmovaps	%xmm15, %xmm2
	vmulss	%xmm6, %xmm15, %xmm5
	vaddss	%xmm5, %xmm3, %xmm8
	vsubss	%xmm3, %xmm8, %xmm7
	vsubss	%xmm7, %xmm8, %xmm10
	vsubss	%xmm10, %xmm3, %xmm3
	vmovaps	%xmm15, %xmm10
	vfmsub213ss	%xmm5, %xmm6, %xmm10    # xmm10 = (xmm6 * xmm10) - xmm5
	vmovaps	%xmm6, %xmm9
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vmovss	4(%rsp), %xmm2                  # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm5
	vsubss	%xmm5, %xmm7, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm5, %xmm2, %xmm5
	vaddss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm5, %xmm3, %xmm8
	vsubss	%xmm3, %xmm8, %xmm15
	vsubss	%xmm15, %xmm8, %xmm11
	vsubss	%xmm11, %xmm3, %xmm3
	vsubss	%xmm15, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vmovaps	%xmm4, %xmm6
	vmovaps	%xmm9, %xmm4
	vmulss	%xmm6, %xmm9, %xmm5
	vmovaps	%xmm6, %xmm11
	vfmsub213ss	%xmm5, %xmm9, %xmm11    # xmm11 = (xmm9 * xmm11) - xmm5
	vmovss	%xmm9, 32(%rsp)                 # 4-byte Spill
	vaddss	%xmm3, %xmm11, %xmm3
	vmovups	208(%rsp), %xmm2                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm2, %xmm10, %xmm11
	vsubss	%xmm10, %xmm11, %xmm15
	vsubss	%xmm15, %xmm11, %xmm9
	vsubss	%xmm9, %xmm10, %xmm9
	vsubss	%xmm15, %xmm2, %xmm10
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm5, %xmm11, %xmm10
	vsubss	%xmm11, %xmm10, %xmm15
	vsubss	%xmm15, %xmm10, %xmm12
	vsubss	%xmm12, %xmm11, %xmm11
	vsubss	%xmm15, %xmm5, %xmm5
	vaddss	%xmm5, %xmm11, %xmm5
	vmovss	%xmm13, 152(%rsp)               # 4-byte Spill
	vaddss	%xmm8, %xmm13, %xmm11
	vsubss	%xmm13, %xmm11, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vsubss	%xmm12, %xmm11, %xmm12
	vsubss	%xmm12, %xmm13, %xmm12
	vaddss	%xmm8, %xmm12, %xmm8
	vaddss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm10, %xmm12, %xmm15
	vsubss	%xmm15, %xmm12, %xmm13
	vsubss	%xmm13, %xmm10, %xmm10
	vsubss	%xmm15, %xmm11, %xmm11
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	40(%rsp), %xmm3, %xmm3          # 4-byte Folded Reload
	vaddss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm3, %xmm9, %xmm3
	vaddss	256(%rsp), %xmm3, %xmm3         # 4-byte Folded Reload
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vaddss	%xmm3, %xmm10, %xmm3
	vfmadd231ss	.LCPI42_7(%rip), %xmm1, %xmm3 # xmm3 = (xmm1 * mem) + xmm3
	vmovss	72(%rsp), %xmm13                # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vfmadd231ss	.LCPI42_7(%rip), %xmm13, %xmm3 # xmm3 = (xmm13 * mem) + xmm3
	vfmadd231ss	.LCPI42_7(%rip), %xmm6, %xmm3 # xmm3 = (xmm6 * mem) + xmm3
	vfmadd231ss	8(%rsp), %xmm4, %xmm3   # 4-byte Folded Reload
                                        # xmm3 = (xmm4 * mem) + xmm3
	vaddss	%xmm3, %xmm12, %xmm5
	vsubss	%xmm5, %xmm12, %xmm8
	vaddss	%xmm3, %xmm8, %xmm3
	vaddss	%xmm5, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm7
	vaddss	%xmm5, %xmm7, %xmm5
	vmovss	20(%rsp), %xmm2                 # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm8, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm8
	vsubss	%xmm7, %xmm14, %xmm2
	vsubss	%xmm14, %xmm2, %xmm9
	vsubss	%xmm9, %xmm2, %xmm10
	vsubss	%xmm10, %xmm14, %xmm10
	vbroadcastss	.LCPI42_3(%rip), %xmm4  # xmm4 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm4, %xmm7, %xmm7
	vsubss	%xmm9, %xmm7, %xmm7
	vaddss	%xmm7, %xmm10, %xmm7
	vmovss	12(%rsp), %xmm6                 # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm8, %xmm6, %xmm9
	vsubss	%xmm6, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm6, %xmm11
	vxorps	%xmm4, %xmm8, %xmm8
	vsubss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm8, %xmm11, %xmm8
	vmovss	(%rsp), %xmm6                   # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm6, %xmm10
	vsubss	%xmm6, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm6, %xmm12
	vxorps	%xmm4, %xmm5, %xmm5
	vsubss	%xmm11, %xmm5, %xmm5
	vaddss	%xmm5, %xmm12, %xmm5
	vaddss	16(%rsp), %xmm5, %xmm5          # 4-byte Folded Reload
	vsubss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm7, %xmm9, %xmm5
	vsubss	%xmm7, %xmm5, %xmm11
	vsubss	%xmm11, %xmm5, %xmm12
	vsubss	%xmm12, %xmm7, %xmm7
	vsubss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm7, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm12, %xmm7, %xmm7
	vsubss	%xmm11, %xmm8, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm10, %xmm9, %xmm8
	vsubss	%xmm9, %xmm8, %xmm11
	vsubss	%xmm11, %xmm8, %xmm12
	vsubss	%xmm12, %xmm9, %xmm9
	vsubss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm3, %xmm7, %xmm3
	vaddss	%xmm3, %xmm8, %xmm7
	vsubss	%xmm7, %xmm8, %xmm8
	vaddss	%xmm3, %xmm8, %xmm3
	vmovss	%xmm3, (%rsp)                   # 4-byte Spill
	vaddss	%xmm7, %xmm5, %xmm3
	vsubss	%xmm3, %xmm5, %xmm5
	vaddss	%xmm7, %xmm5, %xmm4
	vmovss	%xmm4, 12(%rsp)                 # 4-byte Spill
	vaddss	%xmm3, %xmm2, %xmm15
	vsubss	%xmm15, %xmm2, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 20(%rsp)                 # 4-byte Spill
	vdivss	%xmm1, %xmm15, %xmm4
	vmulss	%xmm1, %xmm4, %xmm2
	vmovss	%xmm2, 24(%rsp)                 # 4-byte Spill
	vmovaps	%xmm1, %xmm3
	vfmsub213ss	%xmm2, %xmm4, %xmm3     # xmm3 = (xmm4 * xmm3) - xmm2
	vmulss	%xmm4, %xmm13, %xmm5
	vaddss	%xmm5, %xmm3, %xmm8
	vsubss	%xmm3, %xmm8, %xmm7
	vsubss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vfmsub213ss	%xmm5, %xmm4, %xmm13    # xmm13 = (xmm4 * xmm13) - xmm5
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vmovss	4(%rsp), %xmm2                  # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm5
	vsubss	%xmm5, %xmm7, %xmm10
	vsubss	%xmm10, %xmm8, %xmm8
	vsubss	%xmm5, %xmm2, %xmm5
	vaddss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm5, %xmm3, %xmm8
	vsubss	%xmm3, %xmm8, %xmm10
	vsubss	%xmm10, %xmm8, %xmm12
	vsubss	%xmm12, %xmm3, %xmm3
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vmovss	160(%rsp), %xmm2                # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm4, %xmm5
	vmovaps	%xmm2, %xmm10
	vfmsub213ss	%xmm5, %xmm4, %xmm10    # xmm10 = (xmm4 * xmm10) - xmm5
	vmovss	%xmm4, 16(%rsp)                 # 4-byte Spill
	vaddss	%xmm3, %xmm10, %xmm3
	vmovups	208(%rsp), %xmm6                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm6, %xmm13, %xmm10
	vsubss	%xmm13, %xmm10, %xmm12
	vsubss	%xmm12, %xmm10, %xmm11
	vsubss	%xmm11, %xmm13, %xmm9
	vsubss	%xmm12, %xmm6, %xmm11
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm5, %xmm10, %xmm11
	vsubss	%xmm10, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm10, %xmm10
	vsubss	%xmm12, %xmm5, %xmm5
	vaddss	%xmm5, %xmm10, %xmm5
	vmovss	152(%rsp), %xmm13               # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vaddss	%xmm8, %xmm13, %xmm10
	vsubss	%xmm13, %xmm10, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vsubss	%xmm12, %xmm10, %xmm12
	vsubss	%xmm12, %xmm13, %xmm12
	vaddss	%xmm8, %xmm12, %xmm8
	vaddss	%xmm10, %xmm11, %xmm12
	vsubss	%xmm11, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm11, %xmm11
	vsubss	%xmm13, %xmm10, %xmm10
	vaddss	%xmm10, %xmm11, %xmm10
	vmovss	40(%rsp), %xmm14                # 4-byte Reload
                                        # xmm14 = mem[0],zero,zero,zero
	vaddss	%xmm3, %xmm14, %xmm3
	vaddss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm9, %xmm3
	vaddss	256(%rsp), %xmm3, %xmm3         # 4-byte Folded Reload
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vaddss	%xmm3, %xmm10, %xmm3
	vfmadd231ss	.LCPI42_7(%rip), %xmm1, %xmm3 # xmm3 = (xmm1 * mem) + xmm3
	vmovss	72(%rsp), %xmm6                 # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vfmadd231ss	.LCPI42_7(%rip), %xmm6, %xmm3 # xmm3 = (xmm6 * mem) + xmm3
	vfmadd231ss	.LCPI42_7(%rip), %xmm2, %xmm3 # xmm3 = (xmm2 * mem) + xmm3
	vfmadd231ss	8(%rsp), %xmm4, %xmm3   # 4-byte Folded Reload
                                        # xmm3 = (xmm4 * mem) + xmm3
	vaddss	%xmm3, %xmm12, %xmm5
	vsubss	%xmm5, %xmm12, %xmm8
	vaddss	%xmm3, %xmm8, %xmm3
	vaddss	%xmm5, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm7
	vaddss	%xmm5, %xmm7, %xmm5
	vmovss	24(%rsp), %xmm2                 # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm8, %xmm9
	vsubss	%xmm9, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vsubss	%xmm9, %xmm15, %xmm7
	vsubss	%xmm15, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm10
	vsubss	%xmm10, %xmm15, %xmm10
	vbroadcastss	.LCPI42_3(%rip), %xmm4  # xmm4 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm4, %xmm9, %xmm9
	vsubss	%xmm8, %xmm9, %xmm8
	vaddss	%xmm8, %xmm10, %xmm8
	vmovss	20(%rsp), %xmm11                # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm11, %xmm9
	vsubss	%xmm11, %xmm9, %xmm10
	vmovaps	%xmm11, %xmm12
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm12, %xmm11
	vxorps	%xmm4, %xmm2, %xmm2
	vsubss	%xmm10, %xmm2, %xmm2
	vaddss	%xmm2, %xmm11, %xmm2
	vmovss	12(%rsp), %xmm12                # 4-byte Reload
                                        # xmm12 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm12, %xmm10
	vsubss	%xmm12, %xmm10, %xmm11
	vmovaps	%xmm12, %xmm13
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm13, %xmm12
	vxorps	%xmm4, %xmm5, %xmm5
	vsubss	%xmm11, %xmm5, %xmm5
	vaddss	%xmm5, %xmm12, %xmm5
	vaddss	(%rsp), %xmm5, %xmm5            # 4-byte Folded Reload
	vsubss	%xmm3, %xmm5, %xmm3
	vaddss	%xmm9, %xmm8, %xmm5
	vsubss	%xmm8, %xmm5, %xmm11
	vsubss	%xmm11, %xmm5, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vsubss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm2, %xmm8, %xmm9
	vsubss	%xmm8, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vsubss	%xmm11, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm10, %xmm9, %xmm8
	vsubss	%xmm9, %xmm8, %xmm11
	vsubss	%xmm11, %xmm8, %xmm12
	vsubss	%xmm12, %xmm9, %xmm9
	vsubss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm2, %xmm9, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm3
	vsubss	%xmm3, %xmm8, %xmm8
	vaddss	%xmm2, %xmm8, %xmm2
	vmovss	%xmm2, (%rsp)                   # 4-byte Spill
	vaddss	%xmm3, %xmm5, %xmm8
	vsubss	%xmm8, %xmm5, %xmm2
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 12(%rsp)                 # 4-byte Spill
	vaddss	%xmm7, %xmm8, %xmm2
	vsubss	%xmm2, %xmm7, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vmovss	%xmm3, 20(%rsp)                 # 4-byte Spill
	vdivss	%xmm1, %xmm2, %xmm12
	vmulss	%xmm1, %xmm12, %xmm7
	vmovaps	%xmm1, %xmm3
	vfmsub213ss	%xmm7, %xmm12, %xmm3    # xmm3 = (xmm12 * xmm3) - xmm7
	vmulss	%xmm6, %xmm12, %xmm5
	vaddss	%xmm5, %xmm3, %xmm8
	vsubss	%xmm3, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm3, %xmm3
	vmovaps	%xmm6, %xmm11
	vmovaps	%xmm6, %xmm4
	vfmsub213ss	%xmm5, %xmm12, %xmm11   # xmm11 = (xmm12 * xmm11) - xmm5
	vsubss	%xmm9, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vmovss	4(%rsp), %xmm13                 # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vaddss	%xmm13, %xmm8, %xmm10
	vsubss	%xmm8, %xmm10, %xmm5
	vsubss	%xmm5, %xmm10, %xmm9
	vsubss	%xmm9, %xmm8, %xmm8
	vsubss	%xmm5, %xmm13, %xmm5
	vaddss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm5, %xmm3, %xmm8
	vsubss	%xmm3, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm13
	vsubss	%xmm13, %xmm3, %xmm3
	vsubss	%xmm9, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vmovss	160(%rsp), %xmm9                # 4-byte Reload
                                        # xmm9 = mem[0],zero,zero,zero
	vmulss	%xmm9, %xmm12, %xmm5
	vfmsub213ss	%xmm5, %xmm12, %xmm9    # xmm9 = (xmm12 * xmm9) - xmm5
	vaddss	%xmm3, %xmm9, %xmm3
	vaddss	%xmm3, %xmm14, %xmm3
	vmovups	208(%rsp), %xmm6                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vaddss	%xmm6, %xmm11, %xmm9
	vsubss	%xmm11, %xmm9, %xmm13
	vsubss	%xmm13, %xmm9, %xmm14
	vsubss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm6, %xmm3, %xmm3
	vsubss	%xmm13, %xmm6, %xmm13
	vaddss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm5, %xmm9, %xmm13
	vsubss	%xmm9, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm9, %xmm9
	vsubss	%xmm14, %xmm5, %xmm5
	vaddss	%xmm5, %xmm9, %xmm5
	vmovss	152(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vaddss	%xmm6, %xmm8, %xmm9
	vsubss	%xmm6, %xmm9, %xmm14
	vsubss	%xmm14, %xmm9, %xmm15
	vsubss	%xmm15, %xmm6, %xmm15
	vsubss	%xmm14, %xmm8, %xmm8
	vaddss	%xmm8, %xmm15, %xmm8
	vaddss	%xmm9, %xmm13, %xmm14
	vsubss	%xmm13, %xmm14, %xmm15
	vsubss	%xmm15, %xmm14, %xmm6
	vsubss	%xmm6, %xmm13, %xmm6
	vxorps	%xmm13, %xmm13, %xmm13
	vsubss	%xmm15, %xmm9, %xmm9
	vaddss	%xmm6, %xmm9, %xmm6
	vaddss	%xmm3, %xmm11, %xmm3
	vaddss	256(%rsp), %xmm3, %xmm3         # 4-byte Folded Reload
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vaddss	%xmm6, %xmm3, %xmm3
	vfmadd231ss	%xmm1, %xmm13, %xmm3    # xmm3 = (xmm13 * xmm1) + xmm3
	vfmadd231ss	%xmm4, %xmm13, %xmm3    # xmm3 = (xmm13 * xmm4) + xmm3
	vfmadd231ss	160(%rsp), %xmm13, %xmm3 # 4-byte Folded Reload
                                        # xmm3 = (xmm13 * mem) + xmm3
	vfmadd231ss	8(%rsp), %xmm12, %xmm3  # 4-byte Folded Reload
                                        # xmm3 = (xmm12 * mem) + xmm3
	vaddss	%xmm3, %xmm14, %xmm4
	vsubss	%xmm4, %xmm14, %xmm5
	vaddss	%xmm3, %xmm5, %xmm5
	vaddss	%xmm4, %xmm10, %xmm3
	vsubss	%xmm3, %xmm10, %xmm6
	vaddss	%xmm4, %xmm6, %xmm4
	vaddss	%xmm3, %xmm7, %xmm6
	vsubss	%xmm6, %xmm7, %xmm7
	vaddss	%xmm3, %xmm7, %xmm7
	vbroadcastss	.LCPI42_3(%rip), %xmm10 # xmm10 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm6, %xmm10, %xmm8
	vsubss	%xmm6, %xmm2, %xmm3
	vsubss	%xmm2, %xmm3, %xmm6
	vsubss	%xmm6, %xmm3, %xmm9
	vsubss	%xmm9, %xmm2, %xmm2
	vxorps	%xmm7, %xmm10, %xmm9
	vmovaps	%xmm10, %xmm11
	vsubss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm6, %xmm2, %xmm2
	vmovss	20(%rsp), %xmm8                 # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm7
	vmovaps	%xmm8, %xmm10
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm10, %xmm8
	vsubss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm7, %xmm8, %xmm7
	vmovss	12(%rsp), %xmm10                # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm10, %xmm8
	vsubss	%xmm10, %xmm8, %xmm9
	vmovaps	%xmm10, %xmm14
	vsubss	%xmm9, %xmm8, %xmm10
	vsubss	%xmm10, %xmm14, %xmm10
	vxorps	%xmm4, %xmm11, %xmm4
	vsubss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm4, %xmm10, %xmm4
	vaddss	(%rsp), %xmm4, %xmm4            # 4-byte Folded Reload
	vsubss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm6, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm9
	vsubss	%xmm9, %xmm5, %xmm10
	vsubss	%xmm10, %xmm2, %xmm2
	vsubss	%xmm9, %xmm6, %xmm6
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm7, %xmm2, %xmm6
	vsubss	%xmm2, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm10
	vsubss	%xmm10, %xmm2, %xmm2
	vsubss	%xmm9, %xmm7, %xmm7
	vaddss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm6, %xmm8, %xmm7
	vsubss	%xmm6, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm10
	vsubss	%xmm10, %xmm6, %xmm6
	vsubss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm7, %xmm4
	vsubss	%xmm4, %xmm7, %xmm6
	vaddss	%xmm2, %xmm6, %xmm2
	vaddss	%xmm4, %xmm5, %xmm2
	vsubss	%xmm2, %xmm5, %xmm5
	vaddss	%xmm4, %xmm5, %xmm4
	vaddss	%xmm2, %xmm3, %xmm4
	vsubss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vdivss	%xmm1, %xmm4, %xmm1
	vmovss	%xmm0, 112(%rsp)
	vmovss	32(%rsp), %xmm2                 # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 116(%rsp)
	vmovss	16(%rsp), %xmm2                 # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 120(%rsp)
	vmovss	%xmm12, 124(%rsp)
	vmovss	%xmm1, 128(%rsp)
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	jmp	.LBB42_307
	.p2align	4, 0x90
.LBB42_309:                             #   in Loop: Header=BB42_307 Depth=2
	vmovaps	%xmm1, %xmm0
	leaq	1(%rcx), %rdx
	cmpq	$1, %rcx
	ja	.LBB42_311
.LBB42_310:                             #   in Loop: Header=BB42_307 Depth=2
	movq	%rdx, %rcx
	cmpl	$5, %eax
	jge	.LBB42_311
.LBB42_307:                             #   Parent Loop BB42_302 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	116(%rsp,%rcx,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vucomiss	%xmm13, %xmm0
	setnp	%dl
	sete	%sil
	testb	%dl, %sil
	jne	.LBB42_309
# %bb.308:                              #   in Loop: Header=BB42_307 Depth=2
	movslq	%eax, %rdx
	incl	%eax
	vmovss	%xmm1, 112(%rsp,%rdx,4)
	leaq	1(%rcx), %rdx
	cmpq	$1, %rcx
	jbe	.LBB42_310
.LBB42_311:                             #   in Loop: Header=BB42_302 Depth=1
	cmpl	$4, %eax
	jg	.LBB42_300
# %bb.312:                              #   in Loop: Header=BB42_302 Depth=1
	vmovss	116(%rsp,%rdx,4), %xmm1         # xmm1 = mem[0],zero,zero,zero
	movslq	%eax, %rcx
	vaddss	%xmm1, %xmm0, %xmm2
	vmovss	%xmm2, 112(%rsp,%rcx,4)
	vsubss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 116(%rsp,%rcx,4)
	cmpl	$2, %eax
	jg	.LBB42_300
# %bb.313:                              #   in Loop: Header=BB42_302 Depth=1
	leaq	(%rsp,%rcx,4), %rdi
	addq	$120, %rdi
	movl	$2, %ecx
	subl	%eax, %ecx
	leaq	4(,%rcx,4), %rdx
	xorl	%esi, %esi
	callq	_intel_fast_memset@PLT
	vxorps	%xmm13, %xmm13, %xmm13
	jmp	.LBB42_300
.LBB42_314:
	movl	$90, %eax
	vxorps	%xmm11, %xmm11, %xmm11
	vcvtsi2ss	%eax, %xmm11, %xmm6
	vmulss	%xmm3, %xmm6, %xmm2
	vmovss	%xmm2, 48(%rsp)                 # 4-byte Spill
	vmovaps	%xmm3, %xmm7
	vfmsub213ss	%xmm2, %xmm6, %xmm7     # xmm7 = (xmm6 * xmm7) - xmm2
	vmulss	%xmm4, %xmm6, %xmm11
	vmovaps	%xmm4, %xmm9
	vfmsub213ss	%xmm11, %xmm6, %xmm9    # xmm9 = (xmm6 * xmm9) - xmm11
	vxorps	%xmm2, %xmm2, %xmm2
	vmulss	%xmm3, %xmm2, %xmm8
	vmovaps	%xmm3, %xmm10
	vfmsub213ss	%xmm8, %xmm2, %xmm10    # xmm10 = (xmm2 * xmm10) - xmm8
	vaddss	%xmm7, %xmm11, %xmm12
	vsubss	%xmm7, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm7, %xmm7
	vsubss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm7, %xmm11, %xmm11
	vaddss	%xmm8, %xmm12, %xmm7
	vsubss	%xmm12, %xmm7, %xmm13
	vsubss	%xmm13, %xmm7, %xmm14
	vsubss	%xmm14, %xmm12, %xmm12
	vsubss	%xmm13, %xmm8, %xmm13
	vaddss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm11, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm11, %xmm11
	vmulss	%xmm1, %xmm6, %xmm15
	vsubss	%xmm14, %xmm12, %xmm12
	vmovaps	%xmm1, %xmm14
	vfmsub213ss	%xmm15, %xmm6, %xmm14   # xmm14 = (xmm6 * xmm14) - xmm15
	vaddss	%xmm12, %xmm11, %xmm11
	vmulss	%xmm4, %xmm2, %xmm12
	vaddss	%xmm14, %xmm11, %xmm11
	vmovaps	%xmm4, %xmm14
	vfmsub213ss	%xmm12, %xmm2, %xmm14   # xmm14 = (xmm2 * xmm14) - xmm12
	vaddss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm10, %xmm9, %xmm14
	vsubss	%xmm9, %xmm14, %xmm2
	vsubss	%xmm2, %xmm14, %xmm5
	vsubss	%xmm5, %xmm9, %xmm5
	vaddss	%xmm10, %xmm11, %xmm9
	vsubss	%xmm2, %xmm10, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vaddss	%xmm8, %xmm12, %xmm5
	vsubss	%xmm12, %xmm5, %xmm10
	vsubss	%xmm10, %xmm5, %xmm11
	vsubss	%xmm11, %xmm12, %xmm11
	vsubss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm8, %xmm11, %xmm8
	vaddss	%xmm15, %xmm14, %xmm10
	vsubss	%xmm14, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm12
	vsubss	%xmm12, %xmm14, %xmm12
	vsubss	%xmm11, %xmm15, %xmm11
	vaddss	%xmm11, %xmm12, %xmm11
	vaddss	%xmm5, %xmm13, %xmm12
	vsubss	%xmm5, %xmm12, %xmm14
	vsubss	%xmm14, %xmm12, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm5, %xmm13, %xmm5
	vaddss	%xmm12, %xmm10, %xmm13
	vsubss	%xmm10, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm10, %xmm10
	vsubss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm2, %xmm9, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm2, %xmm11, %xmm2
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm10, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vfmadd231ss	%xmm3, %xmm5, %xmm2     # xmm2 = (xmm5 * xmm3) + xmm2
	vfmadd231ss	%xmm4, %xmm5, %xmm2     # xmm2 = (xmm5 * xmm4) + xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	vfmadd231ss	%xmm1, %xmm3, %xmm2     # xmm2 = (xmm3 * xmm1) + xmm2
	vfmadd213ss	%xmm2, %xmm6, %xmm0     # xmm0 = (xmm6 * xmm0) + xmm2
	vaddss	%xmm0, %xmm13, %xmm1
	vsubss	%xmm1, %xmm13, %xmm2
	vaddss	%xmm0, %xmm2, %xmm3
	vaddss	%xmm1, %xmm7, %xmm5
	vsubss	%xmm5, %xmm7, %xmm2
	vaddss	%xmm1, %xmm2, %xmm4
	vxorps	%xmm2, %xmm2, %xmm2
	vmovss	48(%rsp), %xmm1                 # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm5, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm1
	vucomiss	%xmm2, %xmm0
	vaddss	%xmm5, %xmm1, %xmm1
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB42_316
# %bb.315:
	vxorps	%xmm1, %xmm1, %xmm1
	vblendps	$1, %xmm0, %xmm1, %xmm0         # xmm0 = xmm0[0],xmm1[1,2,3]
	jmp	.LBB42_327
.LBB42_316:
	vcomiss	%xmm0, %xmm2
	jbe	.LBB42_318
# %bb.317:
	vbroadcastss	.LCPI42_0(%rip), %xmm0  # xmm0 = [NaN,NaN,NaN,NaN]
	vmovaps	%xmm0, %xmm1
	jmp	.LBB42_327
.LBB42_318:
	vmovss	%xmm1, 8(%rsp)                  # 4-byte Spill
	vmovss	%xmm4, 48(%rsp)                 # 4-byte Spill
	vmovss	%xmm3, 160(%rsp)                # 4-byte Spill
	vmovups	%xmm0, 224(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vxorps	%xmm2, %xmm2, %xmm2
	vucomiss	%xmm2, %xmm0
	setnp	%cl
	sete	%dl
	vmovd	.LCPI42_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	vmovdqa	%xmm1, %xmm10
	testb	%cl, %dl
	jne	.LBB42_322
# %bb.319:
	movl	%eax, %ecx
	andl	$2139095040, %ecx               # imm = 0x7F800000
	cmpl	$2139095040, %ecx               # imm = 0x7F800000
	je	.LBB42_321
# %bb.320:
	vmovd	%ecx, %xmm10
	jmp	.LBB42_322
.LBB42_321:
	vmovd	.LCPI42_1(%rip), %xmm10         # xmm10 = mem[0],zero,zero,zero
.LBB42_322:
	vucomiss	%xmm2, %xmm0
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	vmovups	224(%rsp), %xmm2                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	jne	.LBB42_326
# %bb.323:
	andl	$2139095040, %eax               # imm = 0x7F800000
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB42_325
# %bb.324:
	movl	$2130706432, %ecx               # imm = 0x7F000000
	subl	%eax, %ecx
	vmovd	%ecx, %xmm1
	jmp	.LBB42_326
.LBB42_325:
	vmovd	.LCPI42_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
.LBB42_326:
	vmovss	.LCPI42_2(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm1, %xmm3
	vmulss	%xmm1, %xmm2, %xmm2
	vmulss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 256(%rsp)                # 4-byte Spill
	vmulss	8(%rsp), %xmm1, %xmm2           # 4-byte Folded Reload
	vmulss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 224(%rsp)                # 4-byte Spill
	vmulss	48(%rsp), %xmm1, %xmm2          # 4-byte Folded Reload
	vmulss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 48(%rsp)                 # 4-byte Spill
	vmulss	160(%rsp), %xmm1, %xmm1         # 4-byte Folded Reload
	movl	$3, %eax
	vcvtsi2ss	%eax, %xmm6, %xmm2
	vmulss	%xmm3, %xmm1, %xmm1
	vmovss	%xmm1, 160(%rsp)                # 4-byte Spill
	vmulss	%xmm4, %xmm2, %xmm1
	vmovaps	%xmm4, %xmm3
	vfmsub213ss	%xmm1, %xmm2, %xmm3     # xmm3 = (xmm2 * xmm3) - xmm1
	vxorps	%xmm14, %xmm14, %xmm14
	vmulss	%xmm2, %xmm14, %xmm5
	vxorps	%xmm6, %xmm6, %xmm6
	vfmsub213ss	%xmm5, %xmm2, %xmm6     # xmm6 = (xmm2 * xmm6) - xmm5
	vaddss	%xmm5, %xmm3, %xmm7
	vsubss	%xmm3, %xmm7, %xmm8
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm3, %xmm3
	vsubss	%xmm8, %xmm5, %xmm8
	vaddss	%xmm3, %xmm8, %xmm8
	vaddss	%xmm7, %xmm14, %xmm3
	vsubss	%xmm7, %xmm3, %xmm9
	vsubss	%xmm9, %xmm3, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vsubss	%xmm9, %xmm14, %xmm9
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm8, %xmm9, %xmm11
	vsubss	%xmm11, %xmm9, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vsubss	%xmm11, %xmm7, %xmm7
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm6, %xmm7, %xmm7
	vaddss	%xmm6, %xmm14, %xmm8
	vsubss	%xmm6, %xmm8, %xmm11
	vsubss	%xmm11, %xmm8, %xmm12
	vsubss	%xmm12, %xmm6, %xmm6
	vsubss	%xmm11, %xmm14, %xmm11
	vaddss	%xmm6, %xmm11, %xmm6
	vaddss	%xmm5, %xmm8, %xmm11
	vsubss	%xmm8, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm8, %xmm8
	vsubss	%xmm12, %xmm5, %xmm5
	vaddss	%xmm5, %xmm8, %xmm5
	vaddss	%xmm9, %xmm14, %xmm8
	vsubss	%xmm14, %xmm8, %xmm12
	vsubss	%xmm12, %xmm9, %xmm9
	vsubss	%xmm12, %xmm8, %xmm12
	vsubss	%xmm12, %xmm14, %xmm12
	vaddss	%xmm9, %xmm12, %xmm9
	vaddss	%xmm8, %xmm11, %xmm12
	vsubss	%xmm11, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm11, %xmm14
	vsubss	%xmm13, %xmm8, %xmm8
	vmovd	%xmm10, 152(%rsp)               # 4-byte Folded Spill
	vdivss	%xmm0, %xmm10, %xmm10
	vxorps	%xmm11, %xmm11, %xmm11
	vaddss	%xmm7, %xmm11, %xmm0
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm8, %xmm14, %xmm7
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vfmadd231ss	%xmm4, %xmm11, %xmm0    # xmm0 = (xmm11 * xmm4) + xmm0
	vfmadd231ss	%xmm11, %xmm11, %xmm0   # xmm0 = (xmm11 * xmm11) + xmm0
	vfmadd231ss	%xmm11, %xmm11, %xmm0   # xmm0 = (xmm11 * xmm11) + xmm0
	vfmadd231ss	%xmm2, %xmm11, %xmm0    # xmm0 = (xmm11 * xmm2) + xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vaddss	%xmm0, %xmm12, %xmm2
	vsubss	%xmm2, %xmm12, %xmm5
	vaddss	%xmm0, %xmm5, %xmm0
	vmovss	%xmm0, 8(%rsp)                  # 4-byte Spill
	vaddss	%xmm2, %xmm3, %xmm0
	vsubss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vmovss	%xmm2, 4(%rsp)                  # 4-byte Spill
	vaddss	%xmm0, %xmm1, %xmm2
	vmovss	%xmm2, 72(%rsp)                 # 4-byte Spill
	vsubss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, 208(%rsp)                # 4-byte Spill
	vmovaps	%xmm10, %xmm0
	vmulss	%xmm10, %xmm10, %xmm4
	vmovaps	%xmm10, %xmm1
	vfmsub213ss	%xmm4, %xmm10, %xmm1    # xmm1 = (xmm10 * xmm1) - xmm4
	vmulss	%xmm6, %xmm10, %xmm2
	vmovaps	%xmm10, %xmm7
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm9
	vsubss	%xmm9, %xmm3, %xmm12
	vsubss	%xmm12, %xmm1, %xmm1
	vmulss	%xmm6, %xmm10, %xmm12
	vsubss	%xmm9, %xmm2, %xmm9
	vaddss	%xmm1, %xmm9, %xmm9
	vaddss	%xmm3, %xmm12, %xmm1
	vsubss	%xmm3, %xmm1, %xmm13
	vsubss	%xmm13, %xmm1, %xmm14
	vsubss	%xmm14, %xmm3, %xmm3
	vsubss	%xmm13, %xmm12, %xmm13
	vaddss	%xmm3, %xmm13, %xmm3
	vxorps	%xmm13, %xmm13, %xmm13
	vfmsub213ss	%xmm2, %xmm10, %xmm13   # xmm13 = (xmm10 * xmm13) - xmm2
	vmovaps	%xmm10, %xmm14
	vfmsub213ss	%xmm12, %xmm6, %xmm14   # xmm14 = (xmm6 * xmm14) - xmm12
	vaddss	%xmm3, %xmm9, %xmm15
	vsubss	%xmm9, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm5
	vsubss	%xmm5, %xmm9, %xmm5
	vsubss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm14, %xmm13, %xmm3
	vsubss	%xmm13, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm9
	vsubss	%xmm9, %xmm13, %xmm9
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm0, %xmm14, %xmm0
	vsubss	%xmm5, %xmm14, %xmm5
	vaddss	%xmm5, %xmm9, %xmm5
	vaddss	%xmm6, %xmm12, %xmm9
	vsubss	%xmm6, %xmm9, %xmm13
	vsubss	%xmm13, %xmm12, %xmm12
	vsubss	%xmm13, %xmm9, %xmm13
	vsubss	%xmm13, %xmm6, %xmm13
	vaddss	%xmm12, %xmm13, %xmm12
	vaddss	%xmm2, %xmm3, %xmm13
	vsubss	%xmm3, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm6
	vsubss	%xmm6, %xmm3, %xmm3
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm15, %xmm9, %xmm3
	vsubss	%xmm9, %xmm3, %xmm6
	vsubss	%xmm6, %xmm3, %xmm14
	vsubss	%xmm14, %xmm9, %xmm9
	vsubss	%xmm6, %xmm15, %xmm6
	vaddss	%xmm6, %xmm9, %xmm6
	vaddss	%xmm3, %xmm13, %xmm9
	vsubss	%xmm13, %xmm9, %xmm14
	vsubss	%xmm14, %xmm9, %xmm15
	vsubss	%xmm15, %xmm13, %xmm13
	vsubss	%xmm14, %xmm3, %xmm3
	vaddss	%xmm3, %xmm13, %xmm3
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vmovss	%xmm10, 40(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm10, %xmm2, %xmm0    # xmm0 = (xmm2 * xmm10) + xmm0
	vfmadd231ss	%xmm2, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm2) + xmm0
	vfmadd231ss	%xmm2, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm2) + xmm0
	vfmadd231ss	%xmm2, %xmm10, %xmm0    # xmm0 = (xmm10 * xmm2) + xmm0
	vaddss	%xmm0, %xmm9, %xmm2
	vsubss	%xmm2, %xmm9, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm0, 32(%rsp)                 # 4-byte Spill
	vaddss	%xmm2, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm10
	vaddss	%xmm0, %xmm4, %xmm8
	vsubss	%xmm8, %xmm4, %xmm2
	vaddss	%xmm0, %xmm2, %xmm9
	vmovss	256(%rsp), %xmm13               # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vmulss	%xmm8, %xmm13, %xmm1
	vmovss	%xmm1, 16(%rsp)                 # 4-byte Spill
	vmovaps	%xmm8, %xmm2
	vfmsub213ss	%xmm1, %xmm13, %xmm2    # xmm2 = (xmm13 * xmm2) - xmm1
	vmulss	%xmm9, %xmm13, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm9, %xmm6
	vfmsub213ss	%xmm3, %xmm13, %xmm6    # xmm6 = (xmm13 * xmm6) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	224(%rsp), %xmm11               # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vmulss	%xmm8, %xmm11, %xmm5
	vaddss	%xmm5, %xmm4, %xmm3
	vsubss	%xmm4, %xmm3, %xmm14
	vsubss	%xmm14, %xmm3, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vmovaps	%xmm8, %xmm15
	vfmsub213ss	%xmm5, %xmm11, %xmm15   # xmm15 = (xmm11 * xmm15) - xmm5
	vsubss	%xmm14, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm14
	vsubss	%xmm14, %xmm5, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vsubss	%xmm14, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vmovaps	%xmm10, %xmm7
	vmovss	%xmm10, (%rsp)                  # 4-byte Spill
	vmulss	%xmm10, %xmm13, %xmm4
	vfmsub213ss	%xmm4, %xmm13, %xmm7    # xmm7 = (xmm13 * xmm7) - xmm4
	vaddss	%xmm7, %xmm2, %xmm2
	vmulss	%xmm9, %xmm11, %xmm7
	vmovaps	%xmm9, %xmm14
	vfmsub213ss	%xmm7, %xmm11, %xmm14   # xmm14 = (xmm11 * xmm14) - xmm7
	vaddss	%xmm2, %xmm14, %xmm2
	vmovss	48(%rsp), %xmm10                # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vmulss	%xmm8, %xmm10, %xmm14
	vmovaps	%xmm8, %xmm1
	vfmsub213ss	%xmm14, %xmm10, %xmm1   # xmm1 = (xmm10 * xmm1) - xmm14
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm6, %xmm15, %xmm2
	vsubss	%xmm6, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm12
	vsubss	%xmm12, %xmm6, %xmm6
	vsubss	%xmm0, %xmm15, %xmm0
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm7, %xmm14, %xmm6
	vsubss	%xmm7, %xmm6, %xmm12
	vsubss	%xmm12, %xmm6, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm12, %xmm14, %xmm12
	vaddss	%xmm7, %xmm12, %xmm7
	vaddss	%xmm4, %xmm2, %xmm12
	vsubss	%xmm2, %xmm12, %xmm14
	vsubss	%xmm14, %xmm12, %xmm15
	vsubss	%xmm15, %xmm2, %xmm2
	vsubss	%xmm14, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm5, %xmm6, %xmm4
	vsubss	%xmm6, %xmm4, %xmm14
	vsubss	%xmm14, %xmm4, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm14, %xmm5, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vaddss	%xmm4, %xmm12, %xmm6
	vsubss	%xmm12, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm4, %xmm4
	vaddss	%xmm4, %xmm12, %xmm4
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vfmadd231ss	160(%rsp), %xmm8, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm8 * mem) + xmm0
	vfmadd231ss	%xmm9, %xmm10, %xmm0    # xmm0 = (xmm10 * xmm9) + xmm0
	vfmadd231ss	(%rsp), %xmm11, %xmm0   # 4-byte Folded Reload
                                        # xmm0 = (xmm11 * mem) + xmm0
	vfmadd231ss	32(%rsp), %xmm13, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm13 * mem) + xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	16(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm4
	vbroadcastss	.LCPI42_3(%rip), %xmm6  # xmm6 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm6, %xmm3, %xmm5
	vmovss	72(%rsp), %xmm11                # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm11, %xmm2
	vsubss	%xmm11, %xmm2, %xmm3
	vsubss	%xmm3, %xmm5, %xmm5
	vsubss	%xmm3, %xmm2, %xmm3
	vsubss	%xmm3, %xmm11, %xmm3
	vaddss	%xmm5, %xmm3, %xmm3
	vmovaps	%xmm6, %xmm8
	vxorps	%xmm6, %xmm4, %xmm5
	vmovss	208(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm7, %xmm4
	vsubss	%xmm7, %xmm4, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm6, %xmm4, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm5, %xmm6, %xmm5
	vxorps	%xmm1, %xmm8, %xmm6
	vmovss	4(%rsp), %xmm8                  # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm8, %xmm1
	vsubss	%xmm8, %xmm1, %xmm7
	vsubss	%xmm7, %xmm6, %xmm6
	vsubss	%xmm7, %xmm1, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	8(%rsp), %xmm6, %xmm6           # 4-byte Folded Reload
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm4, %xmm3, %xmm6
	vsubss	%xmm3, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm5, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm0, 32(%rsp)                 # 4-byte Spill
	vaddss	%xmm1, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm3
	vaddss	%xmm1, %xmm3, %xmm3
	vaddss	%xmm4, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm12
	vmovss	40(%rsp), %xmm10                # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm10, %xmm13
	vmovaps	%xmm10, %xmm2
	vfmsub213ss	%xmm13, %xmm1, %xmm2    # xmm2 = (xmm1 * xmm2) - xmm13
	vxorps	%xmm0, %xmm0, %xmm0
	vmulss	%xmm0, %xmm1, %xmm14
	vaddss	%xmm2, %xmm14, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vsubss	%xmm5, %xmm14, %xmm5
	vaddss	%xmm5, %xmm2, %xmm5
	vmulss	%xmm10, %xmm12, %xmm6
	vaddss	%xmm6, %xmm4, %xmm2
	vsubss	%xmm4, %xmm2, %xmm7
	vsubss	%xmm7, %xmm2, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vmovaps	%xmm10, %xmm8
	vfmsub213ss	%xmm6, %xmm12, %xmm8    # xmm8 = (xmm12 * xmm8) - xmm6
	vsubss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm5, %xmm4
	vxorps	%xmm5, %xmm5, %xmm5
	vfmsub213ss	%xmm14, %xmm1, %xmm5    # xmm5 = (xmm1 * xmm5) - xmm14
	vmulss	%xmm0, %xmm12, %xmm7
	vxorps	%xmm15, %xmm15, %xmm15
	vfmsub213ss	%xmm7, %xmm12, %xmm15   # xmm15 = (xmm12 * xmm15) - xmm7
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm4, %xmm15, %xmm4
	vmulss	%xmm3, %xmm10, %xmm15
	vmovaps	%xmm10, %xmm11
	vfmsub213ss	%xmm15, %xmm3, %xmm11   # xmm11 = (xmm3 * xmm11) - xmm15
	vaddss	%xmm4, %xmm11, %xmm4
	vaddss	%xmm5, %xmm8, %xmm11
	vsubss	%xmm5, %xmm11, %xmm0
	vsubss	%xmm0, %xmm11, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vsubss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm7, %xmm15, %xmm5
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm5, %xmm9
	vsubss	%xmm9, %xmm7, %xmm7
	vsubss	%xmm8, %xmm15, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm14, %xmm11, %xmm8
	vsubss	%xmm11, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm15
	vsubss	%xmm15, %xmm11, %xmm11
	vsubss	%xmm9, %xmm14, %xmm9
	vaddss	%xmm9, %xmm11, %xmm9
	vaddss	%xmm6, %xmm5, %xmm11
	vsubss	%xmm5, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm14, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm11, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm11, %xmm8, %xmm8
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vfmadd231ss	32(%rsp), %xmm10, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm10 * mem) + xmm0
	vxorps	%xmm4, %xmm4, %xmm4
	vfmadd231ss	%xmm3, %xmm4, %xmm0     # xmm0 = (xmm4 * xmm3) + xmm0
	vfmadd231ss	%xmm12, %xmm4, %xmm0    # xmm0 = (xmm4 * xmm12) + xmm0
	vxorps	%xmm3, %xmm3, %xmm3
	vfmadd231ss	%xmm3, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm3) + xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm3
	vaddss	%xmm0, %xmm3, %xmm11
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm8
	vaddss	%xmm0, %xmm13, %xmm12
	vsubss	%xmm12, %xmm13, %xmm1
	vaddss	%xmm0, %xmm1, %xmm13
	vmulss	%xmm12, %xmm12, %xmm10
	vmovaps	%xmm12, %xmm0
	vfmsub213ss	%xmm10, %xmm12, %xmm0   # xmm0 = (xmm12 * xmm0) - xmm10
	vmulss	%xmm13, %xmm12, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm13, %xmm4
	vfmsub213ss	%xmm1, %xmm12, %xmm4    # xmm4 = (xmm12 * xmm4) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm12, %xmm13, %xmm3
	vaddss	%xmm3, %xmm2, %xmm0
	vsubss	%xmm2, %xmm0, %xmm5
	vsubss	%xmm5, %xmm0, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm12, %xmm6
	vfmsub213ss	%xmm3, %xmm13, %xmm6    # xmm6 = (xmm13 * xmm6) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vmovaps	%xmm8, %xmm7
	vmulss	%xmm8, %xmm12, %xmm2
	vmovaps	%xmm8, %xmm5
	vmovaps	%xmm8, %xmm14
	vfmsub213ss	%xmm2, %xmm12, %xmm5    # xmm5 = (xmm12 * xmm5) - xmm2
	vaddss	%xmm5, %xmm1, %xmm1
	vmulss	%xmm13, %xmm13, %xmm5
	vmovaps	%xmm13, %xmm7
	vfmsub213ss	%xmm5, %xmm13, %xmm7    # xmm7 = (xmm13 * xmm7) - xmm5
	vaddss	%xmm7, %xmm1, %xmm1
	vmulss	%xmm12, %xmm8, %xmm7
	vmovaps	%xmm12, %xmm8
	vfmsub213ss	%xmm7, %xmm14, %xmm8    # xmm8 = (xmm14 * xmm8) - xmm7
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm6, %xmm4, %xmm8
	vsubss	%xmm4, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm9, %xmm6, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm7, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm9, %xmm7, %xmm7
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm2, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm9, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm3, %xmm6, %xmm8
	vsubss	%xmm6, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm9, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm7, %xmm8, %xmm6
	vsubss	%xmm7, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vmovss	%xmm11, 32(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm12, %xmm11, %xmm1   # xmm1 = (xmm11 * xmm12) + xmm1
	vmovss	%xmm13, 16(%rsp)                # 4-byte Spill
	vmovss	%xmm14, 40(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm13, %xmm14, %xmm1   # xmm1 = (xmm14 * xmm13) + xmm1
	vfmadd231ss	%xmm14, %xmm13, %xmm1   # xmm1 = (xmm13 * xmm14) + xmm1
	vfmadd231ss	%xmm11, %xmm12, %xmm1   # xmm1 = (xmm12 * xmm11) + xmm1
	vaddss	%xmm1, %xmm6, %xmm2
	vsubss	%xmm2, %xmm6, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm1, (%rsp)                   # 4-byte Spill
	vaddss	%xmm2, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm13
	vaddss	%xmm1, %xmm10, %xmm2
	vsubss	%xmm2, %xmm10, %xmm3
	vaddss	%xmm1, %xmm3, %xmm3
	vmovss	256(%rsp), %xmm14               # 4-byte Reload
                                        # xmm14 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm14, %xmm4
	vmovss	%xmm4, 12(%rsp)                 # 4-byte Spill
	vmovaps	%xmm2, %xmm1
	vfmsub213ss	%xmm4, %xmm14, %xmm1    # xmm1 = (xmm14 * xmm1) - xmm4
	vmulss	%xmm3, %xmm14, %xmm4
	vaddss	%xmm4, %xmm1, %xmm5
	vsubss	%xmm1, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vmovaps	%xmm3, %xmm7
	vfmsub213ss	%xmm4, %xmm14, %xmm7    # xmm7 = (xmm14 * xmm7) - xmm4
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm1, %xmm4
	vmovss	224(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm0, %xmm6
	vaddss	%xmm6, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm8
	vsubss	%xmm8, %xmm1, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vmovaps	%xmm2, %xmm9
	vfmsub213ss	%xmm6, %xmm0, %xmm9     # xmm9 = (xmm0 * xmm9) - xmm6
	vsubss	%xmm8, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm4, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vmovaps	%xmm13, %xmm8
	vmovss	%xmm13, 20(%rsp)                # 4-byte Spill
	vmulss	%xmm13, %xmm14, %xmm5
	vfmsub213ss	%xmm5, %xmm14, %xmm8    # xmm8 = (xmm14 * xmm8) - xmm5
	vaddss	%xmm4, %xmm8, %xmm4
	vmovaps	%xmm0, %xmm13
	vmulss	%xmm3, %xmm0, %xmm8
	vmovaps	%xmm3, %xmm10
	vfmsub213ss	%xmm8, %xmm0, %xmm10    # xmm10 = (xmm0 * xmm10) - xmm8
	vaddss	%xmm4, %xmm10, %xmm4
	vmovss	48(%rsp), %xmm11                # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm11, %xmm10
	vmovaps	%xmm2, %xmm0
	vfmsub213ss	%xmm10, %xmm11, %xmm0   # xmm0 = (xmm11 * xmm0) - xmm10
	vaddss	%xmm0, %xmm4, %xmm0
	vmovss	%xmm0, 24(%rsp)                 # 4-byte Spill
	vaddss	%xmm7, %xmm9, %xmm4
	vsubss	%xmm7, %xmm4, %xmm0
	vsubss	%xmm0, %xmm4, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm10, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm9, %xmm10, %xmm9
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm5, %xmm4, %xmm9
	vsubss	%xmm4, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm6, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm10
	vsubss	%xmm10, %xmm5, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm5, %xmm9, %xmm7
	vsubss	%xmm9, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm15
	vsubss	%xmm15, %xmm9, %xmm9
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm9, %xmm5
	vaddss	24(%rsp), %xmm0, %xmm0          # 4-byte Folded Reload
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vfmadd231ss	160(%rsp), %xmm2, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm2 * mem) + xmm0
	vfmadd231ss	%xmm3, %xmm11, %xmm0    # xmm0 = (xmm11 * xmm3) + xmm0
	vfmadd231ss	20(%rsp), %xmm13, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm13 * mem) + xmm0
	vfmadd231ss	(%rsp), %xmm14, %xmm0   # 4-byte Folded Reload
                                        # xmm0 = (xmm14 * mem) + xmm0
	vaddss	%xmm0, %xmm7, %xmm2
	vsubss	%xmm2, %xmm7, %xmm3
	vaddss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm2, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vmovss	12(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm4
	vaddss	%xmm0, %xmm4, %xmm4
	vbroadcastss	.LCPI42_3(%rip), %xmm8  # xmm8 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm8, %xmm5
	vmovss	72(%rsp), %xmm6                 # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm6, %xmm0
	vsubss	%xmm6, %xmm0, %xmm2
	vsubss	%xmm2, %xmm5, %xmm5
	vsubss	%xmm2, %xmm0, %xmm2
	vsubss	%xmm2, %xmm6, %xmm2
	vaddss	%xmm5, %xmm2, %xmm2
	vxorps	%xmm4, %xmm8, %xmm5
	vmovss	208(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm7, %xmm4
	vsubss	%xmm7, %xmm4, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm6, %xmm4, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm5, %xmm6, %xmm5
	vxorps	%xmm1, %xmm8, %xmm6
	vmovss	4(%rsp), %xmm8                  # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm8, %xmm1
	vsubss	%xmm8, %xmm1, %xmm7
	vsubss	%xmm7, %xmm6, %xmm6
	vsubss	%xmm7, %xmm1, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	8(%rsp), %xmm6, %xmm6           # 4-byte Folded Reload
	vsubss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm4, %xmm2, %xmm6
	vsubss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm5, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm2
	vsubss	%xmm2, %xmm5, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm1, 12(%rsp)                 # 4-byte Spill
	vaddss	%xmm2, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm1
	vaddss	%xmm2, %xmm1, %xmm3
	vaddss	%xmm4, %xmm0, %xmm15
	vsubss	%xmm15, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm1
	vmulss	%xmm12, %xmm15, %xmm0
	vmovss	%xmm0, (%rsp)                   # 4-byte Spill
	vmovaps	%xmm12, %xmm2
	vfmsub213ss	%xmm0, %xmm15, %xmm2    # xmm2 = (xmm15 * xmm2) - xmm0
	vmovss	16(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm15, %xmm4
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vmovaps	%xmm0, %xmm7
	vmovaps	%xmm0, %xmm14
	vfmsub213ss	%xmm4, %xmm15, %xmm7    # xmm7 = (xmm15 * xmm7) - xmm4
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm4
	vmulss	%xmm1, %xmm12, %xmm6
	vaddss	%xmm6, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm8
	vsubss	%xmm8, %xmm2, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vmovaps	%xmm12, %xmm9
	vfmsub213ss	%xmm6, %xmm1, %xmm9     # xmm9 = (xmm1 * xmm9) - xmm6
	vsubss	%xmm8, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm4, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vmovss	40(%rsp), %xmm8                 # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vmulss	%xmm8, %xmm15, %xmm5
	vfmsub213ss	%xmm5, %xmm15, %xmm8    # xmm8 = (xmm15 * xmm8) - xmm5
	vaddss	%xmm4, %xmm8, %xmm4
	vmulss	%xmm0, %xmm1, %xmm8
	vmovaps	%xmm0, %xmm10
	vmovaps	%xmm0, %xmm13
	vfmsub213ss	%xmm8, %xmm1, %xmm10    # xmm10 = (xmm1 * xmm10) - xmm8
	vaddss	%xmm4, %xmm10, %xmm4
	vmulss	%xmm3, %xmm12, %xmm10
	vmovaps	%xmm12, %xmm11
	vfmsub213ss	%xmm10, %xmm3, %xmm11   # xmm11 = (xmm3 * xmm11) - xmm10
	vaddss	%xmm4, %xmm11, %xmm4
	vaddss	%xmm7, %xmm9, %xmm11
	vsubss	%xmm7, %xmm11, %xmm0
	vsubss	%xmm0, %xmm11, %xmm14
	vsubss	%xmm14, %xmm7, %xmm7
	vsubss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm10, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm14
	vsubss	%xmm14, %xmm8, %xmm8
	vsubss	%xmm9, %xmm10, %xmm9
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm5, %xmm11, %xmm9
	vsubss	%xmm11, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm14
	vsubss	%xmm14, %xmm11, %xmm11
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm6, %xmm7, %xmm10
	vsubss	%xmm7, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm14
	vsubss	%xmm14, %xmm7, %xmm7
	vsubss	%xmm11, %xmm6, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm10, %xmm9, %xmm7
	vsubss	%xmm9, %xmm7, %xmm11
	vsubss	%xmm11, %xmm7, %xmm14
	vsubss	%xmm14, %xmm9, %xmm9
	vsubss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vfmadd231ss	12(%rsp), %xmm12, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm12 * mem) + xmm0
	vfmadd231ss	%xmm13, %xmm3, %xmm0    # xmm0 = (xmm3 * xmm13) + xmm0
	vfmadd231ss	40(%rsp), %xmm1, %xmm0  # 4-byte Folded Reload
                                        # xmm0 = (xmm1 * mem) + xmm0
	vfmadd231ss	32(%rsp), %xmm15, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm15 * mem) + xmm0
	vaddss	%xmm0, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm3
	vaddss	%xmm0, %xmm3, %xmm11
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm8
	vmovss	(%rsp), %xmm1                   # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm12
	vsubss	%xmm12, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm13
	vmulss	%xmm12, %xmm12, %xmm10
	vmovaps	%xmm12, %xmm0
	vfmsub213ss	%xmm10, %xmm12, %xmm0   # xmm0 = (xmm12 * xmm0) - xmm10
	vmulss	%xmm13, %xmm12, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm13, %xmm4
	vfmsub213ss	%xmm1, %xmm12, %xmm4    # xmm4 = (xmm12 * xmm4) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm12, %xmm13, %xmm3
	vaddss	%xmm3, %xmm2, %xmm0
	vsubss	%xmm2, %xmm0, %xmm5
	vsubss	%xmm5, %xmm0, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm12, %xmm6
	vfmsub213ss	%xmm3, %xmm13, %xmm6    # xmm6 = (xmm13 * xmm6) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vmovaps	%xmm8, %xmm7
	vmulss	%xmm8, %xmm12, %xmm2
	vmovaps	%xmm8, %xmm5
	vmovaps	%xmm8, %xmm14
	vfmsub213ss	%xmm2, %xmm12, %xmm5    # xmm5 = (xmm12 * xmm5) - xmm2
	vaddss	%xmm5, %xmm1, %xmm1
	vmulss	%xmm13, %xmm13, %xmm5
	vmovaps	%xmm13, %xmm7
	vfmsub213ss	%xmm5, %xmm13, %xmm7    # xmm7 = (xmm13 * xmm7) - xmm5
	vaddss	%xmm7, %xmm1, %xmm1
	vmulss	%xmm12, %xmm8, %xmm7
	vmovaps	%xmm12, %xmm8
	vfmsub213ss	%xmm7, %xmm14, %xmm8    # xmm8 = (xmm14 * xmm8) - xmm7
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm6, %xmm4, %xmm8
	vsubss	%xmm4, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm9, %xmm6, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm7, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm9, %xmm7, %xmm7
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm2, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm9, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm3, %xmm6, %xmm8
	vsubss	%xmm6, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm9, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm7, %xmm8, %xmm6
	vsubss	%xmm7, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vmovss	%xmm11, 32(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm12, %xmm11, %xmm1   # xmm1 = (xmm11 * xmm12) + xmm1
	vmovss	%xmm14, 40(%rsp)                # 4-byte Spill
	vmovss	%xmm13, 16(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm13, %xmm14, %xmm1   # xmm1 = (xmm14 * xmm13) + xmm1
	vfmadd231ss	%xmm14, %xmm13, %xmm1   # xmm1 = (xmm13 * xmm14) + xmm1
	vfmadd231ss	%xmm11, %xmm12, %xmm1   # xmm1 = (xmm12 * xmm11) + xmm1
	vaddss	%xmm1, %xmm6, %xmm2
	vsubss	%xmm2, %xmm6, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm1, (%rsp)                   # 4-byte Spill
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm13
	vaddss	%xmm3, %xmm10, %xmm1
	vsubss	%xmm1, %xmm10, %xmm2
	vaddss	%xmm3, %xmm2, %xmm3
	vmovss	256(%rsp), %xmm14               # 4-byte Reload
                                        # xmm14 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm14, %xmm4
	vmovss	%xmm4, 12(%rsp)                 # 4-byte Spill
	vmovaps	%xmm1, %xmm2
	vfmsub213ss	%xmm4, %xmm14, %xmm2    # xmm2 = (xmm14 * xmm2) - xmm4
	vmulss	%xmm3, %xmm14, %xmm4
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vmovaps	%xmm3, %xmm7
	vfmsub213ss	%xmm4, %xmm14, %xmm7    # xmm7 = (xmm14 * xmm7) - xmm4
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm4
	vmovss	224(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm0, %xmm6
	vaddss	%xmm6, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm8
	vsubss	%xmm8, %xmm2, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vmovaps	%xmm1, %xmm9
	vfmsub213ss	%xmm6, %xmm0, %xmm9     # xmm9 = (xmm0 * xmm9) - xmm6
	vsubss	%xmm8, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm4, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vmovaps	%xmm13, %xmm8
	vmovss	%xmm13, 20(%rsp)                # 4-byte Spill
	vmulss	%xmm13, %xmm14, %xmm5
	vfmsub213ss	%xmm5, %xmm14, %xmm8    # xmm8 = (xmm14 * xmm8) - xmm5
	vaddss	%xmm4, %xmm8, %xmm4
	vmovaps	%xmm0, %xmm13
	vmulss	%xmm3, %xmm0, %xmm8
	vmovaps	%xmm3, %xmm10
	vfmsub213ss	%xmm8, %xmm0, %xmm10    # xmm10 = (xmm0 * xmm10) - xmm8
	vaddss	%xmm4, %xmm10, %xmm4
	vmovss	48(%rsp), %xmm11                # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm11, %xmm10
	vmovaps	%xmm1, %xmm0
	vfmsub213ss	%xmm10, %xmm11, %xmm0   # xmm0 = (xmm11 * xmm0) - xmm10
	vaddss	%xmm0, %xmm4, %xmm0
	vmovss	%xmm0, 24(%rsp)                 # 4-byte Spill
	vaddss	%xmm7, %xmm9, %xmm4
	vsubss	%xmm7, %xmm4, %xmm0
	vsubss	%xmm0, %xmm4, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm10, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm9, %xmm10, %xmm9
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm5, %xmm4, %xmm9
	vsubss	%xmm4, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm6, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm10
	vsubss	%xmm10, %xmm5, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm5, %xmm9, %xmm7
	vsubss	%xmm9, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm15
	vsubss	%xmm15, %xmm9, %xmm9
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm9, %xmm5
	vaddss	24(%rsp), %xmm0, %xmm0          # 4-byte Folded Reload
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vfmadd231ss	160(%rsp), %xmm1, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm1 * mem) + xmm0
	vfmadd231ss	%xmm3, %xmm11, %xmm0    # xmm0 = (xmm11 * xmm3) + xmm0
	vfmadd231ss	20(%rsp), %xmm13, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm13 * mem) + xmm0
	vfmadd231ss	(%rsp), %xmm14, %xmm0   # 4-byte Folded Reload
                                        # xmm0 = (xmm14 * mem) + xmm0
	vaddss	%xmm0, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm3
	vaddss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmovss	12(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm4
	vaddss	%xmm0, %xmm4, %xmm4
	vmovss	72(%rsp), %xmm6                 # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm6, %xmm0
	vsubss	%xmm6, %xmm0, %xmm5
	vmovaps	%xmm6, %xmm7
	vsubss	%xmm5, %xmm0, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vbroadcastss	.LCPI42_3(%rip), %xmm8  # xmm8 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm8, %xmm2
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vmovss	208(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm6
	vmovaps	%xmm7, %xmm9
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm9, %xmm7
	vxorps	%xmm4, %xmm8, %xmm4
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vxorps	%xmm1, %xmm8, %xmm6
	vmovss	4(%rsp), %xmm8                  # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm8, %xmm1
	vsubss	%xmm8, %xmm1, %xmm7
	vmovaps	%xmm8, %xmm9
	vsubss	%xmm7, %xmm1, %xmm8
	vsubss	%xmm8, %xmm9, %xmm8
	vsubss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	8(%rsp), %xmm6, %xmm6           # 4-byte Folded Reload
	vsubss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm5, %xmm2, %xmm6
	vsubss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm1, %xmm5, %xmm4
	vsubss	%xmm5, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm5, %xmm5
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm1, 208(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm6, %xmm3
	vsubss	%xmm3, %xmm6, %xmm4
	vaddss	%xmm2, %xmm4, %xmm5
	vaddss	%xmm3, %xmm0, %xmm8
	vsubss	%xmm8, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm3
	vmulss	%xmm12, %xmm8, %xmm1
	vmovss	%xmm1, 4(%rsp)                  # 4-byte Spill
	vmovaps	%xmm12, %xmm0
	vfmsub213ss	%xmm1, %xmm8, %xmm0     # xmm0 = (xmm8 * xmm0) - xmm1
	vmovss	16(%rsp), %xmm1                 # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm8, %xmm2
	vaddss	%xmm2, %xmm0, %xmm4
	vsubss	%xmm0, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm9
	vsubss	%xmm9, %xmm0, %xmm0
	vmovaps	%xmm1, %xmm9
	vmovaps	%xmm1, %xmm7
	vfmsub213ss	%xmm2, %xmm8, %xmm9     # xmm9 = (xmm8 * xmm9) - xmm2
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vmulss	%xmm3, %xmm12, %xmm6
	vaddss	%xmm6, %xmm4, %xmm2
	vsubss	%xmm4, %xmm2, %xmm10
	vsubss	%xmm10, %xmm2, %xmm11
	vsubss	%xmm11, %xmm4, %xmm4
	vmovaps	%xmm12, %xmm11
	vfmsub213ss	%xmm6, %xmm3, %xmm11    # xmm11 = (xmm3 * xmm11) - xmm6
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm0, %xmm6
	vsubss	%xmm0, %xmm6, %xmm10
	vsubss	%xmm10, %xmm6, %xmm14
	vsubss	%xmm14, %xmm0, %xmm0
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm0, %xmm0
	vmovss	40(%rsp), %xmm10                # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vmulss	%xmm10, %xmm8, %xmm4
	vfmsub213ss	%xmm4, %xmm8, %xmm10    # xmm10 = (xmm8 * xmm10) - xmm4
	vaddss	%xmm0, %xmm10, %xmm0
	vmulss	%xmm1, %xmm3, %xmm10
	vmovaps	%xmm1, %xmm14
	vmovaps	%xmm1, %xmm13
	vfmsub213ss	%xmm10, %xmm3, %xmm14   # xmm14 = (xmm3 * xmm14) - xmm10
	vaddss	%xmm0, %xmm14, %xmm0
	vmulss	%xmm5, %xmm12, %xmm14
	vmovaps	%xmm12, %xmm15
	vfmsub213ss	%xmm14, %xmm5, %xmm15   # xmm15 = (xmm5 * xmm15) - xmm14
	vaddss	%xmm0, %xmm15, %xmm15
	vaddss	%xmm11, %xmm9, %xmm0
	vsubss	%xmm9, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm7
	vsubss	%xmm7, %xmm9, %xmm7
	vsubss	%xmm1, %xmm11, %xmm1
	vaddss	%xmm1, %xmm7, %xmm1
	vaddss	%xmm14, %xmm10, %xmm7
	vsubss	%xmm10, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm11
	vsubss	%xmm11, %xmm10, %xmm10
	vsubss	%xmm9, %xmm14, %xmm9
	vaddss	%xmm9, %xmm10, %xmm9
	vaddss	%xmm4, %xmm0, %xmm10
	vsubss	%xmm0, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm14
	vsubss	%xmm14, %xmm0, %xmm0
	vsubss	%xmm11, %xmm4, %xmm4
	vaddss	%xmm4, %xmm0, %xmm4
	vaddss	%xmm6, %xmm7, %xmm11
	vsubss	%xmm7, %xmm11, %xmm0
	vsubss	%xmm0, %xmm11, %xmm14
	vsubss	%xmm14, %xmm7, %xmm7
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm0, %xmm7, %xmm6
	vaddss	%xmm11, %xmm10, %xmm0
	vsubss	%xmm10, %xmm0, %xmm7
	vsubss	%xmm7, %xmm0, %xmm14
	vmovss	%xmm0, 8(%rsp)                  # 4-byte Spill
	vsubss	%xmm14, %xmm10, %xmm10
	vsubss	%xmm7, %xmm11, %xmm7
	vaddss	%xmm7, %xmm10, %xmm7
	vaddss	%xmm1, %xmm15, %xmm1
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm6
	vfmadd231ss	208(%rsp), %xmm12, %xmm6 # 4-byte Folded Reload
                                        # xmm6 = (xmm12 * mem) + xmm6
	vfmadd231ss	%xmm13, %xmm5, %xmm6    # xmm6 = (xmm5 * xmm13) + xmm6
	vfmadd231ss	40(%rsp), %xmm3, %xmm6  # 4-byte Folded Reload
                                        # xmm6 = (xmm3 * mem) + xmm6
	vfmadd231ss	32(%rsp), %xmm8, %xmm6  # 4-byte Folded Reload
                                        # xmm6 = (xmm8 * mem) + xmm6
	vaddss	%xmm6, %xmm0, %xmm1
	vaddss	%xmm1, %xmm2, %xmm3
	vmovaps	%xmm1, %xmm7
	vmovss	%xmm1, 208(%rsp)                # 4-byte Spill
	vsubss	%xmm3, %xmm2, %xmm5
	vmovss	4(%rsp), %xmm0                  # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vaddss	%xmm3, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm2
	vaddss	%xmm3, %xmm2, %xmm3
	vmovss	256(%rsp), %xmm8                # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm8, %xmm0
	vmovss	%xmm0, 4(%rsp)                  # 4-byte Spill
	vmovaps	%xmm1, %xmm2
	vfmsub213ss	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm2) - xmm0
	vmulss	%xmm3, %xmm8, %xmm4
	vaddss	%xmm4, %xmm2, %xmm9
	vsubss	%xmm2, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm2, %xmm2
	vmovaps	%xmm3, %xmm11
	vfmsub213ss	%xmm4, %xmm8, %xmm11    # xmm11 = (xmm8 * xmm11) - xmm4
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm4
	vmovss	224(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm0, %xmm10
	vaddss	%xmm10, %xmm9, %xmm2
	vsubss	%xmm9, %xmm2, %xmm12
	vsubss	%xmm12, %xmm2, %xmm13
	vsubss	%xmm13, %xmm9, %xmm9
	vmovaps	%xmm1, %xmm13
	vfmsub213ss	%xmm10, %xmm0, %xmm13   # xmm13 = (xmm0 * xmm13) - xmm10
	vsubss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm4, %xmm9, %xmm10
	vsubss	%xmm4, %xmm10, %xmm12
	vsubss	%xmm12, %xmm10, %xmm14
	vsubss	%xmm14, %xmm4, %xmm4
	vsubss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm4, %xmm9, %xmm4
	vaddss	%xmm7, %xmm5, %xmm5
	vmulss	%xmm5, %xmm8, %xmm9
	vmovaps	%xmm5, %xmm12
	vfmsub213ss	%xmm9, %xmm8, %xmm12    # xmm12 = (xmm8 * xmm12) - xmm9
	vaddss	%xmm4, %xmm12, %xmm4
	vmulss	%xmm3, %xmm0, %xmm12
	vmovaps	%xmm3, %xmm14
	vfmsub213ss	%xmm12, %xmm0, %xmm14   # xmm14 = (xmm0 * xmm14) - xmm12
	vaddss	%xmm4, %xmm14, %xmm4
	vmovss	48(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm0, %xmm14
	vmovaps	%xmm1, %xmm15
	vfmsub213ss	%xmm14, %xmm0, %xmm15   # xmm15 = (xmm0 * xmm15) - xmm14
	vaddss	%xmm4, %xmm15, %xmm4
	vaddss	%xmm13, %xmm11, %xmm15
	vsubss	%xmm11, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm7
	vsubss	%xmm7, %xmm11, %xmm7
	vsubss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm14, %xmm12, %xmm7
	vsubss	%xmm12, %xmm7, %xmm11
	vsubss	%xmm11, %xmm7, %xmm13
	vsubss	%xmm13, %xmm12, %xmm12
	vsubss	%xmm11, %xmm14, %xmm11
	vaddss	%xmm11, %xmm12, %xmm11
	vaddss	%xmm9, %xmm15, %xmm12
	vsubss	%xmm15, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm15, %xmm14
	vsubss	%xmm13, %xmm9, %xmm9
	vaddss	%xmm9, %xmm14, %xmm9
	vaddss	%xmm7, %xmm10, %xmm13
	vsubss	%xmm7, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm14, %xmm10, %xmm10
	vaddss	%xmm7, %xmm10, %xmm7
	vaddss	%xmm13, %xmm12, %xmm10
	vsubss	%xmm12, %xmm10, %xmm14
	vsubss	%xmm14, %xmm10, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm0, %xmm12, %xmm0
	vfmadd231ss	160(%rsp), %xmm1, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm1 * mem) + xmm0
	vfmadd231ss	48(%rsp), %xmm3, %xmm0  # 4-byte Folded Reload
                                        # xmm0 = (xmm3 * mem) + xmm0
	vfmadd231ss	224(%rsp), %xmm5, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm5 * mem) + xmm0
	vmovss	8(%rsp), %xmm1                  # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	208(%rsp), %xmm1, %xmm1         # 4-byte Folded Reload
	vaddss	%xmm6, %xmm1, %xmm1
	vfmadd231ss	%xmm1, %xmm8, %xmm0     # xmm0 = (xmm8 * xmm1) + xmm0
	vaddss	%xmm0, %xmm10, %xmm1
	vsubss	%xmm1, %xmm10, %xmm3
	vaddss	%xmm0, %xmm3, %xmm6
	vaddss	%xmm1, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmovss	4(%rsp), %xmm0                  # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vaddss	%xmm3, %xmm0, %xmm2
	vsubss	%xmm2, %xmm0, %xmm4
	movl	$2, %eax
	vxorps	%xmm8, %xmm8, %xmm8
	vcvtsi2ss	%eax, %xmm8, %xmm5
	vaddss	%xmm3, %xmm4, %xmm3
	vmulss	152(%rsp), %xmm5, %xmm4         # 4-byte Folded Reload
	vmulss	%xmm4, %xmm2, %xmm2
	vmulss	%xmm4, %xmm3, %xmm3
	vinsertps	$16, %xmm3, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm3[0],xmm2[2,3]
	vmulss	%xmm4, %xmm1, %xmm1
	vmulss	%xmm4, %xmm6, %xmm2
	vinsertps	$16, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[2,3]
.LBB42_327:
	vxorps	%xmm2, %xmm2, %xmm2
	vucomiss	%xmm2, %xmm0
	setp	%al
	setne	%cl
	orb	%al, %cl
	jne	.LBB42_329
# %bb.328:
	vxorps	%xmm1, %xmm1, %xmm1
	vblendps	$2, %xmm1, %xmm0, %xmm0         # xmm0 = xmm0[0],xmm1[1],xmm0[2,3]
	jmp	.LBB42_340
.LBB42_329:
	vcomiss	%xmm0, %xmm2
	jbe	.LBB42_331
# %bb.330:
	vbroadcastss	.LCPI42_0(%rip), %xmm0  # xmm0 = [NaN,NaN,NaN,NaN]
	vmovaps	%xmm0, %xmm1
	jmp	.LBB42_340
.LBB42_331:
	vmovups	%xmm1, 160(%rsp)                # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	vmovups	%xmm0, 48(%rsp)                 # 16-byte Spill
                                        # AlignMOV convert to UnAlignMOV 
	callq	sqrtf
	vxorps	%xmm2, %xmm2, %xmm2
	vucomiss	%xmm2, %xmm0
	setnp	%cl
	sete	%dl
	vmovd	.LCPI42_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	vmovdqa	%xmm1, %xmm10
	testb	%cl, %dl
	jne	.LBB42_335
# %bb.332:
	movl	%eax, %ecx
	andl	$2139095040, %ecx               # imm = 0x7F800000
	cmpl	$2139095040, %ecx               # imm = 0x7F800000
	je	.LBB42_334
# %bb.333:
	vmovd	%ecx, %xmm10
	jmp	.LBB42_335
.LBB42_334:
	vmovd	.LCPI42_1(%rip), %xmm10         # xmm10 = mem[0],zero,zero,zero
.LBB42_335:
	vucomiss	%xmm2, %xmm0
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB42_339
# %bb.336:
	andl	$2139095040, %eax               # imm = 0x7F800000
	cmpl	$2139095040, %eax               # imm = 0x7F800000
	je	.LBB42_338
# %bb.337:
	movl	$2130706432, %ecx               # imm = 0x7F000000
	subl	%eax, %ecx
	vmovd	%ecx, %xmm1
	jmp	.LBB42_339
.LBB42_338:
	vmovd	.LCPI42_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
.LBB42_339:
	vmovss	.LCPI42_2(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm1, %xmm3
	vmovups	48(%rsp), %xmm5                 # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmulss	%xmm1, %xmm5, %xmm2
	vmulss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 256(%rsp)                # 4-byte Spill
	vmovshdup	%xmm5, %xmm2            # xmm2 = xmm5[1,1,3,3]
	vmulss	%xmm1, %xmm2, %xmm2
	vmulss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 224(%rsp)                # 4-byte Spill
	vmovups	160(%rsp), %xmm5                # 16-byte Reload
                                        # AlignMOV convert to UnAlignMOV 
	vmulss	%xmm1, %xmm5, %xmm2
	vmulss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 48(%rsp)                 # 4-byte Spill
	vmovshdup	%xmm5, %xmm2            # xmm2 = xmm5[1,1,3,3]
	vmulss	%xmm1, %xmm2, %xmm1
	movl	$3, %eax
	vxorps	%xmm8, %xmm8, %xmm8
	vcvtsi2ss	%eax, %xmm8, %xmm2
	vmulss	%xmm3, %xmm1, %xmm1
	vmovss	%xmm1, 160(%rsp)                # 4-byte Spill
	vmulss	%xmm4, %xmm2, %xmm1
	vmovaps	%xmm4, %xmm3
	vfmsub213ss	%xmm1, %xmm2, %xmm3     # xmm3 = (xmm2 * xmm3) - xmm1
	vxorps	%xmm14, %xmm14, %xmm14
	vmulss	%xmm2, %xmm14, %xmm5
	vaddss	%xmm5, %xmm3, %xmm6
	vsubss	%xmm3, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm5, %xmm7
	vaddss	%xmm7, %xmm3, %xmm7
	vaddss	%xmm6, %xmm14, %xmm3
	vsubss	%xmm6, %xmm3, %xmm8
	vsubss	%xmm8, %xmm3, %xmm9
	vsubss	%xmm9, %xmm6, %xmm6
	vsubss	%xmm8, %xmm14, %xmm8
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm6, %xmm7, %xmm8
	vsubss	%xmm7, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm11
	vsubss	%xmm11, %xmm7, %xmm7
	vxorps	%xmm11, %xmm11, %xmm11
	vfmsub213ss	%xmm5, %xmm2, %xmm11    # xmm11 = (xmm2 * xmm11) - xmm5
	vsubss	%xmm9, %xmm6, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm6, %xmm11, %xmm6
	vaddss	%xmm14, %xmm11, %xmm7
	vsubss	%xmm11, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm12
	vsubss	%xmm12, %xmm11, %xmm11
	vsubss	%xmm9, %xmm14, %xmm9
	vaddss	%xmm9, %xmm11, %xmm9
	vaddss	%xmm5, %xmm7, %xmm11
	vsubss	%xmm7, %xmm11, %xmm12
	vsubss	%xmm12, %xmm11, %xmm13
	vsubss	%xmm13, %xmm7, %xmm7
	vsubss	%xmm12, %xmm5, %xmm5
	vaddss	%xmm5, %xmm7, %xmm5
	vaddss	%xmm8, %xmm14, %xmm7
	vsubss	%xmm14, %xmm7, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vsubss	%xmm12, %xmm7, %xmm12
	vsubss	%xmm12, %xmm14, %xmm12
	vaddss	%xmm8, %xmm12, %xmm8
	vaddss	%xmm7, %xmm11, %xmm12
	vsubss	%xmm11, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm11, %xmm11
	vsubss	%xmm13, %xmm7, %xmm7
	vaddss	%xmm7, %xmm11, %xmm7
	vmovd	%xmm10, 152(%rsp)               # 4-byte Folded Spill
	vdivss	%xmm0, %xmm10, %xmm10
	vxorps	%xmm11, %xmm11, %xmm11
	vaddss	%xmm6, %xmm11, %xmm0
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vfmadd231ss	%xmm4, %xmm11, %xmm0    # xmm0 = (xmm11 * xmm4) + xmm0
	vfmadd231ss	%xmm11, %xmm11, %xmm0   # xmm0 = (xmm11 * xmm11) + xmm0
	vfmadd231ss	%xmm11, %xmm11, %xmm0   # xmm0 = (xmm11 * xmm11) + xmm0
	vfmadd231ss	%xmm2, %xmm11, %xmm0    # xmm0 = (xmm11 * xmm2) + xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vaddss	%xmm0, %xmm12, %xmm2
	vsubss	%xmm2, %xmm12, %xmm5
	vaddss	%xmm0, %xmm5, %xmm0
	vmovss	%xmm0, 8(%rsp)                  # 4-byte Spill
	vaddss	%xmm2, %xmm3, %xmm0
	vsubss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm2, %xmm3, %xmm2
	vmovss	%xmm2, 4(%rsp)                  # 4-byte Spill
	vaddss	%xmm0, %xmm1, %xmm2
	vmovss	%xmm2, 72(%rsp)                 # 4-byte Spill
	vsubss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, 208(%rsp)                # 4-byte Spill
	vmovaps	%xmm10, %xmm0
	vmulss	%xmm10, %xmm10, %xmm4
	vmovaps	%xmm10, %xmm1
	vfmsub213ss	%xmm4, %xmm10, %xmm1    # xmm1 = (xmm10 * xmm1) - xmm4
	vmulss	%xmm6, %xmm10, %xmm2
	vmovaps	%xmm10, %xmm7
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm9
	vsubss	%xmm9, %xmm3, %xmm12
	vsubss	%xmm12, %xmm1, %xmm1
	vmulss	%xmm6, %xmm10, %xmm12
	vsubss	%xmm9, %xmm2, %xmm9
	vaddss	%xmm1, %xmm9, %xmm9
	vaddss	%xmm3, %xmm12, %xmm1
	vsubss	%xmm3, %xmm1, %xmm13
	vsubss	%xmm13, %xmm1, %xmm14
	vsubss	%xmm14, %xmm3, %xmm3
	vsubss	%xmm13, %xmm12, %xmm13
	vaddss	%xmm3, %xmm13, %xmm3
	vxorps	%xmm13, %xmm13, %xmm13
	vfmsub213ss	%xmm2, %xmm10, %xmm13   # xmm13 = (xmm10 * xmm13) - xmm2
	vmovaps	%xmm10, %xmm14
	vfmsub213ss	%xmm12, %xmm6, %xmm14   # xmm14 = (xmm6 * xmm14) - xmm12
	vaddss	%xmm3, %xmm9, %xmm15
	vsubss	%xmm9, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm5
	vsubss	%xmm5, %xmm9, %xmm5
	vsubss	%xmm0, %xmm3, %xmm0
	vaddss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm14, %xmm13, %xmm3
	vsubss	%xmm13, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm9
	vsubss	%xmm9, %xmm13, %xmm9
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm0, %xmm14, %xmm0
	vsubss	%xmm5, %xmm14, %xmm5
	vaddss	%xmm5, %xmm9, %xmm5
	vaddss	%xmm6, %xmm12, %xmm9
	vsubss	%xmm6, %xmm9, %xmm13
	vsubss	%xmm13, %xmm12, %xmm12
	vsubss	%xmm13, %xmm9, %xmm13
	vsubss	%xmm13, %xmm6, %xmm13
	vaddss	%xmm12, %xmm13, %xmm12
	vaddss	%xmm2, %xmm3, %xmm13
	vsubss	%xmm3, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm6
	vsubss	%xmm6, %xmm3, %xmm3
	vsubss	%xmm14, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vaddss	%xmm15, %xmm9, %xmm3
	vsubss	%xmm9, %xmm3, %xmm6
	vsubss	%xmm6, %xmm3, %xmm14
	vsubss	%xmm14, %xmm9, %xmm9
	vsubss	%xmm6, %xmm15, %xmm6
	vaddss	%xmm6, %xmm9, %xmm6
	vaddss	%xmm3, %xmm13, %xmm9
	vsubss	%xmm13, %xmm9, %xmm14
	vsubss	%xmm14, %xmm9, %xmm15
	vsubss	%xmm15, %xmm13, %xmm13
	vsubss	%xmm14, %xmm3, %xmm3
	vaddss	%xmm3, %xmm13, %xmm3
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm12, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vmovss	%xmm10, 40(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm10, %xmm2, %xmm0    # xmm0 = (xmm2 * xmm10) + xmm0
	vfmadd231ss	%xmm2, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm2) + xmm0
	vfmadd231ss	%xmm2, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm2) + xmm0
	vfmadd231ss	%xmm2, %xmm10, %xmm0    # xmm0 = (xmm10 * xmm2) + xmm0
	vaddss	%xmm0, %xmm9, %xmm2
	vsubss	%xmm2, %xmm9, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm0, 32(%rsp)                 # 4-byte Spill
	vaddss	%xmm2, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm10
	vaddss	%xmm0, %xmm4, %xmm8
	vsubss	%xmm8, %xmm4, %xmm2
	vaddss	%xmm0, %xmm2, %xmm9
	vmovss	256(%rsp), %xmm13               # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vmulss	%xmm8, %xmm13, %xmm1
	vmovss	%xmm1, 16(%rsp)                 # 4-byte Spill
	vmovaps	%xmm8, %xmm2
	vfmsub213ss	%xmm1, %xmm13, %xmm2    # xmm2 = (xmm13 * xmm2) - xmm1
	vmulss	%xmm9, %xmm13, %xmm3
	vaddss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm9, %xmm6
	vfmsub213ss	%xmm3, %xmm13, %xmm6    # xmm6 = (xmm13 * xmm6) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	224(%rsp), %xmm11               # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vmulss	%xmm8, %xmm11, %xmm5
	vaddss	%xmm5, %xmm4, %xmm3
	vsubss	%xmm4, %xmm3, %xmm14
	vsubss	%xmm14, %xmm3, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vmovaps	%xmm8, %xmm15
	vfmsub213ss	%xmm5, %xmm11, %xmm15   # xmm15 = (xmm11 * xmm15) - xmm5
	vsubss	%xmm14, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm14
	vsubss	%xmm14, %xmm5, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vsubss	%xmm14, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vmovaps	%xmm10, %xmm7
	vmovss	%xmm10, (%rsp)                  # 4-byte Spill
	vmulss	%xmm10, %xmm13, %xmm4
	vfmsub213ss	%xmm4, %xmm13, %xmm7    # xmm7 = (xmm13 * xmm7) - xmm4
	vaddss	%xmm7, %xmm2, %xmm2
	vmulss	%xmm9, %xmm11, %xmm7
	vmovaps	%xmm9, %xmm14
	vfmsub213ss	%xmm7, %xmm11, %xmm14   # xmm14 = (xmm11 * xmm14) - xmm7
	vaddss	%xmm2, %xmm14, %xmm2
	vmovss	48(%rsp), %xmm10                # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vmulss	%xmm8, %xmm10, %xmm14
	vmovaps	%xmm8, %xmm1
	vfmsub213ss	%xmm14, %xmm10, %xmm1   # xmm1 = (xmm10 * xmm1) - xmm14
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm6, %xmm15, %xmm2
	vsubss	%xmm6, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm12
	vsubss	%xmm12, %xmm6, %xmm6
	vsubss	%xmm0, %xmm15, %xmm0
	vaddss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm7, %xmm14, %xmm6
	vsubss	%xmm7, %xmm6, %xmm12
	vsubss	%xmm12, %xmm6, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm12, %xmm14, %xmm12
	vaddss	%xmm7, %xmm12, %xmm7
	vaddss	%xmm4, %xmm2, %xmm12
	vsubss	%xmm2, %xmm12, %xmm14
	vsubss	%xmm14, %xmm12, %xmm15
	vsubss	%xmm15, %xmm2, %xmm2
	vsubss	%xmm14, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm5, %xmm6, %xmm4
	vsubss	%xmm6, %xmm4, %xmm14
	vsubss	%xmm14, %xmm4, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm14, %xmm5, %xmm5
	vaddss	%xmm5, %xmm6, %xmm5
	vaddss	%xmm4, %xmm12, %xmm6
	vsubss	%xmm12, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm4, %xmm4
	vaddss	%xmm4, %xmm12, %xmm4
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vfmadd231ss	160(%rsp), %xmm8, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm8 * mem) + xmm0
	vfmadd231ss	%xmm9, %xmm10, %xmm0    # xmm0 = (xmm10 * xmm9) + xmm0
	vfmadd231ss	(%rsp), %xmm11, %xmm0   # 4-byte Folded Reload
                                        # xmm0 = (xmm11 * mem) + xmm0
	vfmadd231ss	32(%rsp), %xmm13, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm13 * mem) + xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm2
	vaddss	%xmm0, %xmm2, %xmm0
	vaddss	%xmm1, %xmm3, %xmm2
	vsubss	%xmm2, %xmm3, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	16(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm2, %xmm4, %xmm3
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm2, %xmm4, %xmm4
	vbroadcastss	.LCPI42_3(%rip), %xmm6  # xmm6 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm6, %xmm3, %xmm5
	vmovss	72(%rsp), %xmm11                # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm11, %xmm2
	vsubss	%xmm11, %xmm2, %xmm3
	vsubss	%xmm3, %xmm5, %xmm5
	vsubss	%xmm3, %xmm2, %xmm3
	vsubss	%xmm3, %xmm11, %xmm3
	vaddss	%xmm5, %xmm3, %xmm3
	vmovaps	%xmm6, %xmm8
	vxorps	%xmm6, %xmm4, %xmm5
	vmovss	208(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm7, %xmm4
	vsubss	%xmm7, %xmm4, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm6, %xmm4, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm5, %xmm6, %xmm5
	vxorps	%xmm1, %xmm8, %xmm6
	vmovss	4(%rsp), %xmm8                  # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm8, %xmm1
	vsubss	%xmm8, %xmm1, %xmm7
	vsubss	%xmm7, %xmm6, %xmm6
	vsubss	%xmm7, %xmm1, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	8(%rsp), %xmm6, %xmm6           # 4-byte Folded Reload
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm4, %xmm3, %xmm6
	vsubss	%xmm3, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm5, %xmm3, %xmm4
	vsubss	%xmm3, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm3, %xmm3
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vaddss	%xmm0, %xmm1, %xmm0
	vaddss	%xmm0, %xmm5, %xmm1
	vsubss	%xmm1, %xmm5, %xmm3
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm0, 32(%rsp)                 # 4-byte Spill
	vaddss	%xmm1, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm3
	vaddss	%xmm1, %xmm3, %xmm3
	vaddss	%xmm4, %xmm2, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm12
	vmovss	40(%rsp), %xmm10                # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm10, %xmm13
	vmovaps	%xmm10, %xmm2
	vfmsub213ss	%xmm13, %xmm1, %xmm2    # xmm2 = (xmm1 * xmm2) - xmm13
	vxorps	%xmm0, %xmm0, %xmm0
	vmulss	%xmm0, %xmm1, %xmm14
	vaddss	%xmm2, %xmm14, %xmm4
	vsubss	%xmm2, %xmm4, %xmm5
	vsubss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vsubss	%xmm5, %xmm14, %xmm5
	vaddss	%xmm5, %xmm2, %xmm5
	vmulss	%xmm10, %xmm12, %xmm6
	vaddss	%xmm6, %xmm4, %xmm2
	vsubss	%xmm4, %xmm2, %xmm7
	vsubss	%xmm7, %xmm2, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vmovaps	%xmm10, %xmm8
	vfmsub213ss	%xmm6, %xmm12, %xmm8    # xmm8 = (xmm12 * xmm8) - xmm6
	vsubss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm5, %xmm4
	vxorps	%xmm5, %xmm5, %xmm5
	vfmsub213ss	%xmm14, %xmm1, %xmm5    # xmm5 = (xmm1 * xmm5) - xmm14
	vmulss	%xmm0, %xmm12, %xmm7
	vxorps	%xmm15, %xmm15, %xmm15
	vfmsub213ss	%xmm7, %xmm12, %xmm15   # xmm15 = (xmm12 * xmm15) - xmm7
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm4, %xmm15, %xmm4
	vmulss	%xmm3, %xmm10, %xmm15
	vmovaps	%xmm10, %xmm11
	vfmsub213ss	%xmm15, %xmm3, %xmm11   # xmm11 = (xmm3 * xmm11) - xmm15
	vaddss	%xmm4, %xmm11, %xmm4
	vaddss	%xmm5, %xmm8, %xmm11
	vsubss	%xmm5, %xmm11, %xmm0
	vsubss	%xmm0, %xmm11, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vsubss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm0, %xmm5, %xmm0
	vaddss	%xmm7, %xmm15, %xmm5
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm5, %xmm9
	vsubss	%xmm9, %xmm7, %xmm7
	vsubss	%xmm8, %xmm15, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm14, %xmm11, %xmm8
	vsubss	%xmm11, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm15
	vsubss	%xmm15, %xmm11, %xmm11
	vsubss	%xmm9, %xmm14, %xmm9
	vaddss	%xmm9, %xmm11, %xmm9
	vaddss	%xmm6, %xmm5, %xmm11
	vsubss	%xmm5, %xmm11, %xmm14
	vsubss	%xmm14, %xmm11, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm14, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm11, %xmm8, %xmm6
	vsubss	%xmm8, %xmm6, %xmm14
	vsubss	%xmm14, %xmm6, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm11, %xmm8, %xmm8
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vfmadd231ss	32(%rsp), %xmm10, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm10 * mem) + xmm0
	vxorps	%xmm4, %xmm4, %xmm4
	vfmadd231ss	%xmm3, %xmm4, %xmm0     # xmm0 = (xmm4 * xmm3) + xmm0
	vfmadd231ss	%xmm12, %xmm4, %xmm0    # xmm0 = (xmm4 * xmm12) + xmm0
	vxorps	%xmm3, %xmm3, %xmm3
	vfmadd231ss	%xmm3, %xmm1, %xmm0     # xmm0 = (xmm1 * xmm3) + xmm0
	vaddss	%xmm0, %xmm6, %xmm1
	vsubss	%xmm1, %xmm6, %xmm3
	vaddss	%xmm0, %xmm3, %xmm11
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm8
	vaddss	%xmm0, %xmm13, %xmm12
	vsubss	%xmm12, %xmm13, %xmm1
	vaddss	%xmm0, %xmm1, %xmm13
	vmulss	%xmm12, %xmm12, %xmm10
	vmovaps	%xmm12, %xmm0
	vfmsub213ss	%xmm10, %xmm12, %xmm0   # xmm0 = (xmm12 * xmm0) - xmm10
	vmulss	%xmm13, %xmm12, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm13, %xmm4
	vfmsub213ss	%xmm1, %xmm12, %xmm4    # xmm4 = (xmm12 * xmm4) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm12, %xmm13, %xmm3
	vaddss	%xmm3, %xmm2, %xmm0
	vsubss	%xmm2, %xmm0, %xmm5
	vsubss	%xmm5, %xmm0, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm12, %xmm6
	vfmsub213ss	%xmm3, %xmm13, %xmm6    # xmm6 = (xmm13 * xmm6) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vmovaps	%xmm8, %xmm7
	vmulss	%xmm8, %xmm12, %xmm2
	vmovaps	%xmm8, %xmm5
	vmovaps	%xmm8, %xmm14
	vfmsub213ss	%xmm2, %xmm12, %xmm5    # xmm5 = (xmm12 * xmm5) - xmm2
	vaddss	%xmm5, %xmm1, %xmm1
	vmulss	%xmm13, %xmm13, %xmm5
	vmovaps	%xmm13, %xmm7
	vfmsub213ss	%xmm5, %xmm13, %xmm7    # xmm7 = (xmm13 * xmm7) - xmm5
	vaddss	%xmm7, %xmm1, %xmm1
	vmulss	%xmm12, %xmm8, %xmm7
	vmovaps	%xmm12, %xmm8
	vfmsub213ss	%xmm7, %xmm14, %xmm8    # xmm8 = (xmm14 * xmm8) - xmm7
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm6, %xmm4, %xmm8
	vsubss	%xmm4, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm9, %xmm6, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm7, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm9, %xmm7, %xmm7
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm2, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm9, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm3, %xmm6, %xmm8
	vsubss	%xmm6, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm9, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm7, %xmm8, %xmm6
	vsubss	%xmm7, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vmovss	%xmm11, 32(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm12, %xmm11, %xmm1   # xmm1 = (xmm11 * xmm12) + xmm1
	vmovss	%xmm13, 16(%rsp)                # 4-byte Spill
	vmovss	%xmm14, 40(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm13, %xmm14, %xmm1   # xmm1 = (xmm14 * xmm13) + xmm1
	vfmadd231ss	%xmm14, %xmm13, %xmm1   # xmm1 = (xmm13 * xmm14) + xmm1
	vfmadd231ss	%xmm11, %xmm12, %xmm1   # xmm1 = (xmm12 * xmm11) + xmm1
	vaddss	%xmm1, %xmm6, %xmm2
	vsubss	%xmm2, %xmm6, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm1, (%rsp)                   # 4-byte Spill
	vaddss	%xmm2, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm13
	vaddss	%xmm1, %xmm10, %xmm2
	vsubss	%xmm2, %xmm10, %xmm3
	vaddss	%xmm1, %xmm3, %xmm3
	vmovss	256(%rsp), %xmm14               # 4-byte Reload
                                        # xmm14 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm14, %xmm4
	vmovss	%xmm4, 12(%rsp)                 # 4-byte Spill
	vmovaps	%xmm2, %xmm1
	vfmsub213ss	%xmm4, %xmm14, %xmm1    # xmm1 = (xmm14 * xmm1) - xmm4
	vmulss	%xmm3, %xmm14, %xmm4
	vaddss	%xmm4, %xmm1, %xmm5
	vsubss	%xmm1, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vmovaps	%xmm3, %xmm7
	vfmsub213ss	%xmm4, %xmm14, %xmm7    # xmm7 = (xmm14 * xmm7) - xmm4
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm1, %xmm4
	vmovss	224(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm0, %xmm6
	vaddss	%xmm6, %xmm5, %xmm1
	vsubss	%xmm5, %xmm1, %xmm8
	vsubss	%xmm8, %xmm1, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vmovaps	%xmm2, %xmm9
	vfmsub213ss	%xmm6, %xmm0, %xmm9     # xmm9 = (xmm0 * xmm9) - xmm6
	vsubss	%xmm8, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm4, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vmovaps	%xmm13, %xmm8
	vmovss	%xmm13, 20(%rsp)                # 4-byte Spill
	vmulss	%xmm13, %xmm14, %xmm5
	vfmsub213ss	%xmm5, %xmm14, %xmm8    # xmm8 = (xmm14 * xmm8) - xmm5
	vaddss	%xmm4, %xmm8, %xmm4
	vmovaps	%xmm0, %xmm13
	vmulss	%xmm3, %xmm0, %xmm8
	vmovaps	%xmm3, %xmm10
	vfmsub213ss	%xmm8, %xmm0, %xmm10    # xmm10 = (xmm0 * xmm10) - xmm8
	vaddss	%xmm4, %xmm10, %xmm4
	vmovss	48(%rsp), %xmm11                # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm11, %xmm10
	vmovaps	%xmm2, %xmm0
	vfmsub213ss	%xmm10, %xmm11, %xmm0   # xmm0 = (xmm11 * xmm0) - xmm10
	vaddss	%xmm0, %xmm4, %xmm0
	vmovss	%xmm0, 24(%rsp)                 # 4-byte Spill
	vaddss	%xmm7, %xmm9, %xmm4
	vsubss	%xmm7, %xmm4, %xmm0
	vsubss	%xmm0, %xmm4, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm10, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm9, %xmm10, %xmm9
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm5, %xmm4, %xmm9
	vsubss	%xmm4, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm6, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm10
	vsubss	%xmm10, %xmm5, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm5, %xmm9, %xmm7
	vsubss	%xmm9, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm15
	vsubss	%xmm15, %xmm9, %xmm9
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm9, %xmm5
	vaddss	24(%rsp), %xmm0, %xmm0          # 4-byte Folded Reload
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vfmadd231ss	160(%rsp), %xmm2, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm2 * mem) + xmm0
	vfmadd231ss	%xmm3, %xmm11, %xmm0    # xmm0 = (xmm11 * xmm3) + xmm0
	vfmadd231ss	20(%rsp), %xmm13, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm13 * mem) + xmm0
	vfmadd231ss	(%rsp), %xmm14, %xmm0   # 4-byte Folded Reload
                                        # xmm0 = (xmm14 * mem) + xmm0
	vaddss	%xmm0, %xmm7, %xmm2
	vsubss	%xmm2, %xmm7, %xmm3
	vaddss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm2, %xmm1, %xmm0
	vsubss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vmovss	12(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm4
	vaddss	%xmm0, %xmm4, %xmm4
	vbroadcastss	.LCPI42_3(%rip), %xmm8  # xmm8 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm8, %xmm5
	vmovss	72(%rsp), %xmm6                 # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm6, %xmm0
	vsubss	%xmm6, %xmm0, %xmm2
	vsubss	%xmm2, %xmm5, %xmm5
	vsubss	%xmm2, %xmm0, %xmm2
	vsubss	%xmm2, %xmm6, %xmm2
	vaddss	%xmm5, %xmm2, %xmm2
	vxorps	%xmm4, %xmm8, %xmm5
	vmovss	208(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm7, %xmm4
	vsubss	%xmm7, %xmm4, %xmm6
	vsubss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm6, %xmm4, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm5, %xmm6, %xmm5
	vxorps	%xmm1, %xmm8, %xmm6
	vmovss	4(%rsp), %xmm8                  # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm8, %xmm1
	vsubss	%xmm8, %xmm1, %xmm7
	vsubss	%xmm7, %xmm6, %xmm6
	vsubss	%xmm7, %xmm1, %xmm7
	vsubss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	8(%rsp), %xmm6, %xmm6           # 4-byte Folded Reload
	vsubss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm4, %xmm2, %xmm6
	vsubss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm5, %xmm2, %xmm4
	vsubss	%xmm2, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm1, %xmm4, %xmm5
	vsubss	%xmm4, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm2
	vsubss	%xmm2, %xmm5, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm1, 12(%rsp)                 # 4-byte Spill
	vaddss	%xmm2, %xmm6, %xmm4
	vsubss	%xmm4, %xmm6, %xmm1
	vaddss	%xmm2, %xmm1, %xmm3
	vaddss	%xmm4, %xmm0, %xmm15
	vsubss	%xmm15, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm1
	vmulss	%xmm12, %xmm15, %xmm0
	vmovss	%xmm0, (%rsp)                   # 4-byte Spill
	vmovaps	%xmm12, %xmm2
	vfmsub213ss	%xmm0, %xmm15, %xmm2    # xmm2 = (xmm15 * xmm2) - xmm0
	vmovss	16(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm15, %xmm4
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vmovaps	%xmm0, %xmm7
	vmovaps	%xmm0, %xmm14
	vfmsub213ss	%xmm4, %xmm15, %xmm7    # xmm7 = (xmm15 * xmm7) - xmm4
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm4
	vmulss	%xmm1, %xmm12, %xmm6
	vaddss	%xmm6, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm8
	vsubss	%xmm8, %xmm2, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vmovaps	%xmm12, %xmm9
	vfmsub213ss	%xmm6, %xmm1, %xmm9     # xmm9 = (xmm1 * xmm9) - xmm6
	vsubss	%xmm8, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm4, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vmovss	40(%rsp), %xmm8                 # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vmulss	%xmm8, %xmm15, %xmm5
	vfmsub213ss	%xmm5, %xmm15, %xmm8    # xmm8 = (xmm15 * xmm8) - xmm5
	vaddss	%xmm4, %xmm8, %xmm4
	vmulss	%xmm0, %xmm1, %xmm8
	vmovaps	%xmm0, %xmm10
	vmovaps	%xmm0, %xmm13
	vfmsub213ss	%xmm8, %xmm1, %xmm10    # xmm10 = (xmm1 * xmm10) - xmm8
	vaddss	%xmm4, %xmm10, %xmm4
	vmulss	%xmm3, %xmm12, %xmm10
	vmovaps	%xmm12, %xmm11
	vfmsub213ss	%xmm10, %xmm3, %xmm11   # xmm11 = (xmm3 * xmm11) - xmm10
	vaddss	%xmm4, %xmm11, %xmm4
	vaddss	%xmm7, %xmm9, %xmm11
	vsubss	%xmm7, %xmm11, %xmm0
	vsubss	%xmm0, %xmm11, %xmm14
	vsubss	%xmm14, %xmm7, %xmm7
	vsubss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm10, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm14
	vsubss	%xmm14, %xmm8, %xmm8
	vsubss	%xmm9, %xmm10, %xmm9
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm5, %xmm11, %xmm9
	vsubss	%xmm11, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm14
	vsubss	%xmm14, %xmm11, %xmm11
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm6, %xmm7, %xmm10
	vsubss	%xmm7, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm14
	vsubss	%xmm14, %xmm7, %xmm7
	vsubss	%xmm11, %xmm6, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm10, %xmm9, %xmm7
	vsubss	%xmm9, %xmm7, %xmm11
	vsubss	%xmm11, %xmm7, %xmm14
	vsubss	%xmm14, %xmm9, %xmm9
	vsubss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vfmadd231ss	12(%rsp), %xmm12, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm12 * mem) + xmm0
	vfmadd231ss	%xmm13, %xmm3, %xmm0    # xmm0 = (xmm3 * xmm13) + xmm0
	vfmadd231ss	40(%rsp), %xmm1, %xmm0  # 4-byte Folded Reload
                                        # xmm0 = (xmm1 * mem) + xmm0
	vfmadd231ss	32(%rsp), %xmm15, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm15 * mem) + xmm0
	vaddss	%xmm0, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm3
	vaddss	%xmm0, %xmm3, %xmm11
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm8
	vmovss	(%rsp), %xmm1                   # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm1, %xmm12
	vsubss	%xmm12, %xmm1, %xmm1
	vaddss	%xmm0, %xmm1, %xmm13
	vmulss	%xmm12, %xmm12, %xmm10
	vmovaps	%xmm12, %xmm0
	vfmsub213ss	%xmm10, %xmm12, %xmm0   # xmm0 = (xmm12 * xmm0) - xmm10
	vmulss	%xmm13, %xmm12, %xmm1
	vaddss	%xmm1, %xmm0, %xmm2
	vsubss	%xmm0, %xmm2, %xmm3
	vsubss	%xmm3, %xmm2, %xmm4
	vsubss	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm13, %xmm4
	vfmsub213ss	%xmm1, %xmm12, %xmm4    # xmm4 = (xmm12 * xmm4) - xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm12, %xmm13, %xmm3
	vaddss	%xmm3, %xmm2, %xmm0
	vsubss	%xmm2, %xmm0, %xmm5
	vsubss	%xmm5, %xmm0, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm12, %xmm6
	vfmsub213ss	%xmm3, %xmm13, %xmm6    # xmm6 = (xmm13 * xmm6) - xmm3
	vsubss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm3
	vsubss	%xmm1, %xmm3, %xmm5
	vsubss	%xmm5, %xmm3, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vmovaps	%xmm8, %xmm7
	vmulss	%xmm8, %xmm12, %xmm2
	vmovaps	%xmm8, %xmm5
	vmovaps	%xmm8, %xmm14
	vfmsub213ss	%xmm2, %xmm12, %xmm5    # xmm5 = (xmm12 * xmm5) - xmm2
	vaddss	%xmm5, %xmm1, %xmm1
	vmulss	%xmm13, %xmm13, %xmm5
	vmovaps	%xmm13, %xmm7
	vfmsub213ss	%xmm5, %xmm13, %xmm7    # xmm7 = (xmm13 * xmm7) - xmm5
	vaddss	%xmm7, %xmm1, %xmm1
	vmulss	%xmm12, %xmm8, %xmm7
	vmovaps	%xmm12, %xmm8
	vfmsub213ss	%xmm7, %xmm14, %xmm8    # xmm8 = (xmm14 * xmm8) - xmm7
	vaddss	%xmm1, %xmm8, %xmm1
	vaddss	%xmm6, %xmm4, %xmm8
	vsubss	%xmm4, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm9, %xmm6, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm7, %xmm5, %xmm6
	vsubss	%xmm5, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm15
	vsubss	%xmm15, %xmm5, %xmm5
	vsubss	%xmm9, %xmm7, %xmm7
	vaddss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm2, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm9, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vaddss	%xmm3, %xmm6, %xmm8
	vsubss	%xmm6, %xmm8, %xmm9
	vsubss	%xmm9, %xmm8, %xmm15
	vsubss	%xmm15, %xmm6, %xmm6
	vsubss	%xmm9, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm7, %xmm8, %xmm6
	vsubss	%xmm7, %xmm6, %xmm9
	vsubss	%xmm9, %xmm6, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vmovss	%xmm11, 32(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm12, %xmm11, %xmm1   # xmm1 = (xmm11 * xmm12) + xmm1
	vmovss	%xmm14, 40(%rsp)                # 4-byte Spill
	vmovss	%xmm13, 16(%rsp)                # 4-byte Spill
	vfmadd231ss	%xmm13, %xmm14, %xmm1   # xmm1 = (xmm14 * xmm13) + xmm1
	vfmadd231ss	%xmm14, %xmm13, %xmm1   # xmm1 = (xmm13 * xmm14) + xmm1
	vfmadd231ss	%xmm11, %xmm12, %xmm1   # xmm1 = (xmm12 * xmm11) + xmm1
	vaddss	%xmm1, %xmm6, %xmm2
	vsubss	%xmm2, %xmm6, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm1, (%rsp)                   # 4-byte Spill
	vaddss	%xmm2, %xmm0, %xmm3
	vsubss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm13
	vaddss	%xmm3, %xmm10, %xmm1
	vsubss	%xmm1, %xmm10, %xmm2
	vaddss	%xmm3, %xmm2, %xmm3
	vmovss	256(%rsp), %xmm14               # 4-byte Reload
                                        # xmm14 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm14, %xmm4
	vmovss	%xmm4, 12(%rsp)                 # 4-byte Spill
	vmovaps	%xmm1, %xmm2
	vfmsub213ss	%xmm4, %xmm14, %xmm2    # xmm2 = (xmm14 * xmm2) - xmm4
	vmulss	%xmm3, %xmm14, %xmm4
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm6
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm2, %xmm2
	vmovaps	%xmm3, %xmm7
	vfmsub213ss	%xmm4, %xmm14, %xmm7    # xmm7 = (xmm14 * xmm7) - xmm4
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm4
	vmovss	224(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm0, %xmm6
	vaddss	%xmm6, %xmm5, %xmm2
	vsubss	%xmm5, %xmm2, %xmm8
	vsubss	%xmm8, %xmm2, %xmm9
	vsubss	%xmm9, %xmm5, %xmm5
	vmovaps	%xmm1, %xmm9
	vfmsub213ss	%xmm6, %xmm0, %xmm9     # xmm9 = (xmm0 * xmm9) - xmm6
	vsubss	%xmm8, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm6
	vsubss	%xmm4, %xmm6, %xmm8
	vsubss	%xmm8, %xmm6, %xmm10
	vsubss	%xmm10, %xmm4, %xmm4
	vsubss	%xmm8, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vmovaps	%xmm13, %xmm8
	vmovss	%xmm13, 20(%rsp)                # 4-byte Spill
	vmulss	%xmm13, %xmm14, %xmm5
	vfmsub213ss	%xmm5, %xmm14, %xmm8    # xmm8 = (xmm14 * xmm8) - xmm5
	vaddss	%xmm4, %xmm8, %xmm4
	vmovaps	%xmm0, %xmm13
	vmulss	%xmm3, %xmm0, %xmm8
	vmovaps	%xmm3, %xmm10
	vfmsub213ss	%xmm8, %xmm0, %xmm10    # xmm10 = (xmm0 * xmm10) - xmm8
	vaddss	%xmm4, %xmm10, %xmm4
	vmovss	48(%rsp), %xmm11                # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm11, %xmm10
	vmovaps	%xmm1, %xmm0
	vfmsub213ss	%xmm10, %xmm11, %xmm0   # xmm0 = (xmm11 * xmm0) - xmm10
	vaddss	%xmm0, %xmm4, %xmm0
	vmovss	%xmm0, 24(%rsp)                 # 4-byte Spill
	vaddss	%xmm7, %xmm9, %xmm4
	vsubss	%xmm7, %xmm4, %xmm0
	vsubss	%xmm0, %xmm4, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm10, %xmm8, %xmm7
	vsubss	%xmm8, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm15
	vsubss	%xmm15, %xmm8, %xmm8
	vsubss	%xmm9, %xmm10, %xmm9
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm5, %xmm4, %xmm9
	vsubss	%xmm4, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm15
	vsubss	%xmm15, %xmm4, %xmm4
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm6, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm10
	vsubss	%xmm10, %xmm5, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm7, %xmm6
	vaddss	%xmm5, %xmm9, %xmm7
	vsubss	%xmm9, %xmm7, %xmm10
	vsubss	%xmm10, %xmm7, %xmm15
	vsubss	%xmm15, %xmm9, %xmm9
	vsubss	%xmm10, %xmm5, %xmm5
	vaddss	%xmm5, %xmm9, %xmm5
	vaddss	24(%rsp), %xmm0, %xmm0          # 4-byte Folded Reload
	vaddss	%xmm0, %xmm8, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vfmadd231ss	160(%rsp), %xmm1, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm1 * mem) + xmm0
	vfmadd231ss	%xmm3, %xmm11, %xmm0    # xmm0 = (xmm11 * xmm3) + xmm0
	vfmadd231ss	20(%rsp), %xmm13, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm13 * mem) + xmm0
	vfmadd231ss	(%rsp), %xmm14, %xmm0   # 4-byte Folded Reload
                                        # xmm0 = (xmm14 * mem) + xmm0
	vaddss	%xmm0, %xmm7, %xmm1
	vsubss	%xmm1, %xmm7, %xmm3
	vaddss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmovss	12(%rsp), %xmm4                 # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm4
	vaddss	%xmm0, %xmm4, %xmm4
	vmovss	72(%rsp), %xmm6                 # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm6, %xmm0
	vsubss	%xmm6, %xmm0, %xmm5
	vmovaps	%xmm6, %xmm7
	vsubss	%xmm5, %xmm0, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vbroadcastss	.LCPI42_3(%rip), %xmm8  # xmm8 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%xmm2, %xmm8, %xmm2
	vsubss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vmovss	208(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm7, %xmm5
	vsubss	%xmm7, %xmm5, %xmm6
	vmovaps	%xmm7, %xmm9
	vsubss	%xmm6, %xmm5, %xmm7
	vsubss	%xmm7, %xmm9, %xmm7
	vxorps	%xmm4, %xmm8, %xmm4
	vsubss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vxorps	%xmm1, %xmm8, %xmm6
	vmovss	4(%rsp), %xmm8                  # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm8, %xmm1
	vsubss	%xmm8, %xmm1, %xmm7
	vmovaps	%xmm8, %xmm9
	vsubss	%xmm7, %xmm1, %xmm8
	vsubss	%xmm8, %xmm9, %xmm8
	vsubss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	8(%rsp), %xmm6, %xmm6           # 4-byte Folded Reload
	vsubss	%xmm3, %xmm6, %xmm3
	vaddss	%xmm5, %xmm2, %xmm6
	vsubss	%xmm2, %xmm6, %xmm7
	vsubss	%xmm7, %xmm6, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm5
	vsubss	%xmm2, %xmm5, %xmm7
	vsubss	%xmm7, %xmm5, %xmm8
	vsubss	%xmm8, %xmm2, %xmm2
	vsubss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm1, %xmm5, %xmm4
	vsubss	%xmm5, %xmm4, %xmm7
	vsubss	%xmm7, %xmm4, %xmm8
	vsubss	%xmm8, %xmm5, %xmm5
	vsubss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm3
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm1, 208(%rsp)                # 4-byte Spill
	vaddss	%xmm2, %xmm6, %xmm3
	vsubss	%xmm3, %xmm6, %xmm4
	vaddss	%xmm2, %xmm4, %xmm5
	vaddss	%xmm3, %xmm0, %xmm8
	vsubss	%xmm8, %xmm0, %xmm0
	vaddss	%xmm3, %xmm0, %xmm3
	vmulss	%xmm12, %xmm8, %xmm1
	vmovss	%xmm1, 4(%rsp)                  # 4-byte Spill
	vmovaps	%xmm12, %xmm0
	vfmsub213ss	%xmm1, %xmm8, %xmm0     # xmm0 = (xmm8 * xmm0) - xmm1
	vmovss	16(%rsp), %xmm1                 # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm8, %xmm2
	vaddss	%xmm2, %xmm0, %xmm4
	vsubss	%xmm0, %xmm4, %xmm6
	vsubss	%xmm6, %xmm4, %xmm9
	vsubss	%xmm9, %xmm0, %xmm0
	vmovaps	%xmm1, %xmm9
	vmovaps	%xmm1, %xmm7
	vfmsub213ss	%xmm2, %xmm8, %xmm9     # xmm9 = (xmm8 * xmm9) - xmm2
	vsubss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vmulss	%xmm3, %xmm12, %xmm6
	vaddss	%xmm6, %xmm4, %xmm2
	vsubss	%xmm4, %xmm2, %xmm10
	vsubss	%xmm10, %xmm2, %xmm11
	vsubss	%xmm11, %xmm4, %xmm4
	vmovaps	%xmm12, %xmm11
	vfmsub213ss	%xmm6, %xmm3, %xmm11    # xmm11 = (xmm3 * xmm11) - xmm6
	vsubss	%xmm10, %xmm6, %xmm6
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm4, %xmm0, %xmm6
	vsubss	%xmm0, %xmm6, %xmm10
	vsubss	%xmm10, %xmm6, %xmm14
	vsubss	%xmm14, %xmm0, %xmm0
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm0, %xmm0
	vmovss	40(%rsp), %xmm10                # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vmulss	%xmm10, %xmm8, %xmm4
	vfmsub213ss	%xmm4, %xmm8, %xmm10    # xmm10 = (xmm8 * xmm10) - xmm4
	vaddss	%xmm0, %xmm10, %xmm0
	vmulss	%xmm1, %xmm3, %xmm10
	vmovaps	%xmm1, %xmm14
	vmovaps	%xmm1, %xmm13
	vfmsub213ss	%xmm10, %xmm3, %xmm14   # xmm14 = (xmm3 * xmm14) - xmm10
	vaddss	%xmm0, %xmm14, %xmm0
	vmulss	%xmm5, %xmm12, %xmm14
	vmovaps	%xmm12, %xmm15
	vfmsub213ss	%xmm14, %xmm5, %xmm15   # xmm15 = (xmm5 * xmm15) - xmm14
	vaddss	%xmm0, %xmm15, %xmm15
	vaddss	%xmm11, %xmm9, %xmm0
	vsubss	%xmm9, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm7
	vsubss	%xmm7, %xmm9, %xmm7
	vsubss	%xmm1, %xmm11, %xmm1
	vaddss	%xmm1, %xmm7, %xmm1
	vaddss	%xmm14, %xmm10, %xmm7
	vsubss	%xmm10, %xmm7, %xmm9
	vsubss	%xmm9, %xmm7, %xmm11
	vsubss	%xmm11, %xmm10, %xmm10
	vsubss	%xmm9, %xmm14, %xmm9
	vaddss	%xmm9, %xmm10, %xmm9
	vaddss	%xmm4, %xmm0, %xmm10
	vsubss	%xmm0, %xmm10, %xmm11
	vsubss	%xmm11, %xmm10, %xmm14
	vsubss	%xmm14, %xmm0, %xmm0
	vsubss	%xmm11, %xmm4, %xmm4
	vaddss	%xmm4, %xmm0, %xmm4
	vaddss	%xmm6, %xmm7, %xmm11
	vsubss	%xmm7, %xmm11, %xmm0
	vsubss	%xmm0, %xmm11, %xmm14
	vsubss	%xmm14, %xmm7, %xmm7
	vsubss	%xmm0, %xmm6, %xmm0
	vaddss	%xmm0, %xmm7, %xmm6
	vaddss	%xmm11, %xmm10, %xmm0
	vsubss	%xmm10, %xmm0, %xmm7
	vsubss	%xmm7, %xmm0, %xmm14
	vmovss	%xmm0, 8(%rsp)                  # 4-byte Spill
	vsubss	%xmm14, %xmm10, %xmm10
	vsubss	%xmm7, %xmm11, %xmm7
	vaddss	%xmm7, %xmm10, %xmm7
	vaddss	%xmm1, %xmm15, %xmm1
	vaddss	%xmm1, %xmm9, %xmm1
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm6
	vfmadd231ss	208(%rsp), %xmm12, %xmm6 # 4-byte Folded Reload
                                        # xmm6 = (xmm12 * mem) + xmm6
	vfmadd231ss	%xmm13, %xmm5, %xmm6    # xmm6 = (xmm5 * xmm13) + xmm6
	vfmadd231ss	40(%rsp), %xmm3, %xmm6  # 4-byte Folded Reload
                                        # xmm6 = (xmm3 * mem) + xmm6
	vfmadd231ss	32(%rsp), %xmm8, %xmm6  # 4-byte Folded Reload
                                        # xmm6 = (xmm8 * mem) + xmm6
	vaddss	%xmm6, %xmm0, %xmm1
	vaddss	%xmm1, %xmm2, %xmm3
	vmovaps	%xmm1, %xmm7
	vmovss	%xmm1, 208(%rsp)                # 4-byte Spill
	vsubss	%xmm3, %xmm2, %xmm5
	vmovss	4(%rsp), %xmm0                  # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vaddss	%xmm3, %xmm0, %xmm1
	vsubss	%xmm1, %xmm0, %xmm2
	vaddss	%xmm3, %xmm2, %xmm3
	vmovss	256(%rsp), %xmm8                # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm8, %xmm0
	vmovss	%xmm0, 4(%rsp)                  # 4-byte Spill
	vmovaps	%xmm1, %xmm2
	vfmsub213ss	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm2) - xmm0
	vmulss	%xmm3, %xmm8, %xmm4
	vaddss	%xmm4, %xmm2, %xmm9
	vsubss	%xmm2, %xmm9, %xmm10
	vsubss	%xmm10, %xmm9, %xmm11
	vsubss	%xmm11, %xmm2, %xmm2
	vmovaps	%xmm3, %xmm11
	vfmsub213ss	%xmm4, %xmm8, %xmm11    # xmm11 = (xmm8 * xmm11) - xmm4
	vsubss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm4
	vmovss	224(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm0, %xmm10
	vaddss	%xmm10, %xmm9, %xmm2
	vsubss	%xmm9, %xmm2, %xmm12
	vsubss	%xmm12, %xmm2, %xmm13
	vsubss	%xmm13, %xmm9, %xmm9
	vmovaps	%xmm1, %xmm13
	vfmsub213ss	%xmm10, %xmm0, %xmm13   # xmm13 = (xmm0 * xmm13) - xmm10
	vsubss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm4, %xmm9, %xmm10
	vsubss	%xmm4, %xmm10, %xmm12
	vsubss	%xmm12, %xmm10, %xmm14
	vsubss	%xmm14, %xmm4, %xmm4
	vsubss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm4, %xmm9, %xmm4
	vaddss	%xmm7, %xmm5, %xmm5
	vmulss	%xmm5, %xmm8, %xmm9
	vmovaps	%xmm5, %xmm12
	vfmsub213ss	%xmm9, %xmm8, %xmm12    # xmm12 = (xmm8 * xmm12) - xmm9
	vaddss	%xmm4, %xmm12, %xmm4
	vmulss	%xmm3, %xmm0, %xmm12
	vmovaps	%xmm3, %xmm14
	vfmsub213ss	%xmm12, %xmm0, %xmm14   # xmm14 = (xmm0 * xmm14) - xmm12
	vaddss	%xmm4, %xmm14, %xmm4
	vmovss	48(%rsp), %xmm0                 # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm0, %xmm14
	vmovaps	%xmm1, %xmm15
	vfmsub213ss	%xmm14, %xmm0, %xmm15   # xmm15 = (xmm0 * xmm15) - xmm14
	vaddss	%xmm4, %xmm15, %xmm4
	vaddss	%xmm13, %xmm11, %xmm15
	vsubss	%xmm11, %xmm15, %xmm0
	vsubss	%xmm0, %xmm15, %xmm7
	vsubss	%xmm7, %xmm11, %xmm7
	vsubss	%xmm0, %xmm13, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vaddss	%xmm14, %xmm12, %xmm7
	vsubss	%xmm12, %xmm7, %xmm11
	vsubss	%xmm11, %xmm7, %xmm13
	vsubss	%xmm13, %xmm12, %xmm12
	vsubss	%xmm11, %xmm14, %xmm11
	vaddss	%xmm11, %xmm12, %xmm11
	vaddss	%xmm9, %xmm15, %xmm12
	vsubss	%xmm15, %xmm12, %xmm13
	vsubss	%xmm13, %xmm12, %xmm14
	vsubss	%xmm14, %xmm15, %xmm14
	vsubss	%xmm13, %xmm9, %xmm9
	vaddss	%xmm9, %xmm14, %xmm9
	vaddss	%xmm7, %xmm10, %xmm13
	vsubss	%xmm7, %xmm13, %xmm14
	vsubss	%xmm14, %xmm13, %xmm15
	vsubss	%xmm15, %xmm7, %xmm7
	vsubss	%xmm14, %xmm10, %xmm10
	vaddss	%xmm7, %xmm10, %xmm7
	vaddss	%xmm13, %xmm12, %xmm10
	vsubss	%xmm12, %xmm10, %xmm14
	vsubss	%xmm14, %xmm10, %xmm15
	vsubss	%xmm15, %xmm12, %xmm12
	vsubss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm0, %xmm4, %xmm0
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm0, %xmm12, %xmm0
	vfmadd231ss	160(%rsp), %xmm1, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm1 * mem) + xmm0
	vfmadd231ss	48(%rsp), %xmm3, %xmm0  # 4-byte Folded Reload
                                        # xmm0 = (xmm3 * mem) + xmm0
	vfmadd231ss	224(%rsp), %xmm5, %xmm0 # 4-byte Folded Reload
                                        # xmm0 = (xmm5 * mem) + xmm0
	vmovss	8(%rsp), %xmm1                  # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	208(%rsp), %xmm1, %xmm1         # 4-byte Folded Reload
	vaddss	%xmm6, %xmm1, %xmm1
	vfmadd231ss	%xmm1, %xmm8, %xmm0     # xmm0 = (xmm8 * xmm1) + xmm0
	vaddss	%xmm0, %xmm10, %xmm1
	vsubss	%xmm1, %xmm10, %xmm3
	vaddss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm1, %xmm2, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm1, %xmm2, %xmm1
	vmovss	4(%rsp), %xmm4                  # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vaddss	%xmm0, %xmm4, %xmm2
	vsubss	%xmm2, %xmm4, %xmm4
	movl	$2, %eax
	vxorps	%xmm6, %xmm6, %xmm6
	vcvtsi2ss	%eax, %xmm6, %xmm5
	vaddss	%xmm0, %xmm4, %xmm0
	vmulss	152(%rsp), %xmm5, %xmm4         # 4-byte Folded Reload
	vmulss	%xmm4, %xmm2, %xmm2
	vmulss	%xmm4, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0],xmm0[0],xmm2[2,3]
	vmulss	%xmm4, %xmm1, %xmm1
	vmulss	%xmm4, %xmm3, %xmm2
	vinsertps	$16, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[2,3]
.LBB42_340:
	vmovlhps	%xmm1, %xmm0, %xmm0             # xmm0 = xmm0[0],xmm1[0]
	vmovups	%xmm0, 112(%rsp)
	leaq	384(%rsp), %r14
	movq	%r14, 368(%rsp)
	movq	$32, 80(%rsp)
.Ltmp6866:
	leaq	368(%rsp), %rdi
	leaq	80(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp6867:
# %bb.341:
	movq	%rax, 368(%rsp)
	movq	80(%rsp), %rcx
	movq	%rcx, 384(%rsp)
	vmovupd	.L.str.63(%rip), %ymm0
	vmovupd	%ymm0, (%rax)
	movq	%rcx, 376(%rsp)
	movq	368(%rsp), %rax
	movb	$0, (%rax,%rcx)
.Ltmp6869:
	leaq	368(%rsp), %rdi
	leaq	112(%rsp), %rsi
	xorl	%edx, %edx
	vzeroupper
	callq	_Z5printIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp6870:
# %bb.342:
	movq	368(%rsp), %rdi
	cmpq	%r14, %rdi
	je	.LBB42_344
# %bb.343:
	callq	_ZdlPv
.LBB42_344:
	addq	$568, %rsp                      # imm = 0x238
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB42_345:
	.cfi_def_cfa_offset 624
.Ltmp6865:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_346:
.Ltmp6859:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_347:
.Ltmp6816:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_348:
.Ltmp6710:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_349:
.Ltmp6704:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_350:
.Ltmp6661:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_351:
.Ltmp6552:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_352:
.Ltmp6549:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_353:
.Ltmp6546:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_354:
.Ltmp6543:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_355:
.Ltmp6488:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_356:
.Ltmp6485:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_357:
.Ltmp6482:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_358:
.Ltmp6479:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_359:
.Ltmp6424:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_360:
.Ltmp6421:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_361:
.Ltmp6418:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_362:
.Ltmp6415:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_363:
.Ltmp6871:
	movq	%rax, %rbx
	movq	368(%rsp), %rdi
	cmpq	%r14, %rdi
	jne	.LBB42_381
	jmp	.LBB42_382
.LBB42_364:
.Ltmp6868:
	movq	%rax, %rdi
	callq	_Unwind_Resume@PLT
.LBB42_365:
.Ltmp6862:
	movq	%rax, %rbx
	movq	408(%rsp), %rdi
	jmp	.LBB42_369
.LBB42_366:
.Ltmp6856:
	movq	%rax, %rbx
	jmp	.LBB42_441
.LBB42_367:
.Ltmp6800:
	movq	%rax, %rbx
	jmp	.LBB42_441
.LBB42_368:
.Ltmp6707:
	movq	%rax, %rbx
	movq	440(%rsp), %rdi
.LBB42_369:
	cmpq	%r15, %rdi
	je	.LBB42_447
# %bb.370:
	callq	_ZdlPv
	jmp	.LBB42_447
.LBB42_371:
.Ltmp6701:
	movq	%rax, %rbx
	jmp	.LBB42_441
.LBB42_372:
.Ltmp6645:
	movq	%rax, %rbx
	jmp	.LBB42_441
.LBB42_373:
.Ltmp6555:
	movq	%rax, %rbx
	movq	472(%rsp), %rdi
	jmp	.LBB42_380
.LBB42_374:
.Ltmp6540:
	jmp	.LBB42_385
.LBB42_375:
.Ltmp6537:
	jmp	.LBB42_385
.LBB42_376:
.Ltmp6491:
	movq	%rax, %rbx
	movq	504(%rsp), %rdi
	cmpq	%r15, %rdi
	jne	.LBB42_381
	jmp	.LBB42_382
.LBB42_377:
.Ltmp6476:
	jmp	.LBB42_385
.LBB42_378:
.Ltmp6473:
	jmp	.LBB42_385
.LBB42_379:
.Ltmp6427:
	movq	%rax, %rbx
	movq	536(%rsp), %rdi
.LBB42_380:
	cmpq	%r12, %rdi
	je	.LBB42_382
.LBB42_381:
	callq	_ZdlPv
.LBB42_382:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB42_383:
.Ltmp6412:
	jmp	.LBB42_385
.LBB42_384:
.Ltmp6409:
.LBB42_385:
	movq	%rax, %rbx
	leaq	112(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB42_408
.LBB42_386:
.Ltmp6853:
	movq	%rax, %rbx
	jmp	.LBB42_441
.LBB42_387:
.Ltmp6698:
	movq	%rax, %rbx
	jmp	.LBB42_441
.LBB42_388:
.Ltmp6523:
	jmp	.LBB42_407
.LBB42_389:
.Ltmp6507:
	movq	%rax, %rbx
	jmp	.LBB42_409
.LBB42_390:
.Ltmp6459:
	jmp	.LBB42_407
.LBB42_391:
.Ltmp6443:
	movq	%rax, %rbx
	jmp	.LBB42_409
.LBB42_392:
.Ltmp6395:
	jmp	.LBB42_407
.LBB42_393:
.Ltmp6379:
	movq	%rax, %rbx
	jmp	.LBB42_409
.LBB42_394:
.Ltmp6813:
	movq	%rax, %rbx
	jmp	.LBB42_441
.LBB42_395:
.Ltmp6658:
	movq	%rax, %rbx
	jmp	.LBB42_441
.LBB42_396:
.Ltmp6848:
	movq	%rax, %rbx
	jmp	.LBB42_447
.LBB42_397:
.Ltmp6797:
	movq	%rax, %rbx
	jmp	.LBB42_447
.LBB42_398:
.Ltmp6693:
	movq	%rax, %rbx
	jmp	.LBB42_447
.LBB42_399:
.Ltmp6642:
	movq	%rax, %rbx
	jmp	.LBB42_447
.LBB42_400:
.Ltmp6534:
	jmp	.LBB42_407
.LBB42_401:
.Ltmp6518:
	movq	%rax, %rbx
	jmp	.LBB42_409
.LBB42_402:
.Ltmp6502:
	jmp	.LBB42_412
.LBB42_403:
.Ltmp6470:
	jmp	.LBB42_407
.LBB42_404:
.Ltmp6454:
	movq	%rax, %rbx
	jmp	.LBB42_409
.LBB42_405:
.Ltmp6438:
	jmp	.LBB42_412
.LBB42_406:
.Ltmp6406:
.LBB42_407:
	movq	%rax, %rbx
.LBB42_408:
	leaq	80(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB42_409:
	leaq	176(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	leaq	280(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB42_410:
.Ltmp6390:
	movq	%rax, %rbx
	jmp	.LBB42_409
.LBB42_411:
.Ltmp6374:
.LBB42_412:
	movq	%rax, %rbx
	leaq	280(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB42_413:
.Ltmp6837:
	movq	%rax, %rbx
	jmp	.LBB42_447
.LBB42_414:
.Ltmp6682:
	movq	%rax, %rbx
	jmp	.LBB42_447
.LBB42_415:
.Ltmp6786:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_416:
.Ltmp6783:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_417:
.Ltmp6780:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_418:
.Ltmp6732:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_419:
.Ltmp6631:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_420:
.Ltmp6628:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_421:
.Ltmp6625:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_422:
.Ltmp6577:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB42_423:
.Ltmp6713:
	movq	%rax, %rbx
	jmp	.LBB42_447
.LBB42_424:
.Ltmp6558:
	movq	%rax, %rbx
	jmp	.LBB42_447
.LBB42_425:
.Ltmp6764:
	jmp	.LBB42_434
.LBB42_426:
.Ltmp6748:
	jmp	.LBB42_439
.LBB42_427:
.Ltmp6729:
	jmp	.LBB42_431
.LBB42_428:
.Ltmp6609:
	jmp	.LBB42_434
.LBB42_429:
.Ltmp6593:
	jmp	.LBB42_439
.LBB42_430:
.Ltmp6574:
.LBB42_431:
	movq	%rax, %rbx
	leaq	80(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB42_445
.LBB42_432:
.Ltmp6777:
	jmp	.LBB42_434
.LBB42_433:
.Ltmp6622:
.LBB42_434:
	movq	%rax, %rbx
	leaq	176(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB42_440
.LBB42_435:
.Ltmp6759:
	jmp	.LBB42_439
.LBB42_436:
.Ltmp6743:
	movq	%rax, %rbx
	jmp	.LBB42_441
.LBB42_437:
.Ltmp6724:
	jmp	.LBB42_444
.LBB42_438:
.Ltmp6604:
.LBB42_439:
	movq	%rax, %rbx
.LBB42_440:
	leaq	280(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB42_441:
	leaq	80(%rsp), %rdi
	jmp	.LBB42_446
.LBB42_442:
.Ltmp6588:
	movq	%rax, %rbx
	jmp	.LBB42_441
.LBB42_443:
.Ltmp6569:
.LBB42_444:
	movq	%rax, %rbx
.LBB42_445:
	leaq	176(%rsp), %rdi
.LBB42_446:
	callq	_ZN4mpfr6mprealD2Ev
.LBB42_447:
	leaq	112(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end42:
	.size	_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_, .Lfunc_end42-_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.cfi_endproc
	.section	.gcc_except_table._Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"aG",@progbits,_Z6verifyIN7mX_real7qX_real7qx_realIfLNS0_9AlgorithmE0EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.p2align	2, 0x0
GCC_except_table42:
.Lexception35:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase24-.Lttbaseref24
.Lttbaseref24:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end35-.Lcst_begin35
.Lcst_begin35:
	.uleb128 .Lfunc_begin35-.Lfunc_begin35  # >> Call Site 1 <<
	.uleb128 .Ltmp6364-.Lfunc_begin35       #   Call between .Lfunc_begin35 and .Ltmp6364
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6364-.Lfunc_begin35       # >> Call Site 2 <<
	.uleb128 .Ltmp6373-.Ltmp6364            #   Call between .Ltmp6364 and .Ltmp6373
	.uleb128 .Ltmp6374-.Lfunc_begin35       #     jumps to .Ltmp6374
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6375-.Lfunc_begin35       # >> Call Site 3 <<
	.uleb128 .Ltmp6378-.Ltmp6375            #   Call between .Ltmp6375 and .Ltmp6378
	.uleb128 .Ltmp6379-.Lfunc_begin35       #     jumps to .Ltmp6379
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6380-.Lfunc_begin35       # >> Call Site 4 <<
	.uleb128 .Ltmp6389-.Ltmp6380            #   Call between .Ltmp6380 and .Ltmp6389
	.uleb128 .Ltmp6390-.Lfunc_begin35       #     jumps to .Ltmp6390
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6391-.Lfunc_begin35       # >> Call Site 5 <<
	.uleb128 .Ltmp6394-.Ltmp6391            #   Call between .Ltmp6391 and .Ltmp6394
	.uleb128 .Ltmp6395-.Lfunc_begin35       #     jumps to .Ltmp6395
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6396-.Lfunc_begin35       # >> Call Site 6 <<
	.uleb128 .Ltmp6405-.Ltmp6396            #   Call between .Ltmp6396 and .Ltmp6405
	.uleb128 .Ltmp6406-.Lfunc_begin35       #     jumps to .Ltmp6406
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6407-.Lfunc_begin35       # >> Call Site 7 <<
	.uleb128 .Ltmp6408-.Ltmp6407            #   Call between .Ltmp6407 and .Ltmp6408
	.uleb128 .Ltmp6409-.Lfunc_begin35       #     jumps to .Ltmp6409
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6410-.Lfunc_begin35       # >> Call Site 8 <<
	.uleb128 .Ltmp6411-.Ltmp6410            #   Call between .Ltmp6410 and .Ltmp6411
	.uleb128 .Ltmp6412-.Lfunc_begin35       #     jumps to .Ltmp6412
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6413-.Lfunc_begin35       # >> Call Site 9 <<
	.uleb128 .Ltmp6414-.Ltmp6413            #   Call between .Ltmp6413 and .Ltmp6414
	.uleb128 .Ltmp6415-.Lfunc_begin35       #     jumps to .Ltmp6415
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6416-.Lfunc_begin35       # >> Call Site 10 <<
	.uleb128 .Ltmp6417-.Ltmp6416            #   Call between .Ltmp6416 and .Ltmp6417
	.uleb128 .Ltmp6418-.Lfunc_begin35       #     jumps to .Ltmp6418
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6419-.Lfunc_begin35       # >> Call Site 11 <<
	.uleb128 .Ltmp6420-.Ltmp6419            #   Call between .Ltmp6419 and .Ltmp6420
	.uleb128 .Ltmp6421-.Lfunc_begin35       #     jumps to .Ltmp6421
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6422-.Lfunc_begin35       # >> Call Site 12 <<
	.uleb128 .Ltmp6423-.Ltmp6422            #   Call between .Ltmp6422 and .Ltmp6423
	.uleb128 .Ltmp6424-.Lfunc_begin35       #     jumps to .Ltmp6424
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6425-.Lfunc_begin35       # >> Call Site 13 <<
	.uleb128 .Ltmp6426-.Ltmp6425            #   Call between .Ltmp6425 and .Ltmp6426
	.uleb128 .Ltmp6427-.Lfunc_begin35       #     jumps to .Ltmp6427
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6426-.Lfunc_begin35       # >> Call Site 14 <<
	.uleb128 .Ltmp6428-.Ltmp6426            #   Call between .Ltmp6426 and .Ltmp6428
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6428-.Lfunc_begin35       # >> Call Site 15 <<
	.uleb128 .Ltmp6437-.Ltmp6428            #   Call between .Ltmp6428 and .Ltmp6437
	.uleb128 .Ltmp6438-.Lfunc_begin35       #     jumps to .Ltmp6438
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6439-.Lfunc_begin35       # >> Call Site 16 <<
	.uleb128 .Ltmp6442-.Ltmp6439            #   Call between .Ltmp6439 and .Ltmp6442
	.uleb128 .Ltmp6443-.Lfunc_begin35       #     jumps to .Ltmp6443
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6444-.Lfunc_begin35       # >> Call Site 17 <<
	.uleb128 .Ltmp6453-.Ltmp6444            #   Call between .Ltmp6444 and .Ltmp6453
	.uleb128 .Ltmp6454-.Lfunc_begin35       #     jumps to .Ltmp6454
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6455-.Lfunc_begin35       # >> Call Site 18 <<
	.uleb128 .Ltmp6458-.Ltmp6455            #   Call between .Ltmp6455 and .Ltmp6458
	.uleb128 .Ltmp6459-.Lfunc_begin35       #     jumps to .Ltmp6459
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6460-.Lfunc_begin35       # >> Call Site 19 <<
	.uleb128 .Ltmp6469-.Ltmp6460            #   Call between .Ltmp6460 and .Ltmp6469
	.uleb128 .Ltmp6470-.Lfunc_begin35       #     jumps to .Ltmp6470
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6471-.Lfunc_begin35       # >> Call Site 20 <<
	.uleb128 .Ltmp6472-.Ltmp6471            #   Call between .Ltmp6471 and .Ltmp6472
	.uleb128 .Ltmp6473-.Lfunc_begin35       #     jumps to .Ltmp6473
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6474-.Lfunc_begin35       # >> Call Site 21 <<
	.uleb128 .Ltmp6475-.Ltmp6474            #   Call between .Ltmp6474 and .Ltmp6475
	.uleb128 .Ltmp6476-.Lfunc_begin35       #     jumps to .Ltmp6476
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6477-.Lfunc_begin35       # >> Call Site 22 <<
	.uleb128 .Ltmp6478-.Ltmp6477            #   Call between .Ltmp6477 and .Ltmp6478
	.uleb128 .Ltmp6479-.Lfunc_begin35       #     jumps to .Ltmp6479
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6480-.Lfunc_begin35       # >> Call Site 23 <<
	.uleb128 .Ltmp6481-.Ltmp6480            #   Call between .Ltmp6480 and .Ltmp6481
	.uleb128 .Ltmp6482-.Lfunc_begin35       #     jumps to .Ltmp6482
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6483-.Lfunc_begin35       # >> Call Site 24 <<
	.uleb128 .Ltmp6484-.Ltmp6483            #   Call between .Ltmp6483 and .Ltmp6484
	.uleb128 .Ltmp6485-.Lfunc_begin35       #     jumps to .Ltmp6485
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6486-.Lfunc_begin35       # >> Call Site 25 <<
	.uleb128 .Ltmp6487-.Ltmp6486            #   Call between .Ltmp6486 and .Ltmp6487
	.uleb128 .Ltmp6488-.Lfunc_begin35       #     jumps to .Ltmp6488
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6489-.Lfunc_begin35       # >> Call Site 26 <<
	.uleb128 .Ltmp6490-.Ltmp6489            #   Call between .Ltmp6489 and .Ltmp6490
	.uleb128 .Ltmp6491-.Lfunc_begin35       #     jumps to .Ltmp6491
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6490-.Lfunc_begin35       # >> Call Site 27 <<
	.uleb128 .Ltmp6492-.Ltmp6490            #   Call between .Ltmp6490 and .Ltmp6492
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6492-.Lfunc_begin35       # >> Call Site 28 <<
	.uleb128 .Ltmp6501-.Ltmp6492            #   Call between .Ltmp6492 and .Ltmp6501
	.uleb128 .Ltmp6502-.Lfunc_begin35       #     jumps to .Ltmp6502
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6503-.Lfunc_begin35       # >> Call Site 29 <<
	.uleb128 .Ltmp6506-.Ltmp6503            #   Call between .Ltmp6503 and .Ltmp6506
	.uleb128 .Ltmp6507-.Lfunc_begin35       #     jumps to .Ltmp6507
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6508-.Lfunc_begin35       # >> Call Site 30 <<
	.uleb128 .Ltmp6517-.Ltmp6508            #   Call between .Ltmp6508 and .Ltmp6517
	.uleb128 .Ltmp6518-.Lfunc_begin35       #     jumps to .Ltmp6518
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6519-.Lfunc_begin35       # >> Call Site 31 <<
	.uleb128 .Ltmp6522-.Ltmp6519            #   Call between .Ltmp6519 and .Ltmp6522
	.uleb128 .Ltmp6523-.Lfunc_begin35       #     jumps to .Ltmp6523
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6524-.Lfunc_begin35       # >> Call Site 32 <<
	.uleb128 .Ltmp6533-.Ltmp6524            #   Call between .Ltmp6524 and .Ltmp6533
	.uleb128 .Ltmp6534-.Lfunc_begin35       #     jumps to .Ltmp6534
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6535-.Lfunc_begin35       # >> Call Site 33 <<
	.uleb128 .Ltmp6536-.Ltmp6535            #   Call between .Ltmp6535 and .Ltmp6536
	.uleb128 .Ltmp6537-.Lfunc_begin35       #     jumps to .Ltmp6537
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6538-.Lfunc_begin35       # >> Call Site 34 <<
	.uleb128 .Ltmp6539-.Ltmp6538            #   Call between .Ltmp6538 and .Ltmp6539
	.uleb128 .Ltmp6540-.Lfunc_begin35       #     jumps to .Ltmp6540
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6541-.Lfunc_begin35       # >> Call Site 35 <<
	.uleb128 .Ltmp6542-.Ltmp6541            #   Call between .Ltmp6541 and .Ltmp6542
	.uleb128 .Ltmp6543-.Lfunc_begin35       #     jumps to .Ltmp6543
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6544-.Lfunc_begin35       # >> Call Site 36 <<
	.uleb128 .Ltmp6545-.Ltmp6544            #   Call between .Ltmp6544 and .Ltmp6545
	.uleb128 .Ltmp6546-.Lfunc_begin35       #     jumps to .Ltmp6546
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6547-.Lfunc_begin35       # >> Call Site 37 <<
	.uleb128 .Ltmp6548-.Ltmp6547            #   Call between .Ltmp6547 and .Ltmp6548
	.uleb128 .Ltmp6549-.Lfunc_begin35       #     jumps to .Ltmp6549
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6550-.Lfunc_begin35       # >> Call Site 38 <<
	.uleb128 .Ltmp6551-.Ltmp6550            #   Call between .Ltmp6550 and .Ltmp6551
	.uleb128 .Ltmp6552-.Lfunc_begin35       #     jumps to .Ltmp6552
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6553-.Lfunc_begin35       # >> Call Site 39 <<
	.uleb128 .Ltmp6554-.Ltmp6553            #   Call between .Ltmp6553 and .Ltmp6554
	.uleb128 .Ltmp6555-.Lfunc_begin35       #     jumps to .Ltmp6555
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6554-.Lfunc_begin35       # >> Call Site 40 <<
	.uleb128 .Ltmp6556-.Ltmp6554            #   Call between .Ltmp6554 and .Ltmp6556
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6556-.Lfunc_begin35       # >> Call Site 41 <<
	.uleb128 .Ltmp6557-.Ltmp6556            #   Call between .Ltmp6556 and .Ltmp6557
	.uleb128 .Ltmp6558-.Lfunc_begin35       #     jumps to .Ltmp6558
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6559-.Lfunc_begin35       # >> Call Site 42 <<
	.uleb128 .Ltmp6568-.Ltmp6559            #   Call between .Ltmp6559 and .Ltmp6568
	.uleb128 .Ltmp6569-.Lfunc_begin35       #     jumps to .Ltmp6569
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6570-.Lfunc_begin35       # >> Call Site 43 <<
	.uleb128 .Ltmp6573-.Ltmp6570            #   Call between .Ltmp6570 and .Ltmp6573
	.uleb128 .Ltmp6574-.Lfunc_begin35       #     jumps to .Ltmp6574
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6575-.Lfunc_begin35       # >> Call Site 44 <<
	.uleb128 .Ltmp6576-.Ltmp6575            #   Call between .Ltmp6575 and .Ltmp6576
	.uleb128 .Ltmp6577-.Lfunc_begin35       #     jumps to .Ltmp6577
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6578-.Lfunc_begin35       # >> Call Site 45 <<
	.uleb128 .Ltmp6587-.Ltmp6578            #   Call between .Ltmp6578 and .Ltmp6587
	.uleb128 .Ltmp6588-.Lfunc_begin35       #     jumps to .Ltmp6588
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6589-.Lfunc_begin35       # >> Call Site 46 <<
	.uleb128 .Ltmp6592-.Ltmp6589            #   Call between .Ltmp6589 and .Ltmp6592
	.uleb128 .Ltmp6593-.Lfunc_begin35       #     jumps to .Ltmp6593
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6594-.Lfunc_begin35       # >> Call Site 47 <<
	.uleb128 .Ltmp6603-.Ltmp6594            #   Call between .Ltmp6594 and .Ltmp6603
	.uleb128 .Ltmp6604-.Lfunc_begin35       #     jumps to .Ltmp6604
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6605-.Lfunc_begin35       # >> Call Site 48 <<
	.uleb128 .Ltmp6608-.Ltmp6605            #   Call between .Ltmp6605 and .Ltmp6608
	.uleb128 .Ltmp6609-.Lfunc_begin35       #     jumps to .Ltmp6609
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6610-.Lfunc_begin35       # >> Call Site 49 <<
	.uleb128 .Ltmp6621-.Ltmp6610            #   Call between .Ltmp6610 and .Ltmp6621
	.uleb128 .Ltmp6622-.Lfunc_begin35       #     jumps to .Ltmp6622
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6623-.Lfunc_begin35       # >> Call Site 50 <<
	.uleb128 .Ltmp6624-.Ltmp6623            #   Call between .Ltmp6623 and .Ltmp6624
	.uleb128 .Ltmp6625-.Lfunc_begin35       #     jumps to .Ltmp6625
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6626-.Lfunc_begin35       # >> Call Site 51 <<
	.uleb128 .Ltmp6627-.Ltmp6626            #   Call between .Ltmp6626 and .Ltmp6627
	.uleb128 .Ltmp6628-.Lfunc_begin35       #     jumps to .Ltmp6628
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6629-.Lfunc_begin35       # >> Call Site 52 <<
	.uleb128 .Ltmp6630-.Ltmp6629            #   Call between .Ltmp6629 and .Ltmp6630
	.uleb128 .Ltmp6631-.Lfunc_begin35       #     jumps to .Ltmp6631
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6632-.Lfunc_begin35       # >> Call Site 53 <<
	.uleb128 .Ltmp6641-.Ltmp6632            #   Call between .Ltmp6632 and .Ltmp6641
	.uleb128 .Ltmp6642-.Lfunc_begin35       #     jumps to .Ltmp6642
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6643-.Lfunc_begin35       # >> Call Site 54 <<
	.uleb128 .Ltmp6644-.Ltmp6643            #   Call between .Ltmp6643 and .Ltmp6644
	.uleb128 .Ltmp6645-.Lfunc_begin35       #     jumps to .Ltmp6645
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6646-.Lfunc_begin35       # >> Call Site 55 <<
	.uleb128 .Ltmp6657-.Ltmp6646            #   Call between .Ltmp6646 and .Ltmp6657
	.uleb128 .Ltmp6658-.Lfunc_begin35       #     jumps to .Ltmp6658
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6659-.Lfunc_begin35       # >> Call Site 56 <<
	.uleb128 .Ltmp6660-.Ltmp6659            #   Call between .Ltmp6659 and .Ltmp6660
	.uleb128 .Ltmp6661-.Lfunc_begin35       #     jumps to .Ltmp6661
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6662-.Lfunc_begin35       # >> Call Site 57 <<
	.uleb128 .Ltmp6681-.Ltmp6662            #   Call between .Ltmp6662 and .Ltmp6681
	.uleb128 .Ltmp6682-.Lfunc_begin35       #     jumps to .Ltmp6682
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6683-.Lfunc_begin35       # >> Call Site 58 <<
	.uleb128 .Ltmp6692-.Ltmp6683            #   Call between .Ltmp6683 and .Ltmp6692
	.uleb128 .Ltmp6693-.Lfunc_begin35       #     jumps to .Ltmp6693
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6694-.Lfunc_begin35       # >> Call Site 59 <<
	.uleb128 .Ltmp6697-.Ltmp6694            #   Call between .Ltmp6694 and .Ltmp6697
	.uleb128 .Ltmp6698-.Lfunc_begin35       #     jumps to .Ltmp6698
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6699-.Lfunc_begin35       # >> Call Site 60 <<
	.uleb128 .Ltmp6700-.Ltmp6699            #   Call between .Ltmp6699 and .Ltmp6700
	.uleb128 .Ltmp6701-.Lfunc_begin35       #     jumps to .Ltmp6701
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6702-.Lfunc_begin35       # >> Call Site 61 <<
	.uleb128 .Ltmp6703-.Ltmp6702            #   Call between .Ltmp6702 and .Ltmp6703
	.uleb128 .Ltmp6704-.Lfunc_begin35       #     jumps to .Ltmp6704
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6705-.Lfunc_begin35       # >> Call Site 62 <<
	.uleb128 .Ltmp6706-.Ltmp6705            #   Call between .Ltmp6705 and .Ltmp6706
	.uleb128 .Ltmp6707-.Lfunc_begin35       #     jumps to .Ltmp6707
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6708-.Lfunc_begin35       # >> Call Site 63 <<
	.uleb128 .Ltmp6709-.Ltmp6708            #   Call between .Ltmp6708 and .Ltmp6709
	.uleb128 .Ltmp6710-.Lfunc_begin35       #     jumps to .Ltmp6710
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6709-.Lfunc_begin35       # >> Call Site 64 <<
	.uleb128 .Ltmp6711-.Ltmp6709            #   Call between .Ltmp6709 and .Ltmp6711
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6711-.Lfunc_begin35       # >> Call Site 65 <<
	.uleb128 .Ltmp6712-.Ltmp6711            #   Call between .Ltmp6711 and .Ltmp6712
	.uleb128 .Ltmp6713-.Lfunc_begin35       #     jumps to .Ltmp6713
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6714-.Lfunc_begin35       # >> Call Site 66 <<
	.uleb128 .Ltmp6723-.Ltmp6714            #   Call between .Ltmp6714 and .Ltmp6723
	.uleb128 .Ltmp6724-.Lfunc_begin35       #     jumps to .Ltmp6724
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6725-.Lfunc_begin35       # >> Call Site 67 <<
	.uleb128 .Ltmp6728-.Ltmp6725            #   Call between .Ltmp6725 and .Ltmp6728
	.uleb128 .Ltmp6729-.Lfunc_begin35       #     jumps to .Ltmp6729
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6730-.Lfunc_begin35       # >> Call Site 68 <<
	.uleb128 .Ltmp6731-.Ltmp6730            #   Call between .Ltmp6730 and .Ltmp6731
	.uleb128 .Ltmp6732-.Lfunc_begin35       #     jumps to .Ltmp6732
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6733-.Lfunc_begin35       # >> Call Site 69 <<
	.uleb128 .Ltmp6742-.Ltmp6733            #   Call between .Ltmp6733 and .Ltmp6742
	.uleb128 .Ltmp6743-.Lfunc_begin35       #     jumps to .Ltmp6743
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6744-.Lfunc_begin35       # >> Call Site 70 <<
	.uleb128 .Ltmp6747-.Ltmp6744            #   Call between .Ltmp6744 and .Ltmp6747
	.uleb128 .Ltmp6748-.Lfunc_begin35       #     jumps to .Ltmp6748
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6749-.Lfunc_begin35       # >> Call Site 71 <<
	.uleb128 .Ltmp6758-.Ltmp6749            #   Call between .Ltmp6749 and .Ltmp6758
	.uleb128 .Ltmp6759-.Lfunc_begin35       #     jumps to .Ltmp6759
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6760-.Lfunc_begin35       # >> Call Site 72 <<
	.uleb128 .Ltmp6763-.Ltmp6760            #   Call between .Ltmp6760 and .Ltmp6763
	.uleb128 .Ltmp6764-.Lfunc_begin35       #     jumps to .Ltmp6764
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6765-.Lfunc_begin35       # >> Call Site 73 <<
	.uleb128 .Ltmp6776-.Ltmp6765            #   Call between .Ltmp6765 and .Ltmp6776
	.uleb128 .Ltmp6777-.Lfunc_begin35       #     jumps to .Ltmp6777
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6778-.Lfunc_begin35       # >> Call Site 74 <<
	.uleb128 .Ltmp6779-.Ltmp6778            #   Call between .Ltmp6778 and .Ltmp6779
	.uleb128 .Ltmp6780-.Lfunc_begin35       #     jumps to .Ltmp6780
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6781-.Lfunc_begin35       # >> Call Site 75 <<
	.uleb128 .Ltmp6782-.Ltmp6781            #   Call between .Ltmp6781 and .Ltmp6782
	.uleb128 .Ltmp6783-.Lfunc_begin35       #     jumps to .Ltmp6783
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6784-.Lfunc_begin35       # >> Call Site 76 <<
	.uleb128 .Ltmp6785-.Ltmp6784            #   Call between .Ltmp6784 and .Ltmp6785
	.uleb128 .Ltmp6786-.Lfunc_begin35       #     jumps to .Ltmp6786
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6787-.Lfunc_begin35       # >> Call Site 77 <<
	.uleb128 .Ltmp6796-.Ltmp6787            #   Call between .Ltmp6787 and .Ltmp6796
	.uleb128 .Ltmp6797-.Lfunc_begin35       #     jumps to .Ltmp6797
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6798-.Lfunc_begin35       # >> Call Site 78 <<
	.uleb128 .Ltmp6799-.Ltmp6798            #   Call between .Ltmp6798 and .Ltmp6799
	.uleb128 .Ltmp6800-.Lfunc_begin35       #     jumps to .Ltmp6800
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6801-.Lfunc_begin35       # >> Call Site 79 <<
	.uleb128 .Ltmp6812-.Ltmp6801            #   Call between .Ltmp6801 and .Ltmp6812
	.uleb128 .Ltmp6813-.Lfunc_begin35       #     jumps to .Ltmp6813
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6814-.Lfunc_begin35       # >> Call Site 80 <<
	.uleb128 .Ltmp6815-.Ltmp6814            #   Call between .Ltmp6814 and .Ltmp6815
	.uleb128 .Ltmp6816-.Lfunc_begin35       #     jumps to .Ltmp6816
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6817-.Lfunc_begin35       # >> Call Site 81 <<
	.uleb128 .Ltmp6836-.Ltmp6817            #   Call between .Ltmp6817 and .Ltmp6836
	.uleb128 .Ltmp6837-.Lfunc_begin35       #     jumps to .Ltmp6837
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6838-.Lfunc_begin35       # >> Call Site 82 <<
	.uleb128 .Ltmp6847-.Ltmp6838            #   Call between .Ltmp6838 and .Ltmp6847
	.uleb128 .Ltmp6848-.Lfunc_begin35       #     jumps to .Ltmp6848
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6849-.Lfunc_begin35       # >> Call Site 83 <<
	.uleb128 .Ltmp6852-.Ltmp6849            #   Call between .Ltmp6849 and .Ltmp6852
	.uleb128 .Ltmp6853-.Lfunc_begin35       #     jumps to .Ltmp6853
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6854-.Lfunc_begin35       # >> Call Site 84 <<
	.uleb128 .Ltmp6855-.Ltmp6854            #   Call between .Ltmp6854 and .Ltmp6855
	.uleb128 .Ltmp6856-.Lfunc_begin35       #     jumps to .Ltmp6856
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6857-.Lfunc_begin35       # >> Call Site 85 <<
	.uleb128 .Ltmp6858-.Ltmp6857            #   Call between .Ltmp6857 and .Ltmp6858
	.uleb128 .Ltmp6859-.Lfunc_begin35       #     jumps to .Ltmp6859
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6860-.Lfunc_begin35       # >> Call Site 86 <<
	.uleb128 .Ltmp6861-.Ltmp6860            #   Call between .Ltmp6860 and .Ltmp6861
	.uleb128 .Ltmp6862-.Lfunc_begin35       #     jumps to .Ltmp6862
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6863-.Lfunc_begin35       # >> Call Site 87 <<
	.uleb128 .Ltmp6864-.Ltmp6863            #   Call between .Ltmp6863 and .Ltmp6864
	.uleb128 .Ltmp6865-.Lfunc_begin35       #     jumps to .Ltmp6865
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6864-.Lfunc_begin35       # >> Call Site 88 <<
	.uleb128 .Ltmp6866-.Ltmp6864            #   Call between .Ltmp6864 and .Ltmp6866
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6866-.Lfunc_begin35       # >> Call Site 89 <<
	.uleb128 .Ltmp6867-.Ltmp6866            #   Call between .Ltmp6866 and .Ltmp6867
	.uleb128 .Ltmp6868-.Lfunc_begin35       #     jumps to .Ltmp6868
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6869-.Lfunc_begin35       # >> Call Site 90 <<
	.uleb128 .Ltmp6870-.Ltmp6869            #   Call between .Ltmp6869 and .Ltmp6870
	.uleb128 .Ltmp6871-.Lfunc_begin35       #     jumps to .Ltmp6871
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6870-.Lfunc_begin35       # >> Call Site 91 <<
	.uleb128 .Lfunc_end42-.Ltmp6870         #   Call between .Ltmp6870 and .Lfunc_end42
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
.Lcst_end35:
	.byte	1                               # >> Action Record 1 <<
                                        #   Catch TypeInfo 1
	.byte	0                               #   No further actions
	.p2align	2, 0x0
                                        # >> Catch TypeInfos <<
	.long	0                               # TypeInfo 1
.Lttbase24:
	.p2align	2, 0x0
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4, 0x0                          # -- Begin function _Z6verifyIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
.LCPI43_0:
	.quad	0x8000000000000000              #  -0
	.quad	0x8000000000000000              #  -0
	.section	.text._Z6verifyIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"axG",@progbits,_Z6verifyIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.weak	_Z6verifyIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.p2align	4, 0x90
	.type	_Z6verifyIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,@function
_Z6verifyIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_: # 
.Lfunc_begin36:
	.cfi_startproc
	.cfi_personality 3, __gxx_personality_v0
	.cfi_lsda 3, .Lexception36
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$424, %rsp                      # imm = 0x1A8
	.cfi_def_cfa_offset 480
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, 120(%rsp)                  # 8-byte Spill
	movq	%rcx, %rbp
	movq	%rdx, %r12
	movq	%rsi, 8(%rsp)                   # 8-byte Spill
	movq	%rdi, %rbx
	vcvtsi2sdl	(%rdi), %xmm0, %xmm0
	callq	sqrt
	vcvttsd2si	%xmm0, %eax
	movl	%eax, 168(%rsp)                 # 4-byte Spill
	movq	%rbx, 112(%rsp)                 # 8-byte Spill
	movslq	(%rbx), %r15
	movq	%r15, %r13
	shlq	$4, %r13
	testq	%r15, %r15
	movq	$-1, %r14
	cmovnsq	%r13, %r14
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %rbx
	testq	%r15, %r15
	movq	%rbp, 184(%rsp)                 # 8-byte Spill
	je	.LBB43_4
# %bb.1:
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movq	%r13, %rdx
	callq	_intel_fast_memset@PLT
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %r14
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%r13, %rdx
	callq	_intel_fast_memset@PLT
	testl	%r15d, %r15d
	jle	.LBB43_5
# %bb.2:
	movl	$8, %ebp
	xorl	%r15d, %r15d
	movq	184(%rsp), %r13                 # 8-byte Reload
	.p2align	4, 0x90
.LBB43_3:                               # =>This Inner Loop Header: Depth=1
	movq	%r12, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, -8(%rbx,%rbp)
	vmovsd	%xmm1, (%rbx,%rbp)
	movq	%r13, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, -8(%r14,%rbp)
	vmovsd	%xmm1, (%r14,%rbp)
	incq	%r15
	movq	112(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$16, %rbp
	addq	$32, %r13
	addq	$32, %r12
	cmpq	%rax, %r15
	jl	.LBB43_3
	jmp	.LBB43_5
.LBB43_4:
	movq	%r14, %rdi
	callq	_Znam
	movq	%rax, %r14
.LBB43_5:
	movq	8(%rsp), %rdi                   # 8-byte Reload
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, 200(%rsp)                # 8-byte Spill
	vmovsd	%xmm1, 192(%rsp)                # 8-byte Spill
	movl	$160, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rax)
	vmovupd	%xmm0, 16(%rax)
	vmovupd	%xmm0, 32(%rax)
	vmovupd	%xmm0, 48(%rax)
	vmovupd	%xmm0, 64(%rax)
	vmovupd	%xmm0, 80(%rax)
	vmovupd	%xmm0, 96(%rax)
	vmovupd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 128(%rax)
	vmovupd	%xmm0, 144(%rax)
	callq	omp_get_wtime
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
	movq	112(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	movl	%eax, %ecx
	andl	$-4, %ecx
	movq	%rax, %rdx
	shlq	$4, %rdx
	andq	$-64, %rdx
	movl	%eax, %esi
	andl	$3, %esi
	leaq	8(%rdx), %rdi
	xorl	%r8d, %r8d
	vcvtsi2sd	%r8d, %xmm2, %xmm0
	jmp	.LBB43_8
	.p2align	4, 0x90
.LBB43_6:                               #   in Loop: Header=BB43_8 Depth=1
	vmovapd	%xmm0, %xmm3
.LBB43_7:                               #   in Loop: Header=BB43_8 Depth=1
	movq	%r8, %r9
	shlq	$4, %r9
	vmovsd	%xmm3, (%rbp,%r9)
	vmovsd	%xmm1, 8(%rbp,%r9)
	leaq	1(%r8), %r9
	cmpq	$9, %r8
	movq	%r9, %r8
	je	.LBB43_15
.LBB43_8:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB43_11 Depth 2
                                        #     Child Loop BB43_14 Depth 2
	vxorpd	%xmm1, %xmm1, %xmm1
	testl	%eax, %eax
	jle	.LBB43_6
# %bb.9:                                #   in Loop: Header=BB43_8 Depth=1
	vmovapd	%xmm0, %xmm2
	cmpl	$4, %eax
	jb	.LBB43_12
# %bb.10:                               #   in Loop: Header=BB43_8 Depth=1
	xorl	%r9d, %r9d
	vmovapd	%xmm0, %xmm2
	.p2align	4, 0x90
.LBB43_11:                              #   Parent Loop BB43_8 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	(%rbx,%r9), %xmm3               # xmm3 = mem[0],zero
	vmovsd	16(%rbx,%r9), %xmm4             # xmm4 = mem[0],zero
	vmovsd	(%r14,%r9), %xmm5               # xmm5 = mem[0],zero
	vmulsd	%xmm5, %xmm3, %xmm6
	vmovapd	%xmm5, %xmm7
	vfmsub213sd	%xmm6, %xmm3, %xmm7     # xmm7 = (xmm3 * xmm7) - xmm6
	vfmadd231sd	8(%rbx,%r9), %xmm5, %xmm7 # xmm7 = (xmm5 * mem) + xmm7
	vfmadd231sd	8(%r14,%r9), %xmm3, %xmm7 # xmm7 = (xmm3 * mem) + xmm7
	vmovsd	16(%r14,%r9), %xmm3             # xmm3 = mem[0],zero
	vaddsd	%xmm6, %xmm2, %xmm5
	vsubsd	%xmm2, %xmm5, %xmm8
	vsubsd	%xmm8, %xmm5, %xmm9
	vsubsd	%xmm9, %xmm2, %xmm2
	vsubsd	%xmm8, %xmm6, %xmm6
	vaddsd	%xmm6, %xmm2, %xmm2
	vaddsd	%xmm7, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm1, %xmm1
	vmulsd	%xmm3, %xmm4, %xmm2
	vmovapd	%xmm3, %xmm6
	vfmsub213sd	%xmm2, %xmm4, %xmm6     # xmm6 = (xmm4 * xmm6) - xmm2
	vfmadd231sd	24(%rbx,%r9), %xmm3, %xmm6 # xmm6 = (xmm3 * mem) + xmm6
	vfmadd231sd	24(%r14,%r9), %xmm4, %xmm6 # xmm6 = (xmm4 * mem) + xmm6
	vaddsd	%xmm2, %xmm5, %xmm3
	vsubsd	%xmm5, %xmm3, %xmm4
	vsubsd	%xmm4, %xmm3, %xmm7
	vsubsd	%xmm7, %xmm5, %xmm5
	vsubsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm2, %xmm5, %xmm2
	vaddsd	%xmm6, %xmm1, %xmm1
	vmovsd	32(%rbx,%r9), %xmm4             # xmm4 = mem[0],zero
	vmovsd	32(%r14,%r9), %xmm5             # xmm5 = mem[0],zero
	vmulsd	%xmm5, %xmm4, %xmm6
	vmovapd	%xmm5, %xmm7
	vfmsub213sd	%xmm6, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm7) - xmm6
	vfmadd231sd	40(%rbx,%r9), %xmm5, %xmm7 # xmm7 = (xmm5 * mem) + xmm7
	vfmadd231sd	40(%r14,%r9), %xmm4, %xmm7 # xmm7 = (xmm4 * mem) + xmm7
	vaddsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm3, %xmm4
	vsubsd	%xmm3, %xmm4, %xmm2
	vsubsd	%xmm2, %xmm4, %xmm5
	vsubsd	%xmm5, %xmm3, %xmm3
	vsubsd	%xmm2, %xmm6, %xmm2
	vaddsd	%xmm2, %xmm3, %xmm2
	vaddsd	%xmm7, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm1, %xmm1
	vmovsd	48(%rbx,%r9), %xmm2             # xmm2 = mem[0],zero
	vmovsd	48(%r14,%r9), %xmm3             # xmm3 = mem[0],zero
	vmulsd	%xmm3, %xmm2, %xmm5
	vmovapd	%xmm3, %xmm6
	vfmsub213sd	%xmm5, %xmm2, %xmm6     # xmm6 = (xmm2 * xmm6) - xmm5
	vfmadd231sd	56(%rbx,%r9), %xmm3, %xmm6 # xmm6 = (xmm3 * mem) + xmm6
	vfmadd231sd	56(%r14,%r9), %xmm2, %xmm6 # xmm6 = (xmm2 * mem) + xmm6
	vaddsd	%xmm5, %xmm4, %xmm2
	vsubsd	%xmm4, %xmm2, %xmm3
	vsubsd	%xmm3, %xmm2, %xmm7
	vsubsd	%xmm7, %xmm4, %xmm4
	vsubsd	%xmm3, %xmm5, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm3
	vaddsd	%xmm6, %xmm1, %xmm1
	vaddsd	%xmm3, %xmm1, %xmm1
	addq	$64, %r9
	cmpq	%r9, %rdx
	jne	.LBB43_11
.LBB43_12:                              #   in Loop: Header=BB43_8 Depth=1
	vmovapd	%xmm2, %xmm3
	cmpq	%rax, %rcx
	jae	.LBB43_7
# %bb.13:                               #   in Loop: Header=BB43_8 Depth=1
	movq	%rdi, %r9
	movq	%rsi, %r10
	.p2align	4, 0x90
.LBB43_14:                              #   Parent Loop BB43_8 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	-8(%rbx,%r9), %xmm3             # xmm3 = mem[0],zero
	vmovsd	-8(%r14,%r9), %xmm4             # xmm4 = mem[0],zero
	vmulsd	%xmm4, %xmm3, %xmm5
	vmovapd	%xmm4, %xmm6
	vfmsub213sd	%xmm5, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm6) - xmm5
	vfmadd231sd	(%rbx,%r9), %xmm4, %xmm6 # xmm6 = (xmm4 * mem) + xmm6
	vfmadd231sd	(%r14,%r9), %xmm3, %xmm6 # xmm6 = (xmm3 * mem) + xmm6
	vaddsd	%xmm5, %xmm2, %xmm3
	vsubsd	%xmm2, %xmm3, %xmm4
	vsubsd	%xmm4, %xmm3, %xmm7
	vsubsd	%xmm7, %xmm2, %xmm2
	vsubsd	%xmm4, %xmm5, %xmm4
	vaddsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm6, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm1, %xmm1
	addq	$16, %r9
	vmovapd	%xmm3, %xmm2
	decq	%r10
	jne	.LBB43_14
	jmp	.LBB43_7
.LBB43_15:
	callq	omp_get_wtime
	vsubsd	8(%rsp), %xmm0, %xmm0           # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm10, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	128(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp6872:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp6873:
# %bb.16:
.Ltmp6874:
	movq	%rax, %r12
	movq	120(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp6875:
# %bb.17:
.Ltmp6876:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp6877:
# %bb.18:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp6878:
	leaq	80(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp6879:
# %bb.19:
.Ltmp6880:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp6881:
# %bb.20:
.Ltmp6883:
	callq	mpfr_get_default_rounding_mode
.Ltmp6884:
# %bb.21:
.Ltmp6885:
	leaq	80(%rsp), %rdi
	leaq	128(%rsp), %rsi
	movq	120(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp6886:
# %bb.22:
.Ltmp6888:
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6889:
# %bb.23:
.Ltmp6890:
	movq	%rax, %r12
	movq	120(%rsp), %rdi                 # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp6891:
# %bb.24:
.Ltmp6892:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp6893:
# %bb.25:
	movl	%eax, %r15d
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp6894:
	leaq	16(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp6895:
# %bb.26:
.Ltmp6896:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp6897:
# %bb.27:
.Ltmp6899:
	callq	mpfr_get_default_rounding_mode
.Ltmp6900:
# %bb.28:
.Ltmp6901:
	leaq	16(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movq	120(%rsp), %rdx                 # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp6902:
# %bb.29:
.Ltmp6904:
	callq	mpfr_get_default_rounding_mode
.Ltmp6905:
# %bb.30:
.Ltmp6906:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6907:
# %bb.31:
.Ltmp6908:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp6909:
# %bb.32:
.Ltmp6910:
	movl	%eax, %r12d
	leaq	48(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp6911:
# %bb.33:
.Ltmp6912:
	leaq	48(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6913:
# %bb.34:
.Ltmp6915:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp6916:
# %bb.35:
.Ltmp6918:
	leaq	48(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp6919:
# %bb.36:
	vmovsd	%xmm0, 208(%rsp)
	vmovsd	%xmm1, 216(%rsp)
	cmpq	$0, 72(%rsp)
	je	.LBB43_38
# %bb.37:
.Ltmp6921:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6922:
.LBB43_38:
	cmpq	$0, 40(%rsp)
	je	.LBB43_40
# %bb.39:
.Ltmp6924:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6925:
.LBB43_40:
	cmpq	$0, 104(%rsp)
	je	.LBB43_42
# %bb.41:
.Ltmp6927:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6928:
.LBB43_42:
	cmpq	$0, 152(%rsp)
	je	.LBB43_44
# %bb.43:
.Ltmp6930:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6931:
.LBB43_44:
	leaq	408(%rsp), %r15
	movq	%r15, 392(%rsp)
	movl	$544501604, 408(%rsp)           # imm = 0x20746F64
	movw	$32, 412(%rsp)
	movq	$5, 400(%rsp)
.Ltmp6933:
	leaq	392(%rsp), %rdi
	leaq	208(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp6934:
# %bb.45:
	movq	392(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB43_47
# %bb.46:
	callq	_ZdlPv
.LBB43_47:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$160, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rax)
	vmovupd	%xmm0, 16(%rax)
	vmovupd	%xmm0, 32(%rax)
	vmovupd	%xmm0, 48(%rax)
	vmovupd	%xmm0, 64(%rax)
	vmovupd	%xmm0, 80(%rax)
	vmovupd	%xmm0, 96(%rax)
	vmovupd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 128(%rax)
	vmovupd	%xmm0, 144(%rax)
	callq	omp_get_wtime
	vmovsd	%xmm0, 176(%rsp)                # 8-byte Spill
	leaq	8(%rbx), %r15
	xorl	%r12d, %r12d
	vcvtsi2sd	%r12d, %xmm2, %xmm7
	vxorpd	%xmm8, %xmm8, %xmm8
	vmovsd	%xmm7, 224(%rsp)                # 8-byte Spill
	jmp	.LBB43_50
	.p2align	4, 0x90
.LBB43_48:                              #   in Loop: Header=BB43_50 Depth=1
	vmovsd	%xmm0, 160(%rsp)                # 8-byte Spill
	callq	sqrt
	vmovsd	160(%rsp), %xmm1                # 8-byte Reload
                                        # xmm1 = mem[0],zero
	vfnmadd231sd	%xmm0, %xmm0, %xmm1     # xmm1 = -(xmm0 * xmm0) + xmm1
	vaddsd	8(%rsp), %xmm1, %xmm1           # 8-byte Folded Reload
	vaddsd	%xmm0, %xmm0, %xmm2
	vdivsd	%xmm2, %xmm1, %xmm1
	vmovsd	224(%rsp), %xmm7                # 8-byte Reload
                                        # xmm7 = mem[0],zero
	vxorpd	%xmm8, %xmm8, %xmm8
.LBB43_49:                              #   in Loop: Header=BB43_50 Depth=1
	movq	%r12, %rax
	shlq	$4, %rax
	vmovsd	%xmm0, (%rbp,%rax)
	vmovsd	%xmm1, 8(%rbp,%rax)
	leaq	1(%r12), %rax
	cmpq	$9, %r12
	movq	%rax, %r12
	je	.LBB43_64
.LBB43_50:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB43_53 Depth 2
                                        #     Child Loop BB43_56 Depth 2
	movq	112(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovapd	%xmm7, %xmm0
	testl	%eax, %eax
	jle	.LBB43_57
# %bb.51:                               #   in Loop: Header=BB43_50 Depth=1
	vmovapd	%xmm7, %xmm9
	cmpl	$4, %eax
	jb	.LBB43_54
# %bb.52:                               #   in Loop: Header=BB43_50 Depth=1
	movq	%rax, %rcx
	shlq	$4, %rcx
	andq	$-64, %rcx
	xorl	%edx, %edx
	vmovapd	%xmm7, %xmm9
	.p2align	4, 0x90
.LBB43_53:                              #   Parent Loop BB43_50 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	(%rbx,%rdx), %xmm2              # xmm2 = mem[0],zero
	vmovsd	8(%rbx,%rdx), %xmm3             # xmm3 = mem[0],zero
	vmulsd	%xmm2, %xmm2, %xmm4
	vmovapd	%xmm2, %xmm5
	vfmsub213sd	%xmm4, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm5) - xmm4
	vfmadd231sd	%xmm3, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm3) + xmm5
	vfmadd231sd	%xmm3, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm3) + xmm5
	vaddsd	%xmm4, %xmm9, %xmm2
	vsubsd	%xmm9, %xmm2, %xmm3
	vsubsd	%xmm3, %xmm2, %xmm6
	vsubsd	%xmm6, %xmm9, %xmm0
	vsubsd	%xmm3, %xmm4, %xmm3
	vaddsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm5, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovsd	16(%rbx,%rdx), %xmm1            # xmm1 = mem[0],zero
	vmulsd	%xmm1, %xmm1, %xmm3
	vmovapd	%xmm1, %xmm4
	vfmsub213sd	%xmm3, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm4) - xmm3
	vmovsd	24(%rbx,%rdx), %xmm5            # xmm5 = mem[0],zero
	vfmadd231sd	%xmm5, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm5) + xmm4
	vfmadd231sd	%xmm5, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm5) + xmm4
	vaddsd	%xmm3, %xmm2, %xmm1
	vsubsd	%xmm2, %xmm1, %xmm5
	vsubsd	%xmm5, %xmm1, %xmm6
	vsubsd	%xmm6, %xmm2, %xmm2
	vsubsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm4, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vmovsd	32(%rbx,%rdx), %xmm2            # xmm2 = mem[0],zero
	vmulsd	%xmm2, %xmm2, %xmm3
	vmovapd	%xmm2, %xmm4
	vfmsub213sd	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vmovsd	40(%rbx,%rdx), %xmm5            # xmm5 = mem[0],zero
	vfmadd231sd	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vfmadd231sd	%xmm5, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm5) + xmm4
	vaddsd	%xmm3, %xmm1, %xmm2
	vsubsd	%xmm1, %xmm2, %xmm5
	vsubsd	%xmm5, %xmm2, %xmm6
	vsubsd	%xmm6, %xmm1, %xmm1
	vsubsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm0, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm1
	vmovsd	48(%rbx,%rdx), %xmm0            # xmm0 = mem[0],zero
	vmulsd	%xmm0, %xmm0, %xmm3
	vmovapd	%xmm0, %xmm4
	vfmsub213sd	%xmm3, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm4) - xmm3
	vmovsd	56(%rbx,%rdx), %xmm5            # xmm5 = mem[0],zero
	vfmadd231sd	%xmm5, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm5) + xmm4
	vfmadd231sd	%xmm5, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm5) + xmm4
	vaddsd	%xmm3, %xmm2, %xmm9
	vsubsd	%xmm2, %xmm9, %xmm5
	vsubsd	%xmm5, %xmm9, %xmm6
	vsubsd	%xmm6, %xmm2, %xmm2
	vsubsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm1, %xmm1
	addq	$64, %rdx
	cmpq	%rdx, %rcx
	jne	.LBB43_53
.LBB43_54:                              #   in Loop: Header=BB43_50 Depth=1
	movl	%eax, %edx
	andl	$-4, %edx
	vmovapd	%xmm9, %xmm0
	cmpq	%rax, %rdx
	jae	.LBB43_57
# %bb.55:                               #   in Loop: Header=BB43_50 Depth=1
	movq	%rax, %rcx
	subq	%rdx, %rcx
	shlq	$4, %rax
	andq	$-64, %rax
	addq	%r15, %rax
	.p2align	4, 0x90
.LBB43_56:                              #   Parent Loop BB43_50 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	-8(%rax), %xmm2                 # xmm2 = mem[0],zero
	vmovsd	(%rax), %xmm3                   # xmm3 = mem[0],zero
	vmulsd	%xmm2, %xmm2, %xmm4
	vmovapd	%xmm2, %xmm5
	vfmsub213sd	%xmm4, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm5) - xmm4
	vfmadd231sd	%xmm3, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm3) + xmm5
	vfmadd231sd	%xmm3, %xmm2, %xmm5     # xmm5 = (xmm2 * xmm3) + xmm5
	vaddsd	%xmm4, %xmm9, %xmm0
	vsubsd	%xmm9, %xmm0, %xmm2
	vsubsd	%xmm2, %xmm0, %xmm3
	vsubsd	%xmm3, %xmm9, %xmm3
	vsubsd	%xmm2, %xmm4, %xmm2
	vaddsd	%xmm2, %xmm3, %xmm2
	vaddsd	%xmm5, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm1, %xmm1
	addq	$16, %rax
	vmovapd	%xmm0, %xmm9
	decq	%rcx
	jne	.LBB43_56
.LBB43_57:                              #   in Loop: Header=BB43_50 Depth=1
	vaddsd	%xmm1, %xmm0, %xmm2
	vucomisd	%xmm8, %xmm2
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB43_49
# %bb.58:                               #   in Loop: Header=BB43_50 Depth=1
	vucomisd	%xmm8, %xmm0
	vmovapd	%xmm1, %xmm2
	jne	.LBB43_61
# %bb.59:                               #   in Loop: Header=BB43_50 Depth=1
	vmovapd	%xmm1, %xmm2
	jp	.LBB43_61
# %bb.60:                               #   in Loop: Header=BB43_50 Depth=1
	vmovapd	%xmm0, %xmm2
.LBB43_61:                              #   in Loop: Header=BB43_50 Depth=1
	vmovsd	%xmm2, 8(%rsp)                  # 8-byte Spill
	jne	.LBB43_48
# %bb.62:                               #   in Loop: Header=BB43_50 Depth=1
	jp	.LBB43_48
# %bb.63:                               #   in Loop: Header=BB43_50 Depth=1
	vmovapd	%xmm1, %xmm0
	jmp	.LBB43_48
.LBB43_64:
	callq	omp_get_wtime
	vsubsd	176(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm10, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	128(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp6936:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp6937:
# %bb.65:
	movq	%rax, %r13
	movq	120(%rsp), %rax                 # 8-byte Reload
	leaq	32(%rax), %rdi
.Ltmp6938:
	movq	%rdi, 8(%rsp)                   # 8-byte Spill
	callq	mpfr_get_prec
.Ltmp6939:
# %bb.66:
.Ltmp6940:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp6941:
# %bb.67:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp6942:
	leaq	80(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp6943:
# %bb.68:
.Ltmp6944:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6945:
# %bb.69:
.Ltmp6947:
	callq	mpfr_get_default_rounding_mode
.Ltmp6948:
# %bb.70:
.Ltmp6949:
	leaq	80(%rsp), %rdi
	leaq	128(%rsp), %rsi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp6950:
# %bb.71:
.Ltmp6952:
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6953:
# %bb.72:
.Ltmp6954:
	movq	%rax, %r13
	movq	8(%rsp), %rdi                   # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp6955:
# %bb.73:
.Ltmp6956:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp6957:
# %bb.74:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp6958:
	leaq	16(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp6959:
# %bb.75:
.Ltmp6960:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6961:
# %bb.76:
.Ltmp6963:
	callq	mpfr_get_default_rounding_mode
.Ltmp6964:
# %bb.77:
.Ltmp6965:
	leaq	16(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp6966:
# %bb.78:
.Ltmp6968:
	callq	mpfr_get_default_rounding_mode
.Ltmp6969:
# %bb.79:
.Ltmp6970:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp6971:
# %bb.80:
.Ltmp6972:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp6973:
# %bb.81:
.Ltmp6974:
	movl	%eax, %r12d
	leaq	48(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp6975:
# %bb.82:
.Ltmp6976:
	leaq	48(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp6977:
# %bb.83:
.Ltmp6979:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp6980:
# %bb.84:
.Ltmp6982:
	leaq	48(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp6983:
# %bb.85:
	vmovsd	%xmm0, 208(%rsp)
	vmovsd	%xmm1, 216(%rsp)
	cmpq	$0, 72(%rsp)
	je	.LBB43_87
# %bb.86:
.Ltmp6985:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6986:
.LBB43_87:
	cmpq	$0, 40(%rsp)
	je	.LBB43_89
# %bb.88:
.Ltmp6988:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6989:
.LBB43_89:
	cmpq	$0, 104(%rsp)
	je	.LBB43_91
# %bb.90:
.Ltmp6991:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6992:
.LBB43_91:
	cmpq	$0, 152(%rsp)
	je	.LBB43_93
# %bb.92:
.Ltmp6994:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp6995:
.LBB43_93:
	leaq	376(%rsp), %r15
	movq	%r15, 360(%rsp)
	movl	$846033518, 376(%rsp)           # imm = 0x326D726E
	movw	$32, 380(%rsp)
	movq	$5, 368(%rsp)
.Ltmp6997:
	leaq	360(%rsp), %rdi
	leaq	208(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp6998:
# %bb.94:
	movq	360(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB43_96
# %bb.95:
	callq	_ZdlPv
.LBB43_96:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movl	$160, %edi
	callq	_Znam
	movq	%rax, %rbp
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rax)
	vmovupd	%xmm0, 16(%rax)
	vmovupd	%xmm0, 32(%rax)
	vmovupd	%xmm0, 48(%rax)
	vmovupd	%xmm0, 64(%rax)
	vmovupd	%xmm0, 80(%rax)
	vmovupd	%xmm0, 96(%rax)
	vmovupd	%xmm0, 112(%rax)
	vmovupd	%xmm0, 128(%rax)
	vmovupd	%xmm0, 144(%rax)
	callq	omp_get_wtime
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
	movq	112(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	movl	%eax, %ecx
	andl	$-4, %ecx
	movq	%rax, %rdx
	shlq	$4, %rdx
	andq	$-64, %rdx
	movl	%eax, %esi
	andl	$3, %esi
	leaq	(%rdx,%rbx), %rdi
	addq	$8, %rdi
	xorl	%r8d, %r8d
	vcvtsi2sd	%r8d, %xmm2, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovupd	.LCPI43_0(%rip), %xmm2          # xmm2 = [-0.0E+0,-0.0E+0]
                                        # AlignMOV convert to UnAlignMOV 
	jmp	.LBB43_98
	.p2align	4, 0x90
.LBB43_97:                              #   in Loop: Header=BB43_98 Depth=1
	movq	%r8, %r9
	shlq	$4, %r9
	vmovsd	%xmm5, (%rbp,%r9)
	vmovsd	%xmm3, 8(%rbp,%r9)
	leaq	1(%r8), %r9
	cmpq	$9, %r8
	movq	%r9, %r8
	je	.LBB43_115
.LBB43_98:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB43_107 Depth 2
                                        #     Child Loop BB43_103 Depth 2
	vxorpd	%xmm3, %xmm3, %xmm3
	vmovapd	%xmm0, %xmm5
	testl	%eax, %eax
	jle	.LBB43_97
# %bb.99:                               #   in Loop: Header=BB43_98 Depth=1
	vmovapd	%xmm0, %xmm4
	cmpl	$4, %eax
	jae	.LBB43_105
.LBB43_100:                             #   in Loop: Header=BB43_98 Depth=1
	vmovapd	%xmm4, %xmm5
	cmpq	%rax, %rcx
	jae	.LBB43_97
# %bb.101:                              #   in Loop: Header=BB43_98 Depth=1
	movq	%rdi, %r9
	movq	%rsi, %r10
	jmp	.LBB43_103
	.p2align	4, 0x90
.LBB43_102:                             #   in Loop: Header=BB43_103 Depth=2
	vaddsd	%xmm7, %xmm4, %xmm5
	vsubsd	%xmm4, %xmm5, %xmm8
	vsubsd	%xmm8, %xmm5, %xmm9
	vsubsd	%xmm9, %xmm4, %xmm4
	vsubsd	%xmm8, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm4, %xmm4
	vaddsd	%xmm6, %xmm3, %xmm3
	vaddsd	%xmm4, %xmm3, %xmm3
	addq	$16, %r9
	vmovapd	%xmm5, %xmm4
	decq	%r10
	je	.LBB43_97
.LBB43_103:                             #   Parent Loop BB43_98 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	-8(%r9), %xmm7                  # xmm7 = mem[0],zero
	vmovsd	(%r9), %xmm6                    # xmm6 = mem[0],zero
	vaddsd	%xmm6, %xmm7, %xmm5
	vcomisd	%xmm5, %xmm1
	jbe	.LBB43_102
# %bb.104:                              #   in Loop: Header=BB43_103 Depth=2
	vxorpd	%xmm2, %xmm7, %xmm7
	vxorpd	%xmm2, %xmm6, %xmm6
	jmp	.LBB43_102
	.p2align	4, 0x90
.LBB43_105:                             #   in Loop: Header=BB43_98 Depth=1
	xorl	%r9d, %r9d
	vmovapd	%xmm0, %xmm4
	jmp	.LBB43_107
	.p2align	4, 0x90
.LBB43_106:                             #   in Loop: Header=BB43_107 Depth=2
	vaddsd	%xmm7, %xmm5, %xmm4
	vsubsd	%xmm5, %xmm4, %xmm8
	vsubsd	%xmm8, %xmm4, %xmm9
	vsubsd	%xmm9, %xmm5, %xmm5
	vsubsd	%xmm8, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm5, %xmm5
	vaddsd	%xmm6, %xmm3, %xmm3
	vaddsd	%xmm5, %xmm3, %xmm3
	addq	$64, %r9
	cmpq	%r9, %rdx
	je	.LBB43_100
.LBB43_107:                             #   Parent Loop BB43_98 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	(%rbx,%r9), %xmm7               # xmm7 = mem[0],zero
	vmovsd	8(%rbx,%r9), %xmm6              # xmm6 = mem[0],zero
	vaddsd	%xmm6, %xmm7, %xmm5
	vcomisd	%xmm5, %xmm1
	jbe	.LBB43_109
# %bb.108:                              #   in Loop: Header=BB43_107 Depth=2
	vxorpd	%xmm2, %xmm7, %xmm7
	vxorpd	%xmm2, %xmm6, %xmm6
.LBB43_109:                             #   in Loop: Header=BB43_107 Depth=2
	vaddsd	%xmm7, %xmm4, %xmm5
	vsubsd	%xmm4, %xmm5, %xmm8
	vsubsd	%xmm8, %xmm5, %xmm9
	vsubsd	%xmm9, %xmm4, %xmm4
	vsubsd	%xmm8, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm4, %xmm4
	vaddsd	%xmm6, %xmm3, %xmm3
	vaddsd	%xmm4, %xmm3, %xmm4
	vmovsd	16(%rbx,%r9), %xmm7             # xmm7 = mem[0],zero
	vmovsd	24(%rbx,%r9), %xmm6             # xmm6 = mem[0],zero
	vaddsd	%xmm6, %xmm7, %xmm3
	vcomisd	%xmm3, %xmm1
	jbe	.LBB43_111
# %bb.110:                              #   in Loop: Header=BB43_107 Depth=2
	vxorpd	%xmm2, %xmm7, %xmm7
	vxorpd	%xmm2, %xmm6, %xmm6
.LBB43_111:                             #   in Loop: Header=BB43_107 Depth=2
	vaddsd	%xmm7, %xmm5, %xmm3
	vsubsd	%xmm5, %xmm3, %xmm8
	vsubsd	%xmm8, %xmm3, %xmm9
	vsubsd	%xmm9, %xmm5, %xmm5
	vsubsd	%xmm8, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm5, %xmm5
	vaddsd	%xmm6, %xmm4, %xmm4
	vaddsd	%xmm5, %xmm4, %xmm4
	vmovsd	32(%rbx,%r9), %xmm7             # xmm7 = mem[0],zero
	vmovsd	40(%rbx,%r9), %xmm6             # xmm6 = mem[0],zero
	vaddsd	%xmm6, %xmm7, %xmm5
	vcomisd	%xmm5, %xmm1
	jbe	.LBB43_113
# %bb.112:                              #   in Loop: Header=BB43_107 Depth=2
	vxorpd	%xmm2, %xmm7, %xmm7
	vxorpd	%xmm2, %xmm6, %xmm6
.LBB43_113:                             #   in Loop: Header=BB43_107 Depth=2
	vaddsd	%xmm7, %xmm3, %xmm5
	vsubsd	%xmm3, %xmm5, %xmm8
	vsubsd	%xmm8, %xmm5, %xmm9
	vsubsd	%xmm9, %xmm3, %xmm3
	vsubsd	%xmm8, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm4, %xmm4
	vaddsd	%xmm3, %xmm4, %xmm3
	vmovsd	48(%rbx,%r9), %xmm7             # xmm7 = mem[0],zero
	vmovsd	56(%rbx,%r9), %xmm6             # xmm6 = mem[0],zero
	vaddsd	%xmm6, %xmm7, %xmm4
	vcomisd	%xmm4, %xmm1
	jbe	.LBB43_106
# %bb.114:                              #   in Loop: Header=BB43_107 Depth=2
	vxorpd	%xmm2, %xmm7, %xmm7
	vxorpd	%xmm2, %xmm6, %xmm6
	jmp	.LBB43_106
.LBB43_115:
	callq	omp_get_wtime
	vsubsd	8(%rsp), %xmm0, %xmm0           # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm10, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
	leaq	128(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp7000:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp7001:
# %bb.116:
	movq	%rax, %r13
	movq	120(%rsp), %rax                 # 8-byte Reload
	leaq	64(%rax), %rdi
.Ltmp7002:
	movq	%rdi, 8(%rsp)                   # 8-byte Spill
	callq	mpfr_get_prec
.Ltmp7003:
# %bb.117:
.Ltmp7004:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp7005:
# %bb.118:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp7006:
	leaq	80(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp7007:
# %bb.119:
.Ltmp7008:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp7009:
# %bb.120:
.Ltmp7011:
	callq	mpfr_get_default_rounding_mode
.Ltmp7012:
# %bb.121:
.Ltmp7013:
	leaq	80(%rsp), %rdi
	leaq	128(%rsp), %rsi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp7014:
# %bb.122:
.Ltmp7016:
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7017:
# %bb.123:
.Ltmp7018:
	movq	%rax, %r13
	movq	8(%rsp), %rdi                   # 8-byte Reload
	callq	mpfr_get_prec
.Ltmp7019:
# %bb.124:
.Ltmp7020:
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
.Ltmp7021:
# %bb.125:
	movl	%eax, %r12d
	cmpq	%r15, %r13
	cmovgq	%r13, %r15
.Ltmp7022:
	leaq	16(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp7023:
# %bb.126:
.Ltmp7024:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp7025:
# %bb.127:
.Ltmp7027:
	callq	mpfr_get_default_rounding_mode
.Ltmp7028:
# %bb.128:
.Ltmp7029:
	leaq	16(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp7030:
# %bb.129:
.Ltmp7032:
	callq	mpfr_get_default_rounding_mode
.Ltmp7033:
# %bb.130:
.Ltmp7034:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7035:
# %bb.131:
.Ltmp7036:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp7037:
# %bb.132:
.Ltmp7038:
	movl	%eax, %r12d
	leaq	48(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp7039:
# %bb.133:
.Ltmp7040:
	leaq	48(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp7041:
# %bb.134:
.Ltmp7043:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%r15d, %edx
	callq	mpfr_abs
.Ltmp7044:
# %bb.135:
.Ltmp7046:
	leaq	48(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp7047:
# %bb.136:
	vmovsd	%xmm0, 208(%rsp)
	vmovsd	%xmm1, 216(%rsp)
	cmpq	$0, 72(%rsp)
	je	.LBB43_138
# %bb.137:
.Ltmp7049:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7050:
.LBB43_138:
	cmpq	$0, 40(%rsp)
	je	.LBB43_140
# %bb.139:
.Ltmp7052:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7053:
.LBB43_140:
	cmpq	$0, 104(%rsp)
	je	.LBB43_142
# %bb.141:
.Ltmp7055:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7056:
.LBB43_142:
	cmpq	$0, 152(%rsp)
	je	.LBB43_144
# %bb.143:
.Ltmp7058:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7059:
.LBB43_144:
	leaq	344(%rsp), %r15
	movq	%r15, 328(%rsp)
	movl	$1836413793, 344(%rsp)          # imm = 0x6D757361
	movw	$32, 348(%rsp)
	movq	$5, 336(%rsp)
.Ltmp7061:
	leaq	328(%rsp), %rdi
	leaq	208(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp7062:
# %bb.145:
	movq	328(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB43_147
# %bb.146:
	callq	_ZdlPv
.LBB43_147:
	movq	%rbp, %rdi
	callq	_ZdaPv
	movq	112(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %eax
	testl	%eax, %eax
	jle	.LBB43_153
# %bb.148:
	cmpl	$1, %eax
	vmovsd	200(%rsp), %xmm7                # 8-byte Reload
                                        # xmm7 = mem[0],zero
	vmovsd	192(%rsp), %xmm8                # 8-byte Reload
                                        # xmm8 = mem[0],zero
	je	.LBB43_151
# %bb.149:
	movq	%rax, %rcx
	shrq	%rcx
	movl	$24, %edx
	.p2align	4, 0x90
.LBB43_150:                             # =>This Inner Loop Header: Depth=1
	vmovsd	-24(%rbx,%rdx), %xmm0           # xmm0 = mem[0],zero
	vmulsd	%xmm0, %xmm7, %xmm1
	vmovapd	%xmm0, %xmm2
	vfmsub213sd	%xmm1, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm1
	vfmadd231sd	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm0) + xmm2
	vfmadd231sd	-16(%rbx,%rdx), %xmm7, %xmm2 # xmm2 = (xmm7 * mem) + xmm2
	vmovsd	-24(%r14,%rdx), %xmm0           # xmm0 = mem[0],zero
	vmovsd	-8(%r14,%rdx), %xmm3            # xmm3 = mem[0],zero
	vaddsd	%xmm0, %xmm1, %xmm4
	vsubsd	%xmm1, %xmm4, %xmm5
	vsubsd	%xmm5, %xmm4, %xmm6
	vsubsd	%xmm6, %xmm1, %xmm1
	vsubsd	%xmm5, %xmm0, %xmm0
	vaddsd	-16(%r14,%rdx), %xmm2, %xmm2
	vaddsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm0
	vmovsd	%xmm4, -24(%r14,%rdx)
	vmovsd	%xmm0, -16(%r14,%rdx)
	vmovsd	-8(%rbx,%rdx), %xmm0            # xmm0 = mem[0],zero
	vmulsd	%xmm0, %xmm7, %xmm1
	vmovapd	%xmm0, %xmm2
	vfmsub213sd	%xmm1, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm1
	vfmadd231sd	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm0) + xmm2
	vfmadd231sd	(%rbx,%rdx), %xmm7, %xmm2 # xmm2 = (xmm7 * mem) + xmm2
	vaddsd	%xmm3, %xmm1, %xmm0
	vsubsd	%xmm1, %xmm0, %xmm4
	vsubsd	%xmm4, %xmm0, %xmm5
	vsubsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm3, %xmm1, %xmm1
	vaddsd	(%r14,%rdx), %xmm2, %xmm2
	vaddsd	%xmm1, %xmm2, %xmm1
	vmovsd	%xmm0, -8(%r14,%rdx)
	vmovsd	%xmm1, (%r14,%rdx)
	addq	$32, %rdx
	decq	%rcx
	jne	.LBB43_150
.LBB43_151:
	movl	%eax, %ecx
	andl	$-2, %ecx
	cmpq	%rax, %rcx
	jae	.LBB43_153
# %bb.152:
	shlq	$4, %rcx
	vmovsd	(%rbx,%rcx), %xmm0              # xmm0 = mem[0],zero
	vmovsd	200(%rsp), %xmm3                # 8-byte Reload
                                        # xmm3 = mem[0],zero
	vmulsd	%xmm0, %xmm3, %xmm1
	vmovapd	%xmm0, %xmm2
	vfmsub213sd	%xmm1, %xmm3, %xmm2     # xmm2 = (xmm3 * xmm2) - xmm1
	vfmadd231sd	192(%rsp), %xmm0, %xmm2 # 8-byte Folded Reload
                                        # xmm2 = (xmm0 * mem) + xmm2
	vfmadd231sd	8(%rbx,%rcx), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovsd	(%r14,%rcx), %xmm0              # xmm0 = mem[0],zero
	vaddsd	%xmm0, %xmm1, %xmm3
	vsubsd	%xmm1, %xmm3, %xmm4
	vsubsd	%xmm4, %xmm3, %xmm5
	vsubsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm4, %xmm0, %xmm0
	vaddsd	8(%r14,%rcx), %xmm2, %xmm2
	vaddsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm0
	vmovsd	%xmm3, (%r14,%rcx)
	vmovsd	%xmm0, 8(%r14,%rcx)
.LBB43_153:
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	48(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	movq	112(%rsp), %rax                 # 8-byte Reload
	cmpl	$0, (%rax)
	jle	.LBB43_194
# %bb.154:
	movq	120(%rsp), %rax                 # 8-byte Reload
	leaq	256(%rax), %r15
	xorl	%eax, %eax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movq	%r14, %rsi
	jmp	.LBB43_156
	.p2align	4, 0x90
.LBB43_155:                             #   in Loop: Header=BB43_156 Depth=1
	movq	8(%rsp), %rdx                   # 8-byte Reload
	incq	%rdx
	movq	112(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	addq	$32, %r15
	movq	160(%rsp), %rsi                 # 8-byte Reload
	addq	$16, %rsi
	movq	%rdx, %rcx
	movq	%rdx, 8(%rsp)                   # 8-byte Spill
	cmpq	%rax, %rdx
	jge	.LBB43_194
.LBB43_156:                             # =>This Inner Loop Header: Depth=1
.Ltmp7064:
	leaq	80(%rsp), %rdi
	movq	%rsi, 160(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp7065:
# %bb.157:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7067:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp7068:
# %bb.158:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7069:
	movq	%rax, %rbp
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7070:
# %bb.159:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7071:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp7072:
# %bb.160:                              #   in Loop: Header=BB43_156 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp7073:
	leaq	16(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp7074:
# %bb.161:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7075:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp7076:
# %bb.162:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7078:
	callq	mpfr_get_default_rounding_mode
.Ltmp7079:
# %bb.163:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7080:
	leaq	16(%rsp), %rdi
	movq	%r15, %rsi
	leaq	80(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp7081:
# %bb.164:                              #   in Loop: Header=BB43_156 Depth=1
	cmpq	$0, 104(%rsp)
	je	.LBB43_166
# %bb.165:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7083:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7084:
.LBB43_166:                             #   in Loop: Header=BB43_156 Depth=1
.Ltmp7086:
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7087:
# %bb.167:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7088:
	movq	%rax, %rbp
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7089:
# %bb.168:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7090:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp7091:
# %bb.169:                              #   in Loop: Header=BB43_156 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp7092:
	leaq	128(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp7093:
# %bb.170:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7094:
	leaq	128(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp7095:
# %bb.171:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7097:
	callq	mpfr_get_default_rounding_mode
.Ltmp7098:
# %bb.172:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7099:
	leaq	128(%rsp), %rdi
	leaq	16(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp7100:
# %bb.173:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7102:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7103:
# %bb.174:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7104:
	movq	%rax, %rbp
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7105:
# %bb.175:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7106:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp7107:
# %bb.176:                              #   in Loop: Header=BB43_156 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %rbp
	cmovgq	%rbp, %r13
.Ltmp7108:
	leaq	80(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp7109:
# %bb.177:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7110:
	leaq	80(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp7111:
# %bb.178:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7113:
	callq	mpfr_get_default_rounding_mode
.Ltmp7114:
	leaq	48(%rsp), %r12
# %bb.179:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7115:
	leaq	80(%rsp), %rdi
	movq	%r12, %rsi
	leaq	128(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp7116:
# %bb.180:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7118:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp7119:
# %bb.181:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7120:
	movq	%rax, %r12
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7121:
# %bb.182:                              #   in Loop: Header=BB43_156 Depth=1
	movq	%rax, %rbp
	cmpq	%rax, %r12
	je	.LBB43_186
# %bb.183:                              #   in Loop: Header=BB43_156 Depth=1
	cmpq	$0, 72(%rsp)
	je	.LBB43_185
# %bb.184:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7122:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7123:
.LBB43_185:                             #   in Loop: Header=BB43_156 Depth=1
.Ltmp7124:
	leaq	48(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp7125:
.LBB43_186:                             #   in Loop: Header=BB43_156 Depth=1
.Ltmp7126:
	callq	mpfr_get_default_rounding_mode
.Ltmp7127:
# %bb.187:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7128:
	leaq	48(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp7129:
# %bb.188:                              #   in Loop: Header=BB43_156 Depth=1
	cmpq	$0, 104(%rsp)
	je	.LBB43_190
# %bb.189:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7131:
	leaq	80(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7132:
.LBB43_190:                             #   in Loop: Header=BB43_156 Depth=1
	cmpq	$0, 152(%rsp)
	je	.LBB43_192
# %bb.191:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7134:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7135:
.LBB43_192:                             #   in Loop: Header=BB43_156 Depth=1
	cmpq	$0, 40(%rsp)
	je	.LBB43_155
# %bb.193:                              #   in Loop: Header=BB43_156 Depth=1
.Ltmp7137:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7138:
	jmp	.LBB43_155
.LBB43_194:
.Ltmp7140:
	callq	mpfr_get_default_rounding_mode
.Ltmp7141:
# %bb.195:
.Ltmp7142:
	movl	%eax, %ebp
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7143:
# %bb.196:
.Ltmp7144:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp7145:
# %bb.197:
.Ltmp7146:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp7147:
# %bb.198:
.Ltmp7148:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp7149:
# %bb.199:
.Ltmp7151:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp7152:
# %bb.200:
.Ltmp7154:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7155:
# %bb.201:
.Ltmp7156:
	movq	%rax, %r12
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7157:
# %bb.202:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB43_206
# %bb.203:
	cmpq	$0, 72(%rsp)
	je	.LBB43_205
# %bb.204:
.Ltmp7158:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7159:
.LBB43_205:
.Ltmp7160:
	leaq	48(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp7161:
.LBB43_206:
.Ltmp7162:
	callq	mpfr_get_default_rounding_mode
.Ltmp7163:
# %bb.207:
.Ltmp7164:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp7165:
# %bb.208:
	cmpq	$0, 40(%rsp)
	je	.LBB43_210
# %bb.209:
.Ltmp7167:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7168:
.LBB43_210:
	callq	omp_get_wtime
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
	movq	112(%rsp), %rax                 # 8-byte Reload
	movslq	(%rax), %rax
	testq	%rax, %rax
	vmovsd	200(%rsp), %xmm7                # 8-byte Reload
                                        # xmm7 = mem[0],zero
	vmovsd	192(%rsp), %xmm8                # 8-byte Reload
                                        # xmm8 = mem[0],zero
	jle	.LBB43_218
# %bb.211:
	movq	%rax, %rcx
	shrq	%rcx
	movq	%rax, %rdx
	andq	$-2, %rdx
	movq	%rdx, %r9
	shlq	$4, %r9
	leaq	(%rbx,%r9), %rsi
	addq	$8, %rsi
	movq	%rbx, %rdi
	addq	%r9, %rdi
	leaq	(%r14,%r9), %r8
	addq	%r14, %r9
	addq	$8, %r9
	xorl	%r10d, %r10d
	jmp	.LBB43_213
	.p2align	4, 0x90
.LBB43_212:                             #   in Loop: Header=BB43_213 Depth=1
	leal	1(%r10), %r11d
	cmpl	$9, %r10d
	movl	%r11d, %r10d
	je	.LBB43_218
.LBB43_213:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB43_215 Depth 2
	cmpl	$2, %eax
	jb	.LBB43_216
# %bb.214:                              #   in Loop: Header=BB43_213 Depth=1
	movl	$24, %r11d
	movq	%rcx, %r15
	.p2align	4, 0x90
.LBB43_215:                             #   Parent Loop BB43_213 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovsd	-24(%rbx,%r11), %xmm0           # xmm0 = mem[0],zero
	vmulsd	%xmm0, %xmm7, %xmm1
	vmovapd	%xmm0, %xmm2
	vfmsub213sd	%xmm1, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm1
	vfmadd231sd	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm0) + xmm2
	vfmadd231sd	-16(%rbx,%r11), %xmm7, %xmm2 # xmm2 = (xmm7 * mem) + xmm2
	vmovsd	-24(%r14,%r11), %xmm0           # xmm0 = mem[0],zero
	vmovsd	-8(%r14,%r11), %xmm3            # xmm3 = mem[0],zero
	vaddsd	%xmm0, %xmm1, %xmm4
	vsubsd	%xmm1, %xmm4, %xmm5
	vsubsd	%xmm5, %xmm4, %xmm6
	vsubsd	%xmm6, %xmm1, %xmm1
	vsubsd	%xmm5, %xmm0, %xmm0
	vaddsd	-16(%r14,%r11), %xmm2, %xmm2
	vaddsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm0
	vmovsd	%xmm4, -24(%r14,%r11)
	vmovsd	%xmm0, -16(%r14,%r11)
	vmovsd	-8(%rbx,%r11), %xmm0            # xmm0 = mem[0],zero
	vmulsd	%xmm0, %xmm7, %xmm1
	vmovapd	%xmm0, %xmm2
	vfmsub213sd	%xmm1, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm1
	vfmadd231sd	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm0) + xmm2
	vfmadd231sd	(%rbx,%r11), %xmm7, %xmm2 # xmm2 = (xmm7 * mem) + xmm2
	vaddsd	%xmm3, %xmm1, %xmm0
	vsubsd	%xmm1, %xmm0, %xmm4
	vsubsd	%xmm4, %xmm0, %xmm5
	vsubsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm3, %xmm1, %xmm1
	vaddsd	(%r14,%r11), %xmm2, %xmm2
	vaddsd	%xmm1, %xmm2, %xmm1
	vmovsd	%xmm0, -8(%r14,%r11)
	vmovsd	%xmm1, (%r14,%r11)
	addq	$32, %r11
	decq	%r15
	jne	.LBB43_215
.LBB43_216:                             #   in Loop: Header=BB43_213 Depth=1
	cmpq	%rax, %rdx
	jae	.LBB43_212
# %bb.217:                              #   in Loop: Header=BB43_213 Depth=1
	vmovsd	(%rdi), %xmm0                   # xmm0 = mem[0],zero
	vmulsd	%xmm0, %xmm7, %xmm1
	vmovapd	%xmm0, %xmm2
	vfmsub213sd	%xmm1, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) - xmm1
	vfmadd231sd	%xmm0, %xmm8, %xmm2     # xmm2 = (xmm8 * xmm0) + xmm2
	vfmadd231sd	(%rsi), %xmm7, %xmm2    # xmm2 = (xmm7 * mem) + xmm2
	vmovsd	(%r8), %xmm0                    # xmm0 = mem[0],zero
	vaddsd	%xmm0, %xmm1, %xmm3
	vsubsd	%xmm1, %xmm3, %xmm4
	vsubsd	%xmm4, %xmm3, %xmm5
	vsubsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm4, %xmm0, %xmm0
	vaddsd	(%r9), %xmm2, %xmm2
	vaddsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm0
	vmovsd	%xmm3, (%r8)
	vmovsd	%xmm0, 8(%r8)
	jmp	.LBB43_212
.LBB43_218:
	callq	omp_get_wtime
	vsubsd	8(%rsp), %xmm0, %xmm0           # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm9, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp7170:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7171:
# %bb.219:
	movq	%rax, %r12
	movq	120(%rsp), %rax                 # 8-byte Reload
	leaq	96(%rax), %r15
.Ltmp7172:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp7173:
# %bb.220:
.Ltmp7174:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp7175:
# %bb.221:
	movl	%eax, %ebp
	cmpq	%r13, %r12
	cmovgq	%r12, %r13
.Ltmp7176:
	leaq	16(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp7177:
# %bb.222:
.Ltmp7178:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp7179:
# %bb.223:
.Ltmp7181:
	callq	mpfr_get_default_rounding_mode
.Ltmp7182:
# %bb.224:
.Ltmp7183:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movq	%r15, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp7184:
# %bb.225:
.Ltmp7186:
	leaq	16(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp7187:
# %bb.226:
	vmovsd	%xmm0, 80(%rsp)
	vmovsd	%xmm1, 88(%rsp)
	cmpq	$0, 40(%rsp)
	je	.LBB43_228
# %bb.227:
.Ltmp7189:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7190:
.LBB43_228:
	leaq	312(%rsp), %r15
	movq	%r15, 296(%rsp)
	movl	$2037413985, 312(%rsp)          # imm = 0x79707861
	movw	$32, 316(%rsp)
	movq	$5, 304(%rsp)
.Ltmp7192:
	leaq	296(%rsp), %rdi
	leaq	80(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp7193:
# %bb.229:
	movq	296(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB43_231
# %bb.230:
	callq	_ZdlPv
.LBB43_231:
	cmpq	$0, 72(%rsp)
	je	.LBB43_233
# %bb.232:
.Ltmp7195:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7196:
.LBB43_233:
	movl	168(%rsp), %ebp                 # 4-byte Reload
	movslq	%ebp, %r12
	movq	%r12, %r15
	shlq	$4, %r15
	testl	%r12d, %r12d
	movq	$-1, %rdi
	cmovnsq	%r15, %rdi
	callq	_Znam
	movq	%r12, 8(%rsp)                   # 8-byte Spill
	testl	%r12d, %r12d
	movq	%rax, 176(%rsp)                 # 8-byte Spill
	je	.LBB43_246
# %bb.234:
	xorl	%r13d, %r13d
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_intel_fast_memset@PLT
	testl	%ebp, %ebp
	jle	.LBB43_247
# %bb.235:
	cmpl	$4, %ebp
	movq	176(%rsp), %rbp                 # 8-byte Reload
	jb	.LBB43_238
# %bb.236:
	movq	8(%rsp), %r12                   # 8-byte Reload
	shrq	$2, %r12
	movl	$24, %r13d
	movq	184(%rsp), %r15                 # 8-byte Reload
	.p2align	4, 0x90
.LBB43_237:                             # =>This Inner Loop Header: Depth=1
	movq	%r15, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, -48(%r14,%r13,2)
	vmovsd	%xmm1, -40(%r14,%r13,2)
	vmovups	-48(%r14,%r13,2), %xmm0
	vmovups	%xmm0, -48(%rbp,%r13,2)
	leaq	32(%r15), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, -32(%r14,%r13,2)
	vmovsd	%xmm1, -24(%r14,%r13,2)
	vmovups	-32(%r14,%r13,2), %xmm0
	vmovups	%xmm0, -32(%rbp,%r13,2)
	leaq	64(%r15), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, -16(%r14,%r13,2)
	vmovsd	%xmm1, -8(%r14,%r13,2)
	vmovups	-16(%r14,%r13,2), %xmm0
	vmovups	%xmm0, -16(%rbp,%r13,2)
	leaq	96(%r15), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, (%r14,%r13,2)
	vmovsd	%xmm1, 8(%r14,%r13,2)
	vmovupd	(%r14,%r13,2), %xmm0
	vmovupd	%xmm0, (%rbp,%r13,2)
	subq	$-128, %r15
	addq	$32, %r13
	decq	%r12
	jne	.LBB43_237
.LBB43_238:
	movq	8(%rsp), %rcx                   # 8-byte Reload
	movq	%rcx, %rax
	andq	$-4, %rax
	cmpq	%rcx, %rax
	movq	%rbp, %r13
	movq	184(%rsp), %rbp                 # 8-byte Reload
	jae	.LBB43_241
# %bb.239:
	movq	8(%rsp), %rcx                   # 8-byte Reload
	movq	%rcx, %r15
	subq	%rax, %r15
	movq	%rcx, %rax
	shrq	$2, %rax
	movq	%rax, %r12
	shlq	$6, %r12
	shlq	$7, %rax
	addq	%rax, %rbp
	.p2align	4, 0x90
.LBB43_240:                             # =>This Inner Loop Header: Depth=1
	movq	%rbp, %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
	vmovsd	%xmm0, (%r14,%r12)
	vmovsd	%xmm1, 8(%r14,%r12)
	vmovupd	(%r14,%r12), %xmm0
	vmovupd	%xmm0, (%r13,%r12)
	addq	$16, %r12
	addq	$32, %rbp
	decq	%r15
	jne	.LBB43_240
.LBB43_241:
	leaq	8(%r13), %rax
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	vmovsd	200(%rsp), %xmm8                # 8-byte Reload
                                        # xmm8 = mem[0],zero
	vmovsd	192(%rsp), %xmm9                # 8-byte Reload
                                        # xmm9 = mem[0],zero
	movq	8(%rsp), %r8                    # 8-byte Reload
	.p2align	4, 0x90
.LBB43_242:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB43_243 Depth 2
	movq	%rdx, %rsi
	shlq	$4, %rsi
	vmovsd	(%r14,%rsi), %xmm2              # xmm2 = mem[0],zero
	vmulsd	%xmm2, %xmm8, %xmm0
	vmovapd	%xmm2, %xmm1
	vfmsub213sd	%xmm0, %xmm8, %xmm1     # xmm1 = (xmm8 * xmm1) - xmm0
	vfmadd231sd	%xmm2, %xmm9, %xmm1     # xmm1 = (xmm9 * xmm2) + xmm1
	vfmadd231sd	8(%r14,%rsi), %xmm8, %xmm1 # xmm1 = (xmm8 * mem) + xmm1
	movl	%ecx, %esi
	movq	%rax, %rdi
	.p2align	4, 0x90
.LBB43_243:                             #   Parent Loop BB43_242 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movslq	%esi, %rsi
	movq	%rsi, %r9
	shlq	$4, %r9
	vmovsd	(%rbx,%r9), %xmm2               # xmm2 = mem[0],zero
	vmulsd	%xmm0, %xmm2, %xmm3
	vmovapd	%xmm0, %xmm4
	vfmsub213sd	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vfmadd231sd	8(%rbx,%r9), %xmm0, %xmm4 # xmm4 = (xmm0 * mem) + xmm4
	vfmadd231sd	%xmm2, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm2) + xmm4
	vmovsd	-8(%rdi), %xmm2                 # xmm2 = mem[0],zero
	vaddsd	%xmm3, %xmm2, %xmm5
	vsubsd	%xmm2, %xmm5, %xmm6
	vsubsd	%xmm6, %xmm5, %xmm7
	vsubsd	%xmm7, %xmm2, %xmm2
	vsubsd	%xmm6, %xmm3, %xmm3
	vaddsd	%xmm3, %xmm2, %xmm2
	vaddsd	(%rdi), %xmm4, %xmm3
	vaddsd	%xmm2, %xmm3, %xmm2
	vmovsd	%xmm5, -8(%rdi)
	vmovsd	%xmm2, (%rdi)
	addq	$16, %rdi
	incl	%esi
	decq	%r8
	jne	.LBB43_243
# %bb.244:                              #   in Loop: Header=BB43_242 Depth=1
	incq	%rdx
	addl	168(%rsp), %ecx                 # 4-byte Folded Reload
	movq	8(%rsp), %r8                    # 8-byte Reload
	cmpq	%r8, %rdx
	jne	.LBB43_242
# %bb.245:
	movb	$1, %r13b
	jmp	.LBB43_247
.LBB43_246:
	xorl	%r13d, %r13d
.LBB43_247:
	callq	mpfr_get_default_prec
	movq	%rax, %r15
	callq	mpfr_get_default_rounding_mode
	movl	%eax, %ebp
	leaq	48(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
	movq	%r12, %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
	movl	%r13d, 184(%rsp)                # 4-byte Spill
	testb	%r13b, %r13b
	je	.LBB43_288
# %bb.248:
	xorl	%eax, %eax
	movq	%rax, 160(%rsp)                 # 8-byte Spill
	leaq	80(%rsp), %rbp
	movq	176(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB43_250
	.p2align	4, 0x90
.LBB43_249:                             #   in Loop: Header=BB43_250 Depth=1
	movq	160(%rsp), %rcx                 # 8-byte Reload
	incq	%rcx
	movq	224(%rsp), %rsi                 # 8-byte Reload
	addq	$16, %rsi
	movq	%rcx, %rax
	movq	%rcx, 160(%rsp)                 # 8-byte Spill
	cmpq	%rcx, 8(%rsp)                   # 8-byte Folded Reload
	je	.LBB43_288
.LBB43_250:                             # =>This Inner Loop Header: Depth=1
	movq	112(%rsp), %rax                 # 8-byte Reload
	movl	(%rax), %r15d
.Ltmp7198:
	movq	%rbp, %rdi
	movq	%rsi, 224(%rsp)                 # 8-byte Spill
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEN4mpfr6mprealEE4typeERKS6_
.Ltmp7199:
# %bb.251:                              #   in Loop: Header=BB43_250 Depth=1
	movq	160(%rsp), %rax                 # 8-byte Reload
	addl	%r15d, %eax
	addl	$8, %eax
	movslq	%eax, %r12
	shlq	$5, %r12
	addq	120(%rsp), %r12                 # 8-byte Folded Reload
.Ltmp7201:
	movq	%r12, %rdi
	callq	mpfr_get_prec
.Ltmp7202:
# %bb.252:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7203:
	movq	%rax, %r15
	leaq	80(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7204:
# %bb.253:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7205:
	movq	%rax, %rbp
	callq	mpfr_get_default_rounding_mode
.Ltmp7206:
# %bb.254:                              #   in Loop: Header=BB43_250 Depth=1
	movl	%eax, %r13d
	cmpq	%rbp, %r15
	cmovgq	%r15, %rbp
.Ltmp7207:
	leaq	16(%rsp), %rdi
	movq	%rbp, %rsi
	callq	mpfr_init2
.Ltmp7208:
# %bb.255:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7209:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r13d, %edx
	callq	mpfr_set_si
.Ltmp7210:
# %bb.256:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7212:
	callq	mpfr_get_default_rounding_mode
.Ltmp7213:
	leaq	80(%rsp), %rbp
# %bb.257:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7214:
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	movq	%rbp, %rdx
	movl	%eax, %ecx
	callq	mpfr_sub
.Ltmp7215:
# %bb.258:                              #   in Loop: Header=BB43_250 Depth=1
	cmpq	$0, 104(%rsp)
	je	.LBB43_260
# %bb.259:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7217:
	movq	%rbp, %rdi
	callq	mpfr_clear
.Ltmp7218:
.LBB43_260:                             #   in Loop: Header=BB43_250 Depth=1
.Ltmp7220:
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7221:
# %bb.261:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7222:
	movq	%rax, %r15
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7223:
# %bb.262:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7224:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp7225:
# %bb.263:                              #   in Loop: Header=BB43_250 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %r15
	cmovgq	%r15, %r13
.Ltmp7226:
	leaq	128(%rsp), %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp7227:
# %bb.264:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7228:
	leaq	128(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp7229:
# %bb.265:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7231:
	callq	mpfr_get_default_rounding_mode
.Ltmp7232:
# %bb.266:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7233:
	leaq	128(%rsp), %rdi
	leaq	16(%rsp), %rdx
	movq	%rdx, %rsi
	movl	%eax, %ecx
	callq	mpfr_mul
.Ltmp7234:
# %bb.267:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7236:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7237:
# %bb.268:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7238:
	movq	%rax, %r15
	leaq	128(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7239:
# %bb.269:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7240:
	movq	%rax, %r13
	callq	mpfr_get_default_rounding_mode
.Ltmp7241:
# %bb.270:                              #   in Loop: Header=BB43_250 Depth=1
	movl	%eax, %r12d
	cmpq	%r13, %r15
	cmovgq	%r15, %r13
.Ltmp7242:
	movq	%rbp, %rdi
	movq	%r13, %rsi
	callq	mpfr_init2
.Ltmp7243:
# %bb.271:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7244:
	movq	%rbp, %rdi
	xorl	%esi, %esi
	movl	%r12d, %edx
	callq	mpfr_set_si
.Ltmp7245:
# %bb.272:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7247:
	callq	mpfr_get_default_rounding_mode
.Ltmp7248:
	leaq	48(%rsp), %r15
# %bb.273:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7249:
	movq	%rbp, %rdi
	movq	%r15, %rsi
	leaq	128(%rsp), %rdx
	movl	%eax, %ecx
	callq	mpfr_add
.Ltmp7250:
# %bb.274:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7252:
	movq	%r15, %rdi
	callq	mpfr_get_prec
.Ltmp7253:
# %bb.275:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7254:
	movq	%rax, %r15
	movq	%rbp, %rdi
	callq	mpfr_get_prec
.Ltmp7255:
# %bb.276:                              #   in Loop: Header=BB43_250 Depth=1
	movq	%rax, %r12
	cmpq	%rax, %r15
	je	.LBB43_280
# %bb.277:                              #   in Loop: Header=BB43_250 Depth=1
	cmpq	$0, 72(%rsp)
	je	.LBB43_279
# %bb.278:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7256:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7257:
.LBB43_279:                             #   in Loop: Header=BB43_250 Depth=1
.Ltmp7258:
	leaq	48(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp7259:
.LBB43_280:                             #   in Loop: Header=BB43_250 Depth=1
.Ltmp7260:
	callq	mpfr_get_default_rounding_mode
.Ltmp7261:
# %bb.281:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7262:
	leaq	48(%rsp), %rdi
	movq	%rbp, %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp7263:
# %bb.282:                              #   in Loop: Header=BB43_250 Depth=1
	cmpq	$0, 104(%rsp)
	je	.LBB43_284
# %bb.283:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7265:
	movq	%rbp, %rdi
	callq	mpfr_clear
.Ltmp7266:
.LBB43_284:                             #   in Loop: Header=BB43_250 Depth=1
	cmpq	$0, 152(%rsp)
	je	.LBB43_286
# %bb.285:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7268:
	leaq	128(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7269:
.LBB43_286:                             #   in Loop: Header=BB43_250 Depth=1
	cmpq	$0, 40(%rsp)
	je	.LBB43_249
# %bb.287:                              #   in Loop: Header=BB43_250 Depth=1
.Ltmp7271:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7272:
	jmp	.LBB43_249
.LBB43_288:
.Ltmp7274:
	callq	mpfr_get_default_rounding_mode
.Ltmp7275:
# %bb.289:
.Ltmp7276:
	movl	%eax, %ebp
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7277:
# %bb.290:
.Ltmp7278:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp7279:
# %bb.291:
.Ltmp7280:
	movl	%eax, %r15d
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp7281:
# %bb.292:
.Ltmp7282:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%r15d, %edx
	callq	mpfr_set_si
.Ltmp7283:
# %bb.293:
.Ltmp7285:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movl	%ebp, %edx
	callq	mpfr_sqrt
.Ltmp7286:
# %bb.294:
.Ltmp7288:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7289:
# %bb.295:
.Ltmp7290:
	movq	%rax, %r12
	leaq	16(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7291:
# %bb.296:
	movq	%rax, %r15
	cmpq	%rax, %r12
	je	.LBB43_300
# %bb.297:
	cmpq	$0, 72(%rsp)
	je	.LBB43_299
# %bb.298:
.Ltmp7292:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7293:
.LBB43_299:
.Ltmp7294:
	leaq	48(%rsp), %rdi
	movq	%r15, %rsi
	callq	mpfr_init2
.Ltmp7295:
.LBB43_300:
.Ltmp7296:
	callq	mpfr_get_default_rounding_mode
.Ltmp7297:
# %bb.301:
.Ltmp7298:
	leaq	48(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	%eax, %edx
	callq	mpfr_set
.Ltmp7299:
# %bb.302:
	cmpq	$0, 40(%rsp)
	je	.LBB43_304
# %bb.303:
.Ltmp7301:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7302:
.LBB43_304:
	callq	omp_get_wtime
	vmovsd	%xmm0, 112(%rsp)                # 8-byte Spill
	cmpb	$0, 184(%rsp)                   # 1-byte Folded Reload
	vmovsd	200(%rsp), %xmm8                # 8-byte Reload
                                        # xmm8 = mem[0],zero
	vmovsd	192(%rsp), %xmm9                # 8-byte Reload
                                        # xmm9 = mem[0],zero
	je	.LBB43_311
# %bb.305:
	movq	176(%rsp), %rax                 # 8-byte Reload
	addq	$8, %rax
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB43_306:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB43_307 Depth 2
                                        #       Child Loop BB43_308 Depth 3
	xorl	%edx, %edx
	xorl	%esi, %esi
	movq	8(%rsp), %r9                    # 8-byte Reload
	.p2align	4, 0x90
.LBB43_307:                             #   Parent Loop BB43_306 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB43_308 Depth 3
	movq	%rsi, %rdi
	shlq	$4, %rdi
	vmovsd	(%r14,%rdi), %xmm2              # xmm2 = mem[0],zero
	vmulsd	%xmm2, %xmm8, %xmm0
	vmovapd	%xmm2, %xmm1
	vfmsub213sd	%xmm0, %xmm8, %xmm1     # xmm1 = (xmm8 * xmm1) - xmm0
	vfmadd231sd	%xmm2, %xmm9, %xmm1     # xmm1 = (xmm9 * xmm2) + xmm1
	vfmadd231sd	8(%r14,%rdi), %xmm8, %xmm1 # xmm1 = (xmm8 * mem) + xmm1
	movl	%edx, %edi
	movq	%rax, %r8
	.p2align	4, 0x90
.LBB43_308:                             #   Parent Loop BB43_306 Depth=1
                                        #     Parent Loop BB43_307 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movslq	%edi, %rdi
	movq	%rdi, %r10
	shlq	$4, %r10
	vmovsd	(%rbx,%r10), %xmm2              # xmm2 = mem[0],zero
	vmulsd	%xmm0, %xmm2, %xmm3
	vmovapd	%xmm0, %xmm4
	vfmsub213sd	%xmm3, %xmm2, %xmm4     # xmm4 = (xmm2 * xmm4) - xmm3
	vfmadd231sd	8(%rbx,%r10), %xmm0, %xmm4 # xmm4 = (xmm0 * mem) + xmm4
	vfmadd231sd	%xmm2, %xmm1, %xmm4     # xmm4 = (xmm1 * xmm2) + xmm4
	vmovsd	-8(%r8), %xmm2                  # xmm2 = mem[0],zero
	vaddsd	%xmm3, %xmm2, %xmm5
	vsubsd	%xmm2, %xmm5, %xmm6
	vsubsd	%xmm6, %xmm5, %xmm7
	vsubsd	%xmm7, %xmm2, %xmm2
	vsubsd	%xmm6, %xmm3, %xmm3
	vaddsd	%xmm3, %xmm2, %xmm2
	vaddsd	(%r8), %xmm4, %xmm3
	vaddsd	%xmm2, %xmm3, %xmm2
	vmovsd	%xmm5, -8(%r8)
	vmovsd	%xmm2, (%r8)
	addq	$16, %r8
	incl	%edi
	decq	%r9
	jne	.LBB43_308
# %bb.309:                              #   in Loop: Header=BB43_307 Depth=2
	incq	%rsi
	addl	168(%rsp), %edx                 # 4-byte Folded Reload
	movq	8(%rsp), %r9                    # 8-byte Reload
	cmpq	%r9, %rsi
	jne	.LBB43_307
# %bb.310:                              #   in Loop: Header=BB43_306 Depth=1
	incl	%ecx
	cmpl	$10, %ecx
	jne	.LBB43_306
.LBB43_311:
	callq	omp_get_wtime
	vsubsd	112(%rsp), %xmm0, %xmm0         # 8-byte Folded Reload
	movl	$10, %eax
	vcvtsi2sd	%eax, %xmm10, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	movl	$.L.str.57, %edi
	movb	$1, %al
	callq	printf
.Ltmp7304:
	leaq	48(%rsp), %rdi
	callq	mpfr_get_prec
.Ltmp7305:
# %bb.312:
	movq	%rax, %r15
	movq	120(%rsp), %rdi                 # 8-byte Reload
	subq	$-128, %rdi
.Ltmp7306:
	movq	%rdi, %r13
	callq	mpfr_get_prec
.Ltmp7307:
# %bb.313:
.Ltmp7308:
	movq	%rax, %r12
	callq	mpfr_get_default_rounding_mode
.Ltmp7309:
# %bb.314:
	movl	%eax, %ebp
	cmpq	%r12, %r15
	cmovgq	%r15, %r12
.Ltmp7310:
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	callq	mpfr_init2
.Ltmp7311:
# %bb.315:
.Ltmp7312:
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	movl	%ebp, %edx
	callq	mpfr_set_si
.Ltmp7313:
# %bb.316:
.Ltmp7315:
	callq	mpfr_get_default_rounding_mode
.Ltmp7316:
# %bb.317:
.Ltmp7317:
	leaq	16(%rsp), %rdi
	leaq	48(%rsp), %rsi
	movq	%r13, %rdx
	movl	%eax, %ecx
	callq	mpfr_div
.Ltmp7318:
# %bb.318:
.Ltmp7320:
	leaq	16(%rsp), %rdi
	callq	_ZN7mX_real7convertINS_7dX_real7dx_realIdLNS_9AlgorithmE2EEEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueES6_E4typeERKN4mpfr6mprealE
.Ltmp7321:
# %bb.319:
	vmovsd	%xmm0, 80(%rsp)
	vmovsd	%xmm1, 88(%rsp)
	cmpq	$0, 40(%rsp)
	je	.LBB43_321
# %bb.320:
.Ltmp7323:
	leaq	16(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7324:
.LBB43_321:
	leaq	280(%rsp), %r15
	movq	%r15, 264(%rsp)
	movl	$1986880871, 280(%rsp)          # imm = 0x766D6567
	movw	$32, 284(%rsp)
	movq	$5, 272(%rsp)
.Ltmp7326:
	leaq	264(%rsp), %rdi
	leaq	80(%rsp), %rsi
	xorl	%edx, %edx
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp7327:
# %bb.322:
	movq	264(%rsp), %rdi
	cmpq	%r15, %rdi
	je	.LBB43_324
# %bb.323:
	callq	_ZdlPv
.LBB43_324:
	movq	176(%rsp), %rdi                 # 8-byte Reload
	callq	_ZdaPv
	cmpq	$0, 72(%rsp)
	je	.LBB43_326
# %bb.325:
.Ltmp7329:
	leaq	48(%rsp), %rdi
	callq	mpfr_clear
.Ltmp7330:
.LBB43_326:
	movq	%rbx, %rdi
	callq	_ZdaPv
	movq	%r14, %rdi
	callq	_ZdaPv
	xorl	%eax, %eax
	vcvtsi2sd	%eax, %xmm10, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	movl	$2097152, %eax                  # imm = 0x200000
	movl	$1, %ecx
	vcvtsi2sd	%ecx, %xmm10, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	jmp	.LBB43_328
	.p2align	4, 0x90
.LBB43_327:                             #   in Loop: Header=BB43_328 Depth=1
	vdivsd	%xmm3, %xmm1, %xmm6
	vmovapd	%xmm6, %xmm7
	vfnmadd213sd	%xmm1, %xmm3, %xmm7     # xmm7 = -(xmm3 * xmm7) + xmm1
	vaddsd	%xmm0, %xmm7, %xmm7
	vfnmadd231sd	%xmm5, %xmm6, %xmm7     # xmm7 = -(xmm6 * xmm5) + xmm7
	vaddsd	%xmm5, %xmm3, %xmm3
	vdivsd	%xmm3, %xmm7, %xmm5
	vaddsd	%xmm6, %xmm4, %xmm3
	vsubsd	%xmm4, %xmm3, %xmm7
	vsubsd	%xmm7, %xmm3, %xmm8
	vsubsd	%xmm8, %xmm4, %xmm4
	vsubsd	%xmm7, %xmm6, %xmm6
	vaddsd	%xmm6, %xmm4, %xmm4
	vaddsd	%xmm5, %xmm2, %xmm2
	vaddsd	%xmm4, %xmm2, %xmm2
	addl	$-4, %eax
	je	.LBB43_352
.LBB43_328:                             # =>This Inner Loop Header: Depth=1
	vcvtsi2sd	%eax, %xmm10, %xmm4
	vmulsd	%xmm4, %xmm4, %xmm5
	vmovapd	%xmm4, %xmm7
	vfmsub213sd	%xmm5, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm7) - xmm5
	vfmadd231sd	%xmm0, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm0) + xmm7
	vfmadd231sd	%xmm4, %xmm0, %xmm7     # xmm7 = (xmm0 * xmm4) + xmm7
	vmulsd	%xmm5, %xmm5, %xmm4
	vmovapd	%xmm5, %xmm6
	vfmsub213sd	%xmm4, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm6) - xmm4
	vfmadd231sd	%xmm7, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm7) + xmm6
	vfmadd231sd	%xmm7, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm7) + xmm6
	vucomisd	%xmm0, %xmm4
	vmovapd	%xmm6, %xmm5
	jne	.LBB43_331
# %bb.329:                              #   in Loop: Header=BB43_328 Depth=1
	vmovapd	%xmm6, %xmm5
	jp	.LBB43_331
# %bb.330:                              #   in Loop: Header=BB43_328 Depth=1
	vmovapd	%xmm4, %xmm5
.LBB43_331:                             #   in Loop: Header=BB43_328 Depth=1
	jne	.LBB43_334
# %bb.332:                              #   in Loop: Header=BB43_328 Depth=1
	jp	.LBB43_334
# %bb.333:                              #   in Loop: Header=BB43_328 Depth=1
	vmovapd	%xmm6, %xmm4
.LBB43_334:                             #   in Loop: Header=BB43_328 Depth=1
	vdivsd	%xmm4, %xmm1, %xmm6
	vmovapd	%xmm6, %xmm7
	vfnmadd213sd	%xmm1, %xmm4, %xmm7     # xmm7 = -(xmm4 * xmm7) + xmm1
	vaddsd	%xmm0, %xmm7, %xmm7
	vfnmadd231sd	%xmm5, %xmm6, %xmm7     # xmm7 = -(xmm6 * xmm5) + xmm7
	vaddsd	%xmm5, %xmm4, %xmm4
	vdivsd	%xmm4, %xmm7, %xmm5
	vaddsd	%xmm6, %xmm3, %xmm4
	vsubsd	%xmm3, %xmm4, %xmm7
	vsubsd	%xmm7, %xmm4, %xmm8
	vsubsd	%xmm8, %xmm3, %xmm3
	vsubsd	%xmm7, %xmm6, %xmm6
	vaddsd	%xmm6, %xmm3, %xmm3
	vaddsd	%xmm5, %xmm2, %xmm2
	vaddsd	%xmm3, %xmm2, %xmm3
	leal	-1(%rax), %ecx
	vcvtsi2sd	%ecx, %xmm10, %xmm2
	vmulsd	%xmm2, %xmm2, %xmm5
	vmovapd	%xmm2, %xmm7
	vfmsub213sd	%xmm5, %xmm2, %xmm7     # xmm7 = (xmm2 * xmm7) - xmm5
	vfmadd231sd	%xmm0, %xmm2, %xmm7     # xmm7 = (xmm2 * xmm0) + xmm7
	vfmadd231sd	%xmm2, %xmm0, %xmm7     # xmm7 = (xmm0 * xmm2) + xmm7
	vmulsd	%xmm5, %xmm5, %xmm2
	vmovapd	%xmm5, %xmm6
	vfmsub213sd	%xmm2, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm6) - xmm2
	vfmadd231sd	%xmm7, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm7) + xmm6
	vfmadd231sd	%xmm7, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm7) + xmm6
	vucomisd	%xmm0, %xmm2
	vmovapd	%xmm6, %xmm5
	jne	.LBB43_337
# %bb.335:                              #   in Loop: Header=BB43_328 Depth=1
	vmovapd	%xmm6, %xmm5
	jp	.LBB43_337
# %bb.336:                              #   in Loop: Header=BB43_328 Depth=1
	vmovapd	%xmm2, %xmm5
.LBB43_337:                             #   in Loop: Header=BB43_328 Depth=1
	jne	.LBB43_340
# %bb.338:                              #   in Loop: Header=BB43_328 Depth=1
	jp	.LBB43_340
# %bb.339:                              #   in Loop: Header=BB43_328 Depth=1
	vmovapd	%xmm6, %xmm2
.LBB43_340:                             #   in Loop: Header=BB43_328 Depth=1
	vdivsd	%xmm2, %xmm1, %xmm6
	vmovapd	%xmm6, %xmm7
	vfnmadd213sd	%xmm1, %xmm2, %xmm7     # xmm7 = -(xmm2 * xmm7) + xmm1
	vaddsd	%xmm0, %xmm7, %xmm7
	vfnmadd231sd	%xmm5, %xmm6, %xmm7     # xmm7 = -(xmm6 * xmm5) + xmm7
	vaddsd	%xmm5, %xmm2, %xmm2
	vdivsd	%xmm2, %xmm7, %xmm5
	vaddsd	%xmm6, %xmm4, %xmm2
	vsubsd	%xmm4, %xmm2, %xmm7
	vsubsd	%xmm7, %xmm2, %xmm8
	vsubsd	%xmm8, %xmm4, %xmm4
	vsubsd	%xmm7, %xmm6, %xmm6
	vaddsd	%xmm6, %xmm4, %xmm4
	vaddsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm4, %xmm3, %xmm3
	leal	-2(%rax), %ecx
	vcvtsi2sd	%ecx, %xmm9, %xmm4
	vmulsd	%xmm4, %xmm4, %xmm5
	vmovapd	%xmm4, %xmm7
	vfmsub213sd	%xmm5, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm7) - xmm5
	vfmadd231sd	%xmm0, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm0) + xmm7
	vfmadd231sd	%xmm4, %xmm0, %xmm7     # xmm7 = (xmm0 * xmm4) + xmm7
	vmulsd	%xmm5, %xmm5, %xmm4
	vmovapd	%xmm5, %xmm6
	vfmsub213sd	%xmm4, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm6) - xmm4
	vfmadd231sd	%xmm7, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm7) + xmm6
	vfmadd231sd	%xmm7, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm7) + xmm6
	vucomisd	%xmm0, %xmm4
	vmovapd	%xmm6, %xmm5
	jne	.LBB43_343
# %bb.341:                              #   in Loop: Header=BB43_328 Depth=1
	vmovapd	%xmm6, %xmm5
	jp	.LBB43_343
# %bb.342:                              #   in Loop: Header=BB43_328 Depth=1
	vmovapd	%xmm4, %xmm5
.LBB43_343:                             #   in Loop: Header=BB43_328 Depth=1
	jne	.LBB43_346
# %bb.344:                              #   in Loop: Header=BB43_328 Depth=1
	jp	.LBB43_346
# %bb.345:                              #   in Loop: Header=BB43_328 Depth=1
	vmovapd	%xmm6, %xmm4
.LBB43_346:                             #   in Loop: Header=BB43_328 Depth=1
	vdivsd	%xmm4, %xmm1, %xmm6
	vmovapd	%xmm6, %xmm7
	vfnmadd213sd	%xmm1, %xmm4, %xmm7     # xmm7 = -(xmm4 * xmm7) + xmm1
	vaddsd	%xmm0, %xmm7, %xmm7
	vfnmadd231sd	%xmm5, %xmm6, %xmm7     # xmm7 = -(xmm6 * xmm5) + xmm7
	vaddsd	%xmm5, %xmm4, %xmm4
	vdivsd	%xmm4, %xmm7, %xmm5
	vaddsd	%xmm6, %xmm2, %xmm4
	vsubsd	%xmm2, %xmm4, %xmm7
	vsubsd	%xmm7, %xmm4, %xmm8
	vsubsd	%xmm8, %xmm2, %xmm2
	vsubsd	%xmm7, %xmm6, %xmm6
	vaddsd	%xmm6, %xmm2, %xmm2
	vaddsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm2, %xmm3, %xmm2
	leal	-3(%rax), %ecx
	vcvtsi2sd	%ecx, %xmm9, %xmm3
	vmulsd	%xmm3, %xmm3, %xmm5
	vmovapd	%xmm3, %xmm7
	vfmsub213sd	%xmm5, %xmm3, %xmm7     # xmm7 = (xmm3 * xmm7) - xmm5
	vfmadd231sd	%xmm0, %xmm3, %xmm7     # xmm7 = (xmm3 * xmm0) + xmm7
	vfmadd231sd	%xmm3, %xmm0, %xmm7     # xmm7 = (xmm0 * xmm3) + xmm7
	vmulsd	%xmm5, %xmm5, %xmm3
	vmovapd	%xmm5, %xmm6
	vfmsub213sd	%xmm3, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm6) - xmm3
	vfmadd231sd	%xmm7, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm7) + xmm6
	vfmadd231sd	%xmm7, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm7) + xmm6
	vucomisd	%xmm0, %xmm3
	vmovapd	%xmm6, %xmm5
	jne	.LBB43_349
# %bb.347:                              #   in Loop: Header=BB43_328 Depth=1
	vmovapd	%xmm6, %xmm5
	jp	.LBB43_349
# %bb.348:                              #   in Loop: Header=BB43_328 Depth=1
	vmovapd	%xmm3, %xmm5
.LBB43_349:                             #   in Loop: Header=BB43_328 Depth=1
	jne	.LBB43_327
# %bb.350:                              #   in Loop: Header=BB43_328 Depth=1
	jp	.LBB43_327
# %bb.351:                              #   in Loop: Header=BB43_328 Depth=1
	vmovapd	%xmm6, %xmm3
	jmp	.LBB43_327
.LBB43_352:
	movl	$90, %eax
	vcvtsi2sd	%eax, %xmm0, %xmm1
	vmulsd	%xmm3, %xmm1, %xmm0
	vfmsub213sd	%xmm0, %xmm1, %xmm3     # xmm3 = (xmm1 * xmm3) - xmm0
	vfmadd213sd	%xmm3, %xmm1, %xmm2     # xmm2 = (xmm1 * xmm2) + xmm3
	vaddsd	%xmm2, %xmm0, %xmm1
	vxorpd	%xmm3, %xmm3, %xmm3
	vucomisd	%xmm3, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB43_360
# %bb.353:
	vucomisd	%xmm3, %xmm0
	vmovapd	%xmm2, %xmm1
	jne	.LBB43_356
# %bb.354:
	vmovapd	%xmm2, %xmm1
	jp	.LBB43_356
# %bb.355:
	vmovapd	%xmm0, %xmm1
.LBB43_356:
	vmovsd	%xmm1, 168(%rsp)                # 8-byte Spill
	jne	.LBB43_359
# %bb.357:
	jp	.LBB43_359
# %bb.358:
	vmovapd	%xmm2, %xmm0
.LBB43_359:
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
	callq	sqrt
	vmovsd	8(%rsp), %xmm1                  # 8-byte Reload
                                        # xmm1 = mem[0],zero
	vfnmadd231sd	%xmm0, %xmm0, %xmm1     # xmm1 = -(xmm0 * xmm0) + xmm1
	vaddsd	168(%rsp), %xmm1, %xmm1         # 8-byte Folded Reload
	vaddsd	%xmm0, %xmm0, %xmm2
	vdivsd	%xmm2, %xmm1, %xmm2
	vxorpd	%xmm3, %xmm3, %xmm3
.LBB43_360:
	vaddsd	%xmm2, %xmm0, %xmm1
	vucomisd	%xmm3, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB43_368
# %bb.361:
	vxorpd	%xmm1, %xmm1, %xmm1
	vucomisd	%xmm1, %xmm0
	vmovapd	%xmm2, %xmm1
	jne	.LBB43_364
# %bb.362:
	vmovapd	%xmm2, %xmm1
	jp	.LBB43_364
# %bb.363:
	vmovapd	%xmm0, %xmm1
.LBB43_364:
	vmovsd	%xmm1, 168(%rsp)                # 8-byte Spill
	jne	.LBB43_367
# %bb.365:
	jp	.LBB43_367
# %bb.366:
	vmovapd	%xmm2, %xmm0
.LBB43_367:
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
	callq	sqrt
	vmovsd	8(%rsp), %xmm1                  # 8-byte Reload
                                        # xmm1 = mem[0],zero
	vfnmadd231sd	%xmm0, %xmm0, %xmm1     # xmm1 = -(xmm0 * xmm0) + xmm1
	vaddsd	168(%rsp), %xmm1, %xmm1         # 8-byte Folded Reload
	vaddsd	%xmm0, %xmm0, %xmm2
	vdivsd	%xmm2, %xmm1, %xmm2
.LBB43_368:
	vmovsd	%xmm0, 48(%rsp)
	vmovsd	%xmm2, 56(%rsp)
	leaq	248(%rsp), %r14
	movq	%r14, 232(%rsp)
	movq	$32, 16(%rsp)
.Ltmp7332:
	leaq	232(%rsp), %rdi
	leaq	16(%rsp), %rsi
	xorl	%edx, %edx
	callq	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm
.Ltmp7333:
# %bb.369:
	movq	%rax, 232(%rsp)
	movq	16(%rsp), %rcx
	movq	%rcx, 248(%rsp)
	vmovupd	.L.str.63(%rip), %ymm0
	vmovupd	%ymm0, (%rax)
	movq	%rcx, 240(%rsp)
	movq	232(%rsp), %rax
	movb	$0, (%rax,%rcx)
.Ltmp7335:
	leaq	232(%rsp), %rdi
	leaq	48(%rsp), %rsi
	xorl	%edx, %edx
	vzeroupper
	callq	_Z5printIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEENSt9enable_ifIXsr13check_mX_realIT_EE5valueEvE4typeENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKS6_b
.Ltmp7336:
# %bb.370:
	movq	232(%rsp), %rdi
	cmpq	%r14, %rdi
	je	.LBB43_372
# %bb.371:
	callq	_ZdlPv
.LBB43_372:
	addq	$424, %rsp                      # imm = 0x1A8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB43_373:
	.cfi_def_cfa_offset 480
.Ltmp7331:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_374:
.Ltmp7325:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_375:
.Ltmp7303:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_376:
.Ltmp7197:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_377:
.Ltmp7191:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_378:
.Ltmp7169:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_379:
.Ltmp7060:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_380:
.Ltmp7057:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_381:
.Ltmp7054:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_382:
.Ltmp7051:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_383:
.Ltmp6996:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_384:
.Ltmp6993:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_385:
.Ltmp6990:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_386:
.Ltmp6987:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_387:
.Ltmp6932:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_388:
.Ltmp6929:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_389:
.Ltmp6926:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_390:
.Ltmp6923:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_391:
.Ltmp7337:
	movq	%rax, %rbx
	movq	232(%rsp), %rdi
	cmpq	%r14, %rdi
	jne	.LBB43_409
	jmp	.LBB43_410
.LBB43_392:
.Ltmp7334:
	movq	%rax, %rdi
	callq	_Unwind_Resume@PLT
.LBB43_393:
.Ltmp7328:
	movq	%rax, %rbx
	movq	264(%rsp), %rdi
	jmp	.LBB43_397
.LBB43_394:
.Ltmp7322:
	movq	%rax, %rbx
	jmp	.LBB43_467
.LBB43_395:
.Ltmp7287:
	movq	%rax, %rbx
	jmp	.LBB43_467
.LBB43_396:
.Ltmp7194:
	movq	%rax, %rbx
	movq	296(%rsp), %rdi
.LBB43_397:
	cmpq	%r15, %rdi
	je	.LBB43_473
# %bb.398:
	callq	_ZdlPv
	jmp	.LBB43_473
.LBB43_399:
.Ltmp7188:
	movq	%rax, %rbx
	jmp	.LBB43_467
.LBB43_400:
.Ltmp7153:
	movq	%rax, %rbx
	jmp	.LBB43_467
.LBB43_401:
.Ltmp7063:
	movq	%rax, %rbx
	movq	328(%rsp), %rdi
	jmp	.LBB43_408
.LBB43_402:
.Ltmp7048:
	jmp	.LBB43_413
.LBB43_403:
.Ltmp7045:
	jmp	.LBB43_413
.LBB43_404:
.Ltmp6999:
	movq	%rax, %rbx
	movq	360(%rsp), %rdi
	jmp	.LBB43_408
.LBB43_405:
.Ltmp6984:
	jmp	.LBB43_413
.LBB43_406:
.Ltmp6981:
	jmp	.LBB43_413
.LBB43_407:
.Ltmp6935:
	movq	%rax, %rbx
	movq	392(%rsp), %rdi
.LBB43_408:
	cmpq	%r15, %rdi
	je	.LBB43_410
.LBB43_409:
	callq	_ZdlPv
.LBB43_410:
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB43_411:
.Ltmp6920:
	jmp	.LBB43_413
.LBB43_412:
.Ltmp6917:
.LBB43_413:
	movq	%rax, %rbx
	leaq	48(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB43_436
.LBB43_414:
.Ltmp7319:
	movq	%rax, %rbx
	jmp	.LBB43_467
.LBB43_415:
.Ltmp7185:
	movq	%rax, %rbx
	jmp	.LBB43_467
.LBB43_416:
.Ltmp7031:
	jmp	.LBB43_435
.LBB43_417:
.Ltmp7015:
	movq	%rax, %rbx
	jmp	.LBB43_437
.LBB43_418:
.Ltmp6967:
	jmp	.LBB43_435
.LBB43_419:
.Ltmp6951:
	movq	%rax, %rbx
	jmp	.LBB43_437
.LBB43_420:
.Ltmp6903:
	jmp	.LBB43_435
.LBB43_421:
.Ltmp6887:
	movq	%rax, %rbx
	jmp	.LBB43_437
.LBB43_422:
.Ltmp7300:
	movq	%rax, %rbx
	jmp	.LBB43_467
.LBB43_423:
.Ltmp7166:
	movq	%rax, %rbx
	jmp	.LBB43_467
.LBB43_424:
.Ltmp7314:
	movq	%rax, %rbx
	jmp	.LBB43_473
.LBB43_425:
.Ltmp7284:
	movq	%rax, %rbx
	jmp	.LBB43_473
.LBB43_426:
.Ltmp7180:
	movq	%rax, %rbx
	jmp	.LBB43_473
.LBB43_427:
.Ltmp7150:
	movq	%rax, %rbx
	jmp	.LBB43_473
.LBB43_428:
.Ltmp7042:
	jmp	.LBB43_435
.LBB43_429:
.Ltmp7026:
	movq	%rax, %rbx
	jmp	.LBB43_437
.LBB43_430:
.Ltmp7010:
	jmp	.LBB43_440
.LBB43_431:
.Ltmp6978:
	jmp	.LBB43_435
.LBB43_432:
.Ltmp6962:
	movq	%rax, %rbx
	jmp	.LBB43_437
.LBB43_433:
.Ltmp6946:
	jmp	.LBB43_440
.LBB43_434:
.Ltmp6914:
.LBB43_435:
	movq	%rax, %rbx
.LBB43_436:
	leaq	16(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB43_437:
	leaq	80(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	leaq	128(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB43_438:
.Ltmp6898:
	movq	%rax, %rbx
	jmp	.LBB43_437
.LBB43_439:
.Ltmp6882:
.LBB43_440:
	movq	%rax, %rbx
	leaq	128(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.LBB43_441:
.Ltmp7273:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_442:
.Ltmp7270:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_443:
.Ltmp7267:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_444:
.Ltmp7219:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_445:
.Ltmp7139:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_446:
.Ltmp7136:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_447:
.Ltmp7133:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_448:
.Ltmp7085:
	movq	%rax, %rdi
	callq	__clang_call_terminate
.LBB43_449:
.Ltmp7200:
	movq	%rax, %rbx
	jmp	.LBB43_473
.LBB43_450:
.Ltmp7066:
	movq	%rax, %rbx
	jmp	.LBB43_473
.LBB43_451:
.Ltmp7251:
	jmp	.LBB43_463
.LBB43_452:
.Ltmp7235:
	jmp	.LBB43_465
.LBB43_453:
.Ltmp7216:
	jmp	.LBB43_457
.LBB43_454:
.Ltmp7117:
	jmp	.LBB43_463
.LBB43_455:
.Ltmp7101:
	jmp	.LBB43_465
.LBB43_456:
.Ltmp7082:
.LBB43_457:
	movq	%rax, %rbx
	leaq	16(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB43_471
.LBB43_458:
.Ltmp7264:
	jmp	.LBB43_463
.LBB43_459:
.Ltmp7246:
	jmp	.LBB43_465
.LBB43_460:
.Ltmp7230:
	movq	%rax, %rbx
	jmp	.LBB43_467
.LBB43_461:
.Ltmp7211:
	jmp	.LBB43_470
.LBB43_462:
.Ltmp7130:
.LBB43_463:
	movq	%rax, %rbx
	leaq	80(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	jmp	.LBB43_466
.LBB43_464:
.Ltmp7112:
.LBB43_465:
	movq	%rax, %rbx
.LBB43_466:
	leaq	128(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
.LBB43_467:
	leaq	16(%rsp), %rdi
	jmp	.LBB43_472
.LBB43_468:
.Ltmp7096:
	movq	%rax, %rbx
	jmp	.LBB43_467
.LBB43_469:
.Ltmp7077:
.LBB43_470:
	movq	%rax, %rbx
.LBB43_471:
	leaq	80(%rsp), %rdi
.LBB43_472:
	callq	_ZN4mpfr6mprealD2Ev
.LBB43_473:
	leaq	48(%rsp), %rdi
	callq	_ZN4mpfr6mprealD2Ev
	movq	%rbx, %rdi
	callq	_Unwind_Resume@PLT
.Lfunc_end43:
	.size	_Z6verifyIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_, .Lfunc_end43-_Z6verifyIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_
	.cfi_endproc
	.section	.gcc_except_table._Z6verifyIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,"aG",@progbits,_Z6verifyIN7mX_real7dX_real7dx_realIdLNS0_9AlgorithmE2EEEEvRKiRKN4mpfr6mprealEPS8_SB_SB_,comdat
	.p2align	2, 0x0
GCC_except_table43:
.Lexception36:
	.byte	255                             # @LPStart Encoding = omit
	.byte	3                               # @TType Encoding = udata4
	.uleb128 .Lttbase25-.Lttbaseref25
.Lttbaseref25:
	.byte	1                               # Call site Encoding = uleb128
	.uleb128 .Lcst_end36-.Lcst_begin36
.Lcst_begin36:
	.uleb128 .Lfunc_begin36-.Lfunc_begin36  # >> Call Site 1 <<
	.uleb128 .Ltmp6872-.Lfunc_begin36       #   Call between .Lfunc_begin36 and .Ltmp6872
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6872-.Lfunc_begin36       # >> Call Site 2 <<
	.uleb128 .Ltmp6881-.Ltmp6872            #   Call between .Ltmp6872 and .Ltmp6881
	.uleb128 .Ltmp6882-.Lfunc_begin36       #     jumps to .Ltmp6882
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6883-.Lfunc_begin36       # >> Call Site 3 <<
	.uleb128 .Ltmp6886-.Ltmp6883            #   Call between .Ltmp6883 and .Ltmp6886
	.uleb128 .Ltmp6887-.Lfunc_begin36       #     jumps to .Ltmp6887
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6888-.Lfunc_begin36       # >> Call Site 4 <<
	.uleb128 .Ltmp6897-.Ltmp6888            #   Call between .Ltmp6888 and .Ltmp6897
	.uleb128 .Ltmp6898-.Lfunc_begin36       #     jumps to .Ltmp6898
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6899-.Lfunc_begin36       # >> Call Site 5 <<
	.uleb128 .Ltmp6902-.Ltmp6899            #   Call between .Ltmp6899 and .Ltmp6902
	.uleb128 .Ltmp6903-.Lfunc_begin36       #     jumps to .Ltmp6903
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6904-.Lfunc_begin36       # >> Call Site 6 <<
	.uleb128 .Ltmp6913-.Ltmp6904            #   Call between .Ltmp6904 and .Ltmp6913
	.uleb128 .Ltmp6914-.Lfunc_begin36       #     jumps to .Ltmp6914
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6915-.Lfunc_begin36       # >> Call Site 7 <<
	.uleb128 .Ltmp6916-.Ltmp6915            #   Call between .Ltmp6915 and .Ltmp6916
	.uleb128 .Ltmp6917-.Lfunc_begin36       #     jumps to .Ltmp6917
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6918-.Lfunc_begin36       # >> Call Site 8 <<
	.uleb128 .Ltmp6919-.Ltmp6918            #   Call between .Ltmp6918 and .Ltmp6919
	.uleb128 .Ltmp6920-.Lfunc_begin36       #     jumps to .Ltmp6920
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6921-.Lfunc_begin36       # >> Call Site 9 <<
	.uleb128 .Ltmp6922-.Ltmp6921            #   Call between .Ltmp6921 and .Ltmp6922
	.uleb128 .Ltmp6923-.Lfunc_begin36       #     jumps to .Ltmp6923
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6924-.Lfunc_begin36       # >> Call Site 10 <<
	.uleb128 .Ltmp6925-.Ltmp6924            #   Call between .Ltmp6924 and .Ltmp6925
	.uleb128 .Ltmp6926-.Lfunc_begin36       #     jumps to .Ltmp6926
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6927-.Lfunc_begin36       # >> Call Site 11 <<
	.uleb128 .Ltmp6928-.Ltmp6927            #   Call between .Ltmp6927 and .Ltmp6928
	.uleb128 .Ltmp6929-.Lfunc_begin36       #     jumps to .Ltmp6929
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6930-.Lfunc_begin36       # >> Call Site 12 <<
	.uleb128 .Ltmp6931-.Ltmp6930            #   Call between .Ltmp6930 and .Ltmp6931
	.uleb128 .Ltmp6932-.Lfunc_begin36       #     jumps to .Ltmp6932
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6933-.Lfunc_begin36       # >> Call Site 13 <<
	.uleb128 .Ltmp6934-.Ltmp6933            #   Call between .Ltmp6933 and .Ltmp6934
	.uleb128 .Ltmp6935-.Lfunc_begin36       #     jumps to .Ltmp6935
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6934-.Lfunc_begin36       # >> Call Site 14 <<
	.uleb128 .Ltmp6936-.Ltmp6934            #   Call between .Ltmp6934 and .Ltmp6936
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6936-.Lfunc_begin36       # >> Call Site 15 <<
	.uleb128 .Ltmp6945-.Ltmp6936            #   Call between .Ltmp6936 and .Ltmp6945
	.uleb128 .Ltmp6946-.Lfunc_begin36       #     jumps to .Ltmp6946
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6947-.Lfunc_begin36       # >> Call Site 16 <<
	.uleb128 .Ltmp6950-.Ltmp6947            #   Call between .Ltmp6947 and .Ltmp6950
	.uleb128 .Ltmp6951-.Lfunc_begin36       #     jumps to .Ltmp6951
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6952-.Lfunc_begin36       # >> Call Site 17 <<
	.uleb128 .Ltmp6961-.Ltmp6952            #   Call between .Ltmp6952 and .Ltmp6961
	.uleb128 .Ltmp6962-.Lfunc_begin36       #     jumps to .Ltmp6962
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6963-.Lfunc_begin36       # >> Call Site 18 <<
	.uleb128 .Ltmp6966-.Ltmp6963            #   Call between .Ltmp6963 and .Ltmp6966
	.uleb128 .Ltmp6967-.Lfunc_begin36       #     jumps to .Ltmp6967
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6968-.Lfunc_begin36       # >> Call Site 19 <<
	.uleb128 .Ltmp6977-.Ltmp6968            #   Call between .Ltmp6968 and .Ltmp6977
	.uleb128 .Ltmp6978-.Lfunc_begin36       #     jumps to .Ltmp6978
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6979-.Lfunc_begin36       # >> Call Site 20 <<
	.uleb128 .Ltmp6980-.Ltmp6979            #   Call between .Ltmp6979 and .Ltmp6980
	.uleb128 .Ltmp6981-.Lfunc_begin36       #     jumps to .Ltmp6981
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6982-.Lfunc_begin36       # >> Call Site 21 <<
	.uleb128 .Ltmp6983-.Ltmp6982            #   Call between .Ltmp6982 and .Ltmp6983
	.uleb128 .Ltmp6984-.Lfunc_begin36       #     jumps to .Ltmp6984
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6985-.Lfunc_begin36       # >> Call Site 22 <<
	.uleb128 .Ltmp6986-.Ltmp6985            #   Call between .Ltmp6985 and .Ltmp6986
	.uleb128 .Ltmp6987-.Lfunc_begin36       #     jumps to .Ltmp6987
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6988-.Lfunc_begin36       # >> Call Site 23 <<
	.uleb128 .Ltmp6989-.Ltmp6988            #   Call between .Ltmp6988 and .Ltmp6989
	.uleb128 .Ltmp6990-.Lfunc_begin36       #     jumps to .Ltmp6990
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6991-.Lfunc_begin36       # >> Call Site 24 <<
	.uleb128 .Ltmp6992-.Ltmp6991            #   Call between .Ltmp6991 and .Ltmp6992
	.uleb128 .Ltmp6993-.Lfunc_begin36       #     jumps to .Ltmp6993
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6994-.Lfunc_begin36       # >> Call Site 25 <<
	.uleb128 .Ltmp6995-.Ltmp6994            #   Call between .Ltmp6994 and .Ltmp6995
	.uleb128 .Ltmp6996-.Lfunc_begin36       #     jumps to .Ltmp6996
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp6997-.Lfunc_begin36       # >> Call Site 26 <<
	.uleb128 .Ltmp6998-.Ltmp6997            #   Call between .Ltmp6997 and .Ltmp6998
	.uleb128 .Ltmp6999-.Lfunc_begin36       #     jumps to .Ltmp6999
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp6998-.Lfunc_begin36       # >> Call Site 27 <<
	.uleb128 .Ltmp7000-.Ltmp6998            #   Call between .Ltmp6998 and .Ltmp7000
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7000-.Lfunc_begin36       # >> Call Site 28 <<
	.uleb128 .Ltmp7009-.Ltmp7000            #   Call between .Ltmp7000 and .Ltmp7009
	.uleb128 .Ltmp7010-.Lfunc_begin36       #     jumps to .Ltmp7010
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7011-.Lfunc_begin36       # >> Call Site 29 <<
	.uleb128 .Ltmp7014-.Ltmp7011            #   Call between .Ltmp7011 and .Ltmp7014
	.uleb128 .Ltmp7015-.Lfunc_begin36       #     jumps to .Ltmp7015
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7016-.Lfunc_begin36       # >> Call Site 30 <<
	.uleb128 .Ltmp7025-.Ltmp7016            #   Call between .Ltmp7016 and .Ltmp7025
	.uleb128 .Ltmp7026-.Lfunc_begin36       #     jumps to .Ltmp7026
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7027-.Lfunc_begin36       # >> Call Site 31 <<
	.uleb128 .Ltmp7030-.Ltmp7027            #   Call between .Ltmp7027 and .Ltmp7030
	.uleb128 .Ltmp7031-.Lfunc_begin36       #     jumps to .Ltmp7031
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7032-.Lfunc_begin36       # >> Call Site 32 <<
	.uleb128 .Ltmp7041-.Ltmp7032            #   Call between .Ltmp7032 and .Ltmp7041
	.uleb128 .Ltmp7042-.Lfunc_begin36       #     jumps to .Ltmp7042
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7043-.Lfunc_begin36       # >> Call Site 33 <<
	.uleb128 .Ltmp7044-.Ltmp7043            #   Call between .Ltmp7043 and .Ltmp7044
	.uleb128 .Ltmp7045-.Lfunc_begin36       #     jumps to .Ltmp7045
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7046-.Lfunc_begin36       # >> Call Site 34 <<
	.uleb128 .Ltmp7047-.Ltmp7046            #   Call between .Ltmp7046 and .Ltmp7047
	.uleb128 .Ltmp7048-.Lfunc_begin36       #     jumps to .Ltmp7048
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7049-.Lfunc_begin36       # >> Call Site 35 <<
	.uleb128 .Ltmp7050-.Ltmp7049            #   Call between .Ltmp7049 and .Ltmp7050
	.uleb128 .Ltmp7051-.Lfunc_begin36       #     jumps to .Ltmp7051
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7052-.Lfunc_begin36       # >> Call Site 36 <<
	.uleb128 .Ltmp7053-.Ltmp7052            #   Call between .Ltmp7052 and .Ltmp7053
	.uleb128 .Ltmp7054-.Lfunc_begin36       #     jumps to .Ltmp7054
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7055-.Lfunc_begin36       # >> Call Site 37 <<
	.uleb128 .Ltmp7056-.Ltmp7055            #   Call between .Ltmp7055 and .Ltmp7056
	.uleb128 .Ltmp7057-.Lfunc_begin36       #     jumps to .Ltmp7057
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7058-.Lfunc_begin36       # >> Call Site 38 <<
	.uleb128 .Ltmp7059-.Ltmp7058            #   Call between .Ltmp7058 and .Ltmp7059
	.uleb128 .Ltmp7060-.Lfunc_begin36       #     jumps to .Ltmp7060
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7061-.Lfunc_begin36       # >> Call Site 39 <<
	.uleb128 .Ltmp7062-.Ltmp7061            #   Call between .Ltmp7061 and .Ltmp7062
	.uleb128 .Ltmp7063-.Lfunc_begin36       #     jumps to .Ltmp7063
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7062-.Lfunc_begin36       # >> Call Site 40 <<
	.uleb128 .Ltmp7064-.Ltmp7062            #   Call between .Ltmp7062 and .Ltmp7064
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7064-.Lfunc_begin36       # >> Call Site 41 <<
	.uleb128 .Ltmp7065-.Ltmp7064            #   Call between .Ltmp7064 and .Ltmp7065
	.uleb128 .Ltmp7066-.Lfunc_begin36       #     jumps to .Ltmp7066
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7067-.Lfunc_begin36       # >> Call Site 42 <<
	.uleb128 .Ltmp7076-.Ltmp7067            #   Call between .Ltmp7067 and .Ltmp7076
	.uleb128 .Ltmp7077-.Lfunc_begin36       #     jumps to .Ltmp7077
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7078-.Lfunc_begin36       # >> Call Site 43 <<
	.uleb128 .Ltmp7081-.Ltmp7078            #   Call between .Ltmp7078 and .Ltmp7081
	.uleb128 .Ltmp7082-.Lfunc_begin36       #     jumps to .Ltmp7082
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7083-.Lfunc_begin36       # >> Call Site 44 <<
	.uleb128 .Ltmp7084-.Ltmp7083            #   Call between .Ltmp7083 and .Ltmp7084
	.uleb128 .Ltmp7085-.Lfunc_begin36       #     jumps to .Ltmp7085
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7086-.Lfunc_begin36       # >> Call Site 45 <<
	.uleb128 .Ltmp7095-.Ltmp7086            #   Call between .Ltmp7086 and .Ltmp7095
	.uleb128 .Ltmp7096-.Lfunc_begin36       #     jumps to .Ltmp7096
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7097-.Lfunc_begin36       # >> Call Site 46 <<
	.uleb128 .Ltmp7100-.Ltmp7097            #   Call between .Ltmp7097 and .Ltmp7100
	.uleb128 .Ltmp7101-.Lfunc_begin36       #     jumps to .Ltmp7101
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7102-.Lfunc_begin36       # >> Call Site 47 <<
	.uleb128 .Ltmp7111-.Ltmp7102            #   Call between .Ltmp7102 and .Ltmp7111
	.uleb128 .Ltmp7112-.Lfunc_begin36       #     jumps to .Ltmp7112
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7113-.Lfunc_begin36       # >> Call Site 48 <<
	.uleb128 .Ltmp7116-.Ltmp7113            #   Call between .Ltmp7113 and .Ltmp7116
	.uleb128 .Ltmp7117-.Lfunc_begin36       #     jumps to .Ltmp7117
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7118-.Lfunc_begin36       # >> Call Site 49 <<
	.uleb128 .Ltmp7129-.Ltmp7118            #   Call between .Ltmp7118 and .Ltmp7129
	.uleb128 .Ltmp7130-.Lfunc_begin36       #     jumps to .Ltmp7130
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7131-.Lfunc_begin36       # >> Call Site 50 <<
	.uleb128 .Ltmp7132-.Ltmp7131            #   Call between .Ltmp7131 and .Ltmp7132
	.uleb128 .Ltmp7133-.Lfunc_begin36       #     jumps to .Ltmp7133
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7134-.Lfunc_begin36       # >> Call Site 51 <<
	.uleb128 .Ltmp7135-.Ltmp7134            #   Call between .Ltmp7134 and .Ltmp7135
	.uleb128 .Ltmp7136-.Lfunc_begin36       #     jumps to .Ltmp7136
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7137-.Lfunc_begin36       # >> Call Site 52 <<
	.uleb128 .Ltmp7138-.Ltmp7137            #   Call between .Ltmp7137 and .Ltmp7138
	.uleb128 .Ltmp7139-.Lfunc_begin36       #     jumps to .Ltmp7139
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7140-.Lfunc_begin36       # >> Call Site 53 <<
	.uleb128 .Ltmp7149-.Ltmp7140            #   Call between .Ltmp7140 and .Ltmp7149
	.uleb128 .Ltmp7150-.Lfunc_begin36       #     jumps to .Ltmp7150
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7151-.Lfunc_begin36       # >> Call Site 54 <<
	.uleb128 .Ltmp7152-.Ltmp7151            #   Call between .Ltmp7151 and .Ltmp7152
	.uleb128 .Ltmp7153-.Lfunc_begin36       #     jumps to .Ltmp7153
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7154-.Lfunc_begin36       # >> Call Site 55 <<
	.uleb128 .Ltmp7165-.Ltmp7154            #   Call between .Ltmp7154 and .Ltmp7165
	.uleb128 .Ltmp7166-.Lfunc_begin36       #     jumps to .Ltmp7166
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7167-.Lfunc_begin36       # >> Call Site 56 <<
	.uleb128 .Ltmp7168-.Ltmp7167            #   Call between .Ltmp7167 and .Ltmp7168
	.uleb128 .Ltmp7169-.Lfunc_begin36       #     jumps to .Ltmp7169
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7170-.Lfunc_begin36       # >> Call Site 57 <<
	.uleb128 .Ltmp7179-.Ltmp7170            #   Call between .Ltmp7170 and .Ltmp7179
	.uleb128 .Ltmp7180-.Lfunc_begin36       #     jumps to .Ltmp7180
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7181-.Lfunc_begin36       # >> Call Site 58 <<
	.uleb128 .Ltmp7184-.Ltmp7181            #   Call between .Ltmp7181 and .Ltmp7184
	.uleb128 .Ltmp7185-.Lfunc_begin36       #     jumps to .Ltmp7185
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7186-.Lfunc_begin36       # >> Call Site 59 <<
	.uleb128 .Ltmp7187-.Ltmp7186            #   Call between .Ltmp7186 and .Ltmp7187
	.uleb128 .Ltmp7188-.Lfunc_begin36       #     jumps to .Ltmp7188
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7189-.Lfunc_begin36       # >> Call Site 60 <<
	.uleb128 .Ltmp7190-.Ltmp7189            #   Call between .Ltmp7189 and .Ltmp7190
	.uleb128 .Ltmp7191-.Lfunc_begin36       #     jumps to .Ltmp7191
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7192-.Lfunc_begin36       # >> Call Site 61 <<
	.uleb128 .Ltmp7193-.Ltmp7192            #   Call between .Ltmp7192 and .Ltmp7193
	.uleb128 .Ltmp7194-.Lfunc_begin36       #     jumps to .Ltmp7194
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7195-.Lfunc_begin36       # >> Call Site 62 <<
	.uleb128 .Ltmp7196-.Ltmp7195            #   Call between .Ltmp7195 and .Ltmp7196
	.uleb128 .Ltmp7197-.Lfunc_begin36       #     jumps to .Ltmp7197
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7196-.Lfunc_begin36       # >> Call Site 63 <<
	.uleb128 .Ltmp7198-.Ltmp7196            #   Call between .Ltmp7196 and .Ltmp7198
	.byte	0                               #     has no landing pad
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7198-.Lfunc_begin36       # >> Call Site 64 <<
	.uleb128 .Ltmp7199-.Ltmp7198            #   Call between .Ltmp7198 and .Ltmp7199
	.uleb128 .Ltmp7200-.Lfunc_begin36       #     jumps to .Ltmp7200
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7201-.Lfunc_begin36       # >> Call Site 65 <<
	.uleb128 .Ltmp7210-.Ltmp7201            #   Call between .Ltmp7201 and .Ltmp7210
	.uleb128 .Ltmp7211-.Lfunc_begin36       #     jumps to .Ltmp7211
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7212-.Lfunc_begin36       # >> Call Site 66 <<
	.uleb128 .Ltmp7215-.Ltmp7212            #   Call between .Ltmp7212 and .Ltmp7215
	.uleb128 .Ltmp7216-.Lfunc_begin36       #     jumps to .Ltmp7216
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7217-.Lfunc_begin36       # >> Call Site 67 <<
	.uleb128 .Ltmp7218-.Ltmp7217            #   Call between .Ltmp7217 and .Ltmp7218
	.uleb128 .Ltmp7219-.Lfunc_begin36       #     jumps to .Ltmp7219
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7220-.Lfunc_begin36       # >> Call Site 68 <<
	.uleb128 .Ltmp7229-.Ltmp7220            #   Call between .Ltmp7220 and .Ltmp7229
	.uleb128 .Ltmp7230-.Lfunc_begin36       #     jumps to .Ltmp7230
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7231-.Lfunc_begin36       # >> Call Site 69 <<
	.uleb128 .Ltmp7234-.Ltmp7231            #   Call between .Ltmp7231 and .Ltmp7234
	.uleb128 .Ltmp7235-.Lfunc_begin36       #     jumps to .Ltmp7235
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7236-.Lfunc_begin36       # >> Call Site 70 <<
	.uleb128 .Ltmp7245-.Ltmp7236            #   Call between .Ltmp7236 and .Ltmp7245
	.uleb128 .Ltmp7246-.Lfunc_begin36       #     jumps to .Ltmp7246
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7247-.Lfunc_begin36       # >> Call Site 71 <<
	.uleb128 .Ltmp7250-.Ltmp7247            #   Call between .Ltmp7247 and .Ltmp7250
	.uleb128 .Ltmp7251-.Lfunc_begin36       #     jumps to .Ltmp7251
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7252-.Lfunc_begin36       # >> Call Site 72 <<
	.uleb128 .Ltmp7263-.Ltmp7252            #   Call between .Ltmp7252 and .Ltmp7263
	.uleb128 .Ltmp7264-.Lfunc_begin36       #     jumps to .Ltmp7264
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7265-.Lfunc_begin36       # >> Call Site 73 <<
	.uleb128 .Ltmp7266-.Ltmp7265            #   Call between .Ltmp7265 and .Ltmp7266
	.uleb128 .Ltmp7267-.Lfunc_begin36       #     jumps to .Ltmp7267
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7268-.Lfunc_begin36       # >> Call Site 74 <<
	.uleb128 .Ltmp7269-.Ltmp7268            #   Call between .Ltmp7268 and .Ltmp7269
	.uleb128 .Ltmp7270-.Lfunc_begin36       #     jumps to .Ltmp7270
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7271-.Lfunc_begin36       # >> Call Site 75 <<
	.uleb128 .Ltmp7272-.Ltmp7271            #   Call between .Ltmp7271 and .Ltmp7272
	.uleb128 .Ltmp7273-.Lfunc_begin36       #     jumps to .Ltmp7273
	.byte	1                               #   On action: 1
	.uleb128 .Ltmp7274-.Lfunc_begin36       # >> Call Site 76 <<
	.uleb128 .Ltmp7283-.Ltmp7274            #   Call between .Ltmp7274 and .Ltmp7283
	.uleb128 .Ltmp7284-.Lfunc_begin36       #     jumps to .Ltmp7284
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7285-.Lfunc_begin36       # >> Call Site 77 <<
	.uleb128 .Ltmp7286-.Ltmp7285            #   Call between .Ltmp7285 and .Ltmp7286
	.uleb128 .Ltmp7287-.Lfunc_begin36       #     jumps to .Ltmp7287
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7288-.Lfunc_begin36       # >> Call Site 78 <<
	.uleb128 .Ltmp7299-.Ltmp7288            #   Call between .Ltmp7288 and .Ltmp7299
	.uleb128 .Ltmp7300-.Lfunc_begin36       #     jumps to .Ltmp7300
	.byte	0                               #   On action: cleanup
	.uleb128 .Ltmp7301-.Lfunc_begin36       # >> Call Site 79 <<
	.uleb128 .Ltmp730